[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1049.png","content":{"abstract":"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as  results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.","authors":["Ankur Parikh","Xuezhi Wang","Sebastian Gehrmann","Manaal Faruqui","Bhuwan Dhingra","Diyi Yang","Dipanjan Das"],"demo_url":"","keywords":["controlled task","high-precision generation","totto","dataset process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.89","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.870","main.835","main.2476","main.2590","main.2635"],"title":"ToTTo: A Controlled Table-To-Text Generation Dataset","tldr":"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain...","track":"Language Generation"},"forum":"main.1049","id":"main.1049","presentation_id":"38938835"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1339.png","content":{"abstract":"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.  We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks.  A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.","authors":["Jonathan Mallinson","Rico Sennrich","Mirella Lapata"],"demo_url":"","keywords":["sentence simplification","translation","simplification","encoder-decoder models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.415","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2635","main.2412","main.891","main.214","main.2915"],"title":"Zero-Shot Crosslingual Sentence Simplification","tldr":"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.  We propose a zero-...","track":"Language Generation"},"forum":"main.1339","id":"main.1339","presentation_id":"38938888"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1377.png","content":{"abstract":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science.  We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","authors":["Clara Meister","Ryan Cotterell","Tim Vieira"],"demo_url":"","keywords":["language tasks","beam search","decoding","maximum decoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.170","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["TACL.2169","main.2198","main.648","main.3183","main.2307"],"title":"If beam search is the answer, what was the question?","tldr":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhe...","track":"Language Generation"},"forum":"main.1377","id":"main.1377","presentation_id":"38938891"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.148.png","content":{"abstract":"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results. Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.","authors":["Ben Athiwaratkun","Cicero Nogueira dos Santos","Jason Krone","Bing Xiang"],"demo_url":"","keywords":["joint labeling","sentence-level classification","sequence tasks","few-shot learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.27","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2635","main.1720","main.74","main.3216","main.2790"],"title":"Augmented Natural Language for Generative Sequence Labeling","tldr":"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, ou...","track":"Language Generation"},"forum":"main.148","id":"main.148","presentation_id":"38938657"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1482.png","content":{"abstract":"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework\\footnote{\\url{https://github.com/wenhuchen/KGPT}}.","authors":["Wenhu Chen","Yu Su","Xifeng Yan","William Yang Wang"],"demo_url":"","keywords":["data-to-text generation","data-to-text tasks","fully-supervised setting","pre-training learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.697","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3184","main.1631","main.74","main.714","main.1647"],"title":"KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation","tldr":"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, whic...","track":"Language Generation"},"forum":"main.1482","id":"main.1482","presentation_id":"38938913"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1581.png","content":{"abstract":"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the  input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.","authors":["Kalpesh Krishna","John Wieting","Mohit Iyyer"],"demo_url":"","keywords":["style transfer","attribute transfer","unsupervised transfer","paraphrase problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.55","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.888","main.2382","main.1898","main.3227","main.2349"],"title":"Reformulating Unsupervised Style Transfer as Paraphrase Generation","tldr":"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existin...","track":"Language Generation"},"forum":"main.1581","id":"main.1581","presentation_id":"38938942"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1647.png","content":{"abstract":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","authors":["Peng Xu","Mostofa Patwary","Mohammad Shoeybi","Raul Puri","Pascale Fung","Anima Anandkumar","Bryan Catanzaro"],"demo_url":"","keywords":["text generation","pre-trained models","megatron-cntrl","large-scale models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.226","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1928","main.1130","main.1482","main.2650","main.3054"],"title":"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models","tldr":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text...","track":"Language Generation"},"forum":"main.1647","id":"main.1647","presentation_id":"38938958"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1658.png","content":{"abstract":"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88% novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.","authors":["Tuhin Chakrabarty","Smaranda Muresan","Nanyun Peng"],"demo_url":"","keywords":["human imagination","simile generation","mapping properties","sequence model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.524","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.920","main.2758","main.1581","main.2349","main.701"],"title":"Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation","tldr":"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of s...","track":"Language Generation"},"forum":"main.1658","id":"main.1658","presentation_id":"38938962"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.168.png","content":{"abstract":"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.  This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances.  In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance.  As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance.  We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker's intentions and the listener's perceptions in both cases.","authors":["Liye Fu","Susan Fussell","Cristian Danescu-Niculescu-Mizil"],"demo_url":"","keywords":["communication approaches","paraphrases","speaker intentions","technology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.416","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3072","main.2766","main.2561","main.3352","main.2058"],"title":"Facilitating the Communication of Politeness through Fine-Grained Paraphrasing","tldr":"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.  This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse...","track":"Language Generation"},"forum":"main.168","id":"main.168","presentation_id":"38938661"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1687.png","content":{"abstract":"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.","authors":["Hannah Rashkin","Asli Celikyilmaz","Yejin Choi","Jianfeng Gao"],"demo_url":"","keywords":["outline-conditioned generation","plotmachines","neural model","large-scale models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.349","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1928","main.2758","main.2982","main.605","main.2382"],"title":"PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking","tldr":"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline....","track":"Language Generation"},"forum":"main.1687","id":"main.1687","presentation_id":"38938968"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1706.png","content":{"abstract":"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.","authors":["Liying Cheng","Dekun Wu","Lidong Bing","Yan Zhang","Zhanming Jie","Wei Lu","Luo Si"],"demo_url":"","keywords":["knowledge-to-text generation","information loss","kg","graph-to-sequence models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.90","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1231","main.158","main.666","main.531"],"title":"ENT-DESC: Entity Description Generation by Exploring Knowledge Graph","tldr":"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have...","track":"Language Generation"},"forum":"main.1706","id":"main.1706","presentation_id":"38938972"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1707.png","content":{"abstract":"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research.","authors":["Yizhe Zhang","Guoyin Wang","Chunyuan Li","Zhe Gan","Chris Brockett","Bill Dolan"],"demo_url":"","keywords":["language learning","free-form generation","hard-constrained generation","hard-constrained tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.698","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.3483","main.730","main.2382","main.2511"],"title":"POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training","tldr":"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified...","track":"Language Generation"},"forum":"main.1707","id":"main.1707","presentation_id":"38938973"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1898.png","content":{"abstract":"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source--target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods' accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.","authors":["Eric Malmi","Aliaksei Severyn","Sascha Rothe"],"demo_url":"","keywords":["style transfer","sentence fusion","sentiment transfer","masker"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.699","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1581","main.2389","main.247","main.648","main.2382"],"title":"Unsupervised Text Style Transfer with Padded Masked Language Models","tldr":"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source--target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text sp...","track":"Language Generation"},"forum":"main.1898","id":"main.1898","presentation_id":"38939004"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1928.png","content":{"abstract":"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.","authors":["Nader Akoury","Shufan Wang","Josh Whiting","Stephen Hood","Nanyun Peng","Mohit Iyyer"],"demo_url":"","keywords":["story generation","assessing text","story models","storium"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.525","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.252","main.1647","main.2758","main.3054","main.1130"],"title":"STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation","tldr":"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to...","track":"Language Generation"},"forum":"main.1928","id":"main.1928","presentation_id":"38939010"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2012.png","content":{"abstract":"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff's claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court's views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.","authors":["Yiquan Wu","Kun Kuang","Yating Zhang","Xiaozhong Liu","Changlong Sun","Jun Xiao","Yueting Zhuang","Luo Si","Fei Wu"],"demo_url":"","keywords":["court generation","legal ai","automatic generation","fact description"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.56","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2072","main.2962","main.387","main.638","main.362"],"title":"De-Biased Court's View Generation with Causality","tldr":"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) a...","track":"Language Generation"},"forum":"main.2012","id":"main.2012","presentation_id":"38939028"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2198.png","content":{"abstract":"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms \u2013 greedy search, beam search, top-k sampling, and nucleus sampling \u2013 are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.","authors":["Sean Welleck","Ilia Kulikov","Jaedeok Kim","Richard Yuanzhe Pang","Kyunghyun Cho"],"demo_url":"","keywords":["receiving sequences","neural models","recurrent model","common algorithms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.448","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3550","main.1377","main.648","main.2430","main.1445"],"title":"Consistency of a Recurrent Language Model With Respect to Incomplete Decoding","tldr":"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequence...","track":"Language Generation"},"forum":"main.2198","id":"main.2198","presentation_id":"38939066"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2313.png","content":{"abstract":"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen)  model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels.  For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews.  Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.","authors":["Tianlu Wang","Xuezhi Wang","Yao Qin","Ben Packer","Kang Li","Jilin Chen","Alex Beutel","Ed Chi"],"demo_url":"","keywords":["sentiment classification","model re-training","nlp models","cat-gen model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.417","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2914","main.371","demo.104","main.2895","main.426"],"title":"CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation","tldr":"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen)  model that, given an input te...","track":"Language Generation"},"forum":"main.2313","id":"main.2313","presentation_id":"38939090"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2382.png","content":{"abstract":"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model's rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.","authors":["Allison Hegel","Sudha Rao","Asli Celikyilmaz","Bill Dolan"],"demo_url":"","keywords":["sentence-level rewriting","document-level transfer","language generation","generative model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.526","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1130","main.1892","main.648","main.1707","main.3010"],"title":"Substance over Style: Document-Level Targeted Content Transfer","tldr":"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the ch...","track":"Language Generation"},"forum":"main.2382","id":"main.2382","presentation_id":"38939103"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2389.png","content":{"abstract":"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed  exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.","authors":["Jianqiao Li","Chunyuan Li","Guoyin Wang","Hao Fu","Yuhchen Lin","Liqun Chen","Yizhe Zhang","Chenyang Tao","Ruiyi Zhang","Wenlin Wang","Dinghan Shen","Qian Yang","Lawrence Carin"],"demo_url":"","keywords":["testing","ot learning","machine translation","text summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.735","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.648","main.26","main.247","main.3353","main.891"],"title":"Improving Text Generation with Student-Forcing Optimal Transport","tldr":"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens,...","track":"Language Generation"},"forum":"main.2389","id":"main.2389","presentation_id":"38939105"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2422.png","content":{"abstract":"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.","authors":["Xuefeng Bai","Linfeng Song","Yue Zhang"],"demo_url":"","keywords":["amr-to-text generation","text generation","graph encoders","decoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.92","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.870","main.1339","main.2795","main.2098","main.648"],"title":"Online Back-Parsing for AMR-to-Text Generation","tldr":"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being us...","track":"Language Generation"},"forum":"main.2422","id":"main.2422","presentation_id":"38939115"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2430.png","content":{"abstract":"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.","authors":["Tom Bosc","Pascal Vincent"],"demo_url":"","keywords":["generation","memorization","autoregressive models","variational autoencoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.350","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1446","main.3013","main.2851","main.3483","main.1892"],"title":"Do sequence-to-sequence VAEs learn global features of sentences?","tldr":"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation...","track":"Language Generation"},"forum":"main.2430","id":"main.2430","presentation_id":"38939119"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2511.png","content":{"abstract":"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often \u201crambling\u201d without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.","authors":["Xinyu Hua","Lu Wang"],"demo_url":"","keywords":["generation","pre-trained transformers","content-controlled framework","pair"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.57","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1707","main.3398","main.1634","main.1522","main.648"],"title":"PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation","tldr":"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often \u201crambling\u201d without coherently arranged content. In this work, we present a novel content-controlled text generation framewo...","track":"Language Generation"},"forum":"main.2511","id":"main.2511","presentation_id":"38939134"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.252.png","content":{"abstract":"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, con\ufb02icting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.","authors":["Jian Guan","Minlie Huang"],"demo_url":"","keywords":["open-ended generation","story generation","evaluating generation","constructing samples"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.736","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1928","main.1647","main.2758","main.2650","main.2864"],"title":"UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation","tldr":"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many ...","track":"Language Generation"},"forum":"main.252","id":"main.252","presentation_id":"38938672"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2590.png","content":{"abstract":"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.","authors":["Mihir Kale","Abhinav Rastogi"],"demo_url":"","keywords":["natural generation","generation","nlg","domain-independent model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.527","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3353","main.876","TACL.2135","main.2763","main.2641"],"title":"Template Guided Text Generation for Task-Oriented Dialogue","tldr":"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (N...","track":"Language Generation"},"forum":"main.2590","id":"main.2590","presentation_id":"38939152"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2758.png","content":{"abstract":"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset  will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.","authors":["Khyathi Raghavi Chandu","Ruo-Ping Dong","Alan W Black"],"demo_url":"","keywords":["generating narratives","artificial intelligence","prediction steps","generating steps"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.93","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.390","main.2702","main.252","main.1928","main.373"],"title":"Reading Between the Lines: Exploring Infilling in Visual Narratives","tldr":"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The gene...","track":"Language Generation"},"forum":"main.2758","id":"main.2758","presentation_id":"38939186"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2766.png","content":{"abstract":"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis  and discuss the challenges for the computational pun models.","authors":["Zhiwei Yu","Hongyu Zang","Xiaojun Wan"],"demo_url":"","keywords":["punning","error analysis","computational models","homophones"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.229","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.920","main.2208","main.1658","TACL.2013","main.1625"],"title":"Homophonic Pun Generation with Lexically Constrained Rewriting","tldr":"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity...","track":"Language Generation"},"forum":"main.2766","id":"main.2766","presentation_id":"38939190"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2795.png","content":{"abstract":"AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.","authors":["Yan Zhang","Zhijiang Guo","Zhiyang Teng","Wei Lu","Shay B. Cohen","Zuozhu Liu","Lidong Bing"],"demo_url":"","keywords":["amr-to-text generation","graph representations","graph gcns","gcns"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.169","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2422","main.1485","TACL.2121","main.1707","main.471"],"title":"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation","tldr":"AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to e...","track":"Language Generation"},"forum":"main.2795","id":"main.2795","presentation_id":"38939199"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2864.png","content":{"abstract":"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.","authors":["Anthony Chen","Gabriel Stanovsky","Sameer Singh","Matt Gardner"],"demo_url":"","keywords":["reading comprehension","generation problem","mocha","lerc"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.528","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3186","main.2973","main.2258","TACL.2049","main.449"],"title":"MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics","tldr":"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token o...","track":"Language Generation"},"forum":"main.2864","id":"main.2864","presentation_id":"38939212"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2916.png","content":{"abstract":"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.  Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness.  Additionally, we evaluate how a phrase-based data augmentation method can improve performance.  We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.  Data augmentation further improves control on difficult, randomly generated utterance plans.","authors":["Chris Kedzie","Kathleen McKeown"],"demo_url":"","keywords":["natural generation","training","data augmentation","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.419","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3353","main.2389","main.699","main.1006","main.2511"],"title":"Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies","tldr":"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.  Using two task-oriented dialogue generation benchmarks, we systematically...","track":"Language Generation"},"forum":"main.2916","id":"main.2916","presentation_id":"38939225"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2982.png","content":{"abstract":"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.","authors":["Seraphina Goldfarb-Tarrant","Tuhin Chakrabarty","Ralph Weischedel","Nanyun Peng"],"demo_url":"","keywords":["story generation","high-quality planning","large models","plot-generation model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.351","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.390","main.2650","main.2758","main.2511","main.3010"],"title":"Content Planning for Neural Story Generation with Aristotelian Rescoring","tldr":"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be...","track":"Language Generation"},"forum":"main.2982","id":"main.2982","presentation_id":"38939240"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3010.png","content":{"abstract":"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.","authors":["Dongyeop Kang","Eduard Hovy"],"demo_url":"","keywords":["nlp tasks","guiding realization","paragraph task","content prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.529","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1892","main.2377","main.1023","main.2382","main.128"],"title":"Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task","tldr":"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how t...","track":"Language Generation"},"forum":"main.3010","id":"main.3010","presentation_id":"38939248"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3179.png","content":{"abstract":"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.","authors":["Wei-Jen Ko","Avik Ray","Yilin Shen","Hongxia Jin"],"demo_url":"","keywords":["generation responses","regression task","open-domain models","end-to-end classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.352","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.128","main.699","main.215","main.891","main.1700"],"title":"Generating Dialogue Responses from a Semantic Latent Space","tldr":"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multi...","track":"Language Generation"},"forum":"main.3179","id":"main.3179","presentation_id":"38939280"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3184.png","content":{"abstract":"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data's supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.","authors":["Zihao Fu","Bei Shi","Wai Lam","Lidong Bing","Zhiyuan Liu"],"demo_url":"","keywords":["data-to-text task","generation task","dataset problem","over-generation problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.738","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1482","main.1923","main.714","main.3010","main.2342"],"title":"Partially-Aligned Data-to-Text Generation with Distant Supervision","tldr":"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data whic...","track":"Language Generation"},"forum":"main.3184","id":"main.3184","presentation_id":"38939283"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3186.png","content":{"abstract":"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets.  We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.","authors":["Wei-Jen Ko","Te-yuan Chen","Yiyan Huang","Greg Durrett","Junyi Jessy Li"],"demo_url":"","keywords":["inquisitive questions","automatic systems","text comprehension","data-driven approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.530","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2586","main.2973","main.3054","main.41","main.1022"],"title":"Inquisitive Question Generation for High Level Text Comprehension","tldr":"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news arti...","track":"Language Generation"},"forum":"main.3186","id":"main.3186","presentation_id":"38939285"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3270.png","content":{"abstract":"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling.  In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text  generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.","authors":["Lianhui Qin","Vered Shwartz","Peter West","Chandra Bhagavatula","Jena D. Hwang","Ronan Le Bras","Antoine Bosselut","Yejin Choi"],"demo_url":"","keywords":["nonmonotonic tasks","abductive generation","generation","counterfactual revision"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.58","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2415","TACL.2411","TACL.2041","main.623","main.2054"],"title":"Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning","tldr":"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorpora...","track":"Language Generation"},"forum":"main.3270","id":"main.3270","presentation_id":"38939297"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.334.png","content":{"abstract":"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F^2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F^2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.","authors":["Byung-Ju Choi","Jimin Hong","David Park","Sang Wan Lee"],"demo_url":"","keywords":["neural generation","sub-optimal generation","learning model","mefmax"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.737","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1770","main.648","main.2650","main.471","main.2430"],"title":"F2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax","tldr":"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects...","track":"Language Generation"},"forum":"main.334","id":"main.334","presentation_id":"38938686"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3353.png","content":{"abstract":"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task. The system trained using this approach scored 100\\% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.","authors":["Henry Elder","Alexander O'Connor","Jennifer Foster"],"demo_url":"","keywords":["generation","neural systems","data approach","neural approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.230","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2590","main.891","main.3495","main.2389","main.1634"],"title":"How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue","tldr":"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restric...","track":"Language Generation"},"forum":"main.3353","id":"main.3353","presentation_id":"38939317"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3403.png","content":{"abstract":"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.","authors":["Tianxiao Shen","Victor Quach","Regina Barzilay","Tommi Jaakkola"],"demo_url":"","keywords":["text tasks","filling snippets","style transfer","damaged restoration"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.420","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.2382","main.2389","main.1898","main.1892"],"title":"Blank Language Models","tldr":"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The mo...","track":"Language Generation"},"forum":"main.3403","id":"main.3403","presentation_id":"38939329"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3457.png","content":{"abstract":"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply \\method{} to causal generation, the task of predicting a proposition's plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.","authors":["Nathaniel Weir","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":"","keywords":["causal generation","cods","neural models","seqseqs"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.421","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2179","main.2448","main.2650","TACL.2013","main.2005"],"title":"COD3S: Diverse Generation with Discrete Semantic Signatures","tldr":"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and th...","track":"Language Generation"},"forum":"main.3457","id":"main.3457","presentation_id":"38939341"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3483.png","content":{"abstract":"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context.  This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.","authors":["Bin Bi","Chenliang Li","Chen Wu","Ming Yan","Wei Wang","Songfang Huang","Fei Huang","Luo Si"],"demo_url":"","keywords":["natural generation","language tasks","generative answering","conversational generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.700","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1707","TACL.2107","main.2430","main.247","main.1446"],"title":"PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation","tldr":"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transform...","track":"Language Generation"},"forum":"main.3483","id":"main.3483","presentation_id":"38939345"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.648.png","content":{"abstract":"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new  metrics for comparing sparse or truncated distributions: $\\epsilon$-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.","authors":["Pedro Henrique Martins","Zita Marinho","Andr\u00e9 F. T. Martins"],"demo_url":"","keywords":["story completion","dialogue generation","text generators","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.348","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.870","main.1707","main.2389","main.2491","main.3348"],"title":"Sparse Text Generation","tldr":"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncat...","track":"Language Generation"},"forum":"main.648","id":"main.648","presentation_id":"38938749"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.730.png","content":{"abstract":"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.","authors":["Lei Sha"],"demo_url":"","keywords":["lexically generation","real-world applications","lexically-constrained generation","unsupervised problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.701","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1707","main.2590","main.648","main.471","main.3010"],"title":"Gradient-guided Unsupervised Lexically Constrained Text Generation","tldr":"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generat...","track":"Language Generation"},"forum":"main.730","id":"main.730","presentation_id":"38938763"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.870.png","content":{"abstract":"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English.  We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics.  We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.","authors":["Angela Fan","Claire Gardent"],"demo_url":"","keywords":["multilingual generation","cross-lingual embeddings","pretraining","multilingual models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.231","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3116","main.648","main.267","main.2641","main.1649"],"title":"Multilingual AMR-to-Text Generation","tldr":"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an ad...","track":"Language Generation"},"forum":"main.870","id":"main.870","presentation_id":"38938791"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.910.png","content":{"abstract":"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.","authors":["Sebastian Goodman","Nan Ding","Radu Soricut"],"demo_url":"","keywords":["machine benchmark","news benchmarks","sequence models","teacher-forcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.702","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1356","main.2389","TACL.2255","main.2430","main.2078"],"title":"TeaForN: Teacher-Forcing with N-grams","tldr":"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, t...","track":"Language Generation"},"forum":"main.910","id":"main.910","presentation_id":"38938802"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.920.png","content":{"abstract":"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.","authors":["Rajat Agarwal","Katharina Kann"],"demo_url":"","keywords":["computational creativity","generation task","acrostic generation","pretraining"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.94","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2349","main.106","main.1658","main.3093","main.2766"],"title":"Acrostic Poem Generation","tldr":"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task...","track":"Language Generation"},"forum":"main.920","id":"main.920","presentation_id":"38938805"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.923.png","content":{"abstract":"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.","authors":["Haozhe Ji","Pei Ke","Shaohan Huang","Furu Wei","Xiaoyan Zhu","Minlie Huang"],"demo_url":"","keywords":["text tasks","generation","commonsense-aware generation","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.54","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1648","main.666","main.531","main.1647","main.1706"],"title":"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph","tldr":"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate com...","track":"Language Generation"},"forum":"main.923","id":"main.923","presentation_id":"38938806"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.1983.png","content":{"abstract":"Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as \"Obama is a _ by profession\". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as \"Obama worked as a _\" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.","authors":["Zhengbao Jiang","Frank F. Xu","Jun Araki","Graham Neubig"],"demo_url":"","keywords":["querying process","extracting knowledge","language models","lm"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2763","main.2630","TACL.2049","main.41","main.1866"],"title":"How Can We Know What Language Models Know","tldr":"Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as \"Obama is a _ by profession\". These prompts are usually manually created, and quite possibly...","track":"Language Generation"},"forum":"TACL.1983","id":"TACL.1983","presentation_id":"38939396"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2121.png","content":{"abstract":"Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models which encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.","authors":["Leonardo F. R. Ribeiro","Yue Zhang","Claire Gardent","Iryna Gurevych"],"demo_url":"","keywords":["graph-to-text models","global aggregation","node representations","global encoding"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2761","main.782","main.666","main.1231","main.1010"],"title":"Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs","tldr":"Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as...","track":"Language Generation"},"forum":"TACL.2121","id":"TACL.2121","presentation_id":"38939409"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2169.png","content":{"abstract":"Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search since the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is beam search--a pruned version of breadth-first search. Quite surprisingly, beam search often returns better results than exact inference due to beneficial search bias for NLP tasks. In this work, we show that the standard implementation of beam search can be made up to 10x faster in practice. Our method assumes that the scoring function is monotonic in the sequence length, which allows us to safely prune hypotheses that cannot be in the final set of hypotheses early on. We devise effective monotonic approximations to popular nonmonontic scoring functions, including length normalization and mutual information decoding. Lastly, we propose a memory-reduced variant of Best-First Beam Search, which has a similar beneficial search bias in terms of downstream performance, but runs in a fraction of the time.","authors":["Clara Meister","Ryan Cotterell","Tim Vieira"],"demo_url":"","keywords":["nlp tasks","exact search","decoding","heuristic algorithm"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1377","main.2767","main.3348","main.2198","main.730"],"title":"A* Beam Search","tldr":"Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search since the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is bea...","track":"Language Generation"},"forum":"TACL.2169","id":"TACL.2169","presentation_id":"38939414"}]
