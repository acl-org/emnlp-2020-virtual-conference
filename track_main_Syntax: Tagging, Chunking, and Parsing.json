[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1225.png","content":{"abstract":"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.","authors":["Tao Gui","Jiacheng Ye","Qi Zhang","Zhengyan Li","Zichu Fei","Yeyun Gong","Xuanjing Huang"],"demo_url":"","keywords":["label decoding","sequence tasks","parallel decoding","faster prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.181","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1615","main.2636","main.2790","main.148","main.1720"],"title":"Uncertainty-Aware Label Refinement for Sequence Labeling","tldr":"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel tw...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1225","id":"main.1225","presentation_id":"38938868"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1258.png","content":{"abstract":"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework  (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework's default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.","authors":["Amrith Krishna","Ashim Gupta","Deepak Garasangi","Pavankumar Satuluri","Pawan Goyal"],"demo_url":"","keywords":["joint parsing","dependency parsing","structured tasks","morphological parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.388","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1957","main.1943","TACL.2141","main.447"],"title":"Keep it Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit","tldr":"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend t...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1258","id":"main.1258","presentation_id":"38938873"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1494.png","content":{"abstract":"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.","authors":["Steven Cao","Nikita Kitaev","Dan Klein"],"demo_url":"","keywords":["unsupervised parsing","constituency test","grammaticality decisions","unsupervised parser"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.389","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.2684","main.3181","TACL.2141","main.2064"],"title":"Unsupervised Parsing via Constituency Tests","tldr":"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the re...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1494","id":"main.1494","presentation_id":"38938920"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1615.png","content":{"abstract":"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.","authors":["Xinyu Wang","Yong Jiang","Nguyen Bach","Tao Wang","Zhongqiang Huang","Fei Huang","Kewei Tu"],"demo_url":"","keywords":["parallelization","faster prediction","linear-chain model","neural approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.485","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1225","main.3348","main.2636","main.1432","main.2198"],"title":"AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network","tldr":"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training a...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1615","id":"main.1615","presentation_id":"38938950"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1625.png","content":{"abstract":"A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.","authors":["Yoshihide Kato","Shigeki Matsubara"],"demo_url":"","keywords":["gapping construction","coordinated structure","redundant elements","elided elements"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.218","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2419","demo.89","main.1957","main.486","main.447"],"title":"Parsing Gapping Constructions Based on Grammatical and Semantic Roles","tldr":"A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on cons...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1625","id":"main.1625","presentation_id":"38938954"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1755.png","content":{"abstract":"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity. To address this issue, we present a novel nested NER model named HIT. Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary. Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary. Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.","authors":["Yu Wang","Yun Li","Hanghang Tong","Ziye Zhu"],"demo_url":"","keywords":["named recognition","ner","natural processing","sequence approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.486","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2103","main.2974","main.911","main.989","main.2799"],"title":"HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction","tldr":"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approa...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1755","id":"main.1755","presentation_id":"38938982"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1943.png","content":{"abstract":"The connection between dependency trees and spanning trees  is exploited  by the  NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has  missed  an  important  difference  between the  two  structures: only one edge  may emanate from the root in a dependency tree.  We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able  to  learn  that  trees  which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%.  Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan  (1984)  to  dependency  parsing,  which satisfies the constraint without compromising the original runtime.","authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"demo_url":"","keywords":["decoding runtime","dependency","dependency parsing","parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.390","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.447","TACL.2141","main.1258","main.1957","main.1494"],"title":"Please Mind the Root: Decoding Arborescences for Dependency Parsing","tldr":"The connection between dependency trees and spanning trees  is exploited  by the  NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has  missed  an  important  difference  between the  two  structures: o...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1943","id":"main.1943","presentation_id":"38939014"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.207.png","content":{"abstract":"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(n^6) down to O(n^3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra,  Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.","authors":["Caio Corro"],"demo_url":"","keywords":["span-based parsing","fully setting","chart-based algorithm","parser"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.219","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1258","main.447","main.1957","TACL.2141"],"title":"Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n6) down to O(n3)","tldr":"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.207","id":"main.207","presentation_id":"38938664"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2529.png","content":{"abstract":"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.","authors":["Yuanhe Tian","Yan Song","Fei Xia"],"demo_url":"","keywords":["supertagging","combinatory parsing","neural supertagging","parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.487","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3437","main.2650","main.2684","TACL.2411","main.1952"],"title":"Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks","tldr":"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2529","id":"main.2529","presentation_id":"38939138"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2684.png","content":{"abstract":"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Few-shot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.","authors":["Haoyue Shi","Karen Livescu","Kevin Gimpel"],"demo_url":"","keywords":["few-shot parsing","unsupervised parsing","hyperparameter tuning","model selection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.614","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1494","main.143","TACL.2411","main.3540","main.2851"],"title":"On the Role of Supervision in Unsupervised Constituency Parsing","tldr":"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existi...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2684","id":"main.2684","presentation_id":"38939171"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2733.png","content":{"abstract":"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.","authors":["Bosheng Ding","Linlin Liu","Lidong Bing","Canasai Kruengkrai","Thien Hai Nguyen","Shafiq Joty","Luo Si","Chunyan Miao"],"demo_url":"","keywords":["machine learning","generalization","low-resource tasks","named recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.488","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2068","main.2078","main.345","main.148","main.3540"],"title":"DAGA: Data Augmentation with a Generation Approach forLow-resource Tagging Tasks","tldr":"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks ...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2733","id":"main.2733","presentation_id":"38939181"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2895.png","content":{"abstract":"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.","authors":["Wenjuan Han","Liwen Zhang","Yong Jiang","Kewei Tu"],"demo_url":"","keywords":["adversarial attacks","classification problems","structured tasks","nlp tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.182","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2914","demo.104","main.2313","main.47","main.1614"],"title":"Adversarial Attack and Defense of Structured Prediction Models","tldr":"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classifica...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2895","id":"main.2895","presentation_id":"38939221"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3259.png","content":{"abstract":"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.","authors":["Andrew Drozdov","Subendhu Rongali","Yi-Pei Chen","Tim O'Gorman","Mohit Iyyer","Andrew McCallum"],"demo_url":"","keywords":["fine-tuning","unsupervised parsing","deep autoencoder","diora"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.392","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1943","main.3348","main.2684","main.2098"],"title":"Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders","tldr":"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover tha...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3259","id":"main.3259","presentation_id":"38939296"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3282.png","content":{"abstract":"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.  Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.","authors":["Lu Xu","Hao Li","Wei Lu","Lidong Bing"],"demo_url":"","keywords":["aspect extraction","triplet process","aste","pipeline approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.183","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1675","main.3286","main.983","main.3375","main.2261"],"title":"Position-Aware Tagging for Aspect Sentiment Triplet Extraction","tldr":"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pip...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3282","id":"main.3282","presentation_id":"38939300"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3648.png","content":{"abstract":"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval","authors":["Jinlan Fu","Pengfei Liu","Graham Neubig"],"demo_url":"","keywords":["natural tasks","interpretable evaluation","named task","analysis tool"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.489","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1669","main.3216","main.387","main.143"],"title":"Interpretable Multi-dataset Evaluation for Named Entity Recognition","tldr":"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 doe...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3648","id":"main.3648","presentation_id":"38939382"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.447.png","content":{"abstract":"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.","authors":["Anders S\u00f8gaard"],"demo_url":"","keywords":["treebank size","average length","morphological complexity","domain differences"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.220","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2141","main.1943","main.1957","main.2890","main.1901"],"title":"Some Languages Seem Easier to Parse Because Their Treebanks Leak","tldr":"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.447","id":"main.447","presentation_id":"38938710"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.486.png","content":{"abstract":"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.","authors":["David Vilares","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":"","keywords":["discontinuous parsing","sequence labeling","constituent parsing","labeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.221","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.1938","main.1957","main.214","main.2419"],"title":"Discontinuous Constituent Parsing as Sequence Labeling","tldr":"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.486","id":"main.486","presentation_id":"38938720"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.744.png","content":{"abstract":"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.","authors":["Yuanmeng Yan","Keqing He","Hong Xu","Sihong Liu","Fanyu Meng","Min Hu","Weiran Xu"],"demo_url":"","keywords":["neural-based models","robust method","open-vocabulary slots","file name"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.490","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2005","main.891","main.2587","main.143","main.2790"],"title":"Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots","tldr":"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction no...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.744","id":"main.744","presentation_id":"38938765"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.750.png","content":{"abstract":"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.","authors":["Ahmet \u00dcst\u00fcn","Arianna Bisazza","Gosse Bouma","Gertjan van Noord"],"demo_url":"","keywords":["multilingual parsing","soft sharing","multilingual approach","contextual modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.180","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1803","main.2718","main.3116","main.2278","main.267"],"title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing","tldr":"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel mul...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.750","id":"main.750","presentation_id":"38938768"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.1.png","content":{"abstract":"The formalism for Lexical-Functional Grammar (LFG) was introduced in the 1980\u2019s as one of the first constraint-based grammatical formalisms for natural language. It has led to substantial contributions to the linguistic literature and to the construction of large-scale descriptions of particular languages. Investigations of its mathematical properties have shown that, without further restrictions, the recognition, emptiness, and generation problems are undecidable, and that they are intractable in the worst case even with commonly applied restrictions. However, grammars of real languages appear not to invoke the full expressive power of the formalism, as indicated by the fact that algorithms and implementations for recognition and generation have been developed that run\u2014even for broad-coverage grammars\u2014in typically polynomial time. This paper formalizes some restrictions on the notation and its interpretation that are compatible with conventions and principles that have been implicit or informally stated in linguistic theory. We show that LFG grammars that respect these restrictions, although still suitable for the description of natural languages, are equivalent to linear context-free rewriting systems and allow for tractable computation.","authors":["J\u00fcrgen Wedekind","Ronald M. Kaplan"],"demo_url":"","keywords":["recognition","emptiness","tractable computation","constraint-based formalisms"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.593","main.2064","TACL.2141","TACL.2013","main.2179"],"title":"Tractable Lexical-Functional Grammar","tldr":"The formalism for Lexical-Functional Grammar (LFG) was introduced in the 1980\u2019s as one of the first constraint-based grammatical formalisms for natural language. It has led to substantial contributions to the linguistic literature and to the construc...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"CL.1","id":"CL.1","presentation_id":"38939389"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.5.png","content":{"abstract":"Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outside values cannot. We view outside values as functions from inside values to the total value of all derivations, and we analyze outside computation in terms of function composition. This viewpoint helps explain why efficient outside computation is possible in many settings, despite the lack of a general outside algorithm for semiring operations.","authors":["Daniel Gildea"],"demo_url":"","keywords":["outside computation","semiring operations","weighted systems","parsing algorithms"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.1936","demo.86","main.2253","main.2419","main.1625"],"title":"Efficient Outside Computation","tldr":"Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outsid...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"CL.5","id":"CL.5","presentation_id":"38939393"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1936.png","content":{"abstract":"Learning probabilistic context-free grammars from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). ** We proceed via a reparameterisation of (top-down) PCFGs which we call a bottom-up weighted context-free grammar. We show that if the grammar is anchored and satisfies additional restrictions on its ambiguity, then the parameters can be directly related to distributional properties of the anchoring strings; we show the asymptotic correctness of a naive estimator and present some simulations using synthetic data that show that algorithms based on this approach have good finite sample behaviour.","authors":["Alexander Clark","Nathana\u00ebl Fijalkow"],"demo_url":"","keywords":["computational linguistics","learning grammars","distributional learning","pcfgs"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2141","main.1494","main.2179","main.2702","main.2122"],"title":"Consistent Unsupervised Estimators for Anchored PCFGs","tldr":"Learning probabilistic context-free grammars from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs ...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.1936","id":"TACL.1936","presentation_id":"38939394"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2141.png","content":{"abstract":"In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g. lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs which allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone. Code is available at https://github.com/neulab/neural-lpcfg.","authors":["Hao Zhu","Yonatan Bisk","Graham Neubig"],"demo_url":"","keywords":["grammar induction","unsupervised induction","context methods","syntactic formalisms"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.2363","TACL.1936","main.1957","main.2179"],"title":"The Return of Lexical Dependencies: Neural Lexicalized PCFGs","tldr":"In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either c...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.2141","id":"TACL.2141","presentation_id":"38939412"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2411.png","content":{"abstract":"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases. To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model. Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM. Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark. Our findings demonstrate the benefits of syntactic biases, even for representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are helpful in benchmarks of natural language understanding.","authors":["Adhiguna Kuncoro","Lingpeng Kong","Daniel Fried","Dani Yogatama","Laura Rimell","Chris Dyer","Phil Blunsom"],"demo_url":"","keywords":["bert pretraining","structured tasks","natural understanding","textual learners"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2851","main.3635","main.2363","main.1970","TACL.2041"],"title":"Syntactic Structure Distillation Pretraining for Bidirectional Encoders","tldr":"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether s...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.2411","id":"TACL.2411","presentation_id":"38939418"}]
