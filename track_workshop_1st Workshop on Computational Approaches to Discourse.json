[{"content":{"abstract":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an action, and propose a composed variational natural language generator (CLANG), a transformer-based conditional variational autoencoder. CLANG utilizes two latent variables to represent the utterances corresponding to two different independent parts (domain and action) in the intent, and the latent variables are composed together to generate natural examples. Additionally, to improve the generator learning, we adopt the contrastive regularization loss that contrasts the in-class with the out-of-class utterance generation given the intent. To evaluate the quality of the generated utterances, experiments are conducted on the generalized few-shot intent detection task. Empirical results show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets.","authors":["Congying Xia","Caiming Xiong","Philip Yu","Richard Socher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.303","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Composed Variational Natural Language Generation for Few-shot Intents","tldr":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2487","presentation_id":"38940699","rocketchat_channel":"paper-codi2020-2487","speakers":"Congying Xia|Caiming Xiong|Philip Yu|Richard Socher","title":"Composed Variational Natural Language Generation for Few-shot Intents"},{"content":{"abstract":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model. Our results show that the span representation is able to encode a significant amount of coreference information. In addition, we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. Last, our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.","authors":["Patrick Kahardipraja","Olena Vyshnevska","Sharid Lo\u00e1iciga"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Span Representations in Neural Coreference Resolution","tldr":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a prob...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.10","presentation_id":"38939689","rocketchat_channel":"paper-codi2020-10","speakers":"Patrick Kahardipraja|Olena Vyshnevska|Sharid Lo\u00e1iciga","title":"Exploring Span Representations in Neural Coreference Resolution"},{"content":{"abstract":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience\u2019s response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a) text classification labels, indicating which actor\u2019s lines made the studio audience laugh; b) information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles.","authors":["Maolin Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts","tldr":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.11","presentation_id":"38939690","rocketchat_channel":"paper-codi2020-11","speakers":"Maolin Li","title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts"},{"content":{"abstract":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to abstract entities and events. These features are inspired through several sources \u2013 cognitive parameters, pragmatic factors and typological status. We pay attention to the distributions of these features in a dataset containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and feature selection to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for contrastive linguistics, translation studies and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.","authors":["Ekaterina Lapshinova-Koltunski","Kerstin Kunz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Coreference Features in Heterogeneous Data with Text Classification","tldr":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communica...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.13","presentation_id":"38939691","rocketchat_channel":"paper-codi2020-13","speakers":"Ekaterina Lapshinova-Koltunski|Kerstin Kunz","title":"Exploring Coreference Features in Heterogeneous Data with Text Classification"},{"content":{"abstract":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a discourse relation or not. We study the influence of those context-specific embeddings. Further, we show the benefit of training the tasks of connective disambiguation and sense classification together at the same time. The success of our approach is supported by state-of-the-art results.","authors":["Ren\u00e9 Knaebel","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing","tldr":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a disc...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.14","presentation_id":"38939692","rocketchat_channel":"paper-codi2020-14","speakers":"Ren\u00e9 Knaebel|Manfred Stede","title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing"},{"content":{"abstract":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in DSNDM is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.","authors":["Alexander Chernyavskiy","Dmitry Ilvovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking","tldr":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political stateme...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.15","presentation_id":"38939693","rocketchat_channel":"paper-codi2020-15","speakers":"Alexander Chernyavskiy|Dmitry Ilvovsky","title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking"},{"content":{"abstract":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.","authors":["Laurine Huber","Chaker Memmadi","Mathilde Dargnat","Yannick Toussaint"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?","tldr":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.17","presentation_id":"38939694","rocketchat_channel":"paper-codi2020-17","speakers":"Laurine Huber|Chaker Memmadi|Mathilde Dargnat|Yannick Toussaint","title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.18","presentation_id":"38939695","rocketchat_channel":"paper-codi2020-18","speakers":"Patrick Huber|Giuseppe Carenini","title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Discourse Treebanks from Scalable Distant Supervision","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.19","presentation_id":"38939696","rocketchat_channel":"paper-codi2020-19","speakers":"Patrick Huber|Giuseppe Carenini","title":"Large Discourse Treebanks from Scalable Distant Supervision"},{"content":{"abstract":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining different portions of OntoNotes with Twitter data show that selecting text genres for the training data can beat the mere maximization of training data amount. In addition, we inspect several phenomena such as the role of deictic pronouns in conversational data, and present additional results for variant settings. Our best configuration improves the performance of the\u201dout of the box\u201d system by 21.6%.","authors":["Berfin Akta\u015f","Veronika Solopova","Annalena Kohnert","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.222","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adapting Coreference Resolution to Twitter Conversations","tldr":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter con...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.1951","presentation_id":"38940697","rocketchat_channel":"paper-codi2020-1951","speakers":"Berfin Akta\u015f|Veronika Solopova|Annalena Kohnert|Manfred Stede","title":"Adapting Coreference Resolution to Twitter Conversations"},{"content":{"abstract":"","authors":["Diane Litman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse for Argument Mining, and Argument Mining as Discourse","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.20","presentation_id":"38939697","rocketchat_channel":"paper-codi2020-20","speakers":"Diane Litman","title":"Discourse for Argument Mining, and Argument Mining as Discourse"},{"content":{"abstract":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI love you.\u201d We designed a system to allow virtual assistants to take a voice message from one user, convert the point of view of the message, and then deliver the result to its target user. We developed a rule-based model, which integrates a linear text classification model, part-of-speech tagging, and constituency parsing with rule-based transformation methods. We also investigated Neural Machine Translation (NMT) approaches, including LSTMs, CopyNet, and T5. We explored 5 metrics to gauge both naturalness and faithfulness automatically, and we chose to use BLEU plus METEOR for faithfulness and relative perplexity using a separately trained language model (GPT) for naturalness. Transformer-Copynet and T5 performed similarly on faithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a METEOR score of 83.0. CopyNet was the most natural, with a relative perplexity of 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly released our dataset, which is composed of 46,565 crowd-sourced samples.","authors":["Gunhee Lee","Vera Zu","Sai Srujana Buddi","Dennis Liang","Purva Kulkarni","Jack FitzGerald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Converting the Point of View of Messages Spoken to Virtual Assistants","tldr":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI lov...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.208","presentation_id":"38940694","rocketchat_channel":"paper-codi2020-208","speakers":"Gunhee Lee|Vera Zu|Sai Srujana Buddi|Dennis Liang|Purva Kulkarni|Jack FitzGerald","title":"Converting the Point of View of Messages Spoken to Virtual Assistants"},{"content":{"abstract":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.","authors":["Yunmo Chen","Tongfei Chen","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Modeling of Arguments for Event Understanding","tldr":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach all...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.21","presentation_id":"38939698","rocketchat_channel":"paper-codi2020-21","speakers":"Yunmo Chen|Tongfei Chen|Benjamin Van Durme","title":"Joint Modeling of Arguments for Event Understanding"},{"content":{"abstract":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes semi-structured commonsense inference rules and learns to incorporate them in a transformer-based reasoning cell.We assess the model\u2019s performance on two tasks that require different reasoning skills: Abductive Natural Language Inference and Counterfactual Invariance Prediction as a new task. We show that our proposed model improves performance over strong state-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we are, to the best of our knowledge, the first to demonstrate that a model that learns to perform counterfactual reasoning helps predicting the best explanation in an abductive reasoning task. We validate the robustness of the model\u2019s reasoning capabilities by perturbing the knowledge and provide qualitative analysis on the model\u2019s knowledge incorporation capabilities.","authors":["Debjit Paul","Anette Frank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.267","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention","tldr":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes se...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2195","presentation_id":"38940698","rocketchat_channel":"paper-codi2020-2195","speakers":"Debjit Paul|Anette Frank","title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention"},{"content":{"abstract":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.","authors":["Youmna Farag","Josef Valvoda","Helen Yannakoudakis","Ted Briscoe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Neural Discourse Coherence Models","tldr":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensit...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.22","presentation_id":"38939699","rocketchat_channel":"paper-codi2020-22","speakers":"Youmna Farag|Josef Valvoda|Helen Yannakoudakis|Ted Briscoe","title":"Analyzing Neural Discourse Coherence Models"},{"content":{"abstract":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on a Multi-Layer Perceptron study and a Sequential Forward Search experiment, followed by Bayes Factor analysis of the outcomes. The results suggest that recency metrics counting paragraphs and sentences contribute to referential choice prediction more than other recency-related metrics. Based on the results of our analysis, we argue that, sensitivity to discourse structure is important for recency metrics used in determining referring expression forms.","authors":["Fahime Same","Kees van Deemter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse","tldr":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.23","presentation_id":"38939700","rocketchat_channel":"paper-codi2020-23","speakers":"Fahime Same|Kees van Deemter","title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse"},{"content":{"abstract":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \u201cSynthesizer\u201d framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.","authors":["Wen Xiao","Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !","tldr":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechani...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.24","presentation_id":"38939701","rocketchat_channel":"paper-codi2020-24","speakers":"Wen Xiao|Patrick Huber|Giuseppe Carenini","title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !"},{"content":{"abstract":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.","authors":["Li Liang","Zheng Zhao","Bonnie Webber"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extending Implicit Discourse Relation Recognition to the PDTB-3","tldr":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse r...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.26","presentation_id":"38939702","rocketchat_channel":"paper-codi2020-26","speakers":"Li Liang|Zheng Zhao|Bonnie Webber","title":"Extending Implicit Discourse Relation Recognition to the PDTB-3"},{"content":{"abstract":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.","authors":["Chenguang Zhu","Ruochen Xu","Michael Zeng","Xuedong Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining","tldr":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization in...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.263","presentation_id":"38940695","rocketchat_channel":"paper-codi2020-263","speakers":"Chenguang Zhu|Ruochen Xu|Michael Zeng|Xuedong Huang","title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining"},{"content":{"abstract":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En lexicon includes 95 entries, whereas the Tr-En lexicon contains 133 entries. The lexicons constitute the first step of a larger project of developing a multilingual discourse connective lexicon.","authors":["Murathan Kurfal\u0131","Sibel Ozer","Deniz Zeyrek","Am\u00e1lia Mendes"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex","tldr":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.27","presentation_id":"38939703","rocketchat_channel":"paper-codi2020-27","speakers":"Murathan Kurfal\u0131|Sibel Ozer|Deniz Zeyrek|Am\u00e1lia Mendes","title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex"},{"content":{"abstract":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects\u2014lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements.","authors":["Haixia Chai","Wei Zhao","Steffen Eger","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks","tldr":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.28","presentation_id":"38939704","rocketchat_channel":"paper-codi2020-28","speakers":"Haixia Chai|Wei Zhao|Steffen Eger|Michael Strube","title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks"},{"content":{"abstract":"","authors":["Belen Saldias","Deb Roy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.29","presentation_id":"38939705","rocketchat_channel":"paper-codi2020-29","speakers":"Belen Saldias|Deb Roy","title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types"},{"content":{"abstract":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, starting with a strong baseline neural parser unaware of any coreference information, we compare a parser which utilizes only the output of a neural coreference resolver, with a more sophisticated model, where discourse parsing and coreference resolution are jointly learned in a neural multitask fashion. Results indicate that these initial attempts to incorporate coreference information do not boost the performance of discourse parsing in a statistically significant way.","authors":["Grigorii Guz","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Coreference for Discourse Parsing: A Neural Approach","tldr":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, startin...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.31","presentation_id":"38939706","rocketchat_channel":"paper-codi2020-31","speakers":"Grigorii Guz|Giuseppe Carenini","title":"Coreference for Discourse Parsing: A Neural Approach"},{"content":{"abstract":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \\delta-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.","authors":["Rachel Rudinger","Vered Shwartz","Jena D. Hwang","Chandra Bhagavatula","Maxwell Forbes","Ronan Le Bras","Noah A. Smith","Yejin Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.418","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language","tldr":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference ha...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3452","presentation_id":"38940700","rocketchat_channel":"paper-codi2020-3452","speakers":"Rachel Rudinger|Vered Shwartz|Jena D. Hwang|Chandra Bhagavatula|Maxwell Forbes|Ronan Le Bras|Noah A. Smith|Yejin Choi","title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language"},{"content":{"abstract":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model\u2019s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other.","authors":["Yehudit Meged","Avi Caciularu","Vered Shwartz","Ido Dagan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.440","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin","tldr":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as dista...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3598","presentation_id":"38940701","rocketchat_channel":"paper-codi2020-3598","speakers":"Yehudit Meged|Avi Caciularu|Vered Shwartz|Ido Dagan","title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin"},{"content":{"abstract":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A comparative study for the language pair can discover the discourse differences between Spanish and Chinese, and can benefit the Spanish-Chinese translation. In this work, based on a Spanish-Chinese parallel corpus annotated with discourse information, we compare the annotation results between the language pair and analyze how discourse affects Spanish-Chinese translation. The research results in our study can help human translators who work with the language pair.","authors":["Shuyuan Cao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus","tldr":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A compa...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.4","presentation_id":"38939686","rocketchat_channel":"paper-codi2020-4","speakers":"Shuyuan Cao","title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus"},{"content":{"abstract":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.","authors":["Yifan Gao","Piji Li","Wei Bi","Xiaojiang Liu","Michael Lyu","Irwin King"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning","tldr":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.475","presentation_id":"38940696","rocketchat_channel":"paper-codi2020-475","speakers":"Yifan Gao|Piji Li|Wei Bi|Xiaojiang Liu|Michael Lyu|Irwin King","title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning"},{"content":{"abstract":"","authors":["Juntao Yu","Nafise Sadat Moosavi","Silviu Paun","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.6","presentation_id":"38940702","rocketchat_channel":"paper-codi2020-6","speakers":"Juntao Yu|Nafise Sadat Moosavi|Silviu Paun|Massimo Poesio","title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution"},{"content":{"abstract":"","authors":["Juntao Yu","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multitask Learning-Based Neural Bridging Reference Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.7","presentation_id":"38940703","rocketchat_channel":"paper-codi2020-7","speakers":"Juntao Yu|Massimo Poesio","title":"Multitask Learning-Based Neural Bridging Reference Resolution"},{"content":{"abstract":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled. These features were transformed using tf-idf and word embedding; feature selection was done using Principal Component Analysis (PCA). We ran experiments on six combinations of features to predict sequences with Hierarchical Agglomerative Clustering. An analysis of the clustering results indicate that using word embeddings and syntactic features, significantly improved the results.","authors":["Maitreyee Maitreyee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues","tldr":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled....","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.8","presentation_id":"38939687","rocketchat_channel":"paper-codi2020-8","speakers":"Maitreyee Maitreyee","title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues"},{"content":{"abstract":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.","authors":["Sopan Khosla","Carolyn Rose"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Type Information to Improve Entity Coreference Resolution","tldr":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic know...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.9","presentation_id":"38939688","rocketchat_channel":"paper-codi2020-9","speakers":"Sopan Khosla|Carolyn Rose","title":"Using Type Information to Improve Entity Coreference Resolution"}]
