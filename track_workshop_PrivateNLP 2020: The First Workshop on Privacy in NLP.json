[{"content":{"abstract":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution defined by the ground truth labels). In this paper, we show that a malicious modeler, upon obtaining access to the Log-Loss scores on its predictions, can exploit this information to infer all the ground truth labels of arbitrary test datasets with full accuracy. We provide an efficient algorithm to perform this inference. A particularly interesting application where this attack can be exploited is to breach privacy in the setting of Membership Inference Attacks. These attacks exploit the vulnerabilities of exposing models trained on customer data to queries made by an adversary. Privacy auditing tools for measuring leakage from sensitive datasets assess the total privacy leakage based on the adversary\u2019s predictions for datapoint membership. An instance of the proposed attack can hence, cause complete membership privacy breach, obviating any attack model training or access to side knowledge with the adversary. Moreover, our algorithm is agnostic to the model under attack and hence, enables perfect membership inference even for models that do not memorize or overfit. In particular, our observations provide insight into the extent of information leakage from statistical aggregates and how they can be exploited.","authors":["Abhinav Aggarwal","Zekun Xu","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Log-Loss Scores and (No) Privacy","tldr":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.1","presentation_id":"38939769","rocketchat_channel":"paper-privatenlp2020-1","speakers":"Abhinav Aggarwal|Zekun Xu|Oluwaseyi Feyisetan|Nathanael Teissier","title":"On Log-Loss Scores and (No) Privacy"},{"content":{"abstract":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-centric encoders and leverage them to create an interface that allows users to search for privacy settings using free-form queries. In order to achieve this goal, we create a custom Semantic Similarity dataset, which consists of real user queries covering various privacy settings. We then use this dataset to fine-tune a state of the art encoder. Using this fine-tuned encoder, we perform semantic matching between the user queries and the privacy settings to retrieve the most relevant setting. Finally, we also use the encoder to generate embeddings of privacy settings from the top 100 websites and perform unsupervised clustering to learn about the online privacy settings types. We find that the most common type of privacy settings are \u2018Personalization\u2019 and \u2018Notifications\u2019, with coverage of 35.8% and 34.4%, respectively, in our dataset.","authors":["Rishabh Khandelwal","Asmit Nayak","Yao Yao","Kassem Fawaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Surfacing Privacy Settings Using Semantic Matching","tldr":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-cent...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.11","presentation_id":"38939773","rocketchat_channel":"paper-privatenlp2020-11","speakers":"Rishabh Khandelwal|Asmit Nayak|Yao Yao|Kassem Fawaz","title":"Surfacing Privacy Settings Using Semantic Matching"},{"content":{"abstract":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.","authors":["Gavin Kerrigan","Dylan Slack","Jens Tuyls"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Differentially Private Language Models Benefit from Public Pre-training","tldr":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorit...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.12","presentation_id":"38939774","rocketchat_channel":"paper-privatenlp2020-12","speakers":"Gavin Kerrigan|Dylan Slack|Jens Tuyls","title":"Differentially Private Language Models Benefit from Public Pre-training"},{"content":{"abstract":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is first mapped into a continuous embedding space, perturbed by sampling a spherical noise from an appropriate distribution, and then projected back to the discrete vocabulary space. While this allows the perturbation to admit the required metric differential privacy, often the utility of downstream tasks modeled on this perturbed data is low because the spherical noise does not account for the variability in the density around different words in the embedding space. In particular, words in a sparse region are likely unchanged even when the noise scale is large. In this paper, we propose a text perturbation mechanism based on a carefully designed regularized variant of the Mahalanobis metric to overcome this problem. For any given noise scale, this metric adds an elliptical noise to account for the covariance structure in the embedding space. This heterogeneity in the noise scale along different directions helps ensure that the words in the sparse region have sufficient likelihood of replacement without sacrificing the overall utility. We provide a text-perturbation algorithm based on this metric and formally prove its privacy guarantees. Additionally, we empirically show that our mechanism improves the privacy statistics to achieve the same level of utility as compared to the state-of-the-art Laplace mechanism.","authors":["Zekun Xu","Abhinav Aggarwal","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric","tldr":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is firs...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2","presentation_id":"38939770","rocketchat_channel":"paper-privatenlp2020-2","speakers":"Zekun Xu|Abhinav Aggarwal|Oluwaseyi Feyisetan|Nathanael Teissier","title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric"},{"content":{"abstract":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and social networks, as well as the power of modern AI to synthesize and gain insights from this data. This paper presents an approach to detect emotional and informational self-disclosure in natural language. We hypothesize that identifying frame semantics can meaningfully support this task. Specifically, we use Semantic Role Labeling to identify the lexical units and their semantic roles that signal self-disclosure. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.","authors":["Chandan Akiti","Anna Squicciarini","Sarah Rajtmajer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.312","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content","tldr":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal infor...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2534","presentation_id":"38940639","rocketchat_channel":"paper-privatenlp2020-2534","speakers":"Chandan Akiti|Anna Squicciarini|Sarah Rajtmajer","title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content"},{"content":{"abstract":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.","authors":["Yangsibo Huang","Zhao Song","Danqi Chen","Kai Li","Sanjeev Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.123","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TextHide: Tackling Data Privacy in Language Understanding Tasks","tldr":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language unders...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.3","presentation_id":"38939771","rocketchat_channel":"paper-privatenlp2020-3","speakers":"Yangsibo Huang|Zhao Song|Danqi Chen|Kai Li|Sanjeev Arora","title":"TextHide: Tackling Data Privacy in Language Understanding Tasks"},{"content":{"abstract":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations.","authors":["Mitra Bokaie Hosseini","Pragyan K C","Irwin Reyes","Serge Egelman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies","tldr":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulation...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.9","presentation_id":"38939772","rocketchat_channel":"paper-privatenlp2020-9","speakers":"Mitra Bokaie Hosseini|Pragyan K C|Irwin Reyes|Serge Egelman","title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies"}]
