[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1018.png","content":{"abstract":"Bolukbasi et al. (2016) presents one of the first gender  bias  mitigation  techniques  for  word embeddings.   Their  method  takes  pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings.  However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version.  We take inspiration from kernel principal component analysis and derive a  non-linear bias isolation technique.  We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","authors":["Francisco Vargas","Ryan Cotterell"],"demo_url":"","keywords":["word embeddings","analogical task","non-linear mitigation","gender"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.232","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2011","main.3093","main.865","main.802","main.3143"],"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","tldr":"Bolukbasi et al. (2016) presents one of the first gender  bias  mitigation  techniques  for  word embeddings.   Their  method  takes  pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias...","track":"Machine Learning for NLP"},"forum":"main.1018","id":"main.1018","presentation_id":"38938828"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1046.png","content":{"abstract":"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA","authors":["Jiaao Chen","Zhenghui Wang","Ran Tian","Zichao Yang","Diyi Yang"],"demo_url":"","keywords":["named recognition","deep understanding","semi-supervised ner","entity learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.95","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2733","main.989","main.426","main.1458"],"title":"Local Additivity Based Data Augmentation for Semi-supervised NER","tldr":"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data ...","track":"Machine Learning for NLP"},"forum":"main.1046","id":"main.1046","presentation_id":"38938834"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1130.png","content":{"abstract":"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's \\emph{vocabulary}---typically selected before training and permanently fixed later---affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.","authors":["Nikolaos Pappas","Phoebe Mulcaire","Noah A. Smith"],"demo_url":"","keywords":["language modeling","cross-domain settings","language models","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.96","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2363","TACL.2041","main.1892","main.74","main.852"],"title":"Grounded Compositional Outputs for Adaptive Language Modeling","tldr":"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's \\emph{vocabulary}---typically selected b...","track":"Machine Learning for NLP"},"forum":"main.1130","id":"main.1130","presentation_id":"38938850"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1208.png","content":{"abstract":"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.","authors":["Yung-Sung Chuang","Shang-Yu Su","Yun-Nung Chen"],"demo_url":"","keywords":["lll tasks","sequence generation","text tasks","lifelong"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.233","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2838","TACL.2041","main.74","main.1734","main.1485"],"title":"Lifelong Language Knowledge Distillation","tldr":"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2K...","track":"Machine Learning for NLP"},"forum":"main.1208","id":"main.1208","presentation_id":"38938863"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1210.png","content":{"abstract":"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks","authors":["Fabio Massimo Zanzotto","Andrea Santilli","Leonardo Ranaldi","Dario Onorati","Pierfrancesco Tommasino","Francesca Fallucchi"],"demo_url":"","keywords":["natural understanding","inference","syntactic parsers","large-scale learners"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.18","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2411","main.2040","TACL.2141","main.2179","main.1957"],"title":"KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations","tldr":"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose K...","track":"Machine Learning for NLP"},"forum":"main.1210","id":"main.1210","presentation_id":"38938864"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1220.png","content":{"abstract":"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.","authors":["Mengjie Zhao","Tao Lin","Fei Mi","Martin Jaggi","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["masking bert","nlp tasks","downstream tasks","masking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.174","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3337","main.3074","main.247","demo.118","main.1299"],"title":"Masking as an Efficient Alternative to Finetuning for Pretrained Language Models","tldr":"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleve...","track":"Machine Learning for NLP"},"forum":"main.1220","id":"main.1220","presentation_id":"38938867"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1227.png","content":{"abstract":"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.","authors":["Sangwhan Moon","Naoaki Okazaki"],"demo_url":"","keywords":["natural processing","downstream tasks","mitigation","large models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.631","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.2635","main.1960","main.1351","main.858"],"title":"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching","tldr":"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on do...","track":"Machine Learning for NLP"},"forum":"main.1227","id":"main.1227","presentation_id":"38938869"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1351.png","content":{"abstract":"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.","authors":["Vincent Micheli","Martin d'Hoffschmidt","Fran\u00e7ois Fleuret"],"demo_url":"","keywords":["language modeling","sustainable practices","compact models","multiple models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.632","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1960","TACL.2047","main.1631","main.2078"],"title":"On the importance of pre-training data volume for compact language models","tldr":"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multipl...","track":"Machine Learning for NLP"},"forum":"main.1351","id":"main.1351","presentation_id":"38938889"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1356.png","content":{"abstract":"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.","authors":["Demi Guo","Yoon Kim","Alexander Rush"],"demo_url":"","keywords":["sequence-to-sequence problems","scan","semantic parsing","neural networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.447","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.910","main.2430","main.891","main.1960","main.148"],"title":"Sequence-Level Mixed Sample Data Augmentation","tldr":"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-se...","track":"Machine Learning for NLP"},"forum":"main.1356","id":"main.1356","presentation_id":"38938890"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1399.png","content":{"abstract":"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.","authors":["Joseph Fisher","Arpit Mittal","Dave Palfrey","Christos Christodoulopoulos"],"demo_url":"","keywords":["nlp pipelines","triple task","knowledge embeddings","graph embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.595","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1508","main.2721","main.1923","main.1305","main.426"],"title":"Debiasing knowledge graph embeddings","tldr":"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pi...","track":"Machine Learning for NLP"},"forum":"main.1399","id":"main.1399","presentation_id":"38938899"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1458.png","content":{"abstract":"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.","authors":["Nathan Ng","Kyunghyun Cho","Marzyeh Ghassemi"],"demo_url":"","keywords":["data augmentation","ood generalization","robustness benchmarks","ssmba"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.97","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2959","main.745","main.1046","main.3434","main.426"],"title":"SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness","tldr":"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to ...","track":"Machine Learning for NLP"},"forum":"main.1458","id":"main.1458","presentation_id":"38938909"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1490.png","content":{"abstract":"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.","authors":["Florian Mai","Nikolaos Pappas","Ivan Montero","Noah A. Smith","James Henderson"],"demo_url":"","keywords":["conditional tasks","style transfer","style tasks","text autoencoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.491","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.1446","main.3483","main.2635","main.910"],"title":"Plug and Play Autoencoders for Conditional Text Generation","tldr":"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedd...","track":"Machine Learning for NLP"},"forum":"main.1490","id":"main.1490","presentation_id":"38938917"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1498.png","content":{"abstract":"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.","authors":["Alexander Terenin","M\u00e5ns Magnusson","Leif Jonsson"],"demo_url":"","keywords":["data-parallel training","computation","probabilistic models","parallel systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.234","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2792","main.30","main.3348","main.2865"],"title":"Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models","tldr":"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hiera...","track":"Machine Learning for NLP"},"forum":"main.1498","id":"main.1498","presentation_id":"38938922"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1508.png","content":{"abstract":"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node's $k$-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.","authors":["Kian Ahrabian","Aarash Feizi","Yasmin Salehi","William L. Hamilton","Avishek Joey Bose"],"demo_url":"","keywords":["inferring patterns","learning representations","contrastive estimation","contrastive approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.492","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1399","main.2873","main.3434","main.426","main.1460"],"title":"Structure Aware Negative Sampling in Knowledge Graphs","tldr":"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches i...","track":"Machine Learning for NLP"},"forum":"main.1508","id":"main.1508","presentation_id":"38938925"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.151.png","content":{"abstract":"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.","authors":["Kaize Ding","Jianling Wang","Jundong Li","Dingcheng Li","Huan Liu"],"demo_url":"","keywords":["text classification","natural processing","text learning","text task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.399","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1488","main.782","main.2367","main.2995","main.1669"],"title":"Be More with Less: Hypergraph Attention Networks for Inductive Text Classification","tldr":"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on t...","track":"Machine Learning for NLP"},"forum":"main.151","id":"main.151","presentation_id":"38938658"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1528.png","content":{"abstract":"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model---Entities as Experts (EaE)---that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EaE's entity representations are learned directly from text. We show that EaE's learned representations capture sufficient knowledge to answer TriviaQA questions such as \"Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?'', outperforming an encoder-generator Transformer model with 10x the parameters on this task.  According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE's performance.","authors":["Thibault F\u00e9vry","Livio Baldini Soares","Nicholas FitzGerald","Eunsol Choi","Tom Kwiatkowski"],"demo_url":"","keywords":["capturing knowledge","eae","identification entities","language model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.400","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.911","main.1159","main.666","main.605","main.2630"],"title":"Entities as Experts: Sparse Memory Access with Entity Supervision","tldr":"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model---Entities as Experts (EaE)---that can access distinct memories of the entities mentioned in a piece of ...","track":"Machine Learning for NLP"},"forum":"main.1528","id":"main.1528","presentation_id":"38938928"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1574.png","content":{"abstract":"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.","authors":["Canwen Xu","Wangchunshu Zhou","Tao Ge","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["bert compression","model compression","model approach","progressive replacing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.633","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2783","main.956","main.3394","main.2779","main.1219"],"title":"BERT-of-Theseus: Compressing BERT by Progressive Module Replacing","tldr":"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly repla...","track":"Machine Learning for NLP"},"forum":"main.1574","id":"main.1574","presentation_id":"38938938"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1575.png","content":{"abstract":"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.","authors":["Yang Gao","Yi-Fan Li","Yu Lin","Charu Aggarwal","Latifur Khan"],"demo_url":"","keywords":["real-world problems","sentiment classification","ir","machine methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.98","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.493","main.3174","main.1180","main.2068","main.2958"],"title":"SetConv: A New Approach for Learning from Imbalanced Data","tldr":"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (Se...","track":"Machine Learning for NLP"},"forum":"main.1575","id":"main.1575","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1618.png","content":{"abstract":"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, \"Extended Transformer Construction\" (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a \"Contrastive Predictive Coding\" (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.","authors":["Joshua Ainslie","Santiago Ontanon","Chris Alberti","Vaclav Cvicek","Zachary Fisher","Philip Pham","Anirudh Ravula","Sumit Sanghai","Qifan Wang","Li Yang"],"demo_url":"","keywords":["natural tasks","encoding inputs","transformer models","transformer architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.19","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.60","main.3337","main.2635","main.3278","main.3398"],"title":"ETC: Encoding Long and Structured Inputs in Transformers","tldr":"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, \"Extended Transformer Construction\" (ETC), that addresses two key challenges of standard ...","track":"Machine Learning for NLP"},"forum":"main.1618","id":"main.1618","presentation_id":"38938951"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1648.png","content":{"abstract":"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.","authors":["Yanlin Feng","Xinyue Chen","Bill Yuchen Lin","Peifeng Wang","Jun Yan","Xiang Ren"],"demo_url":"","keywords":["multi-hop reasoning","question models","knowledge-aware approach","multi-hop module"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.99","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.923","main.574","main.2761","main.531","main.1123"],"title":"Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering","tldr":"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propos...","track":"Machine Learning for NLP"},"forum":"main.1648","id":"main.1648","presentation_id":"38938959"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1682.png","content":{"abstract":"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations.  Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.","authors":["Jueqing Lu","Lan Du","Ming Liu","Joanna Dipnall"],"demo_url":"","keywords":["fewzero-shot learning","classifications tasks","multi-label classification","classifier"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.235","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2167","main.2793","main.1032","main.148","main.1611"],"title":"Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs","tldr":"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where e...","track":"Machine Learning for NLP"},"forum":"main.1682","id":"main.1682","presentation_id":"38938967"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1734.png","content":{"abstract":"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.","authors":["Sanyuan Chen","Yutai Hou","Yiming Cui","Wanxiang Che","Ting Liu","Xiangzhan Yu"],"demo_url":"","keywords":["pretraining","pretraining tasks","learning tasks","fine-tuning bert-large"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.634","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2838","main.1299","TACL.2041","main.2491","main.74"],"title":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting","tldr":"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performanc...","track":"Machine Learning for NLP"},"forum":"main.1734","id":"main.1734","presentation_id":"38938976"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2040.png","content":{"abstract":"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.","authors":["Prithviraj Sen","Marina Danilevsky","Yunyao Li","Siddhartha Brahma","Matthias Boehm","Laura Chiticariu","Rajasekar Krishnamurthy"],"demo_url":"","keywords":["interpretability models","sentence classification","le","human-machine models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.345","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1613","main.2851","main.3181","main.76","main.1996"],"title":"Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification","tldr":"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form...","track":"Machine Learning for NLP"},"forum":"main.2040","id":"main.2040","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2070.png","content":{"abstract":"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers\u2019 robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs \u2013 dialog response generation and user simulation, where our model outperforms previous strong baselines.","authors":["Kang Min Yoo","Hanbit Lee","Franck Dernoncourt","Trung Bui","Walter Chang","Sang-goo Lee"],"demo_url":"","keywords":["generative augmentation","nlp tasks","dialog tracking","dialog generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.274","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.128","main.3393","main.355","main.3318","main.3179"],"title":"Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation","tldr":"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking fo...","track":"Machine Learning for NLP"},"forum":"main.2070","id":"main.2070","presentation_id":"38939041"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2087.png","content":{"abstract":"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.","authors":["Tu Vu","Tong Wang","Tsendsuren Munkhdalai","Alessandro Sordoni","Adam Trischler","Andrew Mattarella-Micke","Subhransu Maji","Mohit Iyyer"],"demo_url":"","keywords":["language modeling","nlp tasks","text classification","question answering"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.635","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.1923","main.2491","main.2500","main.74"],"title":"Exploring and Predicting Transferability across NLP Tasks","tldr":"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we ...","track":"Machine Learning for NLP"},"forum":"main.2087","id":"main.2087","presentation_id":"38939047"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2221.png","content":{"abstract":"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.","authors":["Ramakanth Pasunuru","Han Guo","Mohit Bansal"],"demo_url":"","keywords":["language tasks","optimization rewards","nlg tasks","question generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.625","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2410","main.2834","main.286","main.1734","main.3672"],"title":"DORB: Dynamically Optimizing Multiple Rewards with Bandits","tldr":"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in...","track":"Machine Learning for NLP"},"forum":"main.2221","id":"main.2221","presentation_id":"38939074"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2271.png","content":{"abstract":"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.","authors":["Yordan Yordanov","Oana-Maria Camburu","Vid Kocijan","Thomas Lukasiewicz"],"demo_url":"","keywords":["hard resolution","commonsense reasoning","pronoun resolution","sequence ranking"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.402","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.767","main.2491","main.143","main.888"],"title":"Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution","tldr":"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categ...","track":"Machine Learning for NLP"},"forum":"main.2271","id":"main.2271","presentation_id":"38939083"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2343.png","content":{"abstract":"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.","authors":["Kevin Clark","Minh-Thang Luong","Quoc Le","Christopher D. Manning"],"demo_url":"","keywords":["representation text","downstream tasks","pre-training","electric"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.20","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2615","main.3483","main.2515","main.247","TACL.2107"],"title":"Pre-Training Transformers as Energy-Based Cloze Models","tldr":"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens...","track":"Machine Learning for NLP"},"forum":"main.2343","id":"main.2343","presentation_id":"38939095"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2357.png","content":{"abstract":"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.","authors":["Lihao Wang","Xiaoqing Zheng"],"demo_url":"","keywords":["grammatical correction","sequence-to-sequence learning","neural networks","gec"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.228","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2047","main.2313","main.3227","demo.118","main.2389"],"title":"Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples","tldr":"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance ...","track":"Machine Learning for NLP"},"forum":"main.2357","id":"main.2357","presentation_id":"38939097"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2370.png","content":{"abstract":"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover's distance (optimal transport), which we refer to as word rotator's distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines. The source code is avaliable at https://github.com/eumesy/wrd","authors":["Sho Yokoi","Ryo Takahashi","Reina Akama","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["assessing similarity","vector converter","word alignment","alignment-based approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.236","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1503","main.1935","main.1901","main.644","main.973"],"title":"Word Rotator's Distance","tldr":"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferi...","track":"Machine Learning for NLP"},"forum":"main.2370","id":"main.2370","presentation_id":"38939100"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2406.png","content":{"abstract":"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.","authors":["Mikhail Galkin","Priyansh Trivedi","Gaurav Maheshwari","Ricardo Usbeck","Jens Lehmann"],"demo_url":"","keywords":["link prediction","kgs","message encoder","message -"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.596","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1706","main.1648","main.1493","main.3517"],"title":"Message Passing for Hyper-Relational Knowledge Graphs","tldr":"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - St...","track":"Machine Learning for NLP"},"forum":"main.2406","id":"main.2406","presentation_id":"38939108"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2448.png","content":{"abstract":"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","authors":["Felix Stahlberg","Shankar Kumar"],"demo_url":"","keywords":["sequence editing","natural tasks","nlp tasks","text normalization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.418","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3299","main.3457","main.1892","main.1503","main.2098"],"title":"Seq2Edits: Sequence Transduction Using Span-level Edit Operations","tldr":"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as...","track":"Machine Learning for NLP"},"forum":"main.2448","id":"main.2448","presentation_id":"38939123"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2452.png","content":{"abstract":"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.","authors":["Piotr Szyma\u0144ski","Kyle Gorman"],"demo_url":"","keywords":["natural models","bayesian technique","k-fold cross-validation","english taggers"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.172","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3115","main.2638","main.457","main.1455","main.3181"],"title":"Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing","tldr":"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the like...","track":"Machine Learning for NLP"},"forum":"main.2452","id":"main.2452","presentation_id":"38939124"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.247.png","content":{"abstract":"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.","authors":["Minki Kang","Moonsu Han","Sung Ju Hwang"],"demo_url":"","keywords":["self-supervised pre-training","question answering","task","reinforcement learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.493","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3483","main.2389","main.1299","main.1898","main.3023"],"title":"Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation","tldr":"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specif...","track":"Machine Learning for NLP"},"forum":"main.247","id":"main.247","presentation_id":"38938671"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2491.png","content":{"abstract":"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT\\textsubscript{Base} on the GLUE benchmark using fewer than a quarter of the training tokens.","authors":["St\u00e9phane Aroca-Ouellette","Frank Rudzicz"],"demo_url":"","keywords":["pre-training","masked modelling","next prediction","nsp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.403","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.1351","main.2635","main.522","main.1631"],"title":"On Losses for Modern Language Models","tldr":"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's e...","track":"Machine Learning for NLP"},"forum":"main.2491","id":"main.2491","presentation_id":"38939128"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2535.png","content":{"abstract":"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT -- a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.","authors":["Tsvetomila Mihaylova","Vlad Niculae","Andr\u00e9 F. T. Martins"],"demo_url":"","keywords":["pipeline systems","ste","latent models","end-to-end training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.171","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2307","main.1446","main.345","main.3292","main.87"],"title":"Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning","tldr":"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-...","track":"Machine Learning for NLP"},"forum":"main.2535","id":"main.2535","presentation_id":"38939140"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2574.png","content":{"abstract":"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.","authors":["Xiaoxiao Guo","Mo Yu","Yupeng Gao","Chuang Gan","Murray Campbell","Shiyu Chang"],"demo_url":"","keywords":["language techniques","language challenges","action generation","if solving"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.624","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1578","main.2982","main.390","main.763","main.645"],"title":"Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning","tldr":"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding...","track":"Machine Learning for NLP"},"forum":"main.2574","id":"main.2574","presentation_id":"38939145"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2615.png","content":{"abstract":"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.","authors":["Shrey Desai","Greg Durrett"],"demo_url":"","keywords":["natural processing","natural inference","paraphrase detection","commonsense reasoning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.21","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2491","main.2893","main.2834","main.1552"],"title":"Calibration of Pre-trained Transformers","tldr":"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an ...","track":"Machine Learning for NLP"},"forum":"main.2615","id":"main.2615","presentation_id":"38939157"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2632.png","content":{"abstract":"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.  We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.  The algorithm is designed to address the exposure bias problem.  On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.  Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.","authors":["Alexander Lin","Jeremy Wohlwend","Howard Chen","Tao Lei"],"demo_url":"","keywords":["natural tasks","knowledge distillation","exposure problem","prototypical tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.494","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1734","main.2430","main.1208","TACL.2041","main.2838"],"title":"Autoregressive Knowledge Distillation through Imitation Learning","tldr":"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-...","track":"Machine Learning for NLP"},"forum":"main.2632","id":"main.2632","presentation_id":"38939159"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2636.png","content":{"abstract":"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.","authors":["Lifu Tu","Tianyu Liu","Kevin Gimpel"],"demo_url":"","keywords":["natural processing","sequence labeling","semantic labeling","parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.449","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.148","main.2430","main.850","main.3348","main.1615"],"title":"An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks","tldr":"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, ...","track":"Machine Learning for NLP"},"forum":"main.2636","id":"main.2636","presentation_id":"38939161"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2783.png","content":{"abstract":"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.","authors":["Wei Zhang","Lu Hou","Yichun Yin","Lifeng Shang","Xiao Chen","Xin Jiang","Qun Liu"],"demo_url":"","keywords":["natural tasks","training process","transformer-based models","bert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.37","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.956","main.3543","main.1485","main.1552","main.3394"],"title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT","tldr":"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained device...","track":"Machine Learning for NLP"},"forum":"main.2783","id":"main.2783","presentation_id":"38939194"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2784.png","content":{"abstract":"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology --left, center, or right--, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.","authors":["Ramy Baly","Giovanni Da San Martino","James Glass","Preslav Nakov"],"demo_url":"","keywords":["article-level prediction","adversarial adaptation","pre-trained transformers","leading ideology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.404","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3644","main.635","main.2651","main.789","main.2430"],"title":"We Can Detect Your Bias: Predicting the Political Ideology of News Articles","tldr":"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology --left, center, or right--, which is well-...","track":"Machine Learning for NLP"},"forum":"main.2784","id":"main.2784","presentation_id":"38939195"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2790.png","content":{"abstract":"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over \\emph{well formed} relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also \\emph{semantically similar}. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.","authors":["Michal Lukasik","Himanshu Jain","Aditya Menon","Seungyeon Kim","Srinadh Bhojanapalli","Felix Yu","Sanjiv Kumar"],"demo_url":"","keywords":["classification","label de-noising","seqseq settings","machine translation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.405","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.1720","main.891","main.1356","main.3609"],"title":"Semantic Label Smoothing for Sequence to Sequence Problems","tldr":"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challe...","track":"Machine Learning for NLP"},"forum":"main.2790","id":"main.2790","presentation_id":"38939196"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2793.png","content":{"abstract":"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient --- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.","authors":["Trapit Bansal","Rishikesh Jha","Tsendsuren Munkhdalai","Andrew McCallum"],"demo_url":"","keywords":["nlp applications","fine-tuning","meta-learning problem","supervised tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.38","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.16","main.74","main.345","main.1482","main.2893"],"title":"Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks","tldr":"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fi...","track":"Machine Learning for NLP"},"forum":"main.2793","id":"main.2793","presentation_id":"38939198"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2818.png","content":{"abstract":"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.","authors":["Jiaji Huang","Xingyu Cai","Kenneth Church"],"demo_url":"","keywords":["monolingual task","bilingual induction","low regime","hubness"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.100","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1901","main.3181","main.2891","main.865","main.3115"],"title":"Improving Bilingual Lexicon Induction for Low Frequency Words","tldr":"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondl...","track":"Machine Learning for NLP"},"forum":"main.2818","id":"main.2818","presentation_id":"38939203"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2834.png","content":{"abstract":"Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs. However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available. In this paper, we study \\emph{ensemble distillation} as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles. We validate this framework on two tasks: named-entity recognition and machine translation. We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.","authors":["Steven Reich","David Mueller","Nicholas Andrews"],"demo_url":"","keywords":["structured prediction","named-entity recognition","machine translation","ensemble distillation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.450","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2615","main.2491","TACL.2411","main.345","main.1923"],"title":"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast\u2014Choose Three","tldr":"Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be use...","track":"Machine Learning for NLP"},"forum":"main.2834","id":"main.2834","presentation_id":"38939205"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2838.png","content":{"abstract":"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained  throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.","authors":["Zirui Wang","Sanket Vaibhav Mehta","Barnabas Poczos","Jaime Carbonell"],"demo_url":"","keywords":["lifelong learning","local adaptation","text benchmarks","multi-task learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.39","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1734","main.1208","TACL.2041","main.74","main.3470"],"title":"Efficient Meta Lifelong-Learning with Limited Memory","tldr":"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained  throughout their lifetime, a challenge known as lifelong learning. Sta...","track":"Machine Learning for NLP"},"forum":"main.2838","id":"main.2838","presentation_id":"38939206"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2865.png","content":{"abstract":"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA  models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.","authors":["Runzhi Tian","Yongyi Mao","Richong Zhang"],"demo_url":"","keywords":["learning models","learning","vae","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.101","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2430","main.1446","main.30","main.1498"],"title":"Learning VAE-LDA Models with Rounded Reparameterization Trick","tldr":"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameteri...","track":"Machine Learning for NLP"},"forum":"main.2865","id":"main.2865","presentation_id":"38939213"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2914.png","content":{"abstract":"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models.  Our code is publicly available at https://github.com/AI-secure/T3/.","authors":["Boxin Wang","Hengzhi Pei","Boyuan Pan","Qian Chen","Shuohang Wang","Bo Li"],"demo_url":"","keywords":["adversarial generation","nlp tasks","sentiment analysis","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.495","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2313","main.2895","main.47","demo.104","main.426"],"title":"T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack","tldr":"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to...","track":"Machine Learning for NLP"},"forum":"main.2914","id":"main.2914","presentation_id":"38939223"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2959.png","content":{"abstract":"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.","authors":["Lingkai Kong","Haoming Jiang","Yuchen Zhuang","Jie Lyu","Tuo Zhao","Chao Zhang"],"demo_url":"","keywords":["augmented training","in-distribution calibration","text classification","expectation error"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.102","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1458","main.3023","main.1834","main.2793","main.1046"],"title":"Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data","tldr":"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method int...","track":"Machine Learning for NLP"},"forum":"main.2959","id":"main.2959","presentation_id":"38939234"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3074.png","content":{"abstract":"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.","authors":["Ziheng Wang","Jeremy Wohlwend","Tao Lei"],"demo_url":"","keywords":["natural tasks","model compression","language tasks","pruning embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.496","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1130","main.1960","main.1351","main.1446"],"title":"Structured Pruning of Large Language Models","tldr":"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises ...","track":"Machine Learning for NLP"},"forum":"main.3074","id":"main.3074","presentation_id":"38939265"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3205.png","content":{"abstract":"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models. Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport. Experimental results on MUSE and VecMap datasets show significant improvement of our models. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed method.","authors":["Xu Zhao","Zihao Wang","Hao Wu","Yong Zhang"],"demo_url":"","keywords":["bilingual induction","prior transport","semi-supervision","bli"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.238","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.865","main.1379","main.471","main.3609","main.410"],"title":"Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction","tldr":"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further impr...","track":"Machine Learning for NLP"},"forum":"main.3205","id":"main.3205","presentation_id":"38939286"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3272.png","content":{"abstract":"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.","authors":["Justin Chiu","Alexander Rush"],"demo_url":"","keywords":["sequence modeling","hidden model","hidden hmm","hidden"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.103","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3348","main.1446","main.2430","main.2198","main.1615"],"title":"Scaling Hidden Markov Language Models","tldr":"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fall...","track":"Machine Learning for NLP"},"forum":"main.3272","id":"main.3272","presentation_id":"38939298"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3348.png","content":{"abstract":"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.","authors":["Sida Gao","Matthew R. Gormley"],"demo_url":"","keywords":["nlp","sequence tagging","named recognition","neural architectures"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.406","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.1615","main.345","main.2430","main.2068"],"title":"Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors","tldr":"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequ...","track":"Machine Learning for NLP"},"forum":"main.3348","id":"main.3348","presentation_id":"38939315"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3394.png","content":{"abstract":"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained  to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.","authors":["Siqi Sun","Zhe Gan","Yuwei Fang","Yu Cheng","Shuohang Wang","Jingjing Liu"],"demo_url":"","keywords":["contrastive distillation","compress models","pre-training stages","existing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.36","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1485","TACL.2411","main.1892","TACL.2041","main.1208"],"title":"Contrastive Distillation on Intermediate Representations for Language Model Compression","tldr":"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions o...","track":"Machine Learning for NLP"},"forum":"main.3394","id":"main.3394","presentation_id":"38939327"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3434.png","content":{"abstract":"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of \\emph{randomly} masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over \\emph{subsets} of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.","authors":["Thuy-Trang Vu","Dinh Phung","Gholamreza Haffari"],"demo_url":"","keywords":["combinatorial problem","unsupervised tasks","named recognition","broad-coverage models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.497","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.426","main.745","main.2313","TACL.2389"],"title":"Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models","tldr":"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small pe...","track":"Machine Learning for NLP"},"forum":"main.3434","id":"main.3434","presentation_id":"38939334"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.345.png","content":{"abstract":"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model.  For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.","authors":["Michelle Yuan","Hsuan-Tien Lin","Jordan Boyd-Graber"],"demo_url":"","keywords":["uncertainty sampling","cold-start setting","pre-trained models","fine-tuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.637","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2793","main.2733","main.3348","main.76","main.2068"],"title":"Cold-start Active Learning through Self-supervised Language Modeling","tldr":"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model.  For instance, uncertainty sampling depends on poorly calibrated mo...","track":"Machine Learning for NLP"},"forum":"main.345","id":"main.345","presentation_id":"38938687"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3470.png","content":{"abstract":"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model\u2019s ability to solve each task. Moreover, the dataset\u2019s structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.","authors":["Orion Weller","Nicholas Lourie","Matt Gardner","Matthew Peters"],"demo_url":"","keywords":["task-oriented evaluation","systematic generalization","machine systems","nlp systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.105","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2342","main.1923","main.2838","main.2491"],"title":"Learning from Task Descriptions","tldr":"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a fra...","track":"Machine Learning for NLP"},"forum":"main.3470","id":"main.3470","presentation_id":"38939344"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.362.png","content":{"abstract":"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks. Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks. In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models. We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors. In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL. We conduct experiments on two English datasets and one Chinese dataset. Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions' consistency.","authors":["Wenqing Chen","Jidong Tian","Liqiang Xiao","Hao He","Yaohui Jin"],"demo_url":"","keywords":["logically tasks","logically mtl","causal inference","label transfer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.173","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.387","TACL.2411","main.1103","main.1191","main.2307"],"title":"Exploring Logically Dependent Multi-task Learning with Causal Inference","tldr":"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignore...","track":"Machine Learning for NLP"},"forum":"main.362","id":"main.362","presentation_id":"38938694"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.371.png","content":{"abstract":"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.","authors":["Siddhant Garg","Goutham Ramakrishnan"],"demo_url":"","keywords":["nlp","generating examples","automatic evaluations","modern models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.498","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2313","main.2914","main.47","main.2357","demo.104"],"title":"BAE: BERT-based Adversarial Examples for Text Classification","tldr":"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to gene...","track":"Machine Learning for NLP"},"forum":"main.371","id":"main.371","presentation_id":"38938695"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.410.png","content":{"abstract":"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space. In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques. We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.","authors":["Pratik Jawanpuria","Mayank Meghwanshi","Bamdev Mishra"],"demo_url":"","keywords":["learning alignment","unsupervised alignment","bilingual induction","cross-lingual similarity"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.240","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.267","main.3116","main.865","CL.2","main.852"],"title":"A Simple Approach to Learning Unsupervised Multilingual Embeddings","tldr":"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-prob...","track":"Machine Learning for NLP"},"forum":"main.410","id":"main.410","presentation_id":"38938704"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.426.png","content":{"abstract":"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.","authors":["Xinyin Ma","Yongliang Shen","Gongfan Fang","Chen Chen","Chenghao Jia","Weiming Lu"],"demo_url":"","keywords":["nlp tasks","nlp","compressing models","text generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.499","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.3434","main.2313","main.1485","main.1046"],"title":"Adversarial Self-Supervised Data-Free Distillation for Text Classification","tldr":"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a reso...","track":"Machine Learning for NLP"},"forum":"main.426","id":"main.426","presentation_id":"38938706"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.47.png","content":{"abstract":"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \\textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at \\url{https://github.com/LinyangLee/BERT-Attack}.","authors":["Linyang Li","Ruotian Ma","Qipeng Guo","Xiangyang Xue","Xipeng Qiu"],"demo_url":"","keywords":["adversarial attacks","downstream tasks","calculation","gradient-based methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.500","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2895","main.60","main.371","demo.104"],"title":"BERT-ATTACK: Adversarial Attack Against BERT Using BERT","tldr":"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack m...","track":"Machine Learning for NLP"},"forum":"main.47","id":"main.47","presentation_id":"38938642"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.493.png","content":{"abstract":"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.","authors":["Liat Ein-Dor","Alon Halfon","Ariel Gera","Eyal Shnarch","Lena Dankin","Leshem Choshen","Marina Danilevsky","Ranit Aharonov","Yoav Katz","Noam Slonim"],"demo_url":"","keywords":["text classification","nlp tasks","bert-based classification","binary classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.638","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2068","main.345","main.1575","main.426","main.1923"],"title":"Active Learning for BERT: An Empirical Study","tldr":"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-train...","track":"Machine Learning for NLP"},"forum":"main.493","id":"main.493","presentation_id":"38938721"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.517.png","content":{"abstract":"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim's labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.","authors":["Nitish Shirish Keskar","Bryan McCann","Caiming Xiong","Richard Socher"],"demo_url":"","keywords":["pre-training","natural processing","victim model","extraction process"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.501","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3116","main.143","main.1997","main.2076","main.2630"],"title":"The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual APIs","tldr":"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim's labels for that data. We di...","track":"Machine Learning for NLP"},"forum":"main.517","id":"main.517","presentation_id":"38938724"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.60.png","content":{"abstract":"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.","authors":["Eric Wallace","Mitchell Stern","Dawn Song"],"demo_url":"","keywords":["machine mt","machine","mt","production systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.446","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2914","main.47","main.2895","main.1614","main.2313"],"title":"Imitation Attacks and Defenses for Black-box Machine Translation Systems","tldr":"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We ...","track":"Machine Learning for NLP"},"forum":"main.60","id":"main.60","presentation_id":"38938644"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.658.png","content":{"abstract":"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.  Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.","authors":["Zhen Han","Peng Chen","Yunpu Ma","Volker Tresp"],"demo_url":"","keywords":["representation kgs","temporal tasks","kgs","embedding approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.593","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2873","main.2877","main.3084","main.973"],"title":"DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion","tldr":"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.  Temporal KGs often exhibit multiple simultaneous non-Euclidean structures,...","track":"Machine Learning for NLP"},"forum":"main.658","id":"main.658","presentation_id":"38938751"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.745.png","content":{"abstract":"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained  on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple  source domains and must make predictions on a domain for which no labelled data has been seen.   Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where  the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training,  to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their  performance, suggesting that large transformer-based models are already relatively robust across  domains. Additionally, we show that mixture of experts leads to significant performance improvements  by comparing several variants of mixing functions, including one novel metric based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective metrics for mixing their predictions.","authors":["Dustin Wright","Isabelle Augenstein"],"demo_url":"","keywords":["unsupervised adaptation","cnns","rnns","domain classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.639","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3434","main.1458","TACL.2389","main.3023","main.493"],"title":"Transformer Based Multi-Source Domain Adaptation","tldr":"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained  on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where ...","track":"Machine Learning for NLP"},"forum":"main.745","id":"main.745","presentation_id":"38938766"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.763.png","content":{"abstract":"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.","authors":["Subhajit Chaudhury","Daiki Kimura","Kartik Talamadupula","Michiaki Tatsubori","Asim Munawar","Ryuki Tachibana"],"demo_url":"","keywords":["irrelevant removal","generalization","observation pruning","reinforcement methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.241","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1578","main.664","main.2389","main.3672","main.2583"],"title":"Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games","tldr":"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for ...","track":"Machine Learning for NLP"},"forum":"main.763","id":"main.763","presentation_id":"38938770"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.825.png","content":{"abstract":"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \\emph{and} document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.","authors":["Xuhui Zhou","Nikolaos Pappas","Noah A. Smith"],"demo_url":"","keywords":["text alignment","citation recommendation","plagiarism detection","predicting relationships"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.407","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.639","main.2382","main.1694","main.2367","main.3010"],"title":"Multilevel Text Alignment with Cross-Document Attention","tldr":"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \\emph{and} document levels....","track":"Machine Learning for NLP"},"forum":"main.825","id":"main.825","presentation_id":"38938779"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.903.png","content":{"abstract":"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.","authors":["Bang An","Jie Lyu","Zhenyi Wang","Chunyuan Li","Changwei Hu","Fei Tan","Ruiyi Zhang","Yifan Hu","Changyou Chen"],"demo_url":"","keywords":["natural applications","attention collapse","neural mechanism","bayesian perspective"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.17","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3513","main.1734","main.1670","main.355","main.2650"],"title":"Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference","tldr":"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. ...","track":"Machine Learning for NLP"},"forum":"main.903","id":"main.903","presentation_id":"38938801"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.973.png","content":{"abstract":"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.","authors":["Max Ryabinin","Sergei Popov","Liudmila Prokhorenkova","Elena Voita"],"demo_url":"","keywords":["word tasks","word embeddings","graphglove","unsupervised representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.594","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2877","TACL.2121","demo.58","main.666"],"title":"Embedding Words in Non-Vector Space with Unsupervised Graph Learning","tldr":"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to ...","track":"Machine Learning for NLP"},"forum":"main.973","id":"main.973","presentation_id":"38938816"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.977.png","content":{"abstract":"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.","authors":["Yexiang Wang","Yi Guo","Siqi Zhu"],"demo_url":"","keywords":["dst","sa","annotation span","multiwoz"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.243","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.689","main.1702","main.1179","main.2209","main.1561"],"title":"Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking","tldr":"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models l...","track":"Machine Learning for NLP"},"forum":"main.977","id":"main.977","presentation_id":"38938817"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2083.png","content":{"abstract":"We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents; a word is generated by a hidden semantic vector encoding its contextual semantic meaning; and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared to existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.","authors":["Lixing Zhu","Deyu Zhou","Yulan He"],"demo_url":"","keywords":["word evaluation","word disambiguation","sentiment classification","generative model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.30","main.2792","main.3093","main.3358"],"title":"A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings","tldr":"We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents; a word is generated by a hidden...","track":"Machine Learning for NLP"},"forum":"TACL.2083","id":"TACL.2083","presentation_id":"38939404"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2093.png","content":{"abstract":"Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (ETM), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the ETM models each word with a categorical distribution whose natural parameter is the inner product between the word's embedding and an embedding of its assigned topic. To fit the ETM, we develop an efficient amortized variational inference algorithm. The ETM discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation (LDA), in terms of both topic quality and predictive performance.","authors":["Adji Bousso Dieng","Francisco Ruiz","David Blei"],"demo_url":"","keywords":["generative documents","topic modeling","topic models","embedded model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2792","main.30","TACL.2083","main.2931","main.1498"],"title":"Topic Modeling in Embedding Spaces","tldr":"Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (ETM), ...","track":"Machine Learning for NLP"},"forum":"TACL.2093","id":"TACL.2093","presentation_id":"38939405"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2095.png","content":{"abstract":"For many NLP applications, such as question answering and summarisation, the goal is to select the best solution from a large space of candidates to meet a particular user\u2019s needs. To address the lack of user or task-specific training data, we propose an interactive text ranking approach that actively selects pairs of candidates, from which the user selects the best. Unlike previous strategies, which attempt to learn a ranking across the whole candidate space, our method employs Bayesian optimisation to focus the user\u2019s labelling effort on high quality candidates and integrate prior knowledge to cope better with small data scenarios. We apply our method to community question answering (cQA) and extractive multi-document summarisation, finding that it significantly outperforms existing interactive approaches. We also show that the ranking function learned by our method is an effective reward function for reinforcement learning, which improves the state of the art for interactive summarisation.","authors":["Edwin Simpson","Yang Gao","Iryna Gurevych"],"demo_url":"","keywords":["nlp applications","question answering","question summarisation","community answering"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.693","main.1023","main.471","main.1949","demo.54"],"title":"Interactive Text Ranking with Bayesian Optimisation: A Case Study on Community QA and Summarisation","tldr":"For many NLP applications, such as question answering and summarisation, the goal is to select the best solution from a large space of candidates to meet a particular user\u2019s needs. To address the lack of user or task-specific training data, we propos...","track":"Machine Learning for NLP"},"forum":"TACL.2095","id":"TACL.2095","presentation_id":"38939406"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2255.png","content":{"abstract":"Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT (Devlin et al., 2019) with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves in-domain model performance, yields effective reduced-size models and increases model stability.","authors":["Roi Reichart","Eyal Ben David","Carmel Rabinovitz"],"demo_url":"","keywords":["domain adaptation","nlp","sentiment setups","in-domain model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2078","main.1733","main.3292","main.910","main.1428"],"title":"PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models","tldr":"Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target d...","track":"Machine Learning for NLP"},"forum":"TACL.2255","id":"TACL.2255","presentation_id":"38939416"}]
