[{"content":{"abstract":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.","authors":["Francois Meyer","Martha Lewis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modelling Lexical Ambiguity with Density Matrices","tldr":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelate...","track":"CoNLL 2020"},"id":"WS-1.100","presentation_id":"38939483","rocketchat_channel":"paper-conll-100","speakers":"Francois Meyer|Martha Lewis","title":"Modelling Lexical Ambiguity with Density Matrices"},{"content":{"abstract":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network\u2019s architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","authors":["William Havard","Laurent Besacier","Jean-Pierre Chevrot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech","tldr":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and the...","track":"CoNLL 2020"},"id":"WS-1.101","presentation_id":"38939484","rocketchat_channel":"paper-conll-101","speakers":"William Havard|Laurent Besacier|Jean-Pierre Chevrot","title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech"},{"content":{"abstract":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the \u201cLarge-scale online biomedical semantic indexing\u201d track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach.","authors":["Dusan Grujicic","Gorjan Radevski","Tinne Tuytelaars","Matthew Blaschko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to ground medical text in a 3D human atlas","tldr":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to r...","track":"CoNLL 2020"},"id":"WS-1.108","presentation_id":"38939485","rocketchat_channel":"paper-conll-108","speakers":"Dusan Grujicic|Gorjan Radevski|Tinne Tuytelaars|Matthew Blaschko","title":"Learning to ground medical text in a 3D human atlas"},{"content":{"abstract":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional types. The multilinear maps of words compose with each other to form sentence representations. We extend the skipgram algorithm from vectors to multi- linear maps to learn these representations and instantiate it on unary and binary maps for transitive verbs. These are evaluated on verb and sentence similarity and disambiguation tasks and a subset of the SICK relatedness dataset. Our model performs better than previous type- driven models and is competitive with state of the art representation learning methods such as BERT and neural sentence encoders.","authors":["Gijs Wijnholds","Mehrnoosh Sadrzadeh","Stephen Clark"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Representation Learning for Type-Driven Composition","tldr":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional...","track":"CoNLL 2020"},"id":"WS-1.109","presentation_id":"38939486","rocketchat_channel":"paper-conll-109","speakers":"Gijs Wijnholds|Mehrnoosh Sadrzadeh|Stephen Clark","title":"Representation Learning for Type-Driven Composition"},{"content":{"abstract":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.","authors":["Romain Couillet","Yagmur Gizem Cinar","Eric Gaussier","Muhammad Imran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word Representations Concentrate and This is Good News!","tldr":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p ...","track":"CoNLL 2020"},"id":"WS-1.113","presentation_id":"38939487","rocketchat_channel":"paper-conll-113","speakers":"Romain Couillet|Yagmur Gizem Cinar|Eric Gaussier|Muhammad Imran","title":"Word Representations Concentrate and This is Good News!"},{"content":{"abstract":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, \u201cLazImpa\u201d, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.","authors":["Mathieu Rita","Rahma Chaabouni","Emmanuel Dupoux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently","tldr":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission o...","track":"CoNLL 2020"},"id":"WS-1.115","presentation_id":"38939488","rocketchat_channel":"paper-conll-115","speakers":"Mathieu Rita|Rahma Chaabouni|Emmanuel Dupoux","title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently"},{"content":{"abstract":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.","authors":["Alex Tamkin","Trisha Singh","Davide Giovanardi","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.125","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Transferability in Pretrained Language Models","tldr":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different ...","track":"CoNLL 2020"},"id":"WS-1.1165_F","presentation_id":"38940643","rocketchat_channel":"paper-conll-1165_F","speakers":"Alex Tamkin|Trisha Singh|Davide Giovanardi|Noah Goodman","title":"Investigating Transferability in Pretrained Language Models"},{"content":{"abstract":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic competence, which includes basic linguistic skills encompassing both referential phenomena and generic knowledge, in particular a) the ability to denote, b) the mastery of the lexicon, or c) the ability to model one\u2019s language use on others. Even though each of those faculties has been extensively tested individually, there is still no computational model that would account for their joint acquisition under the conditions experienced by a human. In this paper, I focus on one particular aspect of this problem: the amount of linguistic data available to the child or machine. I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-the-art performance. I argue that both the nature of the data and the way it is presented to the system matter to acquisition.","authors":["Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Re-solve it: simulating the acquisition of core semantic competences from small data","tldr":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic comp...","track":"CoNLL 2020"},"id":"WS-1.127","presentation_id":"38939489","rocketchat_channel":"paper-conll-127","speakers":"Aur\u00e9lie Herbelot","title":"Re-solve it: simulating the acquisition of core semantic competences from small data"},{"content":{"abstract":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training and evaluation of Named Entity Linking (NEL) tools, since different annotation styles correspond to divergent views on the entities present in the same texts. Such divergence is particularly present in texts from the media domain that contain references to creative works. In this work we present a corpus of 1000 annotated documents selected from the media domain. Each document is presented with multiple gold standard annotations representing various annotation styles. This corpus is used to evaluate a series of Named Entity Linking tools in order to understand the impact of the differences in annotation styles on the reported accuracy when processing highly ambiguous entities such as names of creative works. Relaxed annotation guidelines that include overlap styles lead to better results across all tools.","authors":["Adrian M.P. Brasoveanu","Albert Weichselbraun","Lyndon Nixon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works","tldr":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training an...","track":"CoNLL 2020"},"id":"WS-1.128","presentation_id":"38939490","rocketchat_channel":"paper-conll-128","speakers":"Adrian M.P. Brasoveanu|Albert Weichselbraun|Lyndon Nixon","title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works"},{"content":{"abstract":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to motivate two new metrics that address the issues with the standard test, and which distinguish between class-wise offset concentration (similar directions between pairs of words drawn from different broad classes, such as France-London, China-Ottawa,...) and pairing consistency (the existence of a regular transformation between correctly-matched pairs such as France:Paris::China:Beijing). We show that, while the standard analogy test is flawed, several popular word embeddings do nevertheless encode linguistic regularities.","authors":["Louis Fournier","Emmanuel Dupoux","Ewan Dunbar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analogies minus analogy test: measuring regularities in word embeddings","tldr":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to mot...","track":"CoNLL 2020"},"id":"WS-1.136","presentation_id":"38939491","rocketchat_channel":"paper-conll-136","speakers":"Louis Fournier|Emmanuel Dupoux|Ewan Dunbar","title":"Analogies minus analogy test: measuring regularities in word embeddings"},{"content":{"abstract":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.","authors":["Maria A. Rodriguez","Paola Merlo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word associations and the distance properties of context-aware word embeddings","tldr":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embeddin...","track":"CoNLL 2020"},"id":"WS-1.137","presentation_id":"38939492","rocketchat_channel":"paper-conll-137","speakers":"Maria A. Rodriguez|Paola Merlo","title":"Word associations and the distance properties of context-aware word embeddings"},{"content":{"abstract":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on \u00c6Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear \u03bb-calculus with an accuracy of as high as 70%.","authors":["Konstantinos Kogkalidis","Michael Moortgat","Richard Moot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Proof Nets","tldr":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation o...","track":"CoNLL 2020"},"id":"WS-1.14","presentation_id":"38939465","rocketchat_channel":"paper-conll-14","speakers":"Konstantinos Kogkalidis|Michael Moortgat|Richard Moot","title":"Neural Proof Nets"},{"content":{"abstract":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims\u2019 topics and their possible negative impacts are the main factors affecting their check-worthiness.","authors":["Yavuz Selim Kartal","Mucahid Kutlu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales","tldr":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, avai...","track":"CoNLL 2020"},"id":"WS-1.142","presentation_id":"38939493","rocketchat_channel":"paper-conll-142","speakers":"Yavuz Selim Kartal|Mucahid Kutlu","title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales"},{"content":{"abstract":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.","authors":["Forrest Davis","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse structure interacts with reference but not syntax in neural language models","tldr":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different ...","track":"CoNLL 2020"},"id":"WS-1.144","presentation_id":"38939494","rocketchat_channel":"paper-conll-144","speakers":"Forrest Davis|Marten van Schijndel","title":"Discourse structure interacts with reference but not syntax in neural language models"},{"content":{"abstract":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.","authors":["Robert Hawkins","Minae Kwon","Dorsa Sadigh","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Adaptation for Efficient Machine Communication","tldr":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly a...","track":"CoNLL 2020"},"id":"WS-1.147","presentation_id":"38939495","rocketchat_channel":"paper-conll-147","speakers":"Robert Hawkins|Minae Kwon|Dorsa Sadigh|Noah Goodman","title":"Continual Adaptation for Efficient Machine Communication"},{"content":{"abstract":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing explicit representations for objects and their relations by extracting scene graphs from the images. Utilizing an embedding of this scene graph enables our model to more explicitly reason over objects and their relations during story generation, compared to the global features from an object classifier used in previous work. We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features and show that our approach outperforms previous systems. Our experiments also indicate that our models obtain competitive results on reference-based metrics.","authors":["Xudong Hong","Rakshith Shetty","Asad Sayeed","Khushboo Mehra","Vera Demberg","Bernt Schiele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings","tldr":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing expl...","track":"CoNLL 2020"},"id":"WS-1.149","presentation_id":"38939496","rocketchat_channel":"paper-conll-149","speakers":"Xudong Hong|Rakshith Shetty|Asad Sayeed|Khushboo Mehra|Vera Demberg|Bernt Schiele","title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings"},{"content":{"abstract":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TaxiNLI, a new dataset, that has 10k examples from the MNLI dataset with these taxonomic labels. Through various experiments on TaxiNLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies\u2014a large jump over the previous models\u2014some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.","authors":["Pratik Joshi","Somak Aditya","Aalok Sathe","Monojit Choudhury"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TaxiNLI: Taking a Ride up the NLU Hill","tldr":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remain...","track":"CoNLL 2020"},"id":"WS-1.15","presentation_id":"38939466","rocketchat_channel":"paper-conll-15","speakers":"Pratik Joshi|Somak Aditya|Aalok Sathe|Monojit Choudhury","title":"TaxiNLI: Taking a Ride up the NLU Hill"},{"content":{"abstract":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover, historical variations can be present in aged documents, which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models, and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets, and does not degrade the results for modern datasets.","authors":["Emanuela Boros","Ahmed Hamdi","Elvys Linhares Pontes","Luis Adri\u00e1n Cabrera-Diego","Jose G. Moreno","Nicolas Sidere","Antoine Doucet"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents","tldr":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this ...","track":"CoNLL 2020"},"id":"WS-1.152","presentation_id":"38939497","rocketchat_channel":"paper-conll-152","speakers":"Emanuela Boros|Ahmed Hamdi|Elvys Linhares Pontes|Luis Adri\u00e1n Cabrera-Diego|Jose G. Moreno|Nicolas Sidere|Antoine Doucet","title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents"},{"content":{"abstract":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.","authors":["Steven Derby","Paul Miller","Barry Devereux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models","tldr":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure...","track":"CoNLL 2020"},"id":"WS-1.155","presentation_id":"38939498","rocketchat_channel":"paper-conll-155","speakers":"Steven Derby|Paul Miller|Barry Devereux","title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models"},{"content":{"abstract":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.","authors":["Satwik Bhattamishra","Arkil Patel","Navin Goyal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling","tldr":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their po...","track":"CoNLL 2020"},"id":"WS-1.156","presentation_id":"38939499","rocketchat_channel":"paper-conll-156","speakers":"Satwik Bhattamishra|Arkil Patel|Navin Goyal","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"content":{"abstract":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a partition that places images into equivalence classes based on player position. To model this task, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these models generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.","authors":["Allen Nie","Reuben Cohn-Gordon","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.173","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pragmatic Issue-Sensitive Image Captioning","tldr":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the playe...","track":"CoNLL 2020"},"id":"WS-1.1597_F","presentation_id":"38940644","rocketchat_channel":"paper-conll-1597_F","speakers":"Allen Nie|Reuben Cohn-Gordon|Christopher Potts","title":"Pragmatic Issue-Sensitive Image Captioning"},{"content":{"abstract":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames the task as an inference problem for a general statistical model consisting of observed data (potentially cognate pairs of words), latent variables (the cognacy status of pairs) and unknown global parameters (which sounds correspond between languages). We then give a specific instance of such a model along with an expectation-maximisation algorithm to infer its parameters. We evaluate our system on a corpus of 8140 cognate sets, finding the performance of our method to be comparable to the state of the art. We additionally carry out qualitative analysis demonstrating advantages it has over existing systems. We also suggest several ways our work could be extended within the general theoretical framework we propose.","authors":["Roddy MacSween","Andrew Caines"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Expectation Maximisation Algorithm for Automated Cognate Detection","tldr":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames ...","track":"CoNLL 2020"},"id":"WS-1.162","presentation_id":"38939500","rocketchat_channel":"paper-conll-162","speakers":"Roddy MacSween|Andrew Caines","title":"An Expectation Maximisation Algorithm for Automated Cognate Detection"},{"content":{"abstract":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.","authors":["Debasmita Bhattacharya","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filler-gaps that neural networks fail to generalize","tldr":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap de...","track":"CoNLL 2020"},"id":"WS-1.168","presentation_id":"38939501","rocketchat_channel":"paper-conll-168","speakers":"Debasmita Bhattacharya|Marten van Schijndel","title":"Filler-gaps that neural networks fail to generalize"},{"content":{"abstract":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.","authors":["Qile Zhu","Haidar Khan","Saleh Soltan","Stephen Rawls","Wael Hamza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding","tldr":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, ...","track":"CoNLL 2020"},"id":"WS-1.177","presentation_id":"38939502","rocketchat_channel":"paper-conll-177","speakers":"Qile Zhu|Haidar Khan|Saleh Soltan|Stephen Rawls|Wael Hamza","title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding"},{"content":{"abstract":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments. We use SuspectGuilt to train and assess predictive models which validate the usefulness of the corpus, and show that these models benefit from genre pretraining and joint supervision from the text-level ratings and span-level annotations. Such models might be used as tools for understanding the societal effects of crime reporting.","authors":["Elisa Kreiss","Zijian Wang","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives","tldr":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime ...","track":"CoNLL 2020"},"id":"WS-1.18","presentation_id":"38939467","rocketchat_channel":"paper-conll-18","speakers":"Elisa Kreiss|Zijian Wang|Christopher Potts","title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives"},{"content":{"abstract":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network (RNN) controller a Nondeterministic Stack RNN. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.","authors":["Brian DuSell","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Context-free Languages with Nondeterministic Stack RNNs","tldr":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this dat...","track":"CoNLL 2020"},"id":"WS-1.183","presentation_id":"38939503","rocketchat_channel":"paper-conll-183","speakers":"Brian DuSell|David Chiang","title":"Learning Context-free Languages with Nondeterministic Stack RNNs"},{"content":{"abstract":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representation of the scaffolding and dynamics that guide a coherent narrative. This paper introduces a new model that integrates explicit narrative structure with neural language models, formalizing narrative modeling as a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in which the latent dynamics of the system (i.e. how the state vector transforms over time) is controlled by top-level discrete switching variables. The switching variables represent narrative structure (e.g., sentiment or discourse states), while the latent state vector encodes information on the current state of the narrative. This probabilistic formulation allows us to control generation, and can be learned in a semi-supervised fashion using both labeled and unlabeled data. Additionally, we derive a Gibbs sampler for our model that can \u201cfill in\u201d arbitrary parts of the narrative, guided by the switching variables. Our filled-in (English language) narratives outperform several baselines on both automatic and human evaluations","authors":["Noah Weber","Leena Shekhar","Heeyoung Kwon","Niranjan Balasubramanian","Nathanael Chambers"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Narrative Text in a Switching Dynamical System","tldr":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representa...","track":"CoNLL 2020"},"id":"WS-1.185","presentation_id":"38939504","rocketchat_channel":"paper-conll-185","speakers":"Noah Weber|Leena Shekhar|Heeyoung Kwon|Niranjan Balasubramanian|Nathanael Chambers","title":"Generating Narrative Text in a Switching Dynamical System"},{"content":{"abstract":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. This task is inspired bycomputational and cognitive studies of eventunderstanding, which suggest that understand-ing processes of events is often directed by rec-ognizing the goals, plans or intentions of theprotagonist(s). We develop a large dataset con-taining over 60k event processes, featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10\u02c63\u223c10\u02c64)label vocabularies. We then propose a hybridlearning framework,P2GT, which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. As our experiments indi-cate,P2GTsupports identifying the intent ofprocesses, as well as the fine semantic type ofthe affected object. It also demonstrates the ca-pability of handling few-shot cases, and stronggeneralizability on out-of-domain processes.","authors":["Muhao Chen","Hongming Zhang","Haoyu Wang","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.43","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Are You Trying to Do? Semantic Typing of Event Processes","tldr":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object ...","track":"CoNLL 2020"},"id":"WS-1.189","presentation_id":"38939505","rocketchat_channel":"paper-conll-189","speakers":"Muhao Chen|Hongming Zhang|Haoyu Wang|Dan Roth","title":"What Are You Trying to Do? Semantic Typing of Event Processes"},{"content":{"abstract":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information from it. The corpus has been constructed with two main tasks in mind. The first one, to extract entities about outbreaks such as disease, host, location among others. The second one, to retrieve relations among entities, for instance, in such geographic location fifteen cases of a given disease were reported. Overall, our goal is to offer resources and tools to perform an automated analysis so as to support early detection of disease outbreaks and therefore diminish their spreading.","authors":["Antonella Dellanzo","Viviana Cotik","Jose Ochoa-Luna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.44","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America","tldr":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information f...","track":"CoNLL 2020"},"id":"WS-1.195","presentation_id":"38939506","rocketchat_channel":"paper-conll-195","speakers":"Antonella Dellanzo|Viviana Cotik|Jose Ochoa-Luna","title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America"},{"content":{"abstract":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.","authors":["Nora Kassner","Benno Krojer","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.45","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?","tldr":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present...","track":"CoNLL 2020"},"id":"WS-1.202","presentation_id":"38939507","rocketchat_channel":"paper-conll-202","speakers":"Nora Kassner|Benno Krojer|Hinrich Sch\u00fctze","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"content":{"abstract":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.","authors":["Mark Anderson","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Frailty of Universal POS Tags for Neural UD Parsers","tldr":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear ...","track":"CoNLL 2020"},"id":"WS-1.21","presentation_id":"38939468","rocketchat_channel":"paper-conll-21","speakers":"Mark Anderson|Carlos G\u00f3mez-Rodr\u00edguez","title":"On the Frailty of Universal POS Tags for Neural UD Parsers"},{"content":{"abstract":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent\u2019s language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs.","authors":["Tanmay Parekh","Emily Ahn","Yulia Tsvetkov","Alan W Black"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.46","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues","tldr":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings....","track":"CoNLL 2020"},"id":"WS-1.218","presentation_id":"38939508","rocketchat_channel":"paper-conll-218","speakers":"Tanmay Parekh|Emily Ahn|Yulia Tsvetkov|Alan W Black","title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues"},{"content":{"abstract":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-motor behaviour of typing. Most work to date has focused solely on the timing of keypresses without reference to the linguistic content. In this paper we argue that the identity of the key combinations being produced should impact how they are handled by people with PD, and provide evidence that natural language processing methods can thus be of help in identifying signs of disease. We test the performance of a bi-directional LSTM with convolutional features in distinguishing people with PD from age-matched controls typing in English and Spanish, both in clinics and online.","authors":["Neil Dhir","Mathias Edman","\u00c1lvaro Sanchez Ferro","Tom Stafford","Colin Bannard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.47","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network","tldr":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-m...","track":"CoNLL 2020"},"id":"WS-1.221","presentation_id":"38939509","rocketchat_channel":"paper-conll-221","speakers":"Neil Dhir|Mathias Edman|\u00c1lvaro Sanchez Ferro|Tom Stafford|Colin Bannard","title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network"},{"content":{"abstract":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models\u2019 generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it\u2019s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models.","authors":["Tianyu Liu","Zheng Xin","Xiaoan Ding","Baobao Chang","Zhifang Sui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference","tldr":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is i...","track":"CoNLL 2020"},"id":"WS-1.222","presentation_id":"38939510","rocketchat_channel":"paper-conll-222","speakers":"Tianyu Liu|Zheng Xin|Xiaoan Ding|Baobao Chang|Zhifang Sui","title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference"},{"content":{"abstract":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.","authors":["Tiwalayo Eisape","Noga Zaslavsky","Roger Levy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cloze Distillation Improves Psychometric Predictive Power","tldr":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human n...","track":"CoNLL 2020"},"id":"WS-1.226","presentation_id":"38939511","rocketchat_channel":"paper-conll-226","speakers":"Tiwalayo Eisape|Noga Zaslavsky|Roger Levy","title":"Cloze Distillation Improves Psychometric Predictive Power"},{"content":{"abstract":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. We find that a highly augmented model shows highest accuracy in predicting held-out forms, and investigate other properties of interest learned by our models\u2019 representations. We outline extensions to this architecture that can better capture variation in Indo-Aryan sound change.","authors":["Chundra Cathcart","Taraka Rama"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.50","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping","tldr":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. ...","track":"CoNLL 2020"},"id":"WS-1.234","presentation_id":"38939512","rocketchat_channel":"paper-conll-234","speakers":"Chundra Cathcart|Taraka Rama","title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping"},{"content":{"abstract":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign language as a manual gesture recognition problem. However, signers use other articulators: facial expressions, head and body position and movement to convey linguistic information. Given the important role of non-manual markers, this paper proposes a dataset and presents a use case to stress the importance of including non-manual features to improve the recognition accuracy of signs. To the best of our knowledge no prior publicly available dataset exists that explicitly focuses on non-manual components responsible for the grammar of sign languages. To this end, the proposed dataset contains 28250 videos of signs of high resolution and quality, with annotation of manual and non-manual components. We conducted a series of evaluations in order to investigate whether non-manual components would improve signs\u2019 recognition accuracy. We release the dataset to encourage SLR researchers and help advance current progress in this area toward real-time sign language interpretation. Our dataset will be made publicly available at https://krslproject.github.io/krsl-corpus","authors":["Alfarabi Imashev","Medet Mukushev","Vadim Kimmelman","Anara Sandygulova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.51","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL","tldr":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign...","track":"CoNLL 2020"},"id":"WS-1.247","presentation_id":"38939513","rocketchat_channel":"paper-conll-247","speakers":"Alfarabi Imashev|Medet Mukushev|Vadim Kimmelman|Anara Sandygulova","title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL"},{"content":{"abstract":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor\u2019s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance.","authors":["Tomasz Dwojak","Micha\u0142 Pietruszka","\u0141ukasz Borchmann","Jakub Ch\u0142\u0119dowski","Filip Grali\u0144ski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.52","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Dataset Recycling to Multi-Property Extraction and Beyond","tldr":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introdu...","track":"CoNLL 2020"},"id":"WS-1.258","presentation_id":"38939514","rocketchat_channel":"paper-conll-258","speakers":"Tomasz Dwojak|Micha\u0142 Pietruszka|\u0141ukasz Borchmann|Jakub Ch\u0142\u0119dowski|Filip Grali\u0144ski","title":"From Dataset Recycling to Multi-Property Extraction and Beyond"},{"content":{"abstract":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published neurolinguistic studies of the N400. We find that surprisal can predict N400 amplitude in a wide range of cases, and the cases where it cannot do so provide valuable insight into the neurocognitive processes underlying the response.","authors":["James Michaelov","Benjamin Bergen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How well does surprisal explain N400 amplitude under different experimental conditions?","tldr":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published n...","track":"CoNLL 2020"},"id":"WS-1.259","presentation_id":"38939515","rocketchat_channel":"paper-conll-259","speakers":"James Michaelov|Benjamin Bergen","title":"How well does surprisal explain N400 amplitude under different experimental conditions?"},{"content":{"abstract":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems. Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.","authors":["Leshem Choshen","Dmitry Nikolaev","Yevgeni Berzak","Omri Abend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Classifying Syntactic Errors in Learner Language","tldr":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation sch...","track":"CoNLL 2020"},"id":"WS-1.26","presentation_id":"38939469","rocketchat_channel":"paper-conll-26","speakers":"Leshem Choshen|Dmitry Nikolaev|Yevgeni Berzak|Omri Abend","title":"Classifying Syntactic Errors in Learner Language"},{"content":{"abstract":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowledge. However, designing probing tasks for lesser-resourced languages is tricky, because these often lack largescale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in English. To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study. We show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcomes. We then probe embeddings in a multilingual setup with design choices that lie in a \u2018stable region\u2019, as we identify for English, and find that results on English do not transfer to other languages. Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future.","authors":["Steffen Eger","Johannes Daxenberger","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation","tldr":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowled...","track":"CoNLL 2020"},"id":"WS-1.28","presentation_id":"38939470","rocketchat_channel":"paper-conll-28","speakers":"Steffen Eger|Johannes Daxenberger|Iryna Gurevych","title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation"},{"content":{"abstract":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of these approaches focus strictly on leveraging the co-occurrences of relationship word pairs within sentences. In this paper, we investigate the hypothesis that examples of a lexical relation in a corpus are fundamental to a neural word embedding\u2019s ability to complete analogies involving the relation. Our experiments, in which we remove all known examples of a relation from training corpora, show only marginal degradation in analogy completion performance involving the removed relation. This finding enhances our understanding of neural word embeddings, showing that co-occurrence information of a particular semantic relation is the not the main source of their structural regularity.","authors":["Hsiao-Yu Chiang","Jose Camacho-Collados","Zachary Pardos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding the Source of Semantic Regularities in Word Embeddings","tldr":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of ...","track":"CoNLL 2020"},"id":"WS-1.29","presentation_id":"38939471","rocketchat_channel":"paper-conll-29","speakers":"Hsiao-Yu Chiang|Jose Camacho-Collados|Zachary Pardos","title":"Understanding the Source of Semantic Regularities in Word Embeddings"},{"content":{"abstract":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric element one has propagated in the limited body of computational work on one-anaphora resolution, making this task harder than it is. In the present paper, we resolve this by drawing from an adequate linguistic analysis of the word one in different syntactic environments - once again highlighting the significance of linguistic theory in Natural Language Processing (NLP) tasks. We prepare an annotated corpus marking actual instances of one-anaphora with their textual antecedents, and use the annotations to experiment with state-of-the art neural models for one-anaphora resolution. Apart from presenting a strong neural baseline for this task, we contribute a gold-standard corpus, which is, to the best of our knowledge, the biggest resource on one-anaphora till date.","authors":["Payal Khullar","Arghya Bhattacharya","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Finding The Right One and Resolving it","tldr":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric...","track":"CoNLL 2020"},"id":"WS-1.38","presentation_id":"38939472","rocketchat_channel":"paper-conll-38","speakers":"Payal Khullar|Arghya Bhattacharya|Manish Shrivastava","title":"Finding The Right One and Resolving it"},{"content":{"abstract":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.","authors":["Jonathan Malmaud","Roger Levy","Yevgeni Berzak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension","tldr":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking d...","track":"CoNLL 2020"},"id":"WS-1.49","presentation_id":"38939473","rocketchat_channel":"paper-conll-49","speakers":"Jonathan Malmaud|Roger Levy|Yevgeni Berzak","title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension"},{"content":{"abstract":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","authors":["John P. Lalor","Hong Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation","tldr":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic...","track":"CoNLL 2020"},"id":"WS-1.510_F","presentation_id":"38940641","rocketchat_channel":"paper-conll-510_F","speakers":"John P. Lalor|Hong Yu","title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation"},{"content":{"abstract":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction to syntactically and semantically sound language stimuli. In this study, we asked: how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain\u2019s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain\u2019s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.","authors":["Maryam Hashemzadeh","Greta Kaufeld","Martha White","Andrea E. Martin","Alona Fyshe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.57","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?","tldr":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction t...","track":"CoNLL 2020"},"id":"WS-1.561_F","presentation_id":"38940642","rocketchat_channel":"paper-conll-561_F","speakers":"Maryam Hashemzadeh|Greta Kaufeld|Martha White|Andrea E. Martin|Alona Fyshe","title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?"},{"content":{"abstract":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the dataset and evaluate it with state-of-the-art summarisation methods.","authors":["Yifan Chen","Tamara Polajnar","Colin Batchelor","Simone Teufel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus of Very Short Scientific Summaries","tldr":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first...","track":"CoNLL 2020"},"id":"WS-1.59","presentation_id":"38939474","rocketchat_channel":"paper-conll-59","speakers":"Yifan Chen|Tamara Polajnar|Colin Batchelor|Simone Teufel","title":"A Corpus of Very Short Scientific Summaries"},{"content":{"abstract":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training an LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model\u2019s generated output (its \u2018babbling\u2019), compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.","authors":["Ludovica Pannitto","Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data","tldr":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a chil...","track":"CoNLL 2020"},"id":"WS-1.61","presentation_id":"38939475","rocketchat_channel":"paper-conll-61","speakers":"Ludovica Pannitto|Aur\u00e9lie Herbelot","title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data"},{"content":{"abstract":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively. Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost. We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions. We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.","authors":["Jacqueline van Arkel","Marieke Woensdregt","Mark Dingemanse","Mark Blokpoel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis","tldr":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators...","track":"CoNLL 2020"},"id":"WS-1.63","presentation_id":"38939476","rocketchat_channel":"paper-conll-63","speakers":"Jacqueline van Arkel|Marieke Woensdregt|Mark Dingemanse|Mark Blokpoel","title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis"},{"content":{"abstract":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.","authors":["Cory Shain","Micha Elsner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Acquiring language from speech by learning to remember and predict","tldr":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we pro...","track":"CoNLL 2020"},"id":"WS-1.69","presentation_id":"38939477","rocketchat_channel":"paper-conll-69","speakers":"Cory Shain|Micha Elsner","title":"Acquiring language from speech by learning to remember and predict"},{"content":{"abstract":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where the questioner is not given the context from which answers are drawn, but must reason pragmatically about how to acquire new information, given the shared conversation history. We identify two core challenges: (1) formally defining the informativeness of potential questions, and (2) exploring the prohibitively large space of potential questions to find the good candidates. To generate pragmatic questions, we use reinforcement learning to optimize an informativeness metric we propose, combined with a reward function designed to promote more specific questions. We demonstrate that the resulting pragmatic questioner substantially improves the informativeness and specificity of questions generated over a baseline model, as evaluated by our metrics as well as humans.","authors":["Peng Qi","Yuhao Zhang","Christopher D. Manning"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations","tldr":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where t...","track":"CoNLL 2020"},"id":"WS-1.69_F","presentation_id":"38940640","rocketchat_channel":"paper-conll-69_F","speakers":"Peng Qi|Yuhao Zhang|Christopher D. Manning","title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations"},{"content":{"abstract":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.","authors":["Hongyu Gong","Suma Bhat","Pramod Viswanath"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enriching Word Embeddings with Temporal and Spatial Information","tldr":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may requ...","track":"CoNLL 2020"},"id":"WS-1.7","presentation_id":"38939463","rocketchat_channel":"paper-conll-7","speakers":"Hongyu Gong|Suma Bhat|Pramod Viswanath","title":"Enriching Word Embeddings with Temporal and Spatial Information"},{"content":{"abstract":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus. We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning. We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data. Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-of-the-art models.","authors":["Frederick Reiss","Hong Xu","Bryan Cutler","Karthik Muthuraman","Zachary Eichenberger"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus","tldr":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth ...","track":"CoNLL 2020"},"id":"WS-1.70","presentation_id":"38939478","rocketchat_channel":"paper-conll-70","speakers":"Frederick Reiss|Hong Xu|Bryan Cutler|Karthik Muthuraman|Zachary Eichenberger","title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus"},{"content":{"abstract":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT\u2019s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.","authors":["Gabriella Chronis","Katrin Erk"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships","tldr":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These e...","track":"CoNLL 2020"},"id":"WS-1.73","presentation_id":"38939479","rocketchat_channel":"paper-conll-73","speakers":"Gabriella Chronis|Katrin Erk","title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships"},{"content":{"abstract":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - MQA-RC, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models. However, we show this relationship does not hold true for the XLNet models \u2013 despite the fact that the XLNet performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.","authors":["Ekta Sood","Simon Tannert","Diego Frassinelli","Andreas Bulling","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension","tldr":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new metho...","track":"CoNLL 2020"},"id":"WS-1.8","presentation_id":"38939464","rocketchat_channel":"paper-conll-8","speakers":"Ekta Sood|Simon Tannert|Diego Frassinelli|Andreas Bulling|Ngoc Thang Vu","title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension"},{"content":{"abstract":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty \u2014 entropy-based UID, surprisal-based UID, and pointwise mutual information \u2014 to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.","authors":["Brennan Gonering","Emily Morgan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Processing effort is a poor predictor of cross-linguistic word order frequency","tldr":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Densit...","track":"CoNLL 2020"},"id":"WS-1.83","presentation_id":"38939480","rocketchat_channel":"paper-conll-83","speakers":"Brennan Gonering|Emily Morgan","title":"Processing effort is a poor predictor of cross-linguistic word order frequency"},{"content":{"abstract":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions (about 70%) is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.","authors":["Maja Popovi\u0107"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relations between comprehensibility and adequacy errors in machine translation output","tldr":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all ma...","track":"CoNLL 2020"},"id":"WS-1.88","presentation_id":"38939481","rocketchat_channel":"paper-conll-88","speakers":"Maja Popovi\u0107","title":"Relations between comprehensibility and adequacy errors in machine translation output"},{"content":{"abstract":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier\u2019s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","authors":["Hartger Veeman","Marc Allassonni\u00e8re-Tang","Aleksandrs Berdicevskis","Ali Basirat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment","tldr":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two e...","track":"CoNLL 2020"},"id":"WS-1.96","presentation_id":"38939482","rocketchat_channel":"paper-conll-96","speakers":"Hartger Veeman|Marc Allassonni\u00e8re-Tang|Aleksandrs Berdicevskis|Ali Basirat","title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment"},{"content":{"abstract":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a graph notation. To this end, we designed a novel Plain Graph Notation (PGN) that handles various graphs universally. Then, our parser predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our parser can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the parser tied for 1st place in both cross-framework and cross-lingual tracks.","authors":["Hiroaki Ozaki","Gaku Morio","Yuta Koreeda","Terufumi Morishita","Toshinori Miyoshi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer","tldr":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text...","track":"CoNLL 2020"},"id":"WS-1.Shared1","presentation_id":"38941228","rocketchat_channel":"paper-conll-Shared1","speakers":"Hiroaki Ozaki|Gaku Morio|Yuta Koreeda|Terufumi Morishita|Toshinori Miyoshi","title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer"},{"content":{"abstract":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. Our solution consists of two sub-systems: transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) and iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.","authors":["Longxu Dou","Yunlong Feng","Yuqiu Ji","Wanxiang Che","Ting Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser","tldr":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, A...","track":"CoNLL 2020"},"id":"WS-1.Shared2","presentation_id":"38941229","rocketchat_channel":"paper-conll-Shared2","speakers":"Longxu Dou|Yunlong Feng|Yuqiu Ji|Wanxiang Che|Ting Liu","title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser"},{"content":{"abstract":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.","authors":["Ofir Arviv","Ruixiang Cui","Daniel Hershcovich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers","tldr":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respe...","track":"CoNLL 2020"},"id":"WS-1.Shared3","presentation_id":"38941230","rocketchat_channel":"paper-conll-Shared3","speakers":"Ofir Arviv|Ruixiang Cui|Daniel Hershcovich","title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers"},{"content":{"abstract":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the abstract meaning representation framework and propose a joint state model for the graph-sequence iterative inference of (Cai and Lam, 2020) for a simplified graph-sequence inference. In our joint state model, we update only a single joint state vector during the graph-sequence inference process instead of keeping the dual state vectors, and all other components are exactly the same as in (Cai and Lam, 2020).","authors":["Seung-Hoon Na","Jinwoo Min"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference","tldr":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the...","track":"CoNLL 2020"},"id":"WS-1.Shared4","presentation_id":"38941231","rocketchat_channel":"paper-conll-Shared4","speakers":"Seung-Hoon Na|Jinwoo Min","title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference"},{"content":{"abstract":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.","authors":["David Samuel","Milan Straka"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN","tldr":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the ...","track":"CoNLL 2020"},"id":"WS-1.Shared5","presentation_id":"38941232","rocketchat_channel":"paper-conll-Shared5","speakers":"David Samuel|Milan Straka","title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN"},{"content":{"abstract":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.","authors":["Daniel Zeman","Jan Hajic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FGD at MRP 2020: Prague Tectogrammatical Graphs","tldr":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG ...","track":"CoNLL 2020"},"id":"WS-1.Shared6","presentation_id":"38941233","rocketchat_channel":"paper-conll-Shared6","speakers":"Daniel Zeman|Jan Hajic","title":"FGD at MRP 2020: Prague Tectogrammatical Graphs"},{"content":{"abstract":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.","authors":["Lasha Abzianidze","Johan Bos","Stephan Oepen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs","tldr":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpreta...","track":"CoNLL 2020"},"id":"WS-1.Shared7","presentation_id":"38941234","rocketchat_channel":"paper-conll-Shared7","speakers":"Lasha Abzianidze|Johan Bos|Stephan Oepen","title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs"},{"content":{"abstract":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu","authors":["Stephan Oepen","Omri Abend","Lasha Abzianidze","Johan Bos","Jan Hajic","Daniel Hershcovich","Bin Li","Tim O\u2019Gorman","Nianwen Xue","Daniel Zeman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing","tldr":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the ...","track":"CoNLL 2020"},"id":"WS-1.Shared8","presentation_id":"38941235","rocketchat_channel":"paper-conll-Shared8","speakers":"Stephan Oepen|Omri Abend|Lasha Abzianidze|Johan Bos|Jan Hajic|Daniel Hershcovich|Bin Li|Tim O\u2019Gorman|Nianwen Xue|Daniel Zeman","title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing"}]
