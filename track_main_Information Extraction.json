[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1011.png","content":{"abstract":"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.","authors":["Zequn Sun","Muhao Chen","Wei Hu","Chengming Wang","Jian Dai","Wei Zhang"],"demo_url":"","keywords":["capturing associations","entity alignment","entity inference","nlp applications"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.460","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2877","main.2406","main.1787","main.1460"],"title":"Knowledge Association with Hyperbolic Knowledge Graph Embeddings","tldr":"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are...","track":"Information Extraction"},"forum":"main.1011","id":"main.1011","presentation_id":"38938826"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1116.png","content":{"abstract":"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.","authors":["Haoyu Wang","Muhao Chen","Hongming Zhang","Dan Roth"],"demo_url":"","keywords":["understanding language","temporal extraction","event construction","inducing complexes"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.51","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2972","main.237","main.1421","main.2508","main.1569"],"title":"Joint Constrained Learning for Event-Event Relation Extraction","tldr":"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membersh...","track":"Information Extraction"},"forum":"main.1116","id":"main.1116","presentation_id":"38938847"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1159.png","content":{"abstract":"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.","authors":["Hao Peng","Tianyu Gao","Xu Han","Yankai Lin","Peng Li","Zhiyuan Liu","Maosong Sun","Jie Zhou"],"demo_url":"","keywords":["relation benchmarks","re scenarios","neural models","re models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.298","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2739","main.1669","main.2307","main.2363","main.1528"],"title":"Learning from Context or Names? An Empirical Study on Neural Relation Extraction","tldr":"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these ...","track":"Information Extraction"},"forum":"main.1159","id":"main.1159","presentation_id":"38938855"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1466.png","content":{"abstract":"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to guide a sophisticated walk-based reinforcement learning (RL) model. An alternate approach is to use traditional symbolic methods (e.g., rule induction), which achieve good performance but can be hard to generalize due to the limitation of symbolic representation. In this paper, we propose RuleGuider, which leverages high-quality rules generated by symbolic-based methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets shows that RuleGuider clearly improves the performance of walk-based models without losing interpretability.","authors":["Deren Lei","Gangrong Jiang","Xiaotao Gu","Kexuan Sun","Yuning Mao","Xiang Ren"],"demo_url":"","keywords":["knowledge reasoning","walk-based models","kg","walk-based model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.688","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.684","main.1648","main.607","main.574","main.787"],"title":"Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning","tldr":"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to...","track":"Information Extraction"},"forum":"main.1466","id":"main.1466","presentation_id":"38938912"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1547.png","content":{"abstract":"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.","authors":["Xuming Hu","Lijie Wen","Yusong Xu","Chenwei Zhang","Philip Yu"],"demo_url":"","keywords":["open extraction","extracting facts","adaptive clustering","relation classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.299","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3540","main.1159","main.2640","main.2793","demo.48"],"title":"SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction","tldr":"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or ...","track":"Information Extraction"},"forum":"main.1547","id":"main.1547","presentation_id":"38938930"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1566.png","content":{"abstract":"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.","authors":["Chaojun Xiao","Yuan Yao","Ruobing Xie","Xu Han","Zhiyuan Liu","Maosong Sun","Fen Lin","Leyu Lin"],"demo_url":"","keywords":["sentence-level extraction","document-level extraction","docre","re"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.300","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2367","main.2078","main.2476","main.1339","main.983"],"title":"Denoising Relation Extraction from Document-level Distant Supervision","tldr":"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-lev...","track":"Information Extraction"},"forum":"main.1566","id":"main.1566","presentation_id":"38938935"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1569.png","content":{"abstract":"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.","authors":["Hayley Ross","Jonathon Cai","Bonan Min"],"demo_url":"","keywords":["extracting relations","constructing timelines","time-related answering","temporal parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.689","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1957","main.2972","main.315","main.1561"],"title":"Exploring Contextualized Neural Language Models for Temporal Dependency Parsing","tldr":"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentenc...","track":"Information Extraction"},"forum":"main.1569","id":"main.1569","presentation_id":"38938936"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.158.png","content":{"abstract":"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.","authors":["Shuang Zeng","Runxin Xu","Baobao Chang","Lei Li"],"demo_url":"","keywords":["document-level extraction","sentence-level extraction","graph network","graph"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.127","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.327","main.574","main.2761","main.1706","main.1648"],"title":"Double Graph Based Reasoning for Document-level Relation Extraction","tldr":"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggrega...","track":"Information Extraction"},"forum":"main.158","id":"main.158","presentation_id":"38938659"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1669.png","content":{"abstract":"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence  \"Beethoven composed the Ode to Joy.\", we are expected to extract the triple <Beethoven, composed, Ode to Joy>. In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e.,  by more than 200% in both cases). Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.","authors":["Patrick Hohenecker","Frank Mtumbuka","Vid Kocijan","Thomas Lukasiewicz"],"demo_url":"","keywords":["open extraction","oie","neural architectures","training approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.690","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1159","demo.48","main.471","main.2476","main.2739"],"title":"Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction","tldr":"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence  \"Beethoven composed the Ode to Joy.\", ...","track":"Information Extraction"},"forum":"main.1669","id":"main.1669","presentation_id":"38938963"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1720.png","content":{"abstract":"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by $2.27\\%$--$3.75\\%$ in terms of $F_1$ scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.","authors":["Rongzhi Zhang","Yue Yu","Chao Zhang"],"demo_url":"","keywords":["low-resource tasks","active labeling","mixup","sequence mixup"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.691","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.2790","main.2636","main.1356","main.1225"],"title":"SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup","tldr":"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We pro...","track":"Information Extraction"},"forum":"main.1720","id":"main.1720","presentation_id":"38938974"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1738.png","content":{"abstract":"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches  such  as  BERT.  However,  currentpre-training techniques focus on building lan-guage  modeling  objectives  to  learn  a  gen-eral representation, ignoring the named entity-related  knowledge.   To  this  end,  we  proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge  into  pre-trained  models.   Specifi-cally,  we  first  warm-up  the  model  via  an  en-tity span identification task by training it withWikipedia  anchors,  which  can  be  deemed  asgeneral-typed entities.   Then we leverage thegazetteer-based distant supervision strategy totrain  the  model  extract  coarse-grained  typedentities.   Finally,  we devise a self-supervisedauxiliary task to mine the fine-grained namedentity  knowledge  via  clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks.   Besides,  weshow that our framework gains promising re-sults  without  using  human-labeled  trainingdata,  demonstrating its effectiveness in label-few and low-resource scenarios.","authors":["Xue Mengge","Bowen Yu","Zhenyu Zhang","Tingwen Liu","Yue Zhang","Bin Wang"],"demo_url":"","keywords":["named recognition","bert","en-tity task","pre-trainingapproaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.514","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2947","main.3216","main.1482","main.2799","main.989"],"title":"Coarse-to-Fine Pre-training for Named Entity Recognition","tldr":"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches  such  as  BERT.  However,  currentpre-training techniques focus on building lan-guage  modeling  objectives  to  learn  a  gen-eral representation, ig...","track":"Information Extraction"},"forum":"main.1738","id":"main.1738","presentation_id":"38938977"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1749.png","content":{"abstract":"Conventional approaches to event detection usually require  a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.","authors":["Pengfei Cao","Yubo Chen","Jun Zhao","Taifeng Wang"],"demo_url":"","keywords":["event detection","real-world applications","incremental detection","training problems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.52","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2048","main.3438","main.1116","main.2427","main.3617"],"title":"Incremental Event Detection via Knowledge Consolidation Networks","tldr":"Conventional approaches to event detection usually require  a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it...","track":"Information Extraction"},"forum":"main.1749","id":"main.1749","presentation_id":"38938979"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1787.png","content":{"abstract":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at \\url{https://github.com/thunlp/explore-and-evaluate}.","authors":["Zhiyuan Liu","Yixin Cao","Liangming Pan","Juanzi Li","Zhiyuan Liu","Tat-Seng Chua"],"demo_url":"","keywords":["entity alignment","ea","gnn-based methods","attributed encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.515","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1706","main.2877","main.3216","main.2406","main.2974"],"title":"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","tldr":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. Ho...","track":"Information Extraction"},"forum":"main.1787","id":"main.1787","presentation_id":"38938987"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1817.png","content":{"abstract":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.","authors":["Marcin Kardas","Piotr Czapla","Pontus Stenetorp","Sebastian Ruder","Sebastian Riedel","Ross Taylor","Robert Stojnic"],"demo_url":"","keywords":["machine learning","table subtask","extraction","results extraction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.692","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3646","main.1669","main.1977","demo.72","main.3470"],"title":"AxCell: Automatic Extraction of Results from Machine Learning Papers","tldr":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses severa...","track":"Information Extraction"},"forum":"main.1817","id":"main.1817","presentation_id":"38938992"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1862.png","content":{"abstract":"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation. We then propose a small empirical study to quantify the most common mistake's impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05. We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER. This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics. We finally call for unifying the evaluation setting in end-to-end RE.","authors":["Bruno Taill\u00e9","Vincent Guigue","Geoffrey Scoutheeten","Patrick Gallinari"],"demo_url":"","keywords":["end-to-end articles","language pretraining","meta-analysis","bert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.301","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3517","main.2268","main.1159","main.3648","main.2739"],"title":"Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!","tldr":"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patte...","track":"Information Extraction"},"forum":"main.1862","id":"main.1862","presentation_id":"38938999"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1974.png","content":{"abstract":"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist. And therefore it raises a critical question of whether previous creditable approaches can still work well when facing these challenges. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. To further verify our conclusions, we also construct a new open NER dataset that focuses on entity types with weaker name regularity and lower mention coverage to verify our conclusion. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is critical for the models to generalize to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained encoders.","authors":["Hongyu Lin","Yaojie Lu","Jialong Tang","Xianpei Han","Le Sun","Zhicheng Wei","Nicholas Jing Yuan"],"demo_url":"","keywords":["randomization test","fine-tuning model","ner","creditable approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.592","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1159","main.387","main.1923","main.2943","main.1032"],"title":"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?","tldr":"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER t...","track":"Information Extraction"},"forum":"main.1974","id":"main.1974","presentation_id":"38939021"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1977.png","content":{"abstract":"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.","authors":["Jian Liu","Yubo Chen","Kang Liu","Wei Bi","Xiaojiang Liu"],"demo_url":"","keywords":["event extraction","ee","information task","classification task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.128","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.96","main.1159","main.2608","main.2048","main.2739"],"title":"Event Extraction as Machine Reading Comprehension","tldr":"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, ...","track":"Information Extraction"},"forum":"main.1977","id":"main.1977","presentation_id":"38939023"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2048.png","content":{"abstract":"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.","authors":["Xiaozhi Wang","Ziqi Wang","Xu Han","Wangyi Jiang","Rong Han","Zhiyuan Liu","Juanzi Li","Peng Li","Yankai Lin","Jie Zhou"],"demo_url":"","keywords":["event detection","event","ed","identifying words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.129","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1749","main.1977","main.96","main.2427","main.2972"],"title":"MAVEN: A Massive General Domain Event Detection Dataset","tldr":"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit furth...","track":"Information Extraction"},"forum":"main.2048","id":"main.2048","presentation_id":"38939031"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.237.png","content":{"abstract":"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.","authors":["Manling Li","Qi Zeng","Ying Lin","Kyunghyun Cho","Heng Ji","Jonathan May","Nathanael Chambers","Clare Voss"],"demo_url":"","keywords":["graph induction","downstream extraction","event schemas","event schema"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.50","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1116","main.574","main.2761","main.158","main.666"],"title":"Connecting the Dots: Event Graph Schema Induction with Path Language Modeling","tldr":"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important r...","track":"Information Extraction"},"forum":"main.237","id":"main.237","presentation_id":"38938669"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2426.png","content":{"abstract":"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.","authors":["Ye Liu","Sheng Zhang","Rui Song","Suo Feng","Yanghua Xiao"],"demo_url":"","keywords":["open extraction","question-answering task","information system","kg"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.693","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3462","main.300","main.3646","main.1669","main.1159"],"title":"Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning","tldr":"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information...","track":"Information Extraction"},"forum":"main.2426","id":"main.2426","presentation_id":"38939117"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2427.png","content":{"abstract":"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.","authors":["Lifu Huang","Heng Ji"],"demo_url":"","keywords":["event studies","semi-supervised induction","supervised detection","semi-supervised framework"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.53","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.96","main.1749","main.1116","main.2048","main.2508"],"title":"Semi-supervised New Event Type Induction and Event Detection","tldr":"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatica...","track":"Information Extraction"},"forum":"main.2427","id":"main.2427","presentation_id":"38939118"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2476.png","content":{"abstract":"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.","authors":["Pierre Dognin","Igor Melnyk","Inkit Padhi","Cicero Nogueira dos Santos","Payel Das"],"demo_url":"","keywords":["kb conversion","dual approach","generative models","weak supervision"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.694","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.835","main.471","main.2635","main.1263"],"title":"DualTKB: A Dual Learning Bridge between Text and Knowledge Base","tldr":"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even ...","track":"Information Extraction"},"forum":"main.2476","id":"main.2476","presentation_id":"38939126"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2508.png","content":{"abstract":"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.","authors":["Wenlin Yao","Zeyu Dai","Maitreyi Ramaswamy","Bonan Min","Ruihong Huang"],"demo_url":"","keywords":["discourse analysis","event-centric applications","identification subevents","weakly approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.430","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1421","main.96","main.1977","main.1749"],"title":"Weakly Supervised Subevent Knowledge Acquisition","tldr":"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extr...","track":"Information Extraction"},"forum":"main.2508","id":"main.2508","presentation_id":"38939132"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2608.png","content":{"abstract":"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL's speed and accuracy makes it a viable approach for large-scale real-world scenarios.","authors":["Alan Ramponi","Rob van der Goot","Rosario Lombardo","Barbara Plank"],"demo_url":"","keywords":["biomedical extraction","sequence labeling","large-scale scenarios","beesl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.431","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1977","main.2972","main.96","main.3329","main.3646"],"title":"Biomedical Event Extraction as Sequence Labeling","tldr":"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling...","track":"Information Extraction"},"forum":"main.2608","id":"main.2608","presentation_id":"38939154"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2739.png","content":{"abstract":"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.","authors":["Shachar Rosenman","Alon Jacovi","Yoav Goldberg"],"demo_url":"","keywords":["data process","re collection","sota models","tacred"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.302","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1923","main.2763","main.3470","main.3506"],"title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data","tldr":"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on T...","track":"Information Extraction"},"forum":"main.2739","id":"main.2739","presentation_id":"38939182"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2799.png","content":{"abstract":"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by $6\\%$ to $16\\%$ absolute points over prior meta-learning based systems.","authors":["Yi Yang","Arzoo Katiyar"],"demo_url":"","keywords":["few-shot tasks","nearest learning","structured inference","supervised model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.516","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3217","main.989","TACL.2103","main.1738","main.2974"],"title":"Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning","tldr":"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, ...","track":"Information Extraction"},"forum":"main.2799","id":"main.2799","presentation_id":"38939200"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.287.png","content":{"abstract":"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.","authors":["Kun Qian","Poornima Chozhiyath Raman","Yunyao Li","Lucian Popa"],"demo_url":"","keywords":["entity-related tasks","entity normalization","variant generation","implicit names"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.517","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.1528","main.666","main.327","main.911"],"title":"Learning Structured Representations of Entity Names using ActiveLearning and Weak Supervision","tldr":"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is partic...","track":"Information Extraction"},"forum":"main.287","id":"main.287","presentation_id":"38938677"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2873.png","content":{"abstract":"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human\u2019s ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models. The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.","authors":["Xiaoyu Kou","Yankai Lin","Shaobo Liu","Peng Li","Jie Zhou","Yan Zhang"],"demo_url":"","keywords":["real-world applications","catastrophic problem","graph methods","ge models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.237","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1508","main.658","main.279","TACL.2121"],"title":"Disentangle-based Continual Graph Representation Learning","tldr":"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since...","track":"Information Extraction"},"forum":"main.2873","id":"main.2873","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2877.png","content":{"abstract":"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations. Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.","authors":["Zhichun Wang","Jinjian Yang","Xiaoju Ye"],"demo_url":"","keywords":["knowledge fusion","knowledge integration","kg alignment","knowledge alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.130","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1787","main.2974","main.1706","main.300","main.666"],"title":"Knowledge Graph Alignment with Entity-Pair Embedding","tldr":"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These a...","track":"Information Extraction"},"forum":"main.2877","id":"main.2877","presentation_id":"38939215"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2922.png","content":{"abstract":"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. The data set is publicly available.","authors":["Jiarui Yao","Haoling Qiu","Bonan Min","Nianwen Xue"],"demo_url":"","keywords":["temporal anaphora","temporal annotation","tdgs","representation scheme"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.432","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2512","main.574","main.2739","main.1569","main.3057"],"title":"Annotating Temporal Dependency Graphs via Crowdsourcing","tldr":"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous ...","track":"Information Extraction"},"forum":"main.2922","id":"main.2922","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2938.png","content":{"abstract":"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.","authors":["Dianbo Sui","Yubo Chen","Jun Zhao","Yantao Jia","Yuantao Xie","Weijian Sun"],"demo_url":"","keywords":["privacy protection","knowledge distillation","medical model","privacy-preserving model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.165","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.426","main.748","main.1923","demo.119","demo.71"],"title":"FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction","tldr":"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts a...","track":"Information Extraction"},"forum":"main.2938","id":"main.2938","presentation_id":"38939230"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2947.png","content":{"abstract":"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.","authors":["Chen Jia","Yuefeng Shi","Qinrong Yang","Yue Zhang"],"demo_url":"","keywords":["chinese ner","pre-training","ner fine-tuning","ner"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.518","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.989","demo.49","main.1738","main.3216","main.2635"],"title":"Entity Enhanced BERT Pre-training for Chinese NER","tldr":"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhance...","track":"Information Extraction"},"forum":"main.2947","id":"main.2947","presentation_id":"38939232"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2972.png","content":{"abstract":"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.","authors":["Rujun Han","Yichao Zhou","Nanyun Peng"],"demo_url":"","keywords":["extracting relations","information extraction","natural understanding","maximum inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.461","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1159","main.3462","main.1569","main.850"],"title":"Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction","tldr":"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the ta...","track":"Information Extraction"},"forum":"main.2972","id":"main.2972","presentation_id":"38939236"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2974.png","content":{"abstract":"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.","authors":["Ledell Wu","Fabio Petroni","Martin Josifoski","Sebastian Riedel","Luke Zettlemoyer"],"demo_url":"","keywords":["retrieval","non-zero-shot evaluations","bi-encoder linking","bert-based model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.519","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.300","main.2877","main.1787","main.1755"],"title":"Scalable Zero-shot Entity Linking with Dense Entity Retrieval","tldr":"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is ...","track":"Information Extraction"},"forum":"main.2974","id":"main.2974","presentation_id":"38939238"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3022.png","content":{"abstract":"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).","authors":["Yijun Wang","Changzhi Sun","Yuanbin Wu","Junchi Yan","Peng Gao","Guotong Xie"],"demo_url":"","keywords":["entity task","pre-trained encoder","general-purpose encoder","universal models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.132","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.3287","main.1803","main.1503","main.2877"],"title":"Pre-training Entity Relation Encoder with Intra-span and Inter-span Information","tldr":"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span...","track":"Information Extraction"},"forum":"main.3022","id":"main.3022","presentation_id":"38939251"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3084.png","content":{"abstract":"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.","authors":["Jiapeng Wu","Meng Cao","Jackie Chi Kit Cheung","William L. Hamilton"],"demo_url":"","keywords":["data imputation","tkg tasks","tkgs","time-dependent representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.462","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1465","main.3617","main.2972","main.684","main.300"],"title":"TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion","tldr":"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, thes...","track":"Information Extraction"},"forum":"main.3084","id":"main.3084","presentation_id":"38939266"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3136.png","content":{"abstract":"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product  descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product  information is necessary for the task. Our code and dataset are available at https://github.com/jd-aig/JAVE.","authors":["Tiangang Zhu","Yue Wang","Haoran Li","Youzheng Wu","Xiaodong He","Bowen Zhou"],"demo_url":"","keywords":["e-commerce scenarios","product retrieval","attribute tasks","multimodal method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.166","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2426","main.1787","main.1205","main.3287","main.2273"],"title":"Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product","tldr":"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time...","track":"Information Extraction"},"forum":"main.3136","id":"main.3136","presentation_id":"38939274"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3216.png","content":{"abstract":"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20~million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.","authors":["Jan A. Botha","Zifei Shan","Daniel Gillick"],"demo_url":"","keywords":["multilingual linking","auxiliary task","cross-lingual task","zero- evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.630","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3453","main.2278","main.2974","main.1803","main.1061"],"title":"Entity Linking in 100 Languages","tldr":"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, ne...","track":"Information Extraction"},"forum":"main.3216","id":"main.3216","presentation_id":"38939287"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3231.png","content":{"abstract":"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.","authors":["Hieu Man Duc Trong","Duc Trong Le","Amir Pouran Ben Veyseh","Thuat Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":["detecting events","event detection","ed","manual annotation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.433","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2048","main.1749","main.3101","main.2895","main.3390"],"title":"Introducing a New Dataset for Event Detection in Cybersecurity Texts","tldr":"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In...","track":"Information Extraction"},"forum":"main.3231","id":"main.3231","presentation_id":"38939291"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.327.png","content":{"abstract":"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.","authors":["Difeng Wang","Wei Hu","Ermei Cao","Weijian Sun"],"demo_url":"","keywords":["relation extraction","document-level re","re","entity representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.303","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.158","main.1159","main.911","main.2849","main.3216"],"title":"Global-to-Local Neural Networks for Document-Level Relation Extraction","tldr":"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. I...","track":"Information Extraction"},"forum":"main.327","id":"main.327","presentation_id":"38938684"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3287.png","content":{"abstract":"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders -- a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.","authors":["Jue Wang","Wei Lu"],"demo_url":"","keywords":["named recognition","relation extraction","table-filling problem","representation process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.133","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3022","TACL.2103","main.1787","main.2974","main.3216"],"title":"Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders","tldr":"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they t...","track":"Information Extraction"},"forum":"main.3287","id":"main.3287","presentation_id":"38939302"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3336.png","content":{"abstract":"Personal knowledge about users\u2019 professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.","authors":["Anna Tigunova","Andrew Yates","Paramita Mirza","Gerhard Weikum"],"demo_url":"","keywords":["recommenders","keyword extraction","document retrieval","supervised methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.434","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.1287","main.1390","main.1052","main.2996"],"title":"CHARM: Inferring Personal Attributes from Conversations","tldr":"Personal knowledge about users\u2019 professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source o...","track":"Information Extraction"},"forum":"main.3336","id":"main.3336","presentation_id":"38939312"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3390.png","content":{"abstract":"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.","authors":["Viet Dac Lai","Tuan Ngo Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":["event detection","ed","event prediction","graph networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.435","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.237","main.1488","main.2724","main.1766","TACL.2121"],"title":"Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks","tldr":"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-bas...","track":"Information Extraction"},"forum":"main.3390","id":"main.3390","presentation_id":"38939324"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3453.png","content":{"abstract":"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia\u2019s interlanguage links and thus suffer when the foreign language\u2019s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.","authors":["Xingyu Fu","Weijia Shi","Xiaodong Yu","Zian Zhao","Dan Roth"],"demo_url":"","keywords":["cross-lingual linking","cross-lingual","xel","grounding entities"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.521","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.2630","main.1061","main.2278","main.2363"],"title":"Design Challenges in Low-resource Cross-lingual Entity Linking","tldr":"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, ...","track":"Information Extraction"},"forum":"main.3453","id":"main.3453","presentation_id":"38939339"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3497.png","content":{"abstract":"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence  without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution -- Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.","authors":["Mingming Sun","Wenyue Hua","Zoey Liu","Xin Wang","Kangjie Zheng","Ping Li"],"demo_url":"","keywords":["inference operations","oie algorithms","featured strategies","pipeline"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.167","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["demo.48","main.1179","demo.72","main.3453","main.2342"],"title":"A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression","tldr":"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE...","track":"Information Extraction"},"forum":"main.3497","id":"main.3497","presentation_id":"38939349"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3517.png","content":{"abstract":"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.","authors":["Belinda Z. Li","Sewon Min","Srinivasan Iyer","Yashar Mehdad","Wen-tau Yih"],"demo_url":"","keywords":["mention detection","mention linking","downstream systems","elq"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.522","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2943","main.3507","main.449","main.3216","main.1862"],"title":"Efficient One-Pass End-to-End Entity Linking for Questions","tldr":"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities p...","track":"Information Extraction"},"forum":"main.3517","id":"main.3517","presentation_id":"38939354"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3519.png","content":{"abstract":"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.","authors":["Kai Sun","Richong Zhang","Samuel Mensah","Yongyi Mao","Xudong Liu"],"demo_url":"","keywords":["entity task","relation task","prediction","learning interactions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.304","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1116","main.327","main.861","main.3287","main.605"],"title":"Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations","tldr":"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learni...","track":"Information Extraction"},"forum":"main.3519","id":"main.3519","presentation_id":"38939355"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3617.png","content":{"abstract":"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks.  We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.","authors":["Prachi Jain","Sushant Rathi","Mausam","Soumen Chakrabarti"],"demo_url":"","keywords":["predicting entities","link prediction","time prediction","prediction tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.305","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3084","main.1465","main.3573","main.607","main.1116"],"title":"Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols","tldr":"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time pre...","track":"Information Extraction"},"forum":"main.3617","id":"main.3617","presentation_id":"38939376"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3646.png","content":{"abstract":"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost.   On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that  establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time.  Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination  analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous  analyzers. Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.","authors":["Keshav Kolluru","Vaibhav Adlakha","Samarth Aggarwal","Mausam","Soumen Chakrabarti"],"demo_url":"","keywords":["extractions","-d task","coordination analysis","neural system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.306","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1669","demo.48","main.1159","main.2426","demo.72"],"title":"OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction","tldr":"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost.   On the other hand,sequence labeling appr...","track":"Information Extraction"},"forum":"main.3646","id":"main.3646","presentation_id":"38939380"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.387.png","content":{"abstract":"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.","authors":["Xiangji Zeng","Yunliang Li","Yuchen Zhai","Yin Zhang"],"demo_url":"","keywords":["named recognition","neural models","counterfactual generator","structural model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.590","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2506","main.1923","main.2739","main.911"],"title":"Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition","tldr":"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we d...","track":"Information Extraction"},"forum":"main.387","id":"main.387","presentation_id":"38938699"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.485.png","content":{"abstract":"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles. Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue. In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task. We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve). Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.","authors":["Sopan Khosla","Shikhar Vashishth","Jill Fain Lehman","Carolyn Rose"],"demo_url":"","keywords":["information extraction","communication information","machines","information task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.626","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1622","main.1179","main.1654","main.128","main.689"],"title":"MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge","tldr":"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may diff...","track":"Information Extraction"},"forum":"main.485","id":"main.485","presentation_id":"38938719"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.605.png","content":{"abstract":"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned. In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking. In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions. Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes. We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.","authors":["Jizhi Tang","Yansong Feng","Dongyan Zhao"],"demo_url":"","keywords":["procedural comprehension","state tracking","interactive network","interactive"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.591","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1528","main.1159","main.2377","main.1116","main.911"],"title":"Understanding Procedural Text using Interactive Entity Networks","tldr":"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent effort...","track":"Information Extraction"},"forum":"main.605","id":"main.605","presentation_id":"38938736"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.684.png","content":{"abstract":"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.","authors":["Xin Lv","Xu Han","Lei Hou","Juanzi Li","Zhiyuan Liu","Wei Zhang","Yichi Zhang","Hao Kong","Suhui Wu"],"demo_url":"","keywords":["knowledge completion","reasoning","path search","multi-hop reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.459","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1648","main.3084","main.923","main.300","main.1466"],"title":"Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph","tldr":"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot w...","track":"Information Extraction"},"forum":"main.684","id":"main.684","presentation_id":"38938756"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.861.png","content":{"abstract":"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task  learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.","authors":["Miguel Ballesteros","Rishita Anubhai","Shuai Wang","Nima Pourdamghani","Yogarshi Vyas","Jie Ma","Parminder Bhatia","Kathleen McKeown","Yaser Al-Onaizan"],"demo_url":"","keywords":["predicting relations","transfer","neural architecture","training methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.436","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2579","main.1669","main.883","main.1116","main.1159"],"title":"Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events","tldr":"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Befo...","track":"Information Extraction"},"forum":"main.861","id":"main.861","presentation_id":"38938788"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.883.png","content":{"abstract":"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity's representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3% relative loss in F1 on OntoNotes 5.0.","authors":["Patrick Xia","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":"","keywords":["modeling resolution","incremental algorithm","contextualized encoders","neural components"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.695","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1518","main.3647","main.1621","main.2579","main.891"],"title":"Incremental Neural Coreference Resolution in Constant Memory","tldr":"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scor...","track":"Information Extraction"},"forum":"main.883","id":"main.883","presentation_id":"38938797"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.96.png","content":{"abstract":"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).","authors":["Xinya Du","Claire Cardie"],"demo_url":"","keywords":["event extraction","entity recognition","error propagation","question task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.49","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1977","main.1116","main.2427","main.1421","main.2972"],"title":"Event Extraction by Answering (Almost) Natural Questions","tldr":"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the...","track":"Information Extraction"},"forum":"main.96","id":"main.96","presentation_id":"38938649"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.983.png","content":{"abstract":"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.","authors":["Zhuang Chen","Tieyun Qian"],"demo_url":"","keywords":["aspect extraction","ate","neural taggers","taggers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.164","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3375","main.1159","main.3581","main.1952","main.989"],"title":"Enhancing Aspect Term Extraction with Soft Prototypes","tldr":"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, s...","track":"Information Extraction"},"forum":"main.983","id":"main.983","presentation_id":"38938818"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2103.png","content":{"abstract":"When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outside-to-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving the F1-scores of 85.82%, 84.34%, and 77.36% on ACE-2004, ACE-2005, and GENIA datasets, respectively","authors":["Takashi Shibuya","Eduard Hovy"],"demo_url":"","keywords":["inference","flat tasks","neural model","decoding method"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1755","main.989","main.3216","main.2799","main.1528"],"title":"Nested Named Entity Recognition via Second-best Sequence Learning and Decoding","tldr":"When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an ob...","track":"Information Extraction"},"forum":"TACL.2103","id":"TACL.2103","presentation_id":"38939407"}]
