[{"content":{"abstract":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classification has not been fully explored. We present the first systematic study on how data augmentation techniques impact performance across toxic language classifiers, ranging from shallow logistic regression architectures to BERT \u2013 a state-of-the-art pretrained Transformer network. We compare the performance of eight techniques on very scarce seed datasets. We show that while BERT performed the best, shallow classifiers performed comparably when trained on data augmented with a combination of three techniques, including GPT-2-generated sentences. We discuss the interplay of performance and computational overhead, which can inform the choice of techniques under different constraints.","authors":["Mika Juuti","Tommi Gr\u00f6ndahl","Adrian Flanagan","N. Asokan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.269","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A little goes a long way: Improving toxic language classification despite data scarcity","tldr":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classifi...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2217","presentation_id":"38940137","rocketchat_channel":"paper-woah4-2217","speakers":"Mika Juuti|Tommi Gr\u00f6ndahl|Adrian Flanagan|N. Asokan","title":"A little goes a long way: Improving toxic language classification despite data scarcity"},{"content":{"abstract":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators \u2013 not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists\u2019 experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.","authors":["Ishaan Arora","Julia Guo","Sarah Ita Levitan","Susan McGregor","Julia Hirschberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter","tldr":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annot...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.10","presentation_id":"38939517","rocketchat_channel":"paper-woah4-10","speakers":"Ishaan Arora|Julia Guo|Sarah Ita Levitan|Susan McGregor|Julia Hirschberg","title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter"},{"content":{"abstract":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. However, its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. Here we use a unique situation in Germany where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our pipeline achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97\u2014accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.","authors":["Joshua Garland","Keyan Ghazi-Zahedi","Jean-Gabriel Young","Laurent H\u00e9bert-Dufresne","Mirta Galesic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Countering hate on social media: Large scale classification of hate and counter speech","tldr":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engag...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.13","presentation_id":"38939518","rocketchat_channel":"paper-woah4-13","speakers":"Joshua Garland|Keyan Ghazi-Zahedi|Jean-Gabriel Young|Laurent H\u00e9bert-Dufresne|Mirta Galesic","title":"Countering hate on social media: Large scale classification of hate and counter speech"},{"content":{"abstract":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politically-biased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.","authors":["Maximilian Wich","Jan Bauer","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Impact of politically biased data on hate speech classification","tldr":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their v...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.15","presentation_id":"38939519","rocketchat_channel":"paper-woah4-15","speakers":"Maximilian Wich|Jan Bauer|Georg Groh","title":"Impact of politically biased data on hate speech classification"},{"content":{"abstract":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.","authors":["Michele Banko","Brendon MacKeen","Laurie Ray"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Unified Taxonomy of Harmful Content","tldr":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.16","presentation_id":"38939520","rocketchat_channel":"paper-woah4-16","speakers":"Michele Banko|Brendon MacKeen|Laurie Ray","title":"A Unified Taxonomy of Harmful Content"},{"content":{"abstract":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.","authors":["Kosisochukwu Madukwe","Xiaoying Gao","Bing Xue"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets","tldr":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining ...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.19","presentation_id":"38939521","rocketchat_channel":"paper-woah4-19","speakers":"Kosisochukwu Madukwe|Xiaoying Gao|Bing Xue","title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets"},{"content":{"abstract":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.","authors":["Claire Pershan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach","tldr":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and inter...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2","presentation_id":"38939516","rocketchat_channel":"paper-woah4-2","speakers":"Claire Pershan","title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach"},{"content":{"abstract":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.","authors":["Kadir Bulut Ozler","Kate Kenski","Steve Rains","Yotam Shmargad","Kevin Coe","Steven Bethard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection","tldr":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.24","presentation_id":"38939522","rocketchat_channel":"paper-woah4-24","speakers":"Kadir Bulut Ozler|Kate Kenski|Steve Rains|Yotam Shmargad|Kevin Coe|Steven Bethard","title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection"},{"content":{"abstract":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of \u2018unhealthy online conversation\u2019. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.","authors":["Ilan Price","Jordan Gifford-Moore","Jory Flemming","Saul Musker","Maayan Roichman","Guillaume Sylvain","Nithum Thain","Lucas Dixon","Jeffrey Sorensen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Six Attributes of Unhealthy Conversations","tldr":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.25","presentation_id":"38939523","rocketchat_channel":"paper-woah4-25","speakers":"Ilan Price|Jordan Gifford-Moore|Jory Flemming|Saul Musker|Maayan Roichman|Guillaume Sylvain|Nithum Thain|Lucas Dixon|Jeffrey Sorensen","title":"Six Attributes of Unhealthy Conversations"},{"content":{"abstract":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now. A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or \u0436\u0435\u043d\u0449\u0438\u043d\u0430, \u0447\u0435\u0440\u043d\u044b\u0439, \u0435\u0432\u0440\u0435\u0439) that are not toxic, but serve as triggers for the classifier due to model caveats. In this paper, we describe our efforts towards classifying hate speech in Russian, and propose simple techniques of reducing unintended bias, such as generating training data with language models using terms and words related to protected identities as context and applying word dropout to such words.","authors":["Nadezhda Zueva","Madina Kabirova","Pavel Kalaidin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection","tldr":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both res...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.31","presentation_id":"38939524","rocketchat_channel":"paper-woah4-31","speakers":"Nadezhda Zueva|Madina Kabirova|Pavel Kalaidin","title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection"},{"content":{"abstract":"","authors":["Rosalie Gillett","Nicolas Suzor","Jean Burgess","Bridget Harris","Molly Dragiewicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating takedowns of abuse on Twitter","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.32","presentation_id":"38939525","rocketchat_channel":"paper-woah4-32","speakers":"Rosalie Gillett|Nicolas Suzor|Jean Burgess|Bridget Harris|Molly Dragiewicz","title":"Investigating takedowns of abuse on Twitter"},{"content":{"abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","authors":["Bertie Vidgen","Scott Hale","Ella Guest","Helen Margetts","David Broniatowski","Zeerak Waseem","Austin Botelho","Matthew Hall","Rebekah Tromble"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting East Asian Prejudice on Social Media","tldr":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier t...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.37","presentation_id":"38939526","rocketchat_channel":"paper-woah4-37","speakers":"Bertie Vidgen|Scott Hale|Ella Guest|Helen Margetts|David Broniatowski|Zeerak Waseem|Austin Botelho|Matthew Hall|Rebekah Tromble","title":"Detecting East Asian Prejudice on Social Media"},{"content":{"abstract":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.","authors":["Dante Razo","Sandra K\u00fcbler"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Sampling Bias in Abusive Language Detection","tldr":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.39","presentation_id":"38939527","rocketchat_channel":"paper-woah4-39","speakers":"Dante Razo|Sandra K\u00fcbler","title":"Investigating Sampling Bias in Abusive Language Detection"},{"content":{"abstract":"","authors":["Ian Kivlichan","Olivia Redfield","Rachel Rosen","Raquel Saxe","Nitesh Goyal","Lucy Vasserman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.42","presentation_id":"38939528","rocketchat_channel":"paper-woah4-42","speakers":"Ian Kivlichan|Olivia Redfield|Rachel Rosen|Raquel Saxe|Nitesh Goyal|Lucy Vasserman","title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity"},{"content":{"abstract":"","authors":["Viktorya Vilk","Elodie Vialle","Matt Bailey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.43","presentation_id":"38939529","rocketchat_channel":"paper-woah4-43","speakers":"Viktorya Vilk|Elodie Vialle|Matt Bailey","title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse"},{"content":{"abstract":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.","authors":["Anna Koufakou","Endang Wahyu Pamungkas","Valerio Basile","Viviana Patti"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language","tldr":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features deriv...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.44","presentation_id":"38939530","rocketchat_channel":"paper-woah4-44","speakers":"Anna Koufakou|Endang Wahyu Pamungkas|Valerio Basile|Viviana Patti","title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language"},{"content":{"abstract":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in human-based incivility detection, it is urgent to develop validated and versatile automatic approaches to identifying uncivil posts and comments. This project advances both a neural, BERT-based classifier as well as a logistic regression classifier to identify uncivil comments. The classifier is trained on a dataset of Reddit posts, which are annotated for incivility, and further expanded using a combination of labeled data from Reddit and Twitter. Our best performing model achieves an F1 of 0.802 on our Reddit test set. The final model is not only applicable across social media platforms and their distinct data structures, but also computationally versatile, and - as such - ready to be used on vast volumes of online data. All trained models and annotated data are made available to the research community.","authors":["Sam Davidson","Qiusi Sun","Magdalena Wojcieszak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Developing a New Classifier for Automated Identification of Incivility in Social Media","tldr":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.47","presentation_id":"38939531","rocketchat_channel":"paper-woah4-47","speakers":"Sam Davidson|Qiusi Sun|Magdalena Wojcieszak","title":"Developing a New Classifier for Automated Identification of Incivility in Social Media"},{"content":{"abstract":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that classifiers based on syntactic structure of the text, dependency graphical convolutional networks (DepGCNs) can achieve state-of-the-art performance on abusive language datasets. The overall performance is at par with of strong baselines such as fine-tuned BERT. Further, our GCN-based approach is much more efficient than BERT at inference time making it suitable for real-time detection.","authors":["Kanika Narang","Chris Brew"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Abusive Language Detection using Syntactic Dependency Graphs","tldr":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that cla...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.48","presentation_id":"38939532","rocketchat_channel":"paper-woah4-48","speakers":"Kanika Narang|Chris Brew","title":"Abusive Language Detection using Syntactic Dependency Graphs"},{"content":{"abstract":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comments written by victims of abuse are frequently labelled as hateful, even if they discuss or reclaim slurs. Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. We make two main contributions to this end. First, we provide an annotation guide that outlines 4 main categories of online slur usage, which we further divide into a total of 12 sub-categories. Second, we present a publicly available corpus based on our taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, our taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.","authors":["Jana Kurrek","Haji Mohammad Saleem","Derek Ruths"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage","tldr":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comme...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.49","presentation_id":"38939533","rocketchat_channel":"paper-woah4-49","speakers":"Jana Kurrek|Haji Mohammad Saleem|Derek Ruths","title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage"},{"content":{"abstract":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop computational models to incorporate emotions into textual cues to improve aggression identification. We evaluate our proposed methods on a set of corpora related to the task and show promising results with respect to abusive language detection.","authors":["Niloofar Safi Samghabadi","Afsheen Hatami","Mahsa Shafaei","Sudipta Kar","Thamar Solorio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attending the Emotions to Detect Online Abusive Language","tldr":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.50","presentation_id":"38939534","rocketchat_channel":"paper-woah4-50","speakers":"Niloofar Safi Samghabadi|Afsheen Hatami|Mahsa Shafaei|Sudipta Kar|Thamar Solorio","title":"Attending the Emotions to Detect Online Abusive Language"},{"content":{"abstract":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widely researched problem, with current research having a strong focus on a binary classification of bullying versus non-bullying. This paper proposes a novel approach to enhancing cyberbullying detection through role modeling. We utilise a dataset from ASKfm to perform multi-class classification to detect participant roles (e.g. victim, harasser). Our preliminary results demonstrate promising performance including 0.83 and 0.76 of F1-score for cyberbullying and role classification respectively, outperforming baselines.","authors":["Gathika Rathnayake","Thushari Atapattu","Mahen Herath","Georgia Zhang","Katrina Falkner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing the Identification of Cyberbullying through Participant Roles","tldr":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widel...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.51","presentation_id":"38939535","rocketchat_channel":"paper-woah4-51","speakers":"Gathika Rathnayake|Thushari Atapattu|Mahen Herath|Georgia Zhang|Katrina Falkner","title":"Enhancing the Identification of Cyberbullying through Participant Roles"},{"content":{"abstract":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either \u2018Hateful\u2019, \u2018Normal\u2019 or \u2018Offensive\u2019. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.","authors":["Vebj\u00f8rn Isaksen","Bj\u00f6rn Gamb\u00e4ck"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online","tldr":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.52","presentation_id":"38939536","rocketchat_channel":"paper-woah4-52","speakers":"Vebj\u00f8rn Isaksen|Bj\u00f6rn Gamb\u00e4ck","title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online"},{"content":{"abstract":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics\u2019 keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.","authors":["Isar Nejadgholi","Svetlana Kiritchenko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","tldr":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applie...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.56","presentation_id":"38939537","rocketchat_channel":"paper-woah4-56","speakers":"Isar Nejadgholi|Svetlana Kiritchenko","title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse"},{"content":{"abstract":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.","authors":["Hala Al Kuwatly","Maximilian Wich","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics","tldr":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. O...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.57","presentation_id":"38939538","rocketchat_channel":"paper-woah4-57","speakers":"Hala Al Kuwatly|Maximilian Wich|Georg Groh","title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics"},{"content":{"abstract":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately, machine learning is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in classification performance or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias \u2014 a form of bias that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the annotation behavior from annotators. To do so, we build a graph based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a data set. The proposed method and collected insights can contribute to developing fairer and more reliable hate speech classification models.","authors":["Maximilian Wich","Hala Al Kuwatly","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Annotator Bias with a Graph-Based Approach","tldr":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.58","presentation_id":"38939539","rocketchat_channel":"paper-woah4-58","speakers":"Maximilian Wich|Hala Al Kuwatly|Georg Groh","title":"Investigating Annotator Bias with a Graph-Based Approach"},{"content":{"abstract":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research communities in tackling online abuse. We report on the discussions that took place, and present an analysis of four key issues which emerged: Problems in tackling online abuse, Solutions, Meta concerns and the Ecosystem of content moderation and research. We argue there is a pressing need for NLP research communities to engage with human rights perspectives, and identify four key ways in which NLP research into online abuse could immediately be enhanced to create better and more ethical solutions.","authors":["Vinodkumar Prabhakaran","Zeerak Waseem","Seyi Akiwowo","Bertie Vidgen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020","tldr":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research c...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2020.alw-1.1","presentation_id":"","rocketchat_channel":"paper-woah4-1","speakers":"Vinodkumar Prabhakaran|Zeerak Waseem|Seyi Akiwowo|Bertie Vidgen","title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020"}]
