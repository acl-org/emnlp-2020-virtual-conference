[{"content":{"abstract":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese->English.","authors":["Tanfang Chen","Weiwei Wang","Wenyang Wei","Xing Shi","Xiangang Li","Jieping Ye","Kevin Knight"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.7.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiDi's Machine Translation System for WMT2020","tldr":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1","presentation_id":"38939543","rocketchat_channel":"paper-wmt-1","speakers":"Tanfang Chen|Weiwei Wang|Wenyang Wei|Xing Shi|Xiangang Li|Jieping Ye|Kevin Knight","title":"DiDi's Machine Translation System for WMT2020"},{"content":{"abstract":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7% and 5% relative improvement, over the baseline, in sacreBLEU score on the test set for Pashto and Khmer respectively.","authors":["Muhammad ElNokrashy","Amr Hendy","Mohamed Abdelghaffar","Mohamed Afify","Ahmed Tawfik","Hany Hassan Awadalla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.106.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions","tldr":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.100","presentation_id":"38939612","rocketchat_channel":"paper-wmt-100","speakers":"Muhammad ElNokrashy|Amr Hendy|Mohamed Abdelghaffar|Mohamed Afify|Ahmed Tawfik|Hany Hassan Awadalla","title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions"},{"content":{"abstract":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting. We find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the UNMT system trained on it. Furthermore, we show that document-level data selection should be preferred for training the XLM model when possible. Finally, we show that there is a trade-off between quality and quantity of the data used to train UNMT systems.","authors":["Lukas Edman","Antonio Toral","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.130.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian","tldr":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.101","presentation_id":"38939613","rocketchat_channel":"paper-wmt-101","speakers":"Lukas Edman|Antonio Toral|Gertjan van Noord","title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian"},{"content":{"abstract":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","authors":["Art\u016brs Stafanovi\u010ds","M\u0101rcis Pinnis","Toms Bergmanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.73.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations","tldr":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.102","presentation_id":"38939614","rocketchat_channel":"paper-wmt-102","speakers":"Art\u016brs Stafanovi\u010ds|M\u0101rcis Pinnis|Toms Bergmanis","title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations"},{"content":{"abstract":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model trained with the Fairseq sequence modeling toolkit. Our final system is an ensemble of two transformer models, which ranked first in WMT20 evaluation. One model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are same alphabet languages. The other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.","authors":["Kamalkumar Rathinasamy","Amanpreet Singh","Balaguru Sivasambagupta","Prajna Prasad Neerchal","Vani Sivasankaran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.52.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task","tldr":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.103","presentation_id":"38939615","rocketchat_channel":"paper-wmt-103","speakers":"Kamalkumar Rathinasamy|Amanpreet Singh|Balaguru Sivasambagupta|Prajna Prasad Neerchal|Vani Sivasankaran","title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model, Transformer. We applied various strategies in order to improve our baseline MT systems, e.g. onolin- gual sentence selection for creating synthetic training data, mining monolingual sentences for adapting our MT systems to the task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks.","authors":["Venkatesh Parthasarathy","Akshai Ramesh","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.27.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT System Description for the WMT20 News Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.104","presentation_id":"38939616","rocketchat_channel":"paper-wmt-104","speakers":"Venkatesh Parthasarathy|Akshai Ramesh|Rejwanul Haque|Andy Way","title":"The ADAPT System Description for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical terminologies, and using the state-of-the-art neural MT (NMT) model: Transformer. In order to improve our baseline NMT system, we employ a number of methods, e.g. \u201cpseudo\u201d parallel data selection, monolingual data selection for synthetic corpus creation, mining monolingual sentences for adapting our NMT systems to this task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that systematic addition of the aforementioned techniques to the baseline yields an excellent performance in the English-to-Basque translation task.","authors":["Prashant Nayak","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.91.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical ter...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.105","presentation_id":"38939617","rocketchat_channel":"paper-wmt-105","speakers":"Prashant Nayak|Rejwanul Haque|Andy Way","title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English into French, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from English into German; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions: zero-shot and few-shot domain adaptation.","authors":["Sadaf Abdul Rauf","Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez","Minh Quang Pham","Fran\u00e7ois Yvon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.86.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LIMSI @ WMT 2020","tldr":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English int...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.107","presentation_id":"38939618","rocketchat_channel":"paper-wmt-107","speakers":"Sadaf Abdul Rauf|Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez|Minh Quang Pham|Fran\u00e7ois Yvon","title":"LIMSI @ WMT 2020"},{"content":{"abstract":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions, from Russian to English and from English to Russian. We used official training data, 102 million parallel corpora and 10 million monolingual corpora. Our baseline systems are Transformer models trained with the Sockeye sequence modeling toolkit, supplemented by bi-text data filtering schemes, back-translations, reordering and other related processing methods. The BLEU value of our translation result from Russian to English is 35.7, ranking 5th, while from English to Russian is 39.8, ranking 2th.","authors":["ariel Xv"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.35.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Russian-English Bidirectional Machine Translation System","tldr":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.109","presentation_id":"38939619","rocketchat_channel":"paper-wmt-109","speakers":"ariel Xv","title":"Russian-English Bidirectional Machine Translation System"},{"content":{"abstract":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish -> Portuguese.","authors":["Lovish Madaan","Soumya Sharma","Parag Singla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.46.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task","tldr":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.113","presentation_id":"38939620","rocketchat_channel":"paper-wmt-113","speakers":"Lovish Madaan|Soumya Sharma|Parag Singla","title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task"},{"content":{"abstract":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.","authors":["Keshaw Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.136.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20","tldr":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transforme...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.114","presentation_id":"38939621","rocketchat_channel":"paper-wmt-114","speakers":"Keshaw Singh","title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20"},{"content":{"abstract":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and data augmentation, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our system significantly outperforms all other baselines and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our submission can achieve +5.56 BLEU and -4.57 TER with respect to the official MT baseline.","authors":["Jiayi Wang","Ke Wang","Kai Fan","Yuqi Zhang","Jun Lu","Xin Ge","Yangbin Shi","Yu Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.84.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT","tldr":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Ta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.115","presentation_id":"38939622","rocketchat_channel":"paper-wmt-115","speakers":"Jiayi Wang|Ke Wang|Kai Fan|Yuqi Zhang|Jun Lu|Xin Ge|Yangbin Shi|Yu Zhao","title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT"},{"content":{"abstract":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation.","authors":["Vandan Mujadia","Dipti Sharma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.48.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMT based Similar Language Translation for Hindi - Marathi","tldr":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.116","presentation_id":"38939623","rocketchat_channel":"paper-wmt-116","speakers":"Vandan Mujadia|Dipti Sharma","title":"NMT based Similar Language Translation for Hindi - Marathi"},{"content":{"abstract":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En->Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta->En and En->Ta respectively, and 27.9 and 13.0 for Iu->En and En->Iu respectively.","authors":["Peng-Jen Chen","Ann Lee","Changhan Wang","Naman Goyal","Angela Fan","Mary Williamson","Jiatao Gu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.8.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Facebook AI's WMT20 News Translation Task Submission","tldr":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.117","presentation_id":"38939624","rocketchat_channel":"paper-wmt-117","speakers":"Peng-Jen Chen|Ann Lee|Changhan Wang|Naman Goyal|Angela Fan|Mary Williamson|Jiatao Gu","title":"Facebook AI's WMT20 News Translation Task Submission"},{"content":{"abstract":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.","authors":["Rupjyoti Baruah","Rajesh Kumar Mundotiya","Amit Kumar","Anil kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.126.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLPRL System for Very Low Resource Supervised Machine Translation","tldr":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.118","presentation_id":"38939625","rocketchat_channel":"paper-wmt-118","speakers":"Rupjyoti Baruah|Rajesh Kumar Mundotiya|Amit Kumar|Anil kumar Singh","title":"NLPRL System for Very Low Resource Supervised Machine Translation"},{"content":{"abstract":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT models to learn tag placement via training data augmentation. We study the interactions of tag representation, data augmentation size, tag complexity, and language pair to show the drawbacks and benefits of each method. We construct and release new test sets containing tagged data for three language pairs of varying difficulty.","authors":["Greg Hanneman","Georgiana Dinu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.138.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Should Markup Tags Be Translated?","tldr":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT mode...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.119","presentation_id":"38939626","rocketchat_channel":"paper-wmt-119","speakers":"Greg Hanneman|Georgiana Dinu","title":"How Should Markup Tags Be Translated?"},{"content":{"abstract":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples in order to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 12,432 language pairs that provides competitive translation quality for all language pairs.","authors":["Markus Freitag","Orhan Firat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.66.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Complete Multilingual Neural Machine Translation","tldr":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.12","presentation_id":"38939550","rocketchat_channel":"paper-wmt-12","speakers":"Markus Freitag|Orhan Firat","title":"Complete Multilingual Neural Machine Translation"},{"content":{"abstract":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on news data and fine-tune them on in-domain and pseudo-in-domain web crawled data. Our baseline systems are transformer-big models that are pre-trained on the WMT'19 News Translation task and fine-tuned on pseudo-in-domain web crawled data and in-domain task data. We also experiment with (i) adaptation using speaker and domain tags and (ii) using different types and amounts of preceding context. We observe that contrarily to expectations, exploiting context degrades the results (and on analysis the data is not highly contextual). However using domain tags does improve scores according to the automatic evaluation. Our final primary systems use domain tags and are ensembles of 4 models, with noisy channel reranking of outputs. Our en-de system was ranked second in the shared task while our de-en system outperformed all the other systems.","authors":["Nikita Moghe","Christian Hardmeier","Rachel Bawden"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.58.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task","tldr":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.121","presentation_id":"38939627","rocketchat_channel":"paper-wmt-121","speakers":"Nikita Moghe|Christian Hardmeier|Rachel Bawden","title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task"},{"content":{"abstract":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking results significantly improve the results on the training sets but decrease the test set results. RTMs can achieve to become the 5th among 13 models in ru-en subtask and 5th in the multilingual track of sentence-level Task 1 based on MAE.","authors":["Ergun Bi\u00e7ici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.114.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RTM Ensemble Learning Results at Quality Estimation Task","tldr":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking re...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.122","presentation_id":"38939628","rocketchat_channel":"paper-wmt-122","speakers":"Ergun Bi\u00e7ici","title":"RTM Ensemble Learning Results at Quality Estimation Task"},{"content":{"abstract":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our submission to the WMT20 Chat Translation Task, which consists of two language directions, English \u2013> German and German \u2013> English. The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network. Our system ensembles three models, each with different hyperparameters. Despite being trained on a very small corpus, our model produces surprisingly good results.","authors":["Roweida Mohammed","Mahmoud Al-Ayyoub","Malak Abdullah"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.59.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JUST System for WMT20 Chat Translation Task","tldr":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.123","presentation_id":"38939629","rocketchat_channel":"paper-wmt-123","speakers":"Roweida Mohammed|Mahmoud Al-Ayyoub|Malak Abdullah","title":"JUST System for WMT20 Chat Translation Task"},{"content":{"abstract":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glass-box approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multi-lingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs inTask 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a light-weight alternative to the neural-based models.","authors":["Marina Fomicheva","Shuo Sun","Lisa Yankovskaya","Fr\u00e9d\u00e9ric Blain","Vishrav Chaudhary","Mark Fishel","Francisco Guzm\u00e1n","Lucia Specia"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.116.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task","tldr":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b)...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.124","presentation_id":"38939630","rocketchat_channel":"paper-wmt-124","speakers":"Marina Fomicheva|Shuo Sun|Lisa Yankovskaya|Fr\u00e9d\u00e9ric Blain|Vishrav Chaudhary|Mark Fishel|Francisco Guzm\u00e1n|Lucia Specia","title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (English token count). Our submission focuses on filtering Pashto-English, building on previously successful methods to produce two sets of scores: LASER_LM, a combination of the LASER similarity scores provided in the shared task and perplexity scores from language models, and DCCEF_DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring.","authors":["Felicia Koerner","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.109.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task","tldr":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.125","presentation_id":"38939631","rocketchat_channel":"paper-wmt-125","speakers":"Felicia Koerner|Philipp Koehn","title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task"},{"content":{"abstract":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We participated in ten translation directions for the English/Spanish, English/Portuguese, English/Russian, English/Italian, and English/French language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources.","authors":["Felipe Soares","Delton Vaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.95.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts","tldr":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We particip...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.128","presentation_id":"38939632","rocketchat_channel":"paper-wmt-128","speakers":"Felipe Soares|Delton Vaz","title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts"},{"content":{"abstract":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and build our baseline systems to be morphologically motivated sub-word unit-based Transformer base models that we train using the Marian machine translation toolkit. Additionally, we experiment with different parallel and monolingual data selection schemes, as well as sampled back-translation. Our final models are ensembles of Transformer base and Transformer big models which feature right-to-left re-ranking.","authors":["Rihards Kri\u0161lauks","M\u0101rcis Pinnis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.15.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tilde at WMT 2020: News Task Systems","tldr":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.133","presentation_id":"38939633","rocketchat_channel":"paper-wmt-133","speakers":"Rihards Kri\u0161lauks|M\u0101rcis Pinnis","title":"Tilde at WMT 2020: News Task Systems"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.","authors":["Rebecca Knowles","Samuel Larkin","Darlene Stewart","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.132.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications","tldr":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.135","presentation_id":"38939634","rocketchat_channel":"paper-wmt-135","speakers":"Rebecca Knowles|Samuel Larkin|Darlene Stewart|Patrick Littell","title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications"},{"content":{"abstract":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology.","authors":["Prajit Dhar","Arianna Bisazza","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.9.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020","tldr":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.136","presentation_id":"38939635","rocketchat_channel":"paper-wmt-136","speakers":"Prajit Dhar|Arianna Bisazza|Gertjan van Noord","title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020"},{"content":{"abstract":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.","authors":["J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.139.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT","tldr":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that coll...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.137","presentation_id":"38939636","rocketchat_channel":"paper-wmt-137","speakers":"J\u00f6rg Tiedemann","title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT"},{"content":{"abstract":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.","authors":["Minh Quang Pham","Jitao Xu","Josep Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.63.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Priming Neural Machine Translation","tldr":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.138","presentation_id":"38939637","rocketchat_channel":"paper-wmt-138","speakers":"Minh Quang Pham|Jitao Xu|Josep Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"Priming Neural Machine Translation"},{"content":{"abstract":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for Hindi\u2194Marathi each and 1 NMT systems were developed for Hindi\u2194Marathi using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.","authors":["Atul Kr. Ojha","Priya Rani","Akanksha Bansal","Bharathi Raja Chakravarthi","Ritesh Kumar","John P. McCrae"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.49.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020","tldr":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.139","presentation_id":"38939638","rocketchat_channel":"paper-wmt-139","speakers":"Atul Kr. Ojha|Priya Rani|Akanksha Bansal|Bharathi Raja Chakravarthi|Ritesh Kumar|John P. McCrae","title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020"},{"content":{"abstract":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese\u2013English and Hindi\u2013English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40% smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40% of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.","authors":["Raj Dabre","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.61.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models","tldr":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.14","presentation_id":"38939551","rocketchat_channel":"paper-wmt-14","speakers":"Raj Dabre|Atsushi Fujita","title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.","authors":["Rebecca Knowles","Darlene Stewart","Samuel Larkin","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.13.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for the 2020 Inuktitut-English News Translation Task","tldr":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetune...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.141","presentation_id":"38939639","rocketchat_channel":"paper-wmt-141","speakers":"Rebecca Knowles|Darlene Stewart|Samuel Larkin|Patrick Littell","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"content":{"abstract":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the models and used back-translation for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each model. We also investigate the role of mutual intelligibility in model performance.","authors":["Ife Adebara","El Moatez Billah Nagoudi","Muhammad Abdul Mageed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.42.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers","tldr":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.142","presentation_id":"38939640","rocketchat_channel":"paper-wmt-142","speakers":"Ife Adebara|El Moatez Billah Nagoudi|Muhammad Abdul Mageed","title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers"},{"content":{"abstract":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.","authors":["Ivana Kvapil\u00edkov\u00e1","Tom Kocmi","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.133.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20","tldr":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.143","presentation_id":"38939641","rocketchat_channel":"paper-wmt-143","speakers":"Ivana Kvapil\u00edkov\u00e1|Tom Kocmi|Ond\u0159ej Bojar","title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20"},{"content":{"abstract":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.","authors":["Rachel Bawden","Alexandra Birch","Radina Dobreva","Arturo Oncevay","Antonio Valerio Miceli Barone","Philip Williams"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.5.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task","tldr":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.144","presentation_id":"38939642","rocketchat_channel":"paper-wmt-144","speakers":"Rachel Bawden|Alexandra Birch|Radina Dobreva|Arturo Oncevay|Antonio Valerio Miceli Barone|Philip Williams","title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task"},{"content":{"abstract":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.","authors":["Jo\u00e3o Moura","miguel vera","Daan van Stigt","Fabio Kepler","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.119.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task","tldr":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitte...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.146","presentation_id":"38939643","rocketchat_channel":"paper-wmt-146","speakers":"Jo\u00e3o Moura|miguel vera|Daan van Stigt|Fabio Kepler|Andr\u00e9 F. T. Martins","title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en\u2192ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.","authors":["Karen Hambardzumyan","Hovhannes Tamoyan","Hrant Khachatrian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.88.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs","tldr":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.147","presentation_id":"38939644","rocketchat_channel":"paper-wmt-147","speakers":"Karen Hambardzumyan|Hovhannes Tamoyan|Hrant Khachatrian","title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs"},{"content":{"abstract":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available in our GitHub repository.","authors":["Alexandre Lopes","Rodrigo Nogueira","Roberto Lotufo","Helio Pedrini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.90.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation","tldr":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Port...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.149","presentation_id":"38939645","rocketchat_channel":"paper-wmt-149","speakers":"Alexandre Lopes|Rodrigo Nogueira|Roberto Lotufo|Helio Pedrini","title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation"},{"content":{"abstract":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we implemented a noising module that simulates four types of post-editing errors, and we introduced this module into a Transformer-based multi-source APE model. Our noising module implants errors into texts on the target side of parallel corpora during the training phase to make synthetic MT outputs, increasing the entire number of training samples. We also generated additional training data using the parallel corpora and NMT model that were released for the Quality Estimation task, and we used these data to train our APE model. Experimental results on the WMT20 English-German APE data set show improvements over the baseline in terms of both the TER and BLEU scores: our primary submission achieved an improvement of -3.15 TER and +4.01 BLEU, and our contrastive submission achieved an improvement of -3.34 TER and +4.30 BLEU.","authors":["WonKee Lee","Jaehun Shin","Baikjin Jung","Jihyung Lee","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.83.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Noising Scheme for Data Augmentation in Automatic Post-Editing","tldr":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we imple...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.150","presentation_id":"38939646","rocketchat_channel":"paper-wmt-150","speakers":"WonKee Lee|Jaehun Shin|Baikjin Jung|Jihyung Lee|Jong-Hyeok Lee","title":"Noising Scheme for Data Augmentation in Automatic Post-Editing"},{"content":{"abstract":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard Transformer, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 BLEU on the agent side (en\u2192de), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.","authors":["Calvin Bao","Yow-Ting Shiue","Chujun Song","Jie Li","Marine Carpuat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.56.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation","tldr":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model tr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.153","presentation_id":"38939647","rocketchat_channel":"paper-wmt-153","speakers":"Calvin Bao|Yow-Ting Shiue|Chujun Song|Jie Li|Marine Carpuat","title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation"},{"content":{"abstract":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose strategies from previous years\u2019 efforts with larger datasets and also train models with precomputed word alignments under various settings in an effort to improve translation quality.","authors":["Jeremy Gwinnup","Tim Anderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.20.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The AFRL WMT20 News\u00ad Translation Systems","tldr":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.155","presentation_id":"38939648","rocketchat_channel":"paper-wmt-155","speakers":"Jeremy Gwinnup|Tim Anderson","title":"The AFRL WMT20 News\u00ad Translation Systems"},{"content":{"abstract":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre/post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.","authors":["Ankur Kejriwal","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.108.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20","tldr":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.156","presentation_id":"38939649","rocketchat_channel":"paper-wmt-156","speakers":"Ankur Kejriwal|Philipp Koehn","title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20"},{"content":{"abstract":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95% of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our models do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and robustness.","authors":["David Wan","Chris Kedzie","Faisal Ladhak","Marine Carpuat","Kathleen McKeown"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.141.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Terminology Constraints in Automatic Post-Editing","tldr":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.157","presentation_id":"38939650","rocketchat_channel":"paper-wmt-157","speakers":"David Wan|Chris Kedzie|Faisal Ladhak|Marine Carpuat|Kathleen McKeown","title":"Incorporating Terminology Constraints in Automatic Post-Editing"},{"content":{"abstract":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system's performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.","authors":["Qu Cui","Xiang Geng","Shujian Huang","Jiajun CHEN"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.115.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NJU's submission to the WMT20 QE Shared Task","tldr":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.158","presentation_id":"38939651","rocketchat_channel":"paper-wmt-158","speakers":"Qu Cui|Xiang Geng|Shujian Huang|Jiajun CHEN","title":"NJU's submission to the WMT20 QE Shared Task"},{"content":{"abstract":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.","authors":["Brian Thompson","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.67.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity","tldr":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.16","presentation_id":"38939552","rocketchat_channel":"paper-wmt-16","speakers":"Brian Thompson|Matt Post","title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity"},{"content":{"abstract":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1\u2019s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.","authors":["Chi-kiu Lo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.99.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation","tldr":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.160","presentation_id":"38939652","rocketchat_channel":"paper-wmt-160","speakers":"Chi-kiu Lo","title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation"},{"content":{"abstract":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2\u2019s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.","authors":["Chi-kiu Lo","Samuel Larkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.100.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model","tldr":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with con...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.161","presentation_id":"38939653","rocketchat_channel":"paper-wmt-161","speakers":"Chi-kiu Lo|Samuel Larkin","title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model"},{"content":{"abstract":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation.","authors":["Vikrant Goyal","Anoop Kunchukuttan","Rahul Kejriwal","Siddharth Jain","Amit Bhagwat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.19.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20","tldr":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares conta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.162","presentation_id":"38939654","rocketchat_channel":"paper-wmt-162","speakers":"Vikrant Goyal|Anoop Kunchukuttan|Rahul Kejriwal|Siddharth Jain|Amit Bhagwat","title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20"},{"content":{"abstract":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.","authors":["Minh Quang Pham","Josep Maria Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.72.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation","tldr":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.163","presentation_id":"38939655","rocketchat_channel":"paper-wmt-163","speakers":"Minh Quang Pham|Josep Maria Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation"},{"content":{"abstract":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.","authors":["Sumbal Naz","Sadaf Abdul Rauf","Noor-e- Hira","Sami Ul Haq"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.92.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FJWU participation for the WMT20 Biomedical Translation Task","tldr":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.167","presentation_id":"38939656","rocketchat_channel":"paper-wmt-167","speakers":"Sumbal Naz|Sadaf Abdul Rauf|Noor-e- Hira|Sami Ul Haq","title":"FJWU participation for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.","authors":["Zuchao Li","Hai Zhao","Rui Wang","Kehai Chen","Masao Utiyama","Eiichiro Sumita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.22.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.168","presentation_id":"38939657","rocketchat_channel":"paper-wmt-168","speakers":"Zuchao Li|Hai Zhao|Rui Wang|Kehai Chen|Masao Utiyama|Eiichiro Sumita","title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers' LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.","authors":["Chi-kiu Lo","Eric Joanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.110.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models","tldr":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting se...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.169","presentation_id":"38939658","rocketchat_channel":"paper-wmt-169","speakers":"Chi-kiu Lo|Eric Joanis","title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models"},{"content":{"abstract":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.","authors":["Tom Kocmi","Tomasz Limisiewicz","Gabriel Stanovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.39.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Gender Coreference and Bias Evaluation at WMT 2020","tldr":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.170","presentation_id":"38939659","rocketchat_channel":"paper-wmt-170","speakers":"Tom Kocmi|Tomasz Limisiewicz|Gabriel Stanovsky","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"content":{"abstract":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN<->FR,CS,DE,FI system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.","authors":["Annette Rios","Mathias M\u00fcller","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.64.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation","tldr":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.171","presentation_id":"38939660","rocketchat_channel":"paper-wmt-171","speakers":"Annette Rios|Mathias M\u00fcller|Rico Sennrich","title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation"},{"content":{"abstract":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech \u2194 English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.","authors":["Ulrich Germann","Roman Grundkiewicz","Martin Popel","Radina Dobreva","Nikolay Bogoychev","Kenneth Heafield"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.17.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task","tldr":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower te...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.172","presentation_id":"38939661","rocketchat_channel":"paper-wmt-172","speakers":"Ulrich Germann|Roman Grundkiewicz|Martin Popel|Radina Dobreva|Nikolay Bogoychev|Kenneth Heafield","title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task"},{"content":{"abstract":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-English. All our submissions are MarianNMT-based neural systems. We use more data compared to last year and update our back-translations with better models from the previous year. We show competitive results in terms of BLEU in most directions.","authors":["Alexander Molchanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.25.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PROMT Systems for WMT 2020 Shared News Translation Task","tldr":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.175","presentation_id":"38939662","rocketchat_channel":"paper-wmt-175","speakers":"Alexander Molchanov","title":"PROMT Systems for WMT 2020 Shared News Translation Task"},{"content":{"abstract":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.","authors":["Ulrich Germann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.18.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks","tldr":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.177","presentation_id":"38939663","rocketchat_channel":"paper-wmt-177","speakers":"Ulrich Germann","title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks"},{"content":{"abstract":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensemble technique on different transformer architectures (Deep, Hybrid, Big, Large Transformers). To enlarge the in-domain bilingual corpus, we use back-translation of monolingual in-domain data in the target language as additional in-domain training data. Our systems in German->English and English->German are ranked 1st and 3rd respectively according to the official evaluation results in terms of BLEU scores.","authors":["Xing Wang","Zhaopeng Tu","Longyue Wang","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.97.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task","tldr":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.178","presentation_id":"38939664","rocketchat_channel":"paper-wmt-178","speakers":"Xing Wang|Zhaopeng Tu|Longyue Wang|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed.","authors":["Christian Roest","Lukas Edman","Gosse Minnema","Kevin Kelly","Jennifer Spenader","Antonio Toral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.29.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training","tldr":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of corr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.179","presentation_id":"38939665","rocketchat_channel":"paper-wmt-179","speakers":"Christian Roest|Lukas Edman|Gosse Minnema|Kevin Kelly|Jennifer Spenader|Antonio Toral","title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training"},{"content":{"abstract":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.","authors":["Eleftherios Avramidis","Vivien Macketanz","Ursula Strohriegel","Aljoscha Burchardt","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.38.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation","tldr":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.18","presentation_id":"38939553","rocketchat_channel":"paper-wmt-18","speakers":"Eleftherios Avramidis|Vivien Macketanz|Ursula Strohriegel|Aljoscha Burchardt|Sebastian M\u00f6ller","title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation"},{"content":{"abstract":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.","authors":["Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.14.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Submission for the Inuktitut Language in WMT News 2020","tldr":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.180","presentation_id":"38939666","rocketchat_channel":"paper-wmt-180","speakers":"Tom Kocmi","title":"CUNI Submission for the Inuktitut Language in WMT News 2020"},{"content":{"abstract":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike the simulated scenarios used in particular in much of the unsupervised MT work in the past). We were able to obtain most of the digital data available for Upper Sorbian, a minority language of Germany, which was the original motivation for the Unsupervised MT shared task. As we were defining the task, we also obtained a small amount of parallel data (about 60000 parallel sentences), allowing us to offer a Very Low Resource Supervised MT task as well. Six primary systems participated in the unsupervised shared task, two of these systems used additional data beyond the data released by the organizers. Ten primary systems participated in the very low resource supervised task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.","authors":["Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.80.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT","tldr":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.181","presentation_id":"38939667","rocketchat_channel":"paper-wmt-181","speakers":"Alexander Fraser","title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT"},{"content":{"abstract":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.","authors":["Martin Popel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.28.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training","tldr":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating mu...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.183","presentation_id":"38939668","rocketchat_channel":"paper-wmt-183","speakers":"Martin Popel","title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training"},{"content":{"abstract":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.","authors":["Lei Zhou","Liang Ding","Koichi Takeda"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.125.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns","tldr":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Spec...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.185","presentation_id":"38939669","rocketchat_channel":"paper-wmt-185","speakers":"Lei Zhou|Liang Ding|Koichi Takeda","title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns"},{"content":{"abstract":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese \u2192 English task.","authors":["Shuangzhi Wu","Xing Wang","Longyue Wang","Fangxu Liu","Jun Xie","Zhaopeng Tu","Shuming Shi","Mu Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.34.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transf...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.187","presentation_id":"38939670","rocketchat_channel":"paper-wmt-187","speakers":"Shuangzhi Wu|Xing Wang|Longyue Wang|Fangxu Liu|Jun Xie|Zhaopeng Tu|Shuming Shi|Mu Li","title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German-to-English primary system is ranked the second in terms of BLEU scores.","authors":["Longyue Wang","Zhaopeng Tu","Xing Wang","Li Ding","Liang Ding","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.60.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task","tldr":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.188","presentation_id":"38939671","rocketchat_channel":"paper-wmt-188","speakers":"Longyue Wang|Zhaopeng Tu|Xing Wang|Li Ding|Liang Ding|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task"},{"content":{"abstract":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into German and Chinese by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source/domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year\u2019s results are not directly comparable with last year\u2019s round. However, on both language directions, participants\u2019 submissions show considerable improvements over the baseline results. On English-German, the top ranked system improves over the baseline by -11.35 TER and +16.68 BLEU points, while on EnglishChinese the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year\u2019s round.","authors":["Rajen Chatterjee","Markus Freitag","Matteo Negri","Marco Turchi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.75.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing","tldr":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different senten...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.189","presentation_id":"38939672","rocketchat_channel":"paper-wmt-189","speakers":"Rajen Chatterjee|Markus Freitag|Matteo Negri|Marco Turchi","title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing"},{"content":{"abstract":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We release our preprocessed dataset to encourage evaluations that stress-test systems under multiple data conditions.","authors":["Kelly Marchisio","Kevin Duh","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.68.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When Does Unsupervised Machine Translation Work?","tldr":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar doma...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.19","presentation_id":"38939554","rocketchat_channel":"paper-wmt-19","speakers":"Kelly Marchisio|Kevin Duh|Philipp Koehn","title":"When Does Unsupervised Machine Translation Work?"},{"content":{"abstract":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how to improve BLEU by using diverse automatically paraphrased references (Bawden et al., 2020), extending experiments to the multilingual setting for the WMT2020 metrics shared task and for three base metrics. We compare their capacity to exploit up to 100 additional synthetic references. We find that gains are possible when using additional, automatically paraphrased references, although they are not systematic. However, segment-level correlations, particularly into English, are improved for all three metrics and even with higher numbers of paraphrased references.","authors":["Rachel Bawden","Biao Zhang","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.98.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task","tldr":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.190","presentation_id":"38939673","rocketchat_channel":"paper-wmt-190","speakers":"Rachel Bawden|Biao Zhang|Andre T\u00e4ttar|Matt Post","title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task"},{"content":{"abstract":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En->De for agent utterances and De->En for customer messages. We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments (DDA) to evaluate the agent translations.","authors":["M. Amin Farajian","Ant\u00f3nio V. Lopes","Andr\u00e9 F. T. Martins","Sameen Maruf","Gholamreza Haffari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.3.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Chat Translation","tldr":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German c...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.191","presentation_id":"38939674","rocketchat_channel":"paper-wmt-191","speakers":"M. Amin Farajian|Ant\u00f3nio V. Lopes|Andr\u00e9 F. T. Martins|Sameen Maruf|Gholamreza Haffari","title":"Findings of the WMT 2020 Shared Task on Chat Translation"},{"content":{"abstract":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight language pairs. Five language pairs were previously addressed in past editions of the shared task, namely, English/German, English/French, English/Spanish, English/Portuguese, and English/Chinese. Three additional languages pairs were also introduced this year: English/Russian, English/Italian, and English/Basque. The task addressed the evaluation of both scientific abstracts (all language pairs) and terminologies (English/Basque only). We received submissions from a total of 20 teams. For recurring language pairs, we observed an improvement in the translations in terms of automatic scores and qualitative evaluations, compared to previous years.","authors":["Rachel Bawden","Giorgio Maria Di Nunzio","Cristian Grozea","Inigo Jauregi Unanue","Antonio Jimeno Yepes","Nancy Mah","David Martinez","Aur\u00e9lie N\u00e9v\u00e9ol","Mariana Neves","Maite Oronoz","Olatz Perez-de-Vi\u00f1aspre","Massimo Piccardi","Roland Roller","Amy Siu","Philippe Thomas","Federica Vezzani","Maika Vicente Navarro","Dina Wiemann","Lana Yeganova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.76.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages","tldr":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight lan...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.192","presentation_id":"38939675","rocketchat_channel":"paper-wmt-192","speakers":"Rachel Bawden|Giorgio Maria Di Nunzio|Cristian Grozea|Inigo Jauregi Unanue|Antonio Jimeno Yepes|Nancy Mah|David Martinez|Aur\u00e9lie N\u00e9v\u00e9ol|Mariana Neves|Maite Oronoz|Olatz Perez-de-Vi\u00f1aspre|Massimo Piccardi|Roland Roller|Amy Siu|Philippe Thomas|Federica Vezzani|Maika Vicente Navarro|Dina Wiemann|Lana Yeganova","title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages"},{"content":{"abstract":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs \u2013 English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for \u201dcatastrophic errors\u201d. We received 59 submissions by 11 participating teams from a variety of types of institutions.","authors":["Lucia Specia","Zhenhao Li","Juan Pino","Vishrav Chaudhary","Francisco Guzm\u00e1n","Graham Neubig","Nadir Durrani","Yonatan Belinkov","Philipp Koehn","Hassan Sajjad","Paul Michel","Xian Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.4.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness","tldr":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.193","presentation_id":"38939676","rocketchat_channel":"paper-wmt-193","speakers":"Lucia Specia|Zhenhao Li|Juan Pino|Vishrav Chaudhary|Francisco Guzm\u00e1n|Graham Neubig|Nadir Durrani|Yonatan Belinkov|Philipp Koehn|Hassan Sajjad|Paul Michel|Xian Li","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"content":{"abstract":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with open domain texts, direct assessment annotations, and multiple language pairs: English-German, English-Chinese, Russian-English, Romanian-English, Estonian-English, Sinhala-English and Nepali-English data for the sentence-level subtasks, English-German and English-Chinese for the word-level subtask, and English-French data for the document-level subtask. In addition, we made neural machine translation models available to participants. 19 participating teams from 27 institutions submitted altogether 1374 systems to different task variants and language pairs.","authors":["Lucia Specia","Fr\u00e9d\u00e9ric Blain","Marina Fomicheva","Erick Fonseca","Vishrav Chaudhary","Francisco Guzm\u00e1n","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.79.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Quality Estimation","tldr":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with ope...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.194","presentation_id":"38939677","rocketchat_channel":"paper-wmt-194","speakers":"Lucia Specia|Fr\u00e9d\u00e9ric Blain|Marina Fomicheva|Erick Fonseca|Vishrav Chaudhary|Francisco Guzm\u00e1n|Andr\u00e9 F. T. Martins","title":"Findings of the WMT 2020 Shared Task on Quality Estimation"},{"content":{"abstract":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto\u2013English and Khmer\u2013English and also included the challenge of sentence alignment from document pairs.","authors":["Philipp Koehn","Vishrav Chaudhary","Ahmed El-Kishky","Naman Goyal","Peng-Jen Chen","Francisco Guzm\u00e1n"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.78.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment","tldr":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.195","presentation_id":"38939678","rocketchat_channel":"paper-wmt-195","speakers":"Philipp Koehn|Vishrav Chaudhary|Ahmed El-Kishky|Naman Goyal|Peng-Jen Chen|Francisco Guzm\u00e1n","title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment"},{"content":{"abstract":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Marta R. Costa-juss\u00e0","Fethi Bougares","Olivier Galibert"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.2.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the First Shared Task on Lifelong Learning Machine Translation","tldr":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test dat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.196","presentation_id":"38939679","rocketchat_channel":"paper-wmt-196","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Marta R. Costa-juss\u00e0|Fethi Bougares|Olivier Galibert","title":"Findings of the First Shared Task on Lifelong Learning Machine Translation"},{"content":{"abstract":"This is the main findings paper","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Ond\u0159ej Bojar","Marta R. Costa-juss\u00e0","Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Matthias Huck","Eric Joanis","Tom Kocmi","Philipp Koehn","Chi-kiu Lo","Nikola Ljube\u0161i\u0107","Christof Monz","Makoto Morishita","Masaaki Nagata","Toshiaki Nakazawa","Santanu Pal","Matt Post","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.1.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20)","tldr":"This is the main findings paper...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.197","presentation_id":"38939680","rocketchat_channel":"paper-wmt-197","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Ond\u0159ej Bojar|Marta R. Costa-juss\u00e0|Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Matthias Huck|Eric Joanis|Tom Kocmi|Philipp Koehn|Chi-kiu Lo|Nikola Ljube\u0161i\u0107|Christof Monz|Makoto Morishita|Masaaki Nagata|Toshiaki Nakazawa|Santanu Pal|Matt Post|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"content":{"abstract":"","authors":["Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1971","presentation_id":"38940635","rocketchat_channel":"paper-wmt-1971","speakers":"Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Tom Kocmi","title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task"},{"content":{"abstract":"","authors":["Magdalena Biesialska","Marta R. Costa-juss\u00e0","Nikola Ljube\u0161i\u0107","Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1972","presentation_id":"38940636","rocketchat_channel":"paper-wmt-1972","speakers":"Magdalena Biesialska|Marta R. Costa-juss\u00e0|Nikola Ljube\u0161i\u0107|Santanu Pal|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task"},{"content":{"abstract":"","authors":["Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1973","presentation_id":"38940637","rocketchat_channel":"paper-wmt-1973","speakers":"Ond\u0159ej Bojar","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites"},{"content":{"abstract":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 \"zero-shot\" language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.","authors":["Thibault Sellam","Amy Pu","Hyung Won Chung","Sebastian Gehrmann","Qijun Tan","Markus Freitag","Dipanjan Das","Ankur Parikh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.102.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task","tldr":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.198","presentation_id":"38939681","rocketchat_channel":"paper-wmt-198","speakers":"Thibault Sellam|Amy Pu|Hyung Won Chung|Sebastian Gehrmann|Qijun Tan|Markus Freitag|Dipanjan Das|Ankur Parikh","title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task"},{"content":{"abstract":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining mod- ule adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions.","authors":["Runxin Xu","Zhuo Zhi","Jun Cao","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.112.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Volctrans Parallel Corpus Filtering System for WMT 2020","tldr":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.2","presentation_id":"38939544","rocketchat_channel":"paper-wmt-2","speakers":"Runxin Xu|Zhuo Zhi|Jun Cao|Mingxuan Wang|Lei Li","title":"Volctrans Parallel Corpus Filtering System for WMT 2020"},{"content":{"abstract":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.","authors":["Pere Verg\u00e9s Boncompte","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.54.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages","tldr":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.20","presentation_id":"38939555","rocketchat_channel":"paper-wmt-20","speakers":"Pere Verg\u00e9s Boncompte|Marta R. Costa-juss\u00e0","title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages"},{"content":{"abstract":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to-German and English-to-Chinese. Our approach is based on multi-task fine-tuned cross-lingual language models (XLM), initially pre-trained and further domain-adapted through intermediate training using the translation language model (TLM) approach complemented with a novel self-supervised learning task which aim is to model errors inherent to machine translation outputs. Results obtained on both word and sentence-level QE show that the proposed intermediate training method is complementary to language model domain adaptation and outperforms the fine-tuning only approach.","authors":["Raphael Rubino"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.121.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation","tldr":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.21","presentation_id":"38939556","rocketchat_channel":"paper-wmt-21","speakers":"Raphael Rubino","title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation"},{"content":{"abstract":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.","authors":["Sheila Castilho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.137.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation","tldr":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.22","presentation_id":"38939557","rocketchat_channel":"paper-wmt-22","speakers":"Sheila Castilho","title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation"},{"content":{"abstract":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English \u2194 Czech, English \u2194 Russian, French \u2192 German and Tamil \u2192 English), third in 2 directions (English \u2192 German, English \u2192 Japanese), and fourth in 2 directions (English \u2192 Pashto and and English \u2192 Tamil).","authors":["Tingxun Shi","Shiyu Zhao","Xiaopu Li","Xiaoxue Wang","Qian Zhang","Di Ai","Dawei Dang","Xue Zhengshan","JIE HAO"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.30.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OPPO's Machine Translation Systems for WMT20","tldr":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.23","presentation_id":"38939558","rocketchat_channel":"paper-wmt-23","speakers":"Tingxun Shi|Shiyu Zhao|Xiaopu Li|Xiaoxue Wang|Qian Zhang|Di Ai|Dawei Dang|Xue Zhengshan|JIE HAO","title":"OPPO's Machine Translation Systems for WMT20"},{"content":{"abstract":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.","authors":["Aizhan Imankulova","Masahiro Kaneko","Tosho Hirasawa","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.70.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Multimodal Simultaneous Neural Machine Translation","tldr":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence transla...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.26","presentation_id":"38939559","rocketchat_channel":"paper-wmt-26","speakers":"Aizhan Imankulova|Masahiro Kaneko|Tosho Hirasawa|Mamoru Komachi","title":"Towards Multimodal Simultaneous Neural Machine Translation"},{"content":{"abstract":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of techniques such as back-translation and fine-tuning, which are already widely adopted in translation tasks. We attempted to develop new methods for both synthetic data filtering and reranking. However, the methods turned out to be ineffective, and they provided us with no significant improvement over the baseline. We analyze these negative results to provide insights for future studies.","authors":["Shun Kiyono","Takumi Ito","Ryuto Konno","Makoto Morishita","Jun Suzuki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.12.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task","tldr":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.3","presentation_id":"38939545","rocketchat_channel":"paper-wmt-3","speakers":"Shun Kiyono|Takumi Ito|Ryuto Konno|Makoto Morishita|Jun Suzuki","title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task"},{"content":{"abstract":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.","authors":["Mat\u012bss Rikters","Ryokan Ri","Tong Li","Toshiaki Nakazawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.74.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document-aligned Japanese-English Conversation Parallel Corpus","tldr":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.31","presentation_id":"38939560","rocketchat_channel":"paper-wmt-31","speakers":"Mat\u012bss Rikters|Ryokan Ri|Tong Li|Toshiaki Nakazawa","title":"Document-aligned Japanese-English Conversation Parallel Corpus"},{"content":{"abstract":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of -3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of -5.29 and a BLEU score of +7.32 for the En-Zh subtask.","authors":["Jihyung Lee","WonKee Lee","Jaehun Shin","Baikjin Jung","Young-Kil Kim","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.82.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model","tldr":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, wh...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.32","presentation_id":"38939561","rocketchat_channel":"paper-wmt-32","speakers":"Jihyung Lee|WonKee Lee|Jaehun Shin|Baikjin Jung|Young-Kil Kim|Jong-Hyeok Lee","title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model"},{"content":{"abstract":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two training techniques, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.","authors":["Inigo Jauregi Unanue","Massimo Piccardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.89.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation","tldr":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.35","presentation_id":"38939562","rocketchat_channel":"paper-wmt-35","speakers":"Inigo Jauregi Unanue|Massimo Piccardi","title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation"},{"content":{"abstract":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.","authors":["Vil\u00e9m Zouhar","Tereza Vojt\u011bchov\u00e1","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.41.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WMT20 Document-Level Markable Error Exploration","tldr":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused o...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.36","presentation_id":"38939563","rocketchat_channel":"paper-wmt-36","speakers":"Vil\u00e9m Zouhar|Tereza Vojt\u011bchov\u00e1|Ond\u0159ej Bojar","title":"WMT20 Document-Level Markable Error Exploration"},{"content":{"abstract":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.","authors":["Jind\u0159ich Libovick\u00fd","Viktor Hangya","Helmut Schmid","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.131.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task","tldr":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data wit...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.37","presentation_id":"38939564","rocketchat_channel":"paper-wmt-37","speakers":"Jind\u0159ich Libovick\u00fd|Viktor Hangya|Helmut Schmid|Alexander Fraser","title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task"},{"content":{"abstract":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.","authors":["Jin Xu","Yinuo Guo","Junfeng Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.104.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA","tldr":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that ther...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.39","presentation_id":"38939565","rocketchat_channel":"paper-wmt-39","speakers":"Jin Xu|Yinuo Guo|Junfeng Hu","title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA"},{"content":{"abstract":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuktitut and Inuktitut to English. For each, we trained a single-direction model. However, directions including English, Polish and Czech were derived from a common multilingual base, which was later fine-tuned on each particular direction. For all the translation directions, we used a similar training regime, with iterative training corpora improvement through back-translation and model ensembling. For the En \u2192 Cs direction, we additionally leveraged document-level information by re-ranking the beam output with a separate model.","authors":["Mateusz Krubi\u0144ski","Marcin Chochowski","Bart\u0142omiej Boczek","Miko\u0142aj Koszowski","Adam Dobrowolski","Marcin Szyma\u0144ski","Pawe\u0142 Przybysz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.16.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task","tldr":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuk...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.40","presentation_id":"38939566","rocketchat_channel":"paper-wmt-40","speakers":"Mateusz Krubi\u0144ski|Marcin Chochowski|Bart\u0142omiej Boczek|Miko\u0142aj Koszowski|Adam Dobrowolski|Marcin Szyma\u0144ski|Pawe\u0142 Przybysz","title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task"},{"content":{"abstract":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -> Czech, English -> German, and English -> Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year.","authors":["Yves Scherrer","Alessandro Raganato","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.40.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The MUCOW word sense disambiguation test suite at WMT 2020","tldr":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.42","presentation_id":"38939567","rocketchat_channel":"paper-wmt-42","speakers":"Yves Scherrer|Alessandro Raganato|J\u00f6rg Tiedemann","title":"The MUCOW word sense disambiguation test suite at WMT 2020"},{"content":{"abstract":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling. The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na \u0308\u0131ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.","authors":["Shruti Bhosale","Kyra Yee","Sergey Edunov","Michael Auli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.69.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling","tldr":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy ch...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.43","presentation_id":"38939568","rocketchat_channel":"paper-wmt-43","speakers":"Shruti Bhosale|Kyra Yee|Sergey Edunov|Michael Auli","title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling"},{"content":{"abstract":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Randomised Trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. To train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. In addition we re-score the output of Bicleaner using character-level language models and n-gram saturation.","authors":["Miquel Espl\u00e0-Gomis","V\u00edctor M. S\u00e1nchez-Cartagena","Jaume Zaragoza-Bernabeu","Felipe S\u00e1nchez-Mart\u00ednez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.107.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task","tldr":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Ra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.44","presentation_id":"38939569","rocketchat_channel":"paper-wmt-44","speakers":"Miquel Espl\u00e0-Gomis|V\u00edctor M. S\u00e1nchez-Cartagena|Jaume Zaragoza-Bernabeu|Felipe S\u00e1nchez-Mart\u00ednez","title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task"},{"content":{"abstract":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020 News Translation corpora, and fine-tuned on the APE corpus. Bottleneck Adapter Layers are integrated into the model to prevent over-fitting. We further collect external translations as the augmented MT candidates to improve the performance. The experiment demonstrates that pre-trained NMT models are effective when fine-tuning with the APE corpus of a limited size, and the performance can be further improved with external MT augmentation. Our system achieves competitive results on both directions in the final evaluation.","authors":["Hao Yang","Minghan Wang","Daimeng Wei","Hengchao Shang","Jiaxin Guo","Zongyao Li","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.85.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.45","presentation_id":"38939570","rocketchat_channel":"paper-wmt-45","speakers":"Hao Yang|Minghan Wang|Daimeng Wei|Hengchao Shang|Jiaxin Guo|Zongyao Li|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word/sentence-level sub-tasks for both of En-De/Zh language pairs.","authors":["Minghan Wang","Hao Yang","Hengchao Shang","Daimeng Wei","Jiaxin Guo","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen","Liangyou Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.123.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.46","presentation_id":"38939571","rocketchat_channel":"paper-wmt-46","speakers":"Minghan Wang|Hao Yang|Hengchao Shang|Daimeng Wei|Jiaxin Guo|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen|Liangyou Li","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut->English and Tamil->English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.","authors":["Yuhao Zhang","Ziyang Wang","Runzhe Cao","Binghao Wei","Weiqiao Shan","Shuhan Zhou","Abudurexiti Reheman","Tao Zhou","Xin Zeng","Laohu Wang","Yongyu Mu","Jingnan Zhang","Xiaoqian Liu","Xuanjun Zhou","Yinqiao Li","Bei Li","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.37.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans Machine Translation Systems for WMT20","tldr":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.47","presentation_id":"38939572","rocketchat_channel":"paper-wmt-47","speakers":"Yuhao Zhang|Ziyang Wang|Runzhe Cao|Binghao Wei|Weiqiao Shan|Shuhan Zhou|Abudurexiti Reheman|Tao Zhou|Xin Zeng|Laohu Wang|Yongyu Mu|Jingnan Zhang|Xiaoqian Liu|Xuanjun Zhou|Yinqiao Li|Bei Li|Tong Xiao|Jingbo Zhu","title":"The NiuTrans Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the baseline and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our models such as Back Translation, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.","authors":["Daimeng Wei","Hengchao Shang","Zhanglin Wu","Zhengzhe Yu","Liangyou Li","Jiaxin Guo","Minghan Wang","Hao Yang","Lizhi Lei","Ying Qin","Shiliang Sun"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.31.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task","tldr":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the b...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.48","presentation_id":"38939573","rocketchat_channel":"paper-wmt-48","speakers":"Daimeng Wei|Hengchao Shang|Zhanglin Wu|Zhengzhe Yu|Liangyou Li|Jiaxin Guo|Minghan Wang|Hao Yang|Lizhi Lei|Ying Qin|Shiliang Sun","title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task"},{"content":{"abstract":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared to last year, for some language pairs we dedicated a lot more resources to training, and tried to follow standard best practices to build competitive systems which can achieve good results in the rankings. By using deep and complex architectures we sacrificed direct re-usability of our systems in production environments but evaluation showed that this approach could result in better models that significantly outperform baseline architectures. We submitted two systems to the zero shot robustness task. These submissions are described briefly in this paper as well.","authors":["Csaba Oravecz","Katina Bontcheva","L\u00e1szl\u00f3 Tihanyi","David Kolovratnik","Bhavani Bhaskar","Adrien Lardilleux","Szymon Klocek","Andreas Eisele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.26.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"eTranslation's Submissions to the WMT 2020 News Translation Task","tldr":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.49","presentation_id":"38939574","rocketchat_channel":"paper-wmt-49","speakers":"Csaba Oravecz|Katina Bontcheva|L\u00e1szl\u00f3 Tihanyi|David Kolovratnik|Bhavani Bhaskar|Adrien Lardilleux|Szymon Klocek|Andreas Eisele","title":"eTranslation's Submissions to the WMT 2020 News Translation Task"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.118.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language mod...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.5","presentation_id":"38939546","rocketchat_channel":"paper-wmt-5","speakers":"Dongjun Lee","title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation"},{"content":{"abstract":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model using monolingual data from both the languages by jointly pre-training the encoder and decoder and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).","authors":["Salam Michael Singh","Thoudam Doren Singh","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.135.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020","tldr":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.51","presentation_id":"38939575","rocketchat_channel":"paper-wmt-51","speakers":"Salam Michael Singh|Thoudam Doren Singh|Sivaji Bandyopadhyay","title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020"},{"content":{"abstract":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English<->French, English->German and English->Italian.","authors":["Wei Peng","Jianfeng Liu","Minghan Wang","Liangyou Li","Xupeng Meng","Hao Yang","Qun Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.93.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Huawei's Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine tran...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.52","presentation_id":"38939576","rocketchat_channel":"paper-wmt-52","speakers":"Wei Peng|Jianfeng Liu|Minghan Wang|Liangyou Li|Xupeng Meng|Hao Yang|Qun Liu","title":"Huawei's Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire documents (or bilingual dialogues) at once. We use the same ensemble of such models as our primary submission to all three tasks and achieve competitive results. We also experiment with language model pre-training techniques and evaluate their impact on robustness to noise and out-of-domain translation. For German, Spanish, Italian, and French to English translation in the Biomedical Task, we also submit our recently released multilingual Covid19NMT model.","authors":["Alexandre Berard","Ioan Calapodescu","Vassilina Nikoulina","Jerin Philip"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.57.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020","tldr":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire docume...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.53","presentation_id":"38939577","rocketchat_channel":"paper-wmt-53","speakers":"Alexandre Berard|Ioan Calapodescu|Vassilina Nikoulina|Jerin Philip","title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020"},{"content":{"abstract":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems.","authors":["Maja Popovi\u0107","Alberto Poncelas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.51.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation between similar South-Slavic languages","tldr":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation dir...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.54","presentation_id":"38939578","rocketchat_channel":"paper-wmt-54","speakers":"Maja Popovi\u0107|Alberto Poncelas","title":"Neural Machine Translation between similar South-Slavic languages"},{"content":{"abstract":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.","authors":["Xiangpeng Wei","Ping Guo","Yunpeng Li","Xingsheng Zhang","Luxi Xing","Yue Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.32.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIE's Neural Machine Translation Systems for WMT20","tldr":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.57","presentation_id":"38939579","rocketchat_channel":"paper-wmt-57","speakers":"Xiangpeng Wei|Ping Guo|Yunpeng Li|Xingsheng Zhang|Luxi Xing|Yue Hu","title":"IIE's Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. a) Dual Bilingual GPT-2 model, b) Dual Conditional Cross-Entropy Model and c) IBM word alignment model. The scores of these models are combined by using a positive-unlabeled (PU) learning model and a brute-force search to obtain additional gains. Besides, a few simple but efficient rules are adopted to evaluate the quality and the diversity of the corpus. In the alignment-filtering task, the extraction pipeline of bilingual sentence pairs includes the following steps: bilingual lexicon mining, language identification, sentence segmentation and sentence alignment. The final result shows that, in both filtering and alignment tasks, our system significantly outperforms the LASER-based system.","authors":["Jun Lu","Xin Ge","Yangbin Shi","Yuqi Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.111.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task","tldr":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.58","presentation_id":"38939580","rocketchat_channel":"paper-wmt-58","speakers":"Jun Lu|Xin Ge|Yangbin Shi|Yuqi Zhang","title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task"},{"content":{"abstract":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE<cit.>), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.","authors":["Liwei Wu","Xiao Pan","Zehui Lin","Yaoming ZHU","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.33.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Volctrans Machine Translation System for WMT20","tldr":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.59","presentation_id":"38939581","rocketchat_channel":"paper-wmt-59","speakers":"Liwei Wu|Xiao Pan|Zehui Lin|Yaoming ZHU|Mingxuan Wang|Lei Li","title":"The Volctrans Machine Translation System for WMT20"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our system improves the NMT output by -3.95 and +4.50 in terms of TER and BLEU, respectively.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.81.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Transformers for Neural Automatic Post-Editing","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (M...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.6","presentation_id":"38939547","rocketchat_channel":"paper-wmt-6","speakers":"Dongjun Lee","title":"Cross-Lingual Transformers for Neural Automatic Post-Editing"},{"content":{"abstract":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian\u2192German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German\u2192Upper Sorbian and 35.2 on Upper Sorbian\u2192German.","authors":["Alexandra Chronopoulou","Dario Stojanovski","Viktor Hangya","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.128.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task","tldr":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.60","presentation_id":"38939582","rocketchat_channel":"paper-wmt-60","speakers":"Alexandra Chronopoulou|Dario Stojanovski|Viktor Hangya|Alexander Fraser","title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task"},{"content":{"abstract":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.","authors":["Danielle Saunders","Bill Byrne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.94.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task","tldr":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.62","presentation_id":"38939583","rocketchat_channel":"paper-wmt-62","speakers":"Danielle Saunders|Bill Byrne","title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit systems for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.","authors":["Sourav Dutta","Jesujoba Alabi","Saptarashmi Bandyopadhyay","Dana Ruiter","Josef van Genabith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.129.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian","tldr":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.65","presentation_id":"38939584","rocketchat_channel":"paper-wmt-65","speakers":"Sourav Dutta|Jesujoba Alabi|Saptarashmi Bandyopadhyay|Dana Ruiter|Josef van Genabith","title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian"},{"content":{"abstract":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.","authors":["Jingjing Huo","Christian Herold","Yingbo Gao","Leonard Dahlmann","Shahram Khadivi","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.71.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diving Deep into Context-Aware Neural Machine Translation","tldr":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectur...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.68","presentation_id":"38939585","rocketchat_channel":"paper-wmt-68","speakers":"Jingjing Huo|Christian Herold|Yingbo Gao|Leonard Dahlmann|Shahram Khadivi|Hermann Ney","title":"Diving Deep into Context-Aware Neural Machine Translation"},{"content":{"abstract":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This approach allows the flexible combination of a number of independent component models which are further augmented with back-translation, distillation, fine-tuning with in-domain data, Monte-Carlo Tree Search decoding, and improved uncertainty estimation. In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques. Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set (newstest 2019).","authors":["Lei Yu","Laurent Sartran","Po-Sen Huang","Wojciech Stokowiec","Domenic Donato","Srivatsan Srinivasan","Alek Andreev","Wang Ling","Sona Mokra","Agustin Dal Lago","Yotam Doron","Susannah Young","Phil Blunsom","Chris Dyer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.36.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020","tldr":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This app...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.69","presentation_id":"38939586","rocketchat_channel":"paper-wmt-69","speakers":"Lei Yu|Laurent Sartran|Po-Sen Huang|Wojciech Stokowiec|Domenic Donato|Srivatsan Srinivasan|Alek Andreev|Wang Ling|Sona Mokra|Agustin Dal Lago|Yotam Doron|Susannah Young|Phil Blunsom|Chris Dyer","title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020"},{"content":{"abstract":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems.","authors":["Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.50.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages","tldr":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language f...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.70","presentation_id":"38939587","rocketchat_channel":"paper-wmt-70","speakers":"Santanu Pal|Marcos Zampieri","title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages"},{"content":{"abstract":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of \u00a02x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x\u201311x across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average)","authors":["Biao Zhang","Ivan Titov","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.62.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fast Interleaved Bidirectional Sequence Generation","tldr":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decodin...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.71","presentation_id":"38939588","rocketchat_channel":"paper-wmt-71","speakers":"Biao Zhang|Ivan Titov|Rico Sennrich","title":"Fast Interleaved Bidirectional Sequence Generation"},{"content":{"abstract":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -> German and was ranked second for German -> Upper Sorbian according to BLEU scores. For English\u2013Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.","authors":["Yves Scherrer","Stig-Arne Gr\u00f6nroos","Sami Virpioja"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.134.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks","tldr":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For bot...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.72","presentation_id":"38939589","rocketchat_channel":"paper-wmt-72","speakers":"Yves Scherrer|Stig-Arne Gr\u00f6nroos|Sami Virpioja","title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks"},{"content":{"abstract":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes to train statistical models. Using optimal tokenization scheme among these we created synthetic source side text with back translation. And prune synthetic text with language model scores. This synthetic data was then used along with training data in various settings to build translation models. We also report configuration of the submitted systems and results produced by them.","authors":["Saumitra Yadav","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.55.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020","tldr":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.73","presentation_id":"38939590","rocketchat_channel":"paper-wmt-73","speakers":"Saumitra Yadav|Manish Shrivastava","title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020"},{"content":{"abstract":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.","authors":["Ander Corral","Xabier Saralegi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.87.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation","tldr":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.74","presentation_id":"38939591","rocketchat_channel":"paper-wmt-74","speakers":"Ander Corral|Xabier Saralegi","title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation"},{"content":{"abstract":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.","authors":["Farhan Dhanani","Muhammad Rafi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.43.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attention Transformer Model for Translation of Similar Languages","tldr":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.75","presentation_id":"38939592","rocketchat_channel":"paper-wmt-75","speakers":"Farhan Dhanani|Muhammad Rafi","title":"Attention Transformer Model for Translation of Similar Languages"},{"content":{"abstract":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.","authors":["Markus Freitag","George Foster","David Grangier","Colin Cherry"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.140.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Human-Paraphrased References Improve Neural Machine Translation","tldr":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.76","presentation_id":"38939593","rocketchat_channel":"paper-wmt-76","speakers":"Markus Freitag|George Foster|David Grangier|Colin Cherry","title":"Human-Paraphrased References Improve Neural Machine Translation"},{"content":{"abstract":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive transfer from high resource languages. We use iterative backtranslation to fine-tune the system and benefit from the monolingual data available. In order to measure the effectivity of such methods, we compare our results to a bilingual baseline system.","authors":["Carlos Escolano","Marta R. Costa-juss\u00e0","Jos\u00e9 A. R. Fonollosa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.10.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT","tldr":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.77","presentation_id":"38939594","rocketchat_channel":"paper-wmt-77","speakers":"Carlos Escolano|Marta R. Costa-juss\u00e0|Jos\u00e9 A. R. Fonollosa","title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT"},{"content":{"abstract":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system- and segment-level are analyzed in detail and their shortcomings are pointed out.","authors":["Peter Stanchev","Weiyue Wang","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.103.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Better Evaluation of Metrics for Machine Translation","tldr":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlati...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.8","presentation_id":"38939548","rocketchat_channel":"paper-wmt-8","speakers":"Peter Stanchev|Weiyue Wang|Hermann Ney","title":"Towards a Better Evaluation of Metrics for Machine Translation"},{"content":{"abstract":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for domain Adaptation.","authors":["Luis A. Men\u00e9ndez-Salazar","Grigori Sidorov","Marta R. Costa-Juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.47.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The IPN-CIC team system submission for the WMT 2020 similar language task","tldr":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.80","presentation_id":"38939595","rocketchat_channel":"paper-wmt-80","speakers":"Luis A. Men\u00e9ndez-Salazar|Grigori Sidorov|Marta R. Costa-Juss\u00e0","title":"The IPN-CIC team system submission for the WMT 2020 similar language task"},{"content":{"abstract":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese\u2192English system achieves 36.9 case-sensitive BLEU score, which is thehighest among all submissions.","authors":["Fandong Meng","Jianhao Yan","Yijin Liu","Yuan Gao","Xianfeng Zeng","Qinsong Zeng","Peng Li","Ming Chen","Jie Zhou","Sifan Liu","Hao Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.24.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WeChat Neural Machine Translation Systems for WMT20","tldr":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.81","presentation_id":"38939596","rocketchat_channel":"paper-wmt-81","speakers":"Fandong Meng|Jianhao Yan|Yijin Liu|Yuan Gao|Xianfeng Zeng|Qinsong Zeng|Peng Li|Ming Chen|Jie Zhou|Sifan Liu|Hao Zhou","title":"WeChat Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for \u201cattaching\u201d dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.","authors":["Xing Jie Zhong","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.65.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation","tldr":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.82","presentation_id":"38939597","rocketchat_channel":"paper-wmt-82","speakers":"Xing Jie Zhong|David Chiang","title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation"},{"content":{"abstract":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system\u2019s performance.","authors":["Tucker Berckmann","Berkan Hiziroglu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.127.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Low-Resource Translation as Language Modeling","tldr":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.83","presentation_id":"38939598","rocketchat_channel":"paper-wmt-83","speakers":"Tucker Berckmann|Berkan Hiziroglu","title":"Low-Resource Translation as Language Modeling"},{"content":{"abstract":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compare to the baseline model.","authors":["Jiwan Kim","Soyoon Park","Sangha Kim","Yoonjung Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.11.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task","tldr":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model wi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.84","presentation_id":"38939599","rocketchat_channel":"paper-wmt-84","speakers":"Jiwan Kim|Soyoon Park|Sangha Kim|Yoonjung Choi","title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task"},{"content":{"abstract":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from monolingual data, and combining many NMT systems through n-best list reranking improve translation quality. However, while we observed such improvements on the validation data, we did not observed similar improvements on the test data. Our analysis reveals that the presence of translationese texts in the validation data led us to take decisions in building NMT systems that were not optimal to obtain the best results on the test data.","authors":["Benjamin Marie","Raphael Rubino","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.23.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combination of Neural Machine Translation Systems at WMT20","tldr":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from mono...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.86","presentation_id":"38939600","rocketchat_channel":"paper-wmt-86","speakers":"Benjamin Marie|Raphael Rubino|Atsushi Fujita","title":"Combination of Neural Machine Translation Systems at WMT20"},{"content":{"abstract":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task.","authors":["Chi Hu","Hui Liu","Kai Feng","Chen Xu","Nuo Xu","Zefan Zhou","Shiqin Yan","Yingfeng Luo","Chenglong Wang","Xia Meng","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.117.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task","tldr":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. R...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.87","presentation_id":"38939601","rocketchat_channel":"paper-wmt-87","speakers":"Chi Hu|Hui Liu|Kai Feng|Chen Xu|Nuo Xu|Zefan Zhou|Shiqin Yan|Yingfeng Luo|Chenglong Wang|Xia Meng|Tong Xiao|Jingbo Zhu","title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.","authors":["Akifumi Nakamachi","Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.120.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TMUOU Submission for WMT20 Quality Estimation Shared Task","tldr":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.88","presentation_id":"38939602","rocketchat_channel":"paper-wmt-88","speakers":"Akifumi Nakamachi|Hiroki Shimanaka|Tomoyuki Kajiwara|Mamoru Komachi","title":"TMUOU Submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU scores in the directions of English to Pashto, Pashto to English and Khmer to English (13.1, 23.1 and 25.5 respectively) among all the participants. Our submitted systems are unconstrained and focus on mBART (Multilingual Bidirectional and Auto-Regressive Transformers), back-translation and forward-translation. Also, we apply rules, language model and RoBERTa model to filter monolingual, parallel sentences and synthetic sentences. Besides, we validate the difference of the vocabulary built from monolingual data and parallel data.","authors":["Chao Bei","Hao Zong","Qingmin Liu","Conghu Yuan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.6.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GTCOM Neural Machine Translation Systems for WMT20","tldr":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU sco...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.89","presentation_id":"38939603","rocketchat_channel":"paper-wmt-89","speakers":"Chao Bei|Hao Zong|Qingmin Liu|Conghu Yuan","title":"GTCOM Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems.","authors":["Xabier Soto","Olatz Perez-de-Vi\u00f1aspre","Gorka Labaka","Maite Oronoz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.96.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation","tldr":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora rece...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.9","presentation_id":"38939549","rocketchat_channel":"paper-wmt-9","speakers":"Xabier Soto|Olatz Perez-de-Vi\u00f1aspre|Gorka Labaka|Maite Oronoz","title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation"},{"content":{"abstract":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art.","authors":["Ricardo Rei","Craig Stewart","Ana C Farinha","Alon Lavie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.101.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task","tldr":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.90","presentation_id":"38939604","rocketchat_channel":"paper-wmt-90","speakers":"Ricardo Rei|Craig Stewart|Ana C Farinha|Alon Lavie","title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task"},{"content":{"abstract":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translation directions for this language pair. The trained model is evaluated on the corpus provided by shared task organizers, using BLEU, RIBES, and TER scores. There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively.","authors":["Amit Kumar","Rupjyoti Baruah","Rajesh Kumar Mundotiya","Anil Kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.44.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task","tldr":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.91","presentation_id":"38939605","rocketchat_channel":"paper-wmt-91","speakers":"Amit Kumar|Rupjyoti Baruah|Rajesh Kumar Mundotiya|Anil Kumar Singh","title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task"},{"content":{"abstract":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing neural machine translation system to perform the same task. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years\u2019 state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.","authors":["Haluk A\u00e7ar\u00e7i\u00e7ek","Talha \u00c7olako\u011flu","p\u0131nar ece aktan hatipo\u011flu","Chong Hsuan Huang","Wei Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.105.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning","tldr":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the fil...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.92","presentation_id":"38939606","rocketchat_channel":"paper-wmt-92","speakers":"Haluk A\u00e7ar\u00e7i\u00e7ek|Talha \u00c7olako\u011flu|p\u0131nar ece aktan hatipo\u011flu|Chong Hsuan Huang|Wei Peng","title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning"},{"content":{"abstract":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.","authors":["Tharindu Ranasinghe","Constantin Orasan","Ruslan Mitkov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.122.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TransQuest at WMT2020: Sentence-Level Direct Assessment","tldr":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.93","presentation_id":"38939607","rocketchat_channel":"paper-wmt-93","speakers":"Tharindu Ranasinghe|Constantin Orasan|Ruslan Mitkov","title":"TransQuest at WMT2020: Sentence-Level Direct Assessment"},{"content":{"abstract":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.","authors":["Sami Ul Haq","Sadaf Abdul Rauf","Arsalan Shaukat","Abdullah Saeed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.53.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document Level NMT of Low-Resource Languages with Backtranslation","tldr":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an ext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.94","presentation_id":"38939608","rocketchat_channel":"paper-wmt-94","speakers":"Sami Ul Haq|Sadaf Abdul Rauf|Arsalan Shaukat|Abdullah Saeed","title":"Document Level NMT of Low-Resource Languages with Backtranslation"},{"content":{"abstract":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications: using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a Pearson correlation of 0.664, ranking first (tied) on English-Chinese.","authors":["Haijiang Wu","Zixuan Wang","Qingsong Ma","Xinjie Wen","Ruichen Wang","Xiaoli Wang","Yulin Zhang","Zhipeng Yao","Siyao Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.124.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent submission for WMT20 Quality Estimation Shared Task","tldr":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator m...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.96","presentation_id":"38939609","rocketchat_channel":"paper-wmt-96","speakers":"Haijiang Wu|Zixuan Wang|Qingsong Ma|Xinjie Wen|Ruichen Wang|Xiaoli Wang|Yulin Zhang|Zhipeng Yao|Siyao Peng","title":"Tencent submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).","authors":["Yujin Baek","Zae Myung Kim","Jihyung Moon","Hyunjoong Kim","Eunjeong Park"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.113.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PATQUEST: Papago Translation Quality Estimation","tldr":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former foc...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.97","presentation_id":"38939610","rocketchat_channel":"paper-wmt-97","speakers":"Yujin Baek|Zae Myung Kim|Jihyung Moon|Hyunjoong Kim|Eunjeong Park","title":"PATQUEST: Papago Translation Quality Estimation"},{"content":{"abstract":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We have participated in WMT20 shared task of similar language translation on Hindi-Marathi pair. The main challenge of this task is by utilization of monolingual data and similarity features of similar language pair to overcome the limitation of available parallel data. In this work, we have implemented NMT based model that simultaneously learns bilingual embedding from both the source and target language pairs. Our model has achieved Hindi to Marathi bilingual evaluation understudy (BLEU) score of 11.59, rank-based intuitive bilingual evaluation score (RIBES) score of 57.76 and translation edit rate (TER) score of 79.07 and Marathi to Hindi BLEU score of 15.44, RIBES score of 61.13 and TER score of 75.96.","authors":["Sahinur Rahman Laskar","Abdullah Faiz Ur Rahman Khilji","Partha Pakray","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.45.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hindi-Marathi Cross Lingual Model","tldr":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We hav...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.99","presentation_id":"38939611","rocketchat_channel":"paper-wmt-99","speakers":"Sahinur Rahman Laskar|Abdullah Faiz Ur Rahman Khilji|Partha Pakray|Sivaji Bandyopadhyay","title":"Hindi-Marathi Cross Lingual Model"},{"content":{"abstract":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.","authors":["Fran\u00e7ois Hernandez","Vincent Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.21.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Ubiqus English-Inuktitut System for WMT20","tldr":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. T...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.21","presentation_id":"","rocketchat_channel":"paper-wmt-21","speakers":"Fran\u00e7ois Hernandez|Vincent Nguyen","title":"The Ubiqus English-Inuktitut System for WMT20"},{"content":{"abstract":"This is a placeholder for the metrics task paper as the rsults are not available yet","authors":["Nitika Mathur","Johnny Wei","Qingsong Ma","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.77.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Results of the WMT20 Metrics Shared Task","tldr":"This is a placeholder for the metrics task paper as the rsults are not available yet...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.77","presentation_id":"","rocketchat_channel":"paper-wmt-77","speakers":"Nitika Mathur|Johnny Wei|Qingsong Ma|Ond\u0159ej Bojar","title":"Results of the WMT20 Metrics Shared Task"}]
