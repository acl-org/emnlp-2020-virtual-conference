[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1320.png","content":{"abstract":"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.","authors":["Samson Tan","Shafiq Joty","Lav Varshney","Min-Yen Kan"],"demo_url":"","keywords":["comprehension","fine-tuning models","downstream tasks","nlp systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.455","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3566","main.2847","main.870","main.701","main.2349"],"title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding","tldr":"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.1320","id":"main.1320","presentation_id":"38938886"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1733.png","content":{"abstract":"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as \"black boxes''. We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.","authors":["Peerat Limkonchotiwat","Wannaphong Phatthiyaphaibun","Raheem Sarwar","Ekapol Chuangsuwanich","Sarana Nutanong"],"demo_url":"","keywords":["natural tasks","thai segmentation","transfer learning","filter-and-refine solution"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.315","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","TACL.2255","main.2389","main.2078","main.930"],"title":"Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble","tldr":"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can inter...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.1733","id":"main.1733","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2064.png","content":{"abstract":"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world\u2019s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/","authors":["Aditi Chaudhary","Antonios Anastasopoulos","Adithya Pratapa","David R. Mortensen","Zaid Sheikh","Yulia Tsvetkov","Graham Neubig"],"demo_url":"","keywords":["language preservation","cross-lingual transfer","descriptive language","first-pass specification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.422","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["CL.1","TACL.2013","main.1494","main.2847","main.1258"],"title":"Automatic Extraction of Rules Governing Morphological Agreement","tldr":"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devisin...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2064","id":"main.2064","presentation_id":"38939038"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2208.png","content":{"abstract":"We demonstrate a program that learns to pronounce Chinese text in Mandarin,  without a pronunciation dictionary. From non-parallel streams of Chinese  characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable  accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.","authors":["Christopher Chu","Scot Fang","Kevin Knight"],"demo_url":"","keywords":["unsupervised methods","pronunciations","token-level accuracy","accuracy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.458","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2216","main.2766","main.3391","main.2818","main.1320"],"title":"Learning to Pronounce Chinese Without a Pronunciation Dictionary","tldr":"We demonstrate a program that learns to pronounce Chinese text in Mandarin,  without a pronunciation dictionary. From non-parallel streams of Chinese  characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters a...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2208","id":"main.2208","presentation_id":"38939068"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2424.png","content":{"abstract":"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT\u2019s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT\u2019s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.","authors":["Valentin Hofmann","Janet Pierrehumbert","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["full finetuning","derivation generation","pretrained models","plms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.316","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1280","TACL.2141","main.2675","TACL.2041","main.1892"],"title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model","tldr":"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT\u2019s derivational capabilities in different settings, ranging from using...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2424","id":"main.2424","presentation_id":"38939116"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2675.png","content":{"abstract":"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are  interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely  related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard  monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches out-perform existing ones on all languages by up to 11.4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly low-resource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages.","authors":["Manuel Mager","\u00d6zlem \u00c7etino\u011flu","Katharina Kann"],"demo_url":"","keywords":["morphological generation","canonical segmentation","lstm pointer-generator","sequence-to-sequence model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.423","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3656","main.852","main.143","main.2298","main.522"],"title":"Tackling the Low-resource Challenge for Canonical Segmentation","tldr":"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are  interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2675","id":"main.2675","presentation_id":"38939170"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2847.png","content":{"abstract":"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language's inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.","authors":["Sarah Moeller","Ling Liu","Changbing Yang","Katharina Kann","Mans Hulden"],"demo_url":"","keywords":["linguistic analysis","natural systems","igt-to-paradigms","igtp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.424","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.870","main.143","main.2363","main.1970"],"title":"IGT2P: From Interlinear Glossed Texts to Paradigms","tldr":"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses ab...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2847","id":"main.2847","presentation_id":"38939208"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3046.png","content":{"abstract":"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.","authors":["Ramy Eskander","Smaranda Muresan","Michael Collins"],"demo_url":"","keywords":["part-of-speech tagging","unsupervised approach","cross-lingual projection","bi-lstm architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.391","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2641","main.143","main.852","main.407","main.1061"],"title":"Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios","tldr":"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which PO...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3046","id":"main.3046","presentation_id":"38939256"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3391.png","content":{"abstract":"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As ``alternatives'' to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding. Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs.  Experiments demonstrate that our approach  outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging. The source code is available at https://github.com/abdulrafae/coding_nmt.","authors":["Abdul Rafae Khan","Jia Xu","Weiwei Sun"],"demo_url":"","keywords":["natural tasks","nlp","neural-network-based systems","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.104","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["CL.4","main.648","main.246","main.1613","main.2448"],"title":"Coding Textual Inputs Boosts the Accuracy of Neural Networks","tldr":"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As ``alternatives'' to a text representation, we...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3391","id":"main.3391","presentation_id":"38939325"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3563.png","content":{"abstract":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages' gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","authors":["Arya D. McCarthy","Adina Williams","Shijia Liu","David Yarowsky","Ryan Cotterell"],"demo_url":"","keywords":["cluster evaluation","community detection","grammatical system","gender systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.456","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["CL.2","main.3181","TACL.2013","main.2131","main.2718"],"title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","tldr":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing ...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3563","id":"main.3563","presentation_id":"38939365"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3656.png","content":{"abstract":"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: https://github.com/neulab/InterpretEval","authors":["Jinlan Fu","Pengfei Liu","Qi Zhang","Xuanjing Huang"],"demo_url":"","keywords":["cws task","fine-grained evaluation","negative problem","chinese systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.457","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.930","main.557","main.1733","main.2349","main.3013"],"title":"RethinkCWS: Is Chinese Word Segmentation a Solved Task?","tldr":"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3656","id":"main.3656","presentation_id":"38939384"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.557.png","content":{"abstract":"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.","authors":["Sufeng Duan","Hai Zhao"],"demo_url":"","keywords":["chinese segmentation","chinese","greedy segmentation","smoother training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.317","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","main.930","main.1733","main.618","main.1952"],"title":"Attention Is All You Need for Chinese Word Segmentation","tldr":"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.557","id":"main.557","presentation_id":"38938730"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.930.png","content":{"abstract":"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model. Besides, we utilize a transfer learning method to improve the performance of OOV words. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our method achieves the state-of-the-art performances on all datasets. Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.","authors":["Kaiyu Huang","Degen Huang","Zhuang Liu","Fengran Mo"],"demo_url":"","keywords":["natural","chinese segmentation","chinese","chinese tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.318","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","main.557","main.3013","main.1733","main.16"],"title":"A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation","tldr":"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing metho...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.930","id":"main.930","presentation_id":"38938808"}]
