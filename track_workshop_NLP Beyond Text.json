[{"content":{"abstract":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.","authors":["Elman Mansimov","Mitchell Stern","Mia Chen","Orhan Firat","Jakob Uszkoreit","Puneet Jain"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards End-to-End In-Image Neural Machine Translation","tldr":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model...","track":"NLP Beyond Text"},"id":"WS-23.106","presentation_id":"38939782","rocketchat_channel":"paper-nlpbt2020-106","speakers":"Elman Mansimov|Mitchell Stern|Mia Chen|Orhan Firat|Jakob Uszkoreit|Puneet Jain","title":"Towards End-to-End In-Image Neural Machine Translation"},{"content":{"abstract":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic inputs from a wide range of datasets to challenge, and sometimes surpass, the state-of-the-art in the field. To demonstrate the efficiency of our models, we carefully evaluate their performances on the IEMOCAP, MOSI, MOSEI and MELD dataset. The experiments can be directly replicated and the code is fully open for future researches.","authors":["Jean-Benoit Delbrouck","No\u00e9 Tits","St\u00e9phane Dupont"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition","tldr":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic ...","track":"NLP Beyond Text"},"id":"WS-23.110","presentation_id":"38939779","rocketchat_channel":"paper-nlpbt2020-110","speakers":"Jean-Benoit Delbrouck|No\u00e9 Tits|St\u00e9phane Dupont","title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition"},{"content":{"abstract":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.","authors":["Tejas Srinivasan","Ramon Sanabria","Florian Metze","Desmond Elliott"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multimodal Speech Recognition with Unstructured Audio Masking","tldr":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fix...","track":"NLP Beyond Text"},"id":"WS-23.114","presentation_id":"38939780","rocketchat_channel":"paper-nlpbt2020-114","speakers":"Tejas Srinivasan|Ramon Sanabria|Florian Metze|Desmond Elliott","title":"Multimodal Speech Recognition with Unstructured Audio Masking"},{"content":{"abstract":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.","authors":["Aman Khullar","Udit Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention","tldr":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only util...","track":"NLP Beyond Text"},"id":"WS-23.119","presentation_id":"38939781","rocketchat_channel":"paper-nlpbt2020-119","speakers":"Aman Khullar|Udit Arora","title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention"},{"content":{"abstract":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model\u2019s performance particularly improved on questions that required coreference resolution.","authors":["Muhammad Shah","Shikib Mehri","Tejas Srinivasan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reasoning Over History: Context Aware Visual Dialog","tldr":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existin...","track":"NLP Beyond Text"},"id":"WS-23.122","presentation_id":"38939783","rocketchat_channel":"paper-nlpbt2020-122","speakers":"Muhammad Shah|Shikib Mehri|Tejas Srinivasan","title":"Reasoning Over History: Context Aware Visual Dialog"},{"content":{"abstract":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle","authors":["Chaitanya Ahuja","Dong Won Lee","Ryo Ishii","Louis-Philippe Morency"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.170","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures","tldr":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made a...","track":"NLP Beyond Text"},"id":"WS-23.1589","presentation_id":"38940175","rocketchat_channel":"paper-nlpbt2020-1589","speakers":"Chaitanya Ahuja|Dong Won Lee|Ryo Ishii|Louis-Philippe Morency","title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures"},{"content":{"abstract":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process.","authors":["Wanqing Cui","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.392","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Language: Learning Commonsense from Images for Reasoning","tldr":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thous...","track":"NLP Beyond Text"},"id":"WS-23.3273","presentation_id":"38940176","rocketchat_channel":"paper-nlpbt2020-3273","speakers":"Wanqing Cui|Yanyan Lan|Liang Pang|Jiafeng Guo|Xueqi Cheng","title":"Beyond Language: Learning Commonsense from Images for Reasoning"},{"content":{"abstract":"","authors":["Loic Barrault"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Vision on (Simultaneous) Multimodal Machine Translation","tldr":null,"track":"NLP Beyond Text"},"id":"WS-23.Loic","presentation_id":"38939784","rocketchat_channel":"paper-nlpbt2020-Loic","speakers":"Loic Barrault","title":"A Vision on (Simultaneous) Multimodal Machine Translation"},{"content":{"abstract":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get the summaries, and fuses the summaries. Recently, some multi-modal models based on the architecture of BERT are proposed such as ViLBERT. However, they can only be pretrained on the image-text data. In this paper, we propose an image-text model for sarcasm detection using the pretrained BERT and ResNet without any further pretraining. BERT and ResNet have been pretrained on much larger text or image data than image-text data. We connect the vector spaces of BERT and ResNet to utilize more data. We use the pretrained Multi-Head Attention of BERT to model the text and image. Besides, we propose a 2D-Intra-Attention to extract the relationships between words and images. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Xiaowen Sun","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data","tldr":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get t...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.3","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-3","speakers":"Xinyu Wang|Xiaowen Sun|Tan Yang|Hongbo Wang","title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data"},{"content":{"abstract":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.","authors":["Frank F. Xu","Lei Ji","Botian Shi","Junyi Du","Graham Neubig","Yonatan Bisk","Nan Duan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos","tldr":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quant...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.4","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-4","speakers":"Frank F. Xu|Lei Ji|Botian Shi|Junyi Du|Graham Neubig|Yonatan Bisk|Nan Duan","title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos"},{"content":{"abstract":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped English LibriVox audiobooks and their corresponding English Gutenberg Project e-books to Italian e-books with a set of three complementary methods. Then we aligned the English and the Italian texts using both traditional Gale-Church based alignment methods and a recently proposed tool to perform bilingual sentences alignment computing the cosine similarity of multilingual sentence embeddings. Finally, we forced the alignment between the English audiobooks and the English side of our textual parallel corpus with a text-to-speech and dynamic time warping based forced alignment tool. For each step, we provide the reader with a critical discussion based on detailed evaluation and comparison of the results of the different methods.","authors":["Giuseppe Della Corte","Sara Stymne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation","tldr":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped En...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.5","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-5","speakers":"Giuseppe Della Corte|Sara Stymne","title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation"},{"content":{"abstract":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situations, where the answers are more likely to be sentences rather than single words. To bridge the gap between this natural VQA and existing VQA approaches, a novel unsupervised keyword extraction method is proposed. The method is based on the principle that the full-sentence answers can be decomposed into two parts: one that contains new information answering the question (i.e. keywords), and one that contains information already included in the question. Discriminative decoders were designed to achieve such decomposition, and the method was experimentally implemented on VQA datasets containing full-sentence answers. The results show that the proposed model can accurately extract the keywords without being given explicit annotations describing them.","authors":["Kohei Uehara","Tatsuya Harada"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Keyword Extraction for Full-Sentence VQA","tldr":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situation...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.6","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-6","speakers":"Kohei Uehara|Tatsuya Harada","title":"Unsupervised Keyword Extraction for Full-Sentence VQA"}]
