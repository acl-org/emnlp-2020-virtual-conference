[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1024.png","content":{"abstract":"While being an essential component of spoken language, fillers (e.g. \"um\" or \"uh\") often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two downstream tasks --- predicting a speaker's stance and expressed confidence.","authors":["Tanvi Dinkar","Pierre Colombo","Matthieu Labeau","Chlo\u00e9 Clavel"],"demo_url":"","keywords":["spoken tasks","modelling language","deep embeddings","fillers"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.641","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3257","main.3353","main.1006","main.1455","main.1938"],"title":"The importance of fillers for text representations of speech transcripts","tldr":"While being an essential component of spoken language, fillers (e.g. \"um\" or \"uh\") often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing impr...","track":"Speech and Multimodality"},"forum":"main.1024","id":"main.1024","presentation_id":"38938831"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.106.png","content":{"abstract":"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.","authors":["Elizabeth Nielsen","Mark Steedman","Sharon Goldwater"],"demo_url":"","keywords":["pitch detection","cnn-based model","phenomena","contrast"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.642","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.2349","main.920","main.1613","main.699"],"title":"The role of context in neural pitch accent detection in English","tldr":"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model f...","track":"Speech and Multimodality"},"forum":"main.106","id":"main.106","presentation_id":"38938650"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1402.png","content":{"abstract":"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.","authors":["Ozan Caglayan","Julia Ive","Veneta Haralampieva","Pranava Madhyastha","Lo\u00efc Barrault","Lucia Specia"],"demo_url":"","keywords":["simt","multimodal approaches","simt frameworks","visually-grounded models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.184","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["demo.111","main.888","TACL.2221","main.1572","main.2702"],"title":"Simultaneous Machine Translation with Visual Context","tldr":"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progr...","track":"Speech and Multimodality"},"forum":"main.1402","id":"main.1402","presentation_id":"38938900"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2218.png","content":{"abstract":"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.  While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech.  We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.  Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another.  To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.","authors":["David Gaddy","Dan Klein"],"demo_url":"","keywords":["digitally speech","speech models","emg","silently words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.445","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2915","main.106","TACL.2221","main.2343","main.2208"],"title":"Digital Voicing of Silent Speech","tldr":"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.  While prior work has focused on tr...","track":"Speech and Multimodality"},"forum":"main.2218","id":"main.2218","presentation_id":"38939073"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2383.png","content":{"abstract":"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,860 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.","authors":["Yang Li","Gang Li","Luheng He","Jingjie Zheng","Hong Li","Zhiwei Guan"],"demo_url":"","keywords":["mobile uis","automatically descriptions","widget captioning","multimodal task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.443","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1928","demo.54","main.876","demo.72"],"title":"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements","tldr":"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning...","track":"Speech and Multimodality"},"forum":"main.2383","id":"main.2383","presentation_id":"38939104"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2520.png","content":{"abstract":"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.","authors":["Ramit Sawhney","Piyush Khanna","Arshiya Aggarwal","Taru Jain","Puneet Mathur","Rajiv Ratn Shah"],"demo_url":"","keywords":["natural processing","stock forecasting","financial forecasting","risk modeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.643","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2225","main.1180","main.3486","main.1488","main.1289"],"title":"VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls","tldr":"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique inve...","track":"Speech and Multimodality"},"forum":"main.2520","id":"main.2520","presentation_id":"38939137"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2915.png","content":{"abstract":"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.","authors":["Ashkan Alinejad","Anoop Sarkar"],"demo_url":"","keywords":["automatic task","neural task","speech translation","end-to-end approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.644","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.1339","TACL.2221","main.856","main.2661"],"title":"Effectively pretraining a speech translation decoder with Machine Translation data","tldr":"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the re...","track":"Speech and Multimodality"},"forum":"main.2915","id":"main.2915","presentation_id":"38939224"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2994.png","content":{"abstract":"Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40,000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.","authors":["AmirAli Bagher Zadeh","Yansheng Cao","Simon Hessner","Paul Pu Liang","Soujanya Poria","Louis-Philippe Morency"],"demo_url":"","keywords":["modeling language","natural processing","multilingual studies","cmu-moseas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.141","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2777","main.3566","main.1379","main.870","main.852"],"title":"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","tldr":"Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datase...","track":"Speech and Multimodality"},"forum":"main.2994","id":"main.2994","presentation_id":"38939244"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3239.png","content":{"abstract":"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.","authors":["Wanyun Cui","Guangyu Zheng","Wei Wang"],"demo_url":"","keywords":["natural problem","plain inference","task-agnostic pretraining","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.444","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.376","main.3360","main.76","main.891","main.1834"],"title":"Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning","tldr":"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and vi...","track":"Speech and Multimodality"},"forum":"main.3239","id":"main.3239","presentation_id":"38939293"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3540.png","content":{"abstract":"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance,  self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training,  a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).","authors":["Shaolei Wang","Zhongyuan Wang","Wanxiang Che","Ting Liu"],"demo_url":"","keywords":["disfluency detection","self-supervised techniques","unsupervised paradigm","noisy training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.142","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2733","main.2684","main.1146","main.345","main.471"],"title":"Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection","tldr":"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance,  self-supervised learning techniques, bu...","track":"Speech and Multimodality"},"forum":"main.3540","id":"main.3540","presentation_id":"38939358"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.4.png","content":{"abstract":"The transcription bottleneck is often cited as a major obstacle for efforts to document the world\u2019s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine translation, and recruit linguists to provide narrow phonetic transcriptions and sentence-aligned translations. However, I believe that these approaches are not a good fit with the available data and skills, or with long-established practices that are essentially word based. In seeking a more effective approach, I consider a century of transcription practice and a wide range of computational approaches, before proposing a computational model based on spoken term detection which I call \u201csparse transcription.\u201d This represents a shift away from current assumptions that we transcribe phones, transcribe fully, and transcribe first. Instead, sparse transcription combines the older practice of word-level transcription with interpretive, iterative, and interactive processes which are amenable to wider participation and which open the way to new methods for processing oral languages.","authors":["Steven Bird"],"demo_url":"","keywords":["transcription bottleneck","automatic recognition","machine translation","word-level transcription"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["TACL.2221","main.648","main.3046","main.3257","main.3391"],"title":"Sparse Transcription","tldr":"The transcription bottleneck is often cited as a major obstacle for efforts to document the world\u2019s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine trans...","track":"Speech and Multimodality"},"forum":"CL.4","id":"CL.4","presentation_id":"38939392"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2221.png","content":{"abstract":"The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explores end-to-end trainable direct models that translate without transcribing. However, transcripts can be an indispensable output in practical applications, which often display transcripts alongside the translations to users. ** We make this common requirement explicit and explore the task of jointly transcribing and translating speech. While high accuracy of transcript and translation are crucial, even highly accurate systems can suffer from inconsistencies between both outputs that degrade the user experience. We introduce a methodology to evaluate consistency and compare several modeling approaches, including the traditional cascaded approach and end-to-end models. We find that direct models are poorly suited to the joint transcription/translation task, but that end-to-end models that feature a coupled inference procedure are able to achieve strong consistency. We further introduce simple techniques for directly optimizing for consistency, and analyze the resulting trade-offs between consistency, transcription accuracy, and translation accuracy.","authors":["Matthias Sperber","Hendra Setiawan","Christian Gollan","Udhay Nallasamy","Matthias Paulik"],"demo_url":"","keywords":["speech translation","jointly speech","joint task","speech step"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["CL.4","main.2915","main.1402","main.1100","demo.111"],"title":"Consistent Transcription and Translation of Speech","tldr":"The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explo...","track":"Speech and Multimodality"},"forum":"TACL.2221","id":"TACL.2221","presentation_id":"38939415"}]
