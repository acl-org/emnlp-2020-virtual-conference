[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1052.png","content":{"abstract":"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.","authors":["Julian Michael","Jan A. Botha","Ian Tenney"],"demo_url":"","keywords":["pretrained encoders","elmo","bert","latent learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.552","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2122","main.1159","main.41","main.1130","main.2363"],"title":"Asking without Telling: Exploring Latent Ontologies in Contextual Representations","tldr":"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is th...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1052","id":"main.1052","presentation_id":"38938836"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1205.png","content":{"abstract":"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.","authors":["Pierangelo Lombardo","Alessio Boiardi","Luca Colombo","Angelo Schiavone","Nicol\u00f2 Tamagnone"],"demo_url":"","keywords":["content-based recommenders","construction","top-rank evaluation","semantic models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.249","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1787","TACL.2095","main.693","main.300","main.1949"],"title":"Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models","tldr":"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime exam...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1205","id":"main.1205","presentation_id":"38938862"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1280.png","content":{"abstract":"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the \\textit{embryology} of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at \\url{https://github.com/d223302/albert-embryology}.","authors":["Cheng-Han Chiang","Sung-Feng Huang","Hung-yi Lee"],"demo_url":"","keywords":["pretraining","developmental process","pretrained models","lms"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.553","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2424","TACL.2041","main.2363","main.2414","main.852"],"title":"Pretrained Language Model Embryology: The Birth of ALBERT","tldr":"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent languag...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1280","id":"main.1280","presentation_id":"38938879"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1428.png","content":{"abstract":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","authors":["Charles Welch","Rada Mihalcea","Jonathan K. Kummerfeld"],"demo_url":"","keywords":["nlp applications","language model","language research","byte-pair encoding"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.696","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1130","main.1631","main.1351","main.2078","main.2777"],"title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation","tldr":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1428","id":"main.1428","presentation_id":"38938903"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1551.png","content":{"abstract":"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe's performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.","authors":["Tiago Pimentel","Naomi Saphra","Adina Williams","Ryan Cotterell"],"demo_url":"","keywords":["simplistic tasks","pos labeling","dependency labeling","full parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.254","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2122","main.498","main.3093","main.947","main.1970"],"title":"Pareto Probing: Trading Off Accuracy for Complexity","tldr":"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1551","id":"main.1551","presentation_id":"38938932"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1612.png","content":{"abstract":"To demystify the ``black box\" property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.","authors":["Siwon Kim","Jihun Yi","Eunji Kim","Sungroh Yoon"],"demo_url":"","keywords":["nlp","out-of-distribution problem","sentiment analysis","natural inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.255","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1960","main.2585","main.2650","main.2389","main.3348"],"title":"Interpretation of NLP models through input marginalization","tldr":"To demystify the ``black box\" property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an i...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1612","id":"main.1612","presentation_id":"38938947"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1613.png","content":{"abstract":"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language.  We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.","authors":["Isabel Papadimitriou","Dan Jurafsky"],"demo_url":"","keywords":["analyzing structure","encoding structure","natural acquisition","transfer learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.554","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2851","main.3181","main.1892","main.3115","main.852"],"title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models","tldr":"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce gene...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1613","id":"main.1613","presentation_id":"38938948"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1614.png","content":{"abstract":"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.","authors":["Pepa Atanasova","Dustin Wright","Isabelle Augenstein"],"demo_url":"","keywords":["inference tasks","fact checking","universal generation","adversarial attacks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.256","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2895","main.60","main.47","main.2313"],"title":"Generating Label Cohesive and Well-Formed Adversarial Claims","tldr":"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model i...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1614","id":"main.1614","presentation_id":"38938949"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1626.png","content":{"abstract":"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.","authors":["Hammad Rizwan","Muhammad Haroon Shakeel","Asim Karim"],"demo_url":"","keywords":["automatic detection","hate-speech detection","language models","transfer learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.197","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1997","main.2396","main.3566","main.106","main.3101"],"title":"Hate-Speech and Offensive Language Detection in Roman Urdu","tldr":"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainl...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1626","id":"main.1626","presentation_id":"38938955"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1631.png","content":{"abstract":"Pre-training large language models has become a standard in the natural language processing community.  Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain.  However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required.  In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE.  In this paper we conduct an empirical investigation into known methods to mitigate CF.  We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks.  Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.","authors":["Kristjan Arumae","Qing Sun","Parminder Bhatia"],"demo_url":"","keywords":["pre-training models","natural community","out tasks","clinical recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.394","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.16","main.2491","main.1482","main.1351"],"title":"An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training","tldr":"Pre-training large language models has become a standard in the natural language processing community.  Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain.  However, in...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1631","id":"main.1631","presentation_id":"38938956"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1901.png","content":{"abstract":"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. \"under-training\").","authors":["Ivan Vuli\u0107","Sebastian Ruder","Anders S\u00f8gaard"],"demo_url":"","keywords":["aligning spaces","monolingual training","vector spaces","non-isomorphic spaces"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.257","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2131","main.865","main.2891","main.410","main.447"],"title":"Are All Good Word Vector Spaces Isomorphic?","tldr":"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to resu...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1901","id":"main.1901","presentation_id":"38939005"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1970.png","content":{"abstract":"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.","authors":["Ieva Stali\u016bnait\u0117","Ignacio Iacobacci"],"demo_url":"","keywords":["nlp tasks","conversational task","semantic labeling","contextualized embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.573","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2363","main.143","TACL.2411","TACL.2013","main.2179"],"title":"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","tldr":"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1970","id":"main.1970","presentation_id":"38939019"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1996.png","content":{"abstract":"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.","authors":["Chengyue Jiang","Yinggong Zhao","Shanbo Chu","Libin Shen","Kewei Tu"],"demo_url":"","keywords":["natural applications","training","text classification","neural networks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.258","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3348","main.2040","main.989","main.2851","main.345"],"title":"Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks","tldr":"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressio...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1996","id":"main.1996","presentation_id":"38939025"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2005.png","content":{"abstract":"We study \\emph{semantic collisions}: texts that are semantically unrelated but judged as similar by NLP models.  We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts\\textemdash including paraphrase identification, document retrieval, response suggestion, and extractive summarization\\textemdash are vulnerable to semantic collisions.  For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3.  We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at \\url{https://github.com/csong27/collision-bert}.","authors":["Congzheng Song","Alexander Rush","Vitaly Shmatikov"],"demo_url":"","keywords":["paraphrase identification","document retrieval","response suggestion","extractive summarizationtextemdash"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.344","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3457","main.744","main.3327","main.210","main.371"],"title":"Adversarial Semantic Collisions","tldr":"We study \\emph{semantic collisions}: texts that are semantically unrelated but judged as similar by NLP models.  We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2005","id":"main.2005","presentation_id":"38939027"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.204.png","content":{"abstract":"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.","authors":["Hao Fei","Yafeng Ren","Donghong Ji"],"demo_url":"","keywords":["end tasks","structure integration","main task","semantic- tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.168","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2890","main.1618","main.1210","main.1339","main.2635"],"title":"Retrofitting Structure-aware Transformer Language Model for End Tasks","tldr":"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer struct...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.204","id":"main.204","presentation_id":"38938663"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2122.png","content":{"abstract":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"demo_url":"","keywords":["intrinsic probing","nlp systems","pre-trained representations","extrinsic probing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.15","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.498","main.947","main.2363","main.1052","TACL.2411"],"title":"Intrinsic Probing through Dimension Selection","tldr":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these repres...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2122","id":"main.2122","presentation_id":"38939055"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2179.png","content":{"abstract":"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99%), but generalization accuracy was substantially lower (16--35%) and showed high sensitivity to random seed (+-6--8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.","authors":["Najoung Kim","Tal Linzen"],"demo_url":"","keywords":["compositional generalization","language architectures","cogs","lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.731","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.1970","main.143","main.3457","TACL.2141"],"title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation","tldr":"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2179","id":"main.2179","presentation_id":"38939064"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2215.png","content":{"abstract":"{L}arge {T}ransformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned {BERT}, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained {BERT} weights are potentially useful. We also study the ``good\" subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.","authors":["Sai Prasanna","Anna Rogers","Anna Rumshisky"],"demo_url":"","keywords":["fine-tuned","structured pruning","self-attention heads","self-attention layers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.259","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.763","main.2999","main.618","TACL.2041","main.317"],"title":"When BERT Plays the Lottery, All Tickets Are Winning","tldr":"{L}arge {T}ransformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2215","id":"main.2215","presentation_id":"38939071"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2238.png","content":{"abstract":"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings.  By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be  underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.","authors":["Dallas Card","Peter Henderson","Urvashi Khandelwal","Robin Jia","Kyle Mahowald","Dan Jurafsky"],"demo_url":"","keywords":["human studies","machine translation","power analysis","power analyses"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.745","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.84","main.2268","main.1540","TACL.2055","main.2122"],"title":"With Little Power Comes Great Responsibility","tldr":"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2238","id":"main.2238","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2307.png","content":{"abstract":"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text --- a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.","authors":["Bhargavi Paranjape","Mandar Joshi","John Thickstun","Hannaneh Hajishirzi","Luke Zettlemoyer"],"demo_url":"","keywords":["language understanding","semi-supervised setting","complex models","explainer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.153","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2650","TACL.2411","main.2470","main.1023"],"title":"An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction","tldr":"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text --- a rationale. Models that condition predictions on a concise rationale, while being mor...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2307","id":"main.2307","presentation_id":"38939089"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2331.png","content":{"abstract":"Leveraging large amounts of unlabeled data using  Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.","authors":["Kasturi Bhattacharjee","Miguel Ballesteros","Rishita Anubhai","Smaranda Muresan","Jie Ma","Faisal Ladhak","Yaser Al-Onaizan"],"demo_url":"","keywords":["learning representations","downstream tasks","cross-view cvt","sequence tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.636","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.493","main.345","main.2078","main.2793","main.956"],"title":"To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging","tldr":"Leveraging large amounts of unlabeled data using  Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tas...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2331","id":"main.2331","presentation_id":"38939092"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2391.png","content":{"abstract":"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -- Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation,  open-source framework for evaluating models,  and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.","authors":["Tatiana Shavrina","Alena Fenogenova","Emelyanov Anton","Denis Shevelev","Ekaterina Artemova","Valentin Malykh","Vladislav Mikhailov","Maria Tikhonova","Andrey Chertok","Andrey Evlampiev"],"demo_url":"","keywords":["natural inference","logical operations","human evaluation","evaluating models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.381","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.143","main.835","main.1892","main.623"],"title":"RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark","tldr":"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -- Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their br...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2391","id":"main.2391","presentation_id":"38939106"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2414.png","content":{"abstract":"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.","authors":["Nadir Durrani","Hassan Sajjad","Fahim Dalvi","Yonatan Belinkov"],"demo_url":"","keywords":["neuron-level analysis","linguistic tasks","deep models","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.395","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2851","main.2363","main.2696","main.2893"],"title":"Analyzing Individual Neurons in Pre-trained Language Models","tldr":"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2414","id":"main.2414","presentation_id":"38939111"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2438.png","content":{"abstract":"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why.  We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks.  Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.","authors":["Sean Papay","Roman Klinger","Sebastian Pad\u00f3"],"demo_url":"","keywords":["chunking","ner","code-switching detection","span tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.396","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2087","main.449","main.3470","TACL.2041","main.2363"],"title":"Dissecting Span Identification Tasks with Performance Prediction","tldr":"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2438","id":"main.2438","presentation_id":"38939121"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2585.png","content":{"abstract":"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.","authors":["Hanjie Chen","Yangfeng Ji"],"demo_url":"","keywords":["finding explanations","model interpretability","classification","interpretable classifier"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.347","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.958","main.1612","main.2958","main.1159","main.2040"],"title":"Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers","tldr":"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many exis...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2585","id":"main.2585","presentation_id":"38939149"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2644.png","content":{"abstract":"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","authors":["Nikita Nangia","Clara Vania","Rasika Bhalerao","Samuel R. Bowman"],"demo_url":"","keywords":["nlp tasks","pretrained models","masked models","mlms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.154","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2886","main.834","main.353","TACL.2011","main.2893"],"title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models","tldr":"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicit...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2644","id":"main.2644","presentation_id":"38939165"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2696.png","content":{"abstract":"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.","authors":["Satwik Bhattamishra","Kabir Ahuja","Navin Goyal"],"demo_url":"","keywords":["nlp tasks","construction","transformers","lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.576","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2414","main.1130","main.2179","TACL.2411"],"title":"On the Ability and Limitations of Transformers to Recognize Formal Languages","tldr":"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular la...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2696","id":"main.2696","presentation_id":"38939173"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2705.png","content":{"abstract":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","authors":["Swabha Swayamdipta","Roy Schwartz","Nicholas Lourie","Yizhong Wang","Hannaneh Hajishirzi","Noah A. Smith","Yejin Choi"],"demo_url":"","keywords":["nlp research","out-of-distribution generalization","model optimization","data maps"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.746","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2886","main.3648","main.387","main.2535"],"title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","tldr":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leve...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2705","id":"main.2705","presentation_id":"38939175"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2750.png","content":{"abstract":"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim's factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","authors":["Adithya Pratapa","Sai Muralidhar Jayanthi","Kavya Nerella"],"demo_url":"","keywords":["fact-verification systems","shared tasks","reasoning process","fact-verification"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.629","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1086","main.2054","main.2570","main.3035","main.2416"],"title":"Constrained Fact Verification for FEVER","tldr":"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim's factuality, there is little work on understanding ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2750","id":"main.2750","presentation_id":"38939184"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2763.png","content":{"abstract":"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","authors":["Taylor Shin","Yasaman Razeghi","Robert L. Logan IV","Eric Wallace","Sameer Singh"],"demo_url":"","keywords":["pretraining","fill-in-the-blanks problems","cloze tests","sentiment analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.346","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.1983","main.2630","main.2590","main.3506","main.41"],"title":"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts","tldr":"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging su...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2763","id":"main.2763","presentation_id":"38939188"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2814.png","content":{"abstract":"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using ``fidelity curves'' to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.","authors":["Samuel Carton","Anirudh Rathore","Chenhao Tan"],"demo_url":"","keywords":["evaluating rationales","model retraining","human rationales","rationales"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.747","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2307","main.959","TACL.2049","main.3648","main.2258"],"title":"Evaluating and Characterizing Human Rationales","tldr":"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rational...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2814","id":"main.2814","presentation_id":"38939202"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2886.png","content":{"abstract":"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.  Various metrics have been proposed to quantify biases in model predictions.  In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering.  Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.","authors":["Jieyu Zhao","Kai-Wei Chang"],"demo_url":"","keywords":["evaluating bias","toxicity classification","object tasks","machine techniques"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.155","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.802","main.2076","main.2644","main.2122","main.345"],"title":"LOGAN: Local Group Bias Detection by Clustering","tldr":"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.  Various metrics have been proposed to...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2886","id":"main.2886","presentation_id":"38939216"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2893.png","content":{"abstract":"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.","authors":["Alex Warstadt","Yian Zhang","Xiaocheng Li","Haokun Liu","Samuel R. Bowman"],"demo_url":"","keywords":["self-supervised tasks","language understanding","ambiguous tasks","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.16","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2851","main.3023","main.2793","main.1146"],"title":"Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)","tldr":"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2893","id":"main.2893","presentation_id":"38939219"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2958.png","content":{"abstract":"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND -- a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).","authors":["Piyawat Lertvittayakumjorn","Lucia Specia","Francesca Toni"],"demo_url":"","keywords":["real-world classifiers","classifiers","find","deep classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.24","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2585","main.3540","main.1834","main.1575","main.76"],"title":"FIND: Human-in-the-Loop Debugging Deep Text Classifiers","tldr":"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. Th...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2958","id":"main.2958","presentation_id":"38939233"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2995.png","content":{"abstract":"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.","authors":["Seungtaek Choi","Haeju Park","Jinyoung Yeo","Seung-won Hwang"],"demo_url":"","keywords":["attention supervision","machine-augmented supervision","text tasks","sentiment analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.543","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.151","main.2733","main.1023","main.2476","main.3540"],"title":"Less is More: Attention Supervision with Counterfactuals for Text Classification","tldr":"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this g...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2995","id":"main.2995","presentation_id":"38939245"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3143.png","content":{"abstract":"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Hence, we carry out an empirical study on position embedding of mainstream pre-trained Transformers mainly focusing on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings by feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future works to choose the suitable positional encoding function for specific tasks given the application property.","authors":["Yu-An Wang","Yun-Nung Chen"],"demo_url":"","keywords":["nlp tasks","transformers","position transformers","iconic tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.555","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.1485","TACL.2411","main.2635","main.858"],"title":"What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding","tldr":"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attentio...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3143","id":"main.3143","presentation_id":"38939276"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.32.png","content":{"abstract":"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for `Donald is a' substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.","authors":["Vered Shwartz","Rachel Rudinger","Oyvind Tafjord"],"demo_url":"","keywords":["grounding","downstream tasks","reading probes","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.556","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.2363","main.3181","main.1052","main.2382"],"title":"\u201cYou are grounded!\u201d: Latent Name Artifacts in Pre-trained Language Models","tldr":"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associat...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.32","id":"main.32","presentation_id":"38938640"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3278.png","content":{"abstract":"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.","authors":["Lang Yu","Allyson Ettinger"],"demo_url":"","keywords":["nlp tasks","systematic representations","deep models","phrasal representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.397","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2650","TACL.2411","main.1618","main.1970","main.1130"],"title":"Assessing Phrasal Representation and Composition in Transformers","tldr":"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases,...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3278","id":"main.3278","presentation_id":"38939299"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3304.png","content":{"abstract":"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).","authors":["Bill Yuchen Lin","Seyeon Lee","Rahul Khanna","Xiang Ren"],"demo_url":"","keywords":["probing task","fine-tuning","testing","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.557","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2893","main.2349","main.3183","main.2122"],"title":"Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models","tldr":"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we fi...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3304","id":"main.3304","presentation_id":"38939307"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3513.png","content":{"abstract":"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.","authors":["Aakriti Budhraja","Madhura Pande","Preksha Nema","Pratyush Kumar","Mitesh M. Khapra"],"demo_url":"","keywords":["nlp tasks","pruning heads","fine-tuning","pruning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.260","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.618","main.1552","main.2696","main.1485","main.557"],"title":"On the weak link between importance and prunability of attention heads","tldr":"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3513","id":"main.3513","presentation_id":"38939353"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3529.png","content":{"abstract":"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.","authors":["Sahana Ramnath","Preksha Nema","Deep Sahni","Mitesh M. Khapra"],"demo_url":"","keywords":["nlp tasks","reading answering","contextual understanding","answer prediction"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.261","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1970","main.449","main.2973","main.928","main.3186"],"title":"Towards Interpreting BERT for Reading Comprehension Based QA","tldr":"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight int...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3529","id":"main.3529","presentation_id":"38939356"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.353.png","content":{"abstract":"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are \"hallucinatory\", e.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of 'the doctor removed his mask' is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","authors":["Ana Valeria Gonz\u00e1lez","Maria Barrett","Rasmus Hvingelby","Kellie Webster","Anders S\u00f8gaard"],"demo_url":"","keywords":["nlp tasks","russian","gender bias","coreferential reading"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.209","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.143","main.2278","main.1379","main.623","main.2363"],"title":"Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias","tldr":"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are \"hallucinatory\", e.g., disambiguating ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.353","id":"main.353","presentation_id":"38938689"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3543.png","content":{"abstract":"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes:  General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as  i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure,  which maintains 97% performance while using at-most 10% of the original neurons.","authors":["Fahim Dalvi","Hassan Sajjad","Nadir Durrani","Yonatan Belinkov"],"demo_url":"","keywords":["transformer-based models","pretrained models","bert","xlnet"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.398","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1552","main.2414","main.1446","TACL.2041","main.2615"],"title":"Analyzing Redundancy in Pretrained Transformer Models","tldr":"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundanc...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3543","id":"main.3543","presentation_id":"38939360"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3550.png","content":{"abstract":"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-$(k,m)$, the language of well-nested brackets (of $k$ types) and $m$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use $O(k^{\\frac{m}{2}})$ memory (hidden units) to generate these languages. We prove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with $o(m \\log k)$ hidden units.","authors":["John Hewitt","Michael Hahn","Surya Ganguli","Percy Liang","Christopher D. Manning"],"demo_url":"","keywords":["recurrent networks","rnns","rnn","finite-precision setting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.156","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2198","main.2696","TACL.2411","main.2851","main.1996"],"title":"RNNs can generate bounded hierarchical languages with optimal memory","tldr":"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RN...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3550","id":"main.3550","presentation_id":"38939362"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.359.png","content":{"abstract":"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases.  Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.","authors":["Joe Stacey","Pasquale Minervini","Haim Dubossarsky","Sebastian Riedel","Tim Rockt\u00e4schel"],"demo_url":"","keywords":["neural networks","adversarial training","sentence representations","nli models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.665","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2313","TACL.2129","main.2430","main.3434","main.1834"],"title":"Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training","tldr":"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.359","id":"main.359","presentation_id":"38938692"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.376.png","content":{"abstract":"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.","authors":["Yao-Hung Hubert Tsai","Martin Ma","Muqiao Yang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"demo_url":"","keywords":["human-centric tasks","sentiment analysis","emotion recognition","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.143","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3239","main.2994","main.1670","main.2430","main.1402"],"title":"Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis","tldr":"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentim...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.376","id":"main.376","presentation_id":"38938697"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.457.png","content":{"abstract":"We report that state-of-the-art parsers consistently failed to identify \u201chers\u201d and \u201ctheirs\u201d as pronouns but identified the masculine equivalent \u201chis\u201d. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.","authors":["Robert Munro","Alex (Carmen) Morrison"],"demo_url":"","keywords":["measuring models","parsers","language models","machine models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.157","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3181","main.3115","main.2114","main.32","main.1282"],"title":"Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation","tldr":"We report that state-of-the-art parsers consistently failed to identify \u201chers\u201d and \u201ctheirs\u201d as pronouns but identified the masculine equivalent \u201chis\u201d. We find that the same biases exist in recent language models like BERT. While some of the bias come...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.457","id":"main.457","presentation_id":"38938714"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.498.png","content":{"abstract":"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.","authors":["Ilia Kuznetsov","Iryna Gurevych"],"demo_url":"","keywords":["downstream tasks","pre-training","probing","linguistic tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.13","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2122","main.2363","TACL.2411","main.1970","main.1551"],"title":"A matter of framing: The impact of linguistic formalism on probing results","tldr":"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. W...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.498","id":"main.498","presentation_id":"38938722"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.616.png","content":{"abstract":"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and speci\ufb01c linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The \ufb01ndings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These \ufb01ndings provide insights into the inner workings of Transformers.","authors":["Goro Kobayashi","Tatsuki Kuribayashi","Sho Yokoi","Kentaro Inui"],"demo_url":"","keywords":["natural processing","norm-based analyses","word alignment","transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.574","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1618","main.2696","main.1798","demo.60","main.618"],"title":"Attention is Not Only a Weight: Analyzing Transformers with Vector Norms","tldr":"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.616","id":"main.616","presentation_id":"38938738"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.638.png","content":{"abstract":"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or ''probe'' -- to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either ''the representation being rich in knowledge'', or ''the probe learning the task'', which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the ''good probe'' criteria proposed by the two papers, *selectivity* (Hewitt and Liang, 2019) and *information gain* (Pimentel et al., 2020), are equivalent -- the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.","authors":["Zining Zhu","Frank Rudzicz"],"demo_url":"","keywords":["supervised classification","diagnostic classification","neural representations","diagnostic classifier"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.744","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.947","main.2122","main.498","main.3181","main.1052"],"title":"An information theoretic view on selecting linguistic probes","tldr":"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or ''probe'' -- to perform supervised classification from internal representations. Howev...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.638","id":"main.638","presentation_id":"38938744"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.87.png","content":{"abstract":"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.","authors":["Nicola De Cao","Michael Sejr Schlichtkrull","Wilker Aziz","Ivan Titov"],"demo_url":"","keywords":["model prediction","approximate search","erasure","sentiment classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.262","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2307","main.2834","main.2535","main.3183","TACL.2041"],"title":"How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking","tldr":"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objec...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.87","id":"main.87","presentation_id":"38938648"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.947.png","content":{"abstract":"To measure how well  pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier  trained to predict the  property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect  differences in representations.  For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates \"the amount of effort\" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.","authors":["Elena Voita","Ivan Titov"],"demo_url":"","keywords":["random tasks","estimating mdl","representations","pretrained representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.14","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2122","main.1551","main.638","main.3093","main.2893"],"title":"Information-Theoretic Probing with Minimum Description Length","tldr":"To measure how well  pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier  trained to predict the  property from the representations. Despite widespread adoption of probes, differences...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.947","id":"main.947","presentation_id":"38938809"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.958.png","content":{"abstract":"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.","authors":["Pepa Atanasova","Jakob Grue Simonsen","Christina Lioma","Isabelle Augenstein"],"demo_url":"","keywords":["downstream tasks","machine learning","explainability techniques","diverse techniques"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.263","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2307","main.2585","main.76","main.2650"],"title":"A Diagnostic Study of Explainability Techniques for Text Classification","tldr":"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.958","id":"main.958","presentation_id":"38938813"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.959.png","content":{"abstract":"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"demo_url":"","keywords":["reasoning process","user study","model selection","explainable systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.575","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2228","main.1022","TACL.2049","main.2258","main.2380"],"title":"F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering","tldr":"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.959","id":"main.959","presentation_id":"38938814"}]
