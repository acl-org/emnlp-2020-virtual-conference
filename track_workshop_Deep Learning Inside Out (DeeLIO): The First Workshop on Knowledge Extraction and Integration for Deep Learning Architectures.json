[{"content":{"abstract":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common misuse of Chinese idioms leads to error in corpus and causes error in the learned semantic representation of Chinese idioms. In this paper, we introduce the definition written by Chinese experts to correct the misuse. We propose a model for the Chinese idiom cloze test integrating various information effectively. We propose an attention mechanism called Attribute Attention to balance the weight of different attributes among different descriptions of the Chinese idiom. Besides the given candidates of every blank, we also try to choose the answer from all Chinese idioms that appear in the dataset as the extra loss due to the uniqueness and specificity of Chinese idioms. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Hongsheng Zhao","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test","tldr":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common mi...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1","presentation_id":"38939724","rocketchat_channel":"paper-deelio-1","speakers":"Xinyu Wang|Hongsheng Zhao|Tan Yang|Hongbo Wang","title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test"},{"content":{"abstract":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.","authors":["Guanglin Niu","Bo Li","Yongfei Zhang","Shiliang Pu","Jingyang Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.105","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding","tldr":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1008","presentation_id":"38940167","rocketchat_channel":"paper-deelio-1008","speakers":"Guanglin Niu|Bo Li|Yongfei Zhang|Shiliang Pu|Jingyang Li","title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding"},{"content":{"abstract":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available.","authors":["Hoang Nguyen","Chenwei Zhang","Congying Xia","Philip Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.108","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection","tldr":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1039","presentation_id":"38940168","rocketchat_channel":"paper-deelio-1039","speakers":"Hoang Nguyen|Chenwei Zhang|Congying Xia|Philip Yu","title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection"},{"content":{"abstract":"","authors":["Kung-Hsiang Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs","tldr":null,"track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1059","presentation_id":"38940169","rocketchat_channel":"paper-deelio-1059","speakers":"Kung-Hsiang Huang","title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs"},{"content":{"abstract":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attributes, even for common entities. To improve the precision of model-based entity-attribute extraction, we propose attribute-aware embeddings, which embeds entities and attributes in the same space by the similarity of their attributes. Our model, EANET, learns these embeddings by representing entities as a weighted sum of their attributes and concatenates these embeddings to mention level features. EANET achieves up to 91% classification accuracy, outperforming strong baselines and achieves 83% precision on manually labeled high confidence extractions, outperforming Biperpedia (Gupta et al., 2014), a previous state-of-the-art for large scale entity-attribute extraction.","authors":["Dan Iter","Xiao Yu","Fangtao Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings","tldr":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attrib...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.12","presentation_id":"38939729","rocketchat_channel":"paper-deelio-12","speakers":"Dan Iter|Xiao Yu|Fangtao Li","title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings"},{"content":{"abstract":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task\u2019s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.","authors":["Xin Guo","Yu Tian","Qinghan Xue","Panos Lampropoulos","Steven Eliuk","Kenneth Barner","Xiaolong Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.164","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Learning Long Short Term Memory","tldr":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell i...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1524","presentation_id":"38940170","rocketchat_channel":"paper-deelio-1524","speakers":"Xin Guo|Yu Tian|Qinghan Xue|Panos Lampropoulos|Steven Eliuk|Kenneth Barner|Xiaolong Wang","title":"Continual Learning Long Short Term Memory"},{"content":{"abstract":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we present OSCR (Ontology-based Semantic Composition Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pre-training. We evaluated the performance of BERT pre-trained on Wikipedia with and without OSCR by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33.3%, 18.6%, and 4% improved accuracy compared to pre-training BERT without OSCR.","authors":["Travis Goodwin","Dina Demner-Fushman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization","tldr":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we prese...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.16","presentation_id":"38939730","rocketchat_channel":"paper-deelio-16","speakers":"Travis Goodwin|Dina Demner-Fushman","title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization"},{"content":{"abstract":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) is learning target concept vector representations from scratch which requires more number of training instances. Our model is based on RoBERTa and target concept embeddings. In our model, we integrate a) target concept information in the form of target concept vectors generated by encoding target concept descriptions using SRoBERTa, state-of-the-art RoBERTa based sentence embedding model and b) domain lexicon knowledge by enriching target concept vectors with synonym relationship knowledge using retrofitting algorithm. It is the first attempt in MCN to exploit both target concept information as well as domain lexicon knowledge in the form of retrofitted target concept vectors. Our model outperforms all the existing models with an accuracy improvement up to 1.36% on three standard datasets. Further, our model when trained only on mapping lexicon synonyms achieves up to 4.87% improvement in accuracy.","authors":["Katikapalli Subramanyam Kalyan","Sivanesan Sangeetha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts","tldr":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.17","presentation_id":"38939731","rocketchat_channel":"paper-deelio-17","speakers":"Katikapalli Subramanyam Kalyan|Sivanesan Sangeetha","title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts"},{"content":{"abstract":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes.","authors":["Ting-Yun Chang","Yang Liu","Karthik Gopalakrishnan","Behnam Hedayatnia","Pei Zhou","Dilek Hakkani-Tur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks","tldr":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to pe...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.18","presentation_id":"38939732","rocketchat_channel":"paper-deelio-18","speakers":"Ting-Yun Chang|Yang Liu|Karthik Gopalakrishnan|Behnam Hedayatnia|Pei Zhou|Dilek Hakkani-Tur","title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks"},{"content":{"abstract":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for our work is the BERT assumption according to which a large number of NLP tasks can be solved with pre-trained Transformers with no substantial task-specific changes of the architecture. However, our experiments show that the encoding strategy can have a great impact on the quality of the fine-tuning. The combination between cross-encoding and multi-input models worked better than one cross-encoder and allowed us to achieve comparable results with the state-of-the-art without the use of any external data.","authors":["Sonia Cibu","Anca Marginean"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Commonsense Statements Identification and Explanation with Transformer based Encoders","tldr":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for ou...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.20","presentation_id":"38939733","rocketchat_channel":"paper-deelio-20","speakers":"Sonia Cibu|Anca Marginean","title":"Commonsense Statements Identification and Explanation with Transformer based Encoders"},{"content":{"abstract":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.","authors":["Marjan Albooyeh","Rishab Goel","Seyed Mehran Kazemi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.241","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Out-of-Sample Representation Learning for Knowledge Graphs","tldr":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2047","presentation_id":"38940171","rocketchat_channel":"paper-deelio-2047","speakers":"Marjan Albooyeh|Rishab Goel|Seyed Mehran Kazemi","title":"Out-of-Sample Representation Learning for Knowledge Graphs"},{"content":{"abstract":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH assumes that a context surrounding a hyponym is more informative than that of a hypernym, it has never been tested on visual objects. Since our perception is tightly associated with language, it is meaningful to explore whether the DIH holds on visual objects. To this end, we consider visual objects as the context of a word and represent a word as a bag of visual objects found in images associated with the word. This allows us to test the feasibility of the visual DIH. To better distinguish word pairs in a hypernym relation from other relations such as co-hypernyms, we also propose a new measurable function that takes into account both the difference in the generality of meaning and similarity of meaning between words. Our experimental results show that the DIH holds on visual objects and that the proposed method combined with the proposed function outperforms existing unsupervised representation methods.","authors":["Masayasu Muraoka","Tetsuya Nasukawa","Bishwaranjan Bhattacharjee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.246","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment","tldr":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH as...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2085","presentation_id":"38940172","rocketchat_channel":"paper-deelio-2085","speakers":"Masayasu Muraoka|Tetsuya Nasukawa|Bishwaranjan Bhattacharjee","title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment"},{"content":{"abstract":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge graph embeddings and fine-grain entity type representations. Our work also shows that jointly modeling both structured knowledge tuples and language improves both.","authors":["Rajat Patel","Francis Ferraro"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling","tldr":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge gr...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.22","presentation_id":"38939734","rocketchat_channel":"paper-deelio-22","speakers":"Rajat Patel|Francis Ferraro","title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling"},{"content":{"abstract":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional information retrieval step (IR) and a kNN search over a large datastore of an embedded text collection. Our contributions are as follows: i) BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training. ii) We show that BERT often identifies the correct response category (e.g., US city), but only kNN recovers the factually correct answer (e.g.,\u201cMiami\u201d). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN can easily handle facts not covered by BERT\u2019s training set, e.g., recent events.","authors":["Nora Kassner","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.307","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA","tldr":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we comb...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2513","presentation_id":"38940173","rocketchat_channel":"paper-deelio-2513","speakers":"Nora Kassner|Hinrich Sch\u00fctze","title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA"},{"content":{"abstract":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train classifiers without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates \u201cweak\u201d supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages: by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier: leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of word translations.","authors":["Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.323","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher","tldr":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2666","presentation_id":"38940174","rocketchat_channel":"paper-deelio-2666","speakers":"Giannis Karamanolakis|Daniel Hsu|Luis Gravano","title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher"},{"content":{"abstract":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner.","authors":["Xiaoyu Chen","Rohan Badlani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relation Extraction with Contextualized Relation Embedding","tldr":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.4","presentation_id":"38939725","rocketchat_channel":"paper-deelio-4","speakers":"Xiaoyu Chen|Rohan Badlani","title":"Relation Extraction with Contextualized Relation Embedding"},{"content":{"abstract":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In attacks of this variety, the adversary substitutes words with synonyms. Since synonym substitution perturbations aim to satisfy all lexical, grammatical, and semantic constraints, they are difficult to detect with automatic syntax check as well as by humans. In this paper, we propose a structure-free defensive method that is capable of improving the performance of DNN-based models with both clean and adversarial data. Our findings show that replacing the embeddings of the important words in the input samples with the average of their synonyms\u2019 embeddings can significantly improve the generalization of DNN-based classifiers. By doing so, we reduce model sensitivity to particular words in the input samples. Our results indicate that the proposed defense is not only capable of defending against adversarial attacks, but is also capable of improving the performance of DNN-based models when tested on benign data. On average, the proposed defense improved the classification accuracy of the CNN and Bi-LSTM models by 41.30% and 55.66%, respectively, when tested under adversarial attacks. Extended investigation shows that our defensive method can improve the robustness of nonneural models, achieving an average of 17.62% and 22.93% classification accuracy increase on the SVM and XGBoost models, respectively. The proposed defensive method has also shown an average of 26.60% classification accuracy improvement when tested with the infamous BERT model. Our algorithm is generic enough to be applied in any NLP domain and to any model trained on any natural language.","authors":["Basemah Alshemali","Jugal Kalita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generalization to Mitigate Synonym Substitution Attacks","tldr":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In at...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.6","presentation_id":"38939726","rocketchat_channel":"paper-deelio-6","speakers":"Basemah Alshemali|Jugal Kalita","title":"Generalization to Mitigate Synonym Substitution Attacks"},{"content":{"abstract":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors. The resulting entity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no expensive further pre-training of the BERT encoder. We evaluate E-BERT on unsupervised question answering (QA), supervised relation classification (RC) and entity linking (EL). On all three tasks, E-BERT outperforms BERT and other baselines. We also show quantitatively that the original BERT model is overly reliant on the surface form of entity names (e.g., guessing that someone with an Italian-sounding name speaks Italian), and that E-BERT mitigates this problem.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.71","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT","tldr":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entit...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.696","presentation_id":"38940166","rocketchat_channel":"paper-deelio-696","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"content":{"abstract":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.","authors":["Steven Y. Feng","Varun Gangal","Dongyeop Kang","Teruko Mitamura","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GenAug: Data Augmentation for Finetuning Text Generators","tldr":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose a...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.7","presentation_id":"38939727","rocketchat_channel":"paper-deelio-7","speakers":"Steven Y. Feng|Varun Gangal|Dongyeop Kang|Teruko Mitamura|Eduard Hovy","title":"GenAug: Data Augmentation for Finetuning Text Generators"},{"content":{"abstract":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, joint pre-training (i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective) may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/wluper/retrograph.","authors":["Anne Lauscher","Olga Majewska","Leonardo F. R. Ribeiro","Iryna Gurevych","Nikolai Rozanov","Goran Glava\u0161"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers","tldr":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.9","presentation_id":"38939728","rocketchat_channel":"paper-deelio-9","speakers":"Anne Lauscher|Olga Majewska|Leonardo F. R. Ribeiro|Iryna Gurevych|Nikolai Rozanov|Goran Glava\u0161","title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"},{"content":{"abstract":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.","authors":["Adyasha Maharana","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.333","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension","tldr":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2797","presentation_id":"38940111","rocketchat_channel":"paper-deelio-2797","speakers":"Adyasha Maharana|Mohit Bansal","title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension"},{"content":{"abstract":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at https://github.com/weakrules/Denoise-multi-weak-sources.","authors":["Wendi Ren","Yinghao Li","Hanting Su","David Kartchner","Cassie Mitchell","Chao Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.334","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Denoising Multi-Source Weak Supervision for Neural Text Classification","tldr":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2800","presentation_id":"38940139","rocketchat_channel":"paper-deelio-2800","speakers":"Wendi Ren|Yinghao Li|Hanting Su|David Kartchner|Cassie Mitchell|Chao Zhang","title":"Denoising Multi-Source Weak Supervision for Neural Text Classification"},{"content":{"abstract":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG\u02c6C, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG\u02c6C consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG\u02c6C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.","authors":["Yiben Yang","Chaitanya Malaviya","Jared Fernandez","Swabha Swayamdipta","Ronan Le Bras","Ji-Ping Wang","Chandra Bhagavatula","Yejin Choi","Doug Downey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.90","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generative Data Augmentation for Commonsense Reasoning","tldr":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models c...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.884","presentation_id":"38940138","rocketchat_channel":"paper-deelio-884","speakers":"Yiben Yang|Chaitanya Malaviya|Jared Fernandez|Swabha Swayamdipta|Ronan Le Bras|Ji-Ping Wang|Chandra Bhagavatula|Yejin Choi|Doug Downey","title":"Generative Data Augmentation for Commonsense Reasoning"}]
