[{"content":{"abstract":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.","authors":["Ali Akbar Septiandri","Yosef Ardhito Winatmoko","Ilham Firdausi Putra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?","tldr":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent develo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1","presentation_id":"38939419","rocketchat_channel":"paper-sustainlp2020-1","speakers":"Ali Akbar Septiandri|Yosef Ardhito Winatmoko|Ilham Firdausi Putra","title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?"},{"content":{"abstract":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but they are seldom available and expensive to hire. This has lead to the thriving of crowdsourcing platforms such as Amazon Mechanical Turk (AMT). However, the annotations provided by one worker cannot be used directly to train the model due to the lack of expertise. Existing literature in annotation aggregation focuses on binary and multi-choice problems. In contrast, little work has been done on complex tasks such as sequence labeling with imbalanced classes, a ubiquitous task in Natural Language Processing (NLP), and Bio-Informatics. We propose OptSLA, an Optimization-based Sequential Label Aggregation method, that jointly considers the characteristics of sequential labeling tasks, workers reliabilities, and advanced deep learning techniques to conquer the challenge. We evaluate our model on crowdsourced data for named entity recognition task. Our results show that the proposed OptSLA outperforms the state-of-the-art aggregation methods, and the results are easier to interpret.","authors":["Nasim Sabetpour","Adithya Kulkarni","Qi Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.119","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation","tldr":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but th...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1098","presentation_id":"38940107","rocketchat_channel":"paper-sustainlp2020-1098","speakers":"Nasim Sabetpour|Adithya Kulkarni|Qi Li","title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation"},{"content":{"abstract":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.","authors":["Moshe Wasserblat","Oren Pereg","Peter Izsak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring the Boundaries of Low-Resource BERT Distillation","tldr":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.12","presentation_id":"38939426","rocketchat_channel":"paper-sustainlp2020-12","speakers":"Moshe Wasserblat|Oren Pereg|Peter Izsak","title":"Exploring the Boundaries of Low-Resource BERT Distillation"},{"content":{"abstract":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT - BERT F1 delta, at 5% of BioBERT\u2019s CO 2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.134","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","tldr":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1286","presentation_id":"38940121","rocketchat_channel":"paper-sustainlp2020-1286","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"content":{"abstract":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.","authors":["Sosuke Kobayashi","Sho Yokoi","Jun Suzuki","Kentaro Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Estimation of Influence of a Training Instance","tldr":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.13","presentation_id":"38939427","rocketchat_channel":"paper-sustainlp2020-13","speakers":"Sosuke Kobayashi|Sho Yokoi|Jun Suzuki|Kentaro Inui","title":"Efficient Estimation of Influence of a Training Instance"},{"content":{"abstract":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.","authors":["Yi-Te Hsu","Sarthak Garg","Yi-Hsiu Liao","Ilya Chatsviorkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Inference For Neural Machine Translation","tldr":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.14","presentation_id":"38939429","rocketchat_channel":"paper-sustainlp2020-14","speakers":"Yi-Te Hsu|Sarthak Garg|Yi-Hsiu Liao|Ilya Chatsviorkin","title":"Efficient Inference For Neural Machine Translation"},{"content":{"abstract":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations \u2013 a main performance bottleneck. Consequently, our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets.","authors":["Yatin Chaudhary","Pankaj Gupta","Khushbu Saxena","Vivek Kulkarni","Thomas Runkler","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.152","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TopicBERT for Energy Efficient Document Classification","tldr":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1418","presentation_id":"38940122","rocketchat_channel":"paper-sustainlp2020-1418","speakers":"Yatin Chaudhary|Pankaj Gupta|Khushbu Saxena|Vivek Kulkarni|Thomas Runkler|Hinrich Sch\u00fctze","title":"TopicBERT for Energy Efficient Document Classification"},{"content":{"abstract":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in F1 that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both PyTorch and TensorFlow.","authors":["Brian Lester","Daniel Pressel","Amy Hemmeter","Sagnik Ray Choudhury","Srinivas Bangalore"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.166","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers","tldr":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. C...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1537","presentation_id":"38940105","rocketchat_channel":"paper-sustainlp2020-1537","speakers":"Brian Lester|Daniel Pressel|Amy Hemmeter|Sagnik Ray Choudhury|Srinivas Bangalore","title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers"},{"content":{"abstract":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.","authors":["Alicia Tsai","Laurent El Ghaoui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm","tldr":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrai...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.17","presentation_id":"38939430","rocketchat_channel":"paper-sustainlp2020-17","speakers":"Alicia Tsai|Laurent El Ghaoui","title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm"},{"content":{"abstract":"","authors":["Kunal Chawla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1887","presentation_id":"38940140","rocketchat_channel":"paper-sustainlp2020-1887","speakers":"Kunal Chawla","title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization"},{"content":{"abstract":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization (HMF) that achieves this dual objective. HMF improves low-rank matrix factorization (LMF) techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-timethan pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks (Translation, Intent Detection,Language Modeling) and show that for similar accuracy values and compression factors, HMF can achieve more than 2.32x faster inference run-time than pruning and 16.77% better accuracy than LMF.","authors":["Urmish Thakker","Jesse Beu","Dibakar Gope","Ganesh Dasika","Matthew Mattina"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Rank and run-time aware compression of NLP Applications","tldr":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2","presentation_id":"38939420","rocketchat_channel":"paper-sustainlp2020-2","speakers":"Urmish Thakker|Jesse Beu|Dibakar Gope|Ganesh Dasika|Matthew Mattina","title":"Rank and run-time aware compression of NLP Applications"},{"content":{"abstract":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.","authors":["Jiezhong Qiu","Hao Ma","Omer Levy","Wen-tau Yih","Sinong Wang","Jie Tang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.232","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Blockwise Self-Attention for Long Document Understanding","tldr":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/infere...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2015","presentation_id":"38940119","rocketchat_channel":"paper-sustainlp2020-2015","speakers":"Jiezhong Qiu|Hao Ma|Omer Levy|Wen-tau Yih|Sinong Wang|Jie Tang","title":"Blockwise Self-Attention for Long Document Understanding"},{"content":{"abstract":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of severe information loss. In this paper, we present a simple but effective unsupervised neural generative semantic hashing method with a focus on few-bits hashing. Our model is built upon variational autoencoder and represents each hash bit as a Bernoulli variable, which allows the model to be end-to-end trainable. To address the issue of information loss, we introduce a set of auxiliary implicit topic vectors. With the aid of these topic vectors, the generated hash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves significant improvements over state-of-the-art semantic hashing methods in few-bits hashing.","authors":["Fanghua Ye","Jarana Manotumruksa","Emine Yilmaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.233","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling","tldr":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guarante...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2017","presentation_id":"38940106","rocketchat_channel":"paper-sustainlp2020-2017","speakers":"Fanghua Ye|Jarana Manotumruksa|Emine Yilmaz","title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling"},{"content":{"abstract":"","authors":["Jiecao Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2182","presentation_id":"38940104","rocketchat_channel":"paper-sustainlp2020-2182","speakers":"Jiecao Chen","title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling"},{"content":{"abstract":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","authors":["Yuxiang Wu","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering","tldr":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have show...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.22","presentation_id":"38939431","rocketchat_channel":"paper-sustainlp2020-22","speakers":"Yuxiang Wu|Pasquale Minervini|Pontus Stenetorp|Sebastian Riedel","title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering"},{"content":{"abstract":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational cost during inference can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an encoder and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this encoder, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher accuracy and lower computational cost once the system is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the computational cost when multiple tasks are performed on the same text.","authors":["Jingfei Du","Myle Ott","Haoran Li","Xing Zhou","Veselin Stoyanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.271","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference","tldr":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a sin...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2230","presentation_id":"38940109","rocketchat_channel":"paper-sustainlp2020-2230","speakers":"Jingfei Du|Myle Ott|Haoran Li|Xing Zhou|Veselin Stoyanov","title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference"},{"content":{"abstract":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.","authors":["Giorgos Vernikos","Katerina Margatina","Alexandra Chronopoulou","Ion Androutsopoulos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.278","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain Adversarial Fine-Tuning as an Effective Regularizer","tldr":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2288","presentation_id":"38940129","rocketchat_channel":"paper-sustainlp2020-2288","speakers":"Giorgos Vernikos|Katerina Margatina|Alexandra Chronopoulou|Ion Androutsopoulos","title":"Domain Adversarial Fine-Tuning as an Effective Regularizer"},{"content":{"abstract":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.","authors":["Zhiheng Huang","Davis Liang","Peng Xu","Bing Xiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.298","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improve Transformer Models with Better Relative Position Embeddings","tldr":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2453","presentation_id":"38940108","rocketchat_channel":"paper-sustainlp2020-2453","speakers":"Zhiheng Huang|Davis Liang|Peng Xu|Bing Xiang","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"content":{"abstract":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from \u201cgenuine\u201d ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.","authors":["Zhao Wang","Aron Culotta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.308","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Spurious Correlations for Robust Text Classification","tldr":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we pr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2516","presentation_id":"38940117","rocketchat_channel":"paper-sustainlp2020-2516","speakers":"Zhao Wang|Aron Culotta","title":"Identifying Spurious Correlations for Robust Text Classification"},{"content":{"abstract":"","authors":["Urmish Thakker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Doped Structured Matrices for Extreme Compression of LSTM Models","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.27","presentation_id":"38940744","rocketchat_channel":"paper-sustainlp2020-27","speakers":"Urmish Thakker","title":"Doped Structured Matrices for Extreme Compression of LSTM Models"},{"content":{"abstract":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.","authors":["Cennet Oguz","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings","tldr":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.28","presentation_id":"38939432","rocketchat_channel":"paper-sustainlp2020-28","speakers":"Cennet Oguz|Ngoc Thang Vu","title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings"},{"content":{"abstract":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.","authors":["Ji Xin","Rodrigo Nogueira","Yaoliang Yu","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Early Exiting BERT for Efficient Document Ranking","tldr":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for d...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.29","presentation_id":"38939433","rocketchat_channel":"paper-sustainlp2020-29","speakers":"Ji Xin|Rodrigo Nogueira|Yaoliang Yu|Jimmy Lin","title":"Early Exiting BERT for Efficient Document Ranking"},{"content":{"abstract":"","authors":["Patrick Xia","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incremental Neural Coreference Resolution in Constant Memory","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3","presentation_id":"38939421","rocketchat_channel":"paper-sustainlp2020-3","speakers":"Patrick Xia|Jo\u00e3o Sedoc|Benjamin Van Durme","title":"Incremental Neural Coreference Resolution in Constant Memory"},{"content":{"abstract":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of these approaches still need to be trained on very large datasets. In this paper, we introduce BeGanKP, a new conditional GAN model to address the problem of Keyphrase Generation in a low-resource scenario. Our main contribution relies in the Discriminator\u2019s architecture: a new BERT-based module which is able to distinguish between the generated and humancurated KPs reliably. Its characteristics allow us to use it in a low-resource scenario, where only a small amount of training data are available, obtaining an efficient Generator. The resulting architecture achieves, on five public datasets, competitive results with respect to the state-of-the-art approaches, using less than 1% of the training data.","authors":["Giuseppe Lancioni","Saida S.Mohamed","Beatrice Portelli","Giuseppe Serra","Carlo Tasso"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Keyphrase Generation with GANs in Low-Resources Scenarios","tldr":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.30","presentation_id":"38939434","rocketchat_channel":"paper-sustainlp2020-30","speakers":"Giuseppe Lancioni|Saida S.Mohamed|Beatrice Portelli|Giuseppe Serra|Carlo Tasso","title":"Keyphrase Generation with GANs in Low-Resources Scenarios"},{"content":{"abstract":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.","authors":["Umanga Bista","Alexander Mathews","Aditya Menon","Lexing Xie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.367","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy","tldr":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information pres...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3078","presentation_id":"38940131","rocketchat_channel":"paper-sustainlp2020-3078","speakers":"Umanga Bista|Alexander Mathews|Aditya Menon|Lexing Xie","title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy"},{"content":{"abstract":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we can get benefits similar to an ensemble of classifiers with a fraction of the resources required.We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique that encourages the model to develop an internal representation of the problem at hand which is beneficial to multiple output units of the classifier at the same time. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.","authors":["Norbert Kis-Szab\u00f3","G\u00e1bor Berend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles","tldr":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we c...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.32","presentation_id":"38939435","rocketchat_channel":"paper-sustainlp2020-32","speakers":"Norbert Kis-Szab\u00f3|G\u00e1bor Berend","title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles"},{"content":{"abstract":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that \u201csome\u201d labeled in-domain data can be worse than none at all.","authors":["Xinyu Zhang","Andrew Yates","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data","tldr":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.34","presentation_id":"38939436","rocketchat_channel":"paper-sustainlp2020-34","speakers":"Xinyu Zhang|Andrew Yates|Jimmy Lin","title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data"},{"content":{"abstract":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed model is able to generate fluent questions with high completeness and outperforms the strongest baseline by 20.8% in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU","authors":["Dan Su","Yan Xu","Wenliang Dai","Ziwei Ji","Tiezheng Yu","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.416","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-hop Question Generation with Graph Convolutional Network","tldr":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3444","presentation_id":"38940120","rocketchat_channel":"paper-sustainlp2020-3444","speakers":"Dan Su|Yan Xu|Wenliang Dai|Ziwei Ji|Tiezheng Yu|Pascale Fung","title":"Multi-hop Question Generation with Graph Convolutional Network"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3459","presentation_id":"38940124","rocketchat_channel":"paper-sustainlp2020-3459","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"","authors":["Rajarshi Das"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3526","presentation_id":"38940133","rocketchat_channel":"paper-sustainlp2020-3526","speakers":"Rajarshi Das","title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion"},{"content":{"abstract":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8\u00d7 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3\u00d7 reduction in run-time memory footprints and 3.5\u00d7 speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.","authors":["Insoo Chung","Byeongwook Kim","Yoonjung Choi","Se Jung Kwon","Yongkweon Jeon","Baeseong Park","Sangha Kim","Dongsoo Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.433","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation","tldr":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quan...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3562","presentation_id":"38940118","rocketchat_channel":"paper-sustainlp2020-3562","speakers":"Insoo Chung|Byeongwook Kim|Yoonjung Choi|Se Jung Kwon|Yongkweon Jeon|Baeseong Park|Sangha Kim|Dongsoo Lee","title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"},{"content":{"abstract":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that determines which existing source model would minimize error for transfer learning to a given target. This technique does not require learning for prediction, and avoids computational costs of trail-and-error. We have evaluated this technique on nine datasets across diverse domains, including newswire, user forums, air flight booking, cybersecurity news, etc. We show that it per-forms better than existing techniques such as fine-tuning over vanilla BERT, or curriculum learning over the largest dataset on top of BERT, resulting in average F1 score gains in excess of 3%. Moreover, our technique consistently selects the best model using fewer tries.","authors":["Parul Awasthy","Bishwaranjan Bhattacharjee","John Kender","Radu Florian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks","tldr":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that dete...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.36","presentation_id":"38939437","rocketchat_channel":"paper-sustainlp2020-36","speakers":"Parul Awasthy|Bishwaranjan Bhattacharjee|John Kender|Radu Florian","title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks"},{"content":{"abstract":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.","authors":["Julian Eisenschlos","Syrine Krichene","Thomas M\u00fcller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding tables with intermediate pre-training","tldr":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on t...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.361","presentation_id":"38940134","rocketchat_channel":"paper-sustainlp2020-361","speakers":"Julian Eisenschlos|Syrine Krichene|Thomas M\u00fcller","title":"Understanding tables with intermediate pre-training"},{"content":{"abstract":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.","authors":["Amine Abdaoui","Camille Pradel","Gr\u00e9goire Sigel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Load What You Need: Smaller Versions of Mutlilingual BERT","tldr":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.37","presentation_id":"38939438","rocketchat_channel":"paper-sustainlp2020-37","speakers":"Amine Abdaoui|Camille Pradel|Gr\u00e9goire Sigel","title":"Load What You Need: Smaller Versions of Mutlilingual BERT"},{"content":{"abstract":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert","authors":["Forrest Iandola","Albert Shaw","Ravi Krishna","Kurt Keutzer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?","tldr":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.38","presentation_id":"38939439","rocketchat_channel":"paper-sustainlp2020-38","speakers":"Forrest Iandola|Albert Shaw|Ravi Krishna|Kurt Keutzer","title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"},{"content":{"abstract":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification. However, it lacks sufficient contextual information enhancement and thus is less useful for sequence labelling tasks such as chunking and named entity recognition (NER). To address this limitation, we propose masked adversarial training (MAT) to improve robustness from contextual information in sequence labelling. MAT masks or replaces some words in the sentence when computing adversarial loss from perturbed inputs and consequently enhances model robustness using more context-level information. In our experiments, our method shows significant improvements on accuracy and robustness of sequence labelling. By further incorporating with ELMo embeddings, our model achieves better or comparable results to state-of-the-art on CoNLL 2000 and 2003 benchmarks using much less parameters.","authors":["Luoxin Chen","Xinyue Liu","Weitong Ruan","Jianhua Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training","tldr":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input n...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.381","presentation_id":"38940127","rocketchat_channel":"paper-sustainlp2020-381","speakers":"Luoxin Chen|Xinyue Liu|Weitong Ruan|Jianhua Lu","title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training"},{"content":{"abstract":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource constraint devices. These models try to minimize resource requirements like RAM and storage without hurting the accuracy much. We utilized these models on multiple benchmark natural language processing tasks, which were sentimental analysis, spam message detection, emotion analysis and fake news classification. The experiment results shows that the tree-based algorithm, Bonsai, surpassed the rest of the machine learning algorithms by achieve higher accuracy scores while having significantly lower memory usage.","authors":["Raj Pranesh","Ambesh Shekhar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing","tldr":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource cons...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.39","presentation_id":"38939440","rocketchat_channel":"paper-sustainlp2020-39","speakers":"Raj Pranesh|Ambesh Shekhar","title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing"},{"content":{"abstract":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.","authors":["Qingqing Cao","Aruna Balasubramanian","Niranjan Balasubramanian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Accurate and Reliable Energy Measurement of NLP Models","tldr":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate beca...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.42","presentation_id":"38939441","rocketchat_channel":"paper-sustainlp2020-42","speakers":"Qingqing Cao|Aruna Balasubramanian|Niranjan Balasubramanian","title":"Towards Accurate and Reliable Energy Measurement of NLP Models"},{"content":{"abstract":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.","authors":["Young Jin Kim","Hany Hassan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding","tldr":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficien...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.43","presentation_id":"38939442","rocketchat_channel":"paper-sustainlp2020-43","speakers":"Young Jin Kim|Hany Hassan","title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"},{"content":{"abstract":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.","authors":["Ariadna Quattoni","Xavier Carreras"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A comparison between CNNs and WFAs for Sequence Classification","tldr":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.45","presentation_id":"38939443","rocketchat_channel":"paper-sustainlp2020-45","speakers":"Ariadna Quattoni|Xavier Carreras","title":"A comparison between CNNs and WFAs for Sequence Classification"},{"content":{"abstract":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of pseudo-positive labels. We validate that the effectiveness of augmented labels are comparable to positives, such that ours outperform state-of-the-arts without augmentation.","authors":["Seungtaek Choi","Myeongho Jeong","Jinyoung Yeo","Seung-won Hwang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactual Augmentation for Training Next Response Selection","tldr":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.46","presentation_id":"38939444","rocketchat_channel":"paper-sustainlp2020-46","speakers":"Seungtaek Choi|Myeongho Jeong|Jinyoung Yeo|Seung-won Hwang","title":"Counterfactual Augmentation for Training Next Response Selection"},{"content":{"abstract":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as \u2018Entailment\u2019, \u2018Neutral\u2019, and \u2018Contradiction\u2019 for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2% data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.","authors":["Swaroop Mishra","Bhavdeep Singh Sachdeva"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Need to Create Big Datasets to Learn a Task?","tldr":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.47","presentation_id":"38939445","rocketchat_channel":"paper-sustainlp2020-47","speakers":"Swaroop Mishra|Bhavdeep Singh Sachdeva","title":"Do We Need to Create Big Datasets to Learn a Task?"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.49","presentation_id":"38939446","rocketchat_channel":"paper-sustainlp2020-49","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.","authors":["Harshil Shah","Julien Fauqueur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models","tldr":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.5","presentation_id":"38939422","rocketchat_channel":"paper-sustainlp2020-5","speakers":"Harshil Shah|Julien Fauqueur","title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models"},{"content":{"abstract":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed \u2013 non-learnable \u2013 attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.","authors":["Alessandro Raganato","Yves Scherrer","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation","tldr":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.512","presentation_id":"38940110","rocketchat_channel":"paper-sustainlp2020-512","speakers":"Alessandro Raganato|Yves Scherrer|J\u00f6rg Tiedemann","title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"},{"content":{"abstract":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.","authors":["Zhao Jinman","Shawn Zhong","Xiaomin Zhang","Yingyu Liang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding","tldr":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.547","presentation_id":"38940115","rocketchat_channel":"paper-sustainlp2020-547","speakers":"Zhao Jinman|Shawn Zhong|Xiaomin Zhang|Yingyu Liang","title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding"},{"content":{"abstract":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.","authors":["Kumar Shridhar","Harshil Jain","Akshat Agarwal","Denis Kleyko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End to End Binarized Neural Networks for Text Classification","tldr":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.6","presentation_id":"38939423","rocketchat_channel":"paper-sustainlp2020-6","speakers":"Kumar Shridhar|Harshil Jain|Akshat Agarwal|Denis Kleyko","title":"End to End Binarized Neural Networks for Text Classification"},{"content":{"abstract":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.","authors":["Zi Lin","Jeremiah Liu","Zi Yang","Nan Hua","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.64","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior","tldr":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which p...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.651","presentation_id":"38940112","rocketchat_channel":"paper-sustainlp2020-651","speakers":"Zi Lin|Jeremiah Liu|Zi Yang|Nan Hua|Dan Roth","title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"},{"content":{"abstract":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.","authors":["Mingzhu Wu","Nafise Sadat Moosavi","Andreas R\u00fcckl\u00e9","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.74","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases","tldr":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge abo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.724","presentation_id":"38940113","rocketchat_channel":"paper-sustainlp2020-724","speakers":"Mingzhu Wu|Nafise Sadat Moosavi|Andreas R\u00fcckl\u00e9|Iryna Gurevych","title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases"},{"content":{"abstract":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of pretrained language models (PLMs), we investigate how to incorporate large PKM into PLMs that can be finetuned for a wide variety of downstream NLP tasks. We define a new memory usage metric, and careful observation using this metric reveals that most memory slots remain outdated during the training of PKM-augmented models. To train better PLMs by tackling this issue, we propose simple but effective solutions: (1) initialization from the model weights pretrained without memory and (2) augmenting PKM by addition rather than replacing a feed-forward network. We verify that both of them are crucial for the pretraining of PKM-augmented PLMs, enhancing memory utilization and downstream performance. Code and pretrained weights are available at https://github.com/clovaai/pkm-transformers.","authors":["Gyuwan Kim","Tae Hwan Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.362","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Product Key Memory for Pretrained Language Models","tldr":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal langua...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.8","presentation_id":"38939424","rocketchat_channel":"paper-sustainlp2020-8","speakers":"Gyuwan Kim|Tae Hwan Jung","title":"Large Product Key Memory for Pretrained Language Models"},{"content":{"abstract":"","authors":["Vivek Gupta","Ankit Saw","Pegah Nokhiz","Praneeth Netrapalli","Piyush Rai","Partha Talukdar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"P-SIF: Document Embeddings using Partition Averaging","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.9","presentation_id":"38939425","rocketchat_channel":"paper-sustainlp2020-9","speakers":"Vivek Gupta|Ankit Saw|Pegah Nokhiz|Praneeth Netrapalli|Piyush Rai|Partha Talukdar","title":"P-SIF: Document Embeddings using Partition Averaging"},{"content":{"abstract":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on domain-specific, labeled data. In this paper, we propose ESTeR , an unsupervised model for identifying emotions using a novel similarity function based on random walks on graphs. Our model combines large-scale word co-occurrence information with word-associations from lexicons avoiding not only the dependence on labeled datasets, but also an explicit mapping of words to latent spaces used in emotion-enriched word embeddings. Our similarity function can also be computed efficiently. We study a range of datasets including recent tweets related to COVID-19 to illustrate the superior performance of our model and report insights on public emotions during the on-going pandemic.","authors":["Sujatha Das Gollapalli","Polina Rozenshtein","See-Kiong Ng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.93","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection","tldr":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using comp...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.929","presentation_id":"38940135","rocketchat_channel":"paper-sustainlp2020-929","speakers":"Sujatha Das Gollapalli|Polina Rozenshtein|See-Kiong Ng","title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection"},{"content":{"abstract":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20\u00d7 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.","authors":["Alex Wang","Thomas Wolf"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview of the SustaiNLP 2020 Shared Task","tldr":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We des...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2020.sustainlp-1.24","presentation_id":"","rocketchat_channel":"paper-sustainlp2020-24","speakers":"Alex Wang|Thomas Wolf","title":"Overview of the SustaiNLP 2020 Shared Task"}]
