[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1030.png","content":{"abstract":"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.","authors":["Hrituraj Singh","Sumit Shekhar"],"demo_url":"","keywords":["answering questions","natural questions","sequential localization","question encoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.264","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2120","main.319","main.3398","main.3054","main.1123"],"title":"STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering","tldr":"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1030","id":"main.1030","presentation_id":"38938832"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1071.png","content":{"abstract":"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images.  In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and  text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.","authors":["Bowen Zhang","Hexiang Hu","Vihan Jain","Eugene Ie","Fei Sha"],"demo_url":"","keywords":["cross-modal retrieval","referring expression","compositional recognition","pre-training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.60","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2702","main.666","main.531","main.1052","main.1231"],"title":"Learning to Represent Image and Text with Denotation Graph","tldr":"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers t...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1071","id":"main.1071","presentation_id":"38938839"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1085.png","content":{"abstract":"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating \\textit{commonsense} captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset ``Video-to-Commonsense (V2C)\" that contains $\\sim9k$ videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.","authors":["Zhiyuan Fang","Tejas Gokhale","Pratyay Banerjee","Chitta Baral","Yezhou Yang"],"demo_url":"","keywords":["captioning","video understanding","video captioning","generating captions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.61","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2839","main.2322","main.1009","main.284","main.702"],"title":"Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning","tldr":"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and trans...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1085","id":"main.1085","presentation_id":"38938840"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1113.png","content":{"abstract":"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments.","authors":["Alexander Ku","Peter Anderson","Roma Patel","Eugene Ie","Jason Baldridge"],"demo_url":"","keywords":["multitask learning","embodied agents","vln","rxr"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.356","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.995","main.647","main.355","main.1402","main.3360"],"title":"Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding","tldr":"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by a...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1113","id":"main.1113","presentation_id":"38938846"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1196.png","content":{"abstract":"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model's robustness.","authors":["Zujie Liang","Weitao Jiang","Haifeng Hu","Jiaying Zhu"],"demo_url":"","keywords":["visual","generating samples","augmentation","self-supervised mechanism"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.265","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3183","TACL.2055","main.387","TACL.2129","main.2258"],"title":"Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering","tldr":"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1196","id":"main.1196","presentation_id":"38938860"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1388.png","content":{"abstract":"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work.","authors":["Xisen Jin","Junyi Du","Arka Sadhu","Ram Nevatia","Xiang Ren"],"demo_url":"","keywords":["visually task","continual phrases","visually-grounded task","compositional generalization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.158","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2702","main.3360","TACL.2041","main.3470","main.2838"],"title":"Visually Grounded Continual Learning of Compositional Phrases","tldr":"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning tas...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1388","id":"main.1388","presentation_id":"38938894"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1455.png","content":{"abstract":"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle  tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.  Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.","authors":["Yonatan Bisk","Ari Holtzman","Jesse Thomason","Jacob Andreas","Yoshua Bengio","Joyce Chai","Mirella Lapata","Angeliki Lazaridou","Jonathan May","Aleksandr Nisnevich","Nicolas Pinto","Joseph Turian"],"demo_url":"","keywords":["language research","tasks","linguistic communication","natural processing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.703","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2638","main.3115","main.2363","main.1130","main.1613"],"title":"Experience Grounds Language","tldr":"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle  tasks after b...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1455","id":"main.1455","presentation_id":"38938907"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1578.png","content":{"abstract":"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame.","authors":["Shunyu Yao","Rohan Rao","Matthew Hausknecht","Karthik Narasimhan"],"demo_url":"","keywords":["autonomous agents","contextual model","calm","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.704","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.763","main.2574","main.699","TACL.2143","main.2410"],"title":"Keep CALM and Explore: Language Models for Action Generation in Text-based Games","tldr":"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates a...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1578","id":"main.1578","presentation_id":"38938940"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1670.png","content":{"abstract":"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.","authors":["Jack Hessel","Lillian Lee"],"demo_url":"","keywords":["modeling interactions","multimodal tasks","visual answering","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.62","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3183","main.284","main.376","TACL.2041","main.317"],"title":"Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!","tldr":"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1670","id":"main.1670","presentation_id":"38938964"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2058.png","content":{"abstract":"Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.","authors":["Ece Takmaz","Mario Giulianelli","Sandro Pezzelle","Arabella Sinclair","Raquel Fern\u00e1ndez"],"demo_url":"","keywords":["generation model","reference system","visual context","dialogue context"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.353","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2141","main.689","main.1522","main.3157","main.916"],"title":"Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts","tldr":"Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting p...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2058","id":"main.2058","presentation_id":"38939036"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2083.png","content":{"abstract":"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.","authors":["Qinxin Wang","Hao Tan","Sheng Shen","Michael Mahoney","Zhewei Yao"],"demo_url":"","keywords":["phrase localization","visually-aware representations","weakly-supervised scenarios","ablation studies"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.159","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2273","main.3609","main.148","main.3360","main.2702"],"title":"MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding","tldr":"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-availab...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2083","id":"main.2083","presentation_id":"38939046"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2273.png","content":{"abstract":"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as ``kitchen'' and ``bedroom'', and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating ``granite'' with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.","authors":["Gregory Yauney","Jack Hessel","David Mimno"],"demo_url":"","keywords":["image-text approaches","granular annotation","unsupervised method","object detection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.160","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2083","main.2702","main.143","main.1052","main.2342"],"title":"Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents","tldr":"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, u...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2273","id":"main.2273","presentation_id":"38939084"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2322.png","content":{"abstract":"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs---a natural expression of information need---from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.","authors":["Adam Fisch","Kenton Lee","Ming-Wei Chang","Jonathan Clark","Regina Barzilay"],"demo_url":"","keywords":["image task","visual images","captioning","capwap"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.705","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.284","main.1923","main.1085","main.3183","main.317"],"title":"CapWAP: Image Captioning with a Purpose","tldr":"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioni...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2322","id":"main.2322","presentation_id":"38939091"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2702.png","content":{"abstract":"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy.  We show that using an extension of probabilistic context-free grammar  model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more `abstract' categories (e.g., +55.1% recall on VPs).","authors":["Yanpeng Zhao","Ivan Titov"],"demo_url":"","keywords":["exploiting groundings","language understanding","gradient estimates","fully-differentiable learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.354","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1388","main.3360","main.2122","main.2083","TACL.2411"],"title":"Visually Grounded Compound PCFGs","tldr":"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2702","id":"main.2702","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.279.png","content":{"abstract":"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.","authors":["Zhenjie Zhao","Evangelos Papalexakis","Xiaojuan Ma"],"demo_url":"","keywords":["human-robot interaction","physical learning","natural processing","model generalization"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.266","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.666","main.2873","main.684","main.923","main.1010"],"title":"Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization","tldr":"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer fro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.279","id":"main.279","presentation_id":"38938674"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2839.png","content":{"abstract":"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work.","authors":["Jie Lei","Licheng Yu","Tamara Berg","Mohit Bansal"],"demo_url":"","keywords":["video-and-language prediction","ai models","vlep","adversarial procedure"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.706","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1085","main.645","main.1797","main.3179","TACL.2143"],"title":"What is More Likely to Happen Next? Video-and-Language Future Event Prediction","tldr":"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of co...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2839","id":"main.2839","presentation_id":"38939207"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.284.png","content":{"abstract":"Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family \u2013 LXMERT \u2013 finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT\u2019s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.","authors":["Jaemin Cho","Jiasen Lu","Dustin Schwenk","Hannaneh Hajishirzi","Aniruddha Kembhavi"],"demo_url":"","keywords":["multimodal tasks","visual answering","visual grounding","generative captioning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.707","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2322","TACL.2041","main.3360","main.2758","main.3183"],"title":"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","tldr":"Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual gro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.284","id":"main.284","presentation_id":"38938675"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2853.png","content":{"abstract":"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.","authors":["Linjie Li","Yen-Chun Chen","Yu Cheng","Zhe Gan","Licheng Yu","Jingjing Liu"],"demo_url":"","keywords":["large-scale learning","pre-training tasks","video-subtitle matching","text-based retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.161","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.355","TACL.2107","main.1113","main.3483","main.628"],"title":"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training","tldr":"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal f...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2853","id":"main.2853","presentation_id":"38939211"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2991.png","content":{"abstract":"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50\\% of the training data, SSCR achieves a comparable result to using complete data.","authors":["Tsu-Jui Fu","Xin Wang","Scott Grafton","Miguel Eckstein","William Yang Wang"],"demo_url":"","keywords":["iterative tasks","data scarcity","editing tasks","self-supervised scenario"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.357","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2758","main.284","main.373","main.2072","main.2382"],"title":"SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning","tldr":"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2991","id":"main.2991","presentation_id":"38939243"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3183.png","content":{"abstract":"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present \\textit{MUTANT}, a training paradigm that exposes the model to perceptually similar, yet semantically distinct \\textit{mutations} of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, \\textit{MUTANT} does not rely on the knowledge about the nature of train and test answer distributions. \\textit{MUTANT} establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.","authors":["Tejas Gokhale","Pratyay Banerjee","Chitta Baral","Yezhou Yang"],"demo_url":"","keywords":["generalization","ood generalization","question answering","training paradigm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.63","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1196","main.1837","main.3140","main.1022","TACL.2041"],"title":"MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering","tldr":"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a pro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3183","id":"main.3183","presentation_id":"38939282"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3360.png","content":{"abstract":"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named \u201cvokenization\u201d that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call \u201cvokens\u201d). The \u201cvokenizer\u201d is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.","authors":["Hao Tan","Mohit Bansal"],"demo_url":"","keywords":["speaking","writing","text-only self-supervision","pure-language tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.162","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.407","main.1892","main.1130","main.1388","main.2083"],"title":"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision","tldr":"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a vi...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3360","id":"main.3360","presentation_id":"38939320"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3504.png","content":{"abstract":"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable.  In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance?  Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others.  Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.","authors":["Wanrong Zhu","Xin Wang","Pradyumna Narayana","Kazoo Sone","Sugato Basu","William Yang Wang"],"demo_url":"","keywords":["visually generation","vision-and-language tasks","cider","utilities"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.708","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1388","main.3360","main.1928","main.2864","main.1923"],"title":"Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations","tldr":"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmark...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3504","id":"main.3504","presentation_id":"38939350"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.355.png","content":{"abstract":"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt  BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.","authors":["Yue Wang","Shafiq Joty","Michael Lyu","Irwin King","Caiming Xiong","Steven C.H. Hoi"],"demo_url":"","keywords":["visual dialog","vision-language task","visual tasks","answer ranking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.269","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2070","main.1113","main.647","main.2853","TACL.2041"],"title":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","tldr":"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such int...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.355","id":"main.355","presentation_id":"38938690"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.445.png","content":{"abstract":"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively \"easy:\" speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are \"grounded\" and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.","authors":["Jack Hessel","Zhenhai Zhu","Bo Pang","Radu Soricut"],"demo_url":"","keywords":["pretraining","video tasks","pretraining model","features"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.709","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.2839","main.2758","main.3580","main.1085"],"title":"Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube","tldr":"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech re...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.445","id":"main.445","presentation_id":"38938709"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.593.png","content":{"abstract":"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.","authors":["Oskar van der Wal","Silvan de Boer","Elia Bruni","Dieuwke Hupkes"],"demo_url":"","keywords":["unsupervised techniques","ugi techniques","referential games","emergent languages"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.270","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["CL.1","TACL.2141","main.3115","TACL.1936","TACL.2013"],"title":"The Grammar of Emergent Languages","tldr":"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.593","id":"main.593","presentation_id":"38938733"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.635.png","content":{"abstract":"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.","authors":["Reuben Tan","Bryan Plummer","Kate Saenko"],"demo_url":"","keywords":["large-scale disinformation","detecting inconsistencies","defense mechanism","adversaries"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.163","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2784","main.47","main.1614","main.2914","TACL.2129"],"title":"Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News","tldr":"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and in...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.635","id":"main.635","presentation_id":"38938743"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.647.png","content":{"abstract":"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans -- an Observer and a Locator -- complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task -- providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer's location within 3m in unseen buildings, vs. 70.4% for human Locators.","authors":["Meera Hahn","Jacob Krantz","Dhruv Batra","Devi Parikh","James Rehg","Stefan Lee","Peter Anderson"],"demo_url":"","keywords":["cooperative task","embodied dialog","cooperative localization","locator"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.59","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1113","main.355","main.373","main.1797","main.995"],"title":"Where Are You? Localization from Embodied Dialog","tldr":"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans -- an Observer and a Locator -- complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views whi...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.647","id":"main.647","presentation_id":"38938748"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.76.png","content":{"abstract":"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model\u2019s structure. We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.","authors":["Weixin Liang","James Zou","Zhou Yu"],"demo_url":"","keywords":["active learning","data learning","learning","visual tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.355","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2851","main.345","main.1611","main.1923","main.2040"],"title":"ALICE: Active Learning with Contrastive Natural Language Explanations","tldr":"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.76","id":"main.76","presentation_id":"38938646"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.995.png","content":{"abstract":"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.  We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.","authors":["Yicong Hong","Cristian Rodriguez","Qi Wu","Stephen Gould"],"demo_url":"","keywords":["vision-and-language navigation","navigation","agent","sub-instruction modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.271","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1113","main.355","main.647","main.1402","TACL.2041"],"title":"Sub-Instruction Aware Vision-and-Language Navigation","tldr":"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visua...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.995","id":"main.995","presentation_id":"38938820"}]
