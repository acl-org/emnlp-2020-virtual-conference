[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1004.png","content":{"abstract":"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.","authors":["Wenxuan Zhang","Yang Deng","Jing Ma","Wai Lam"],"demo_url":"","keywords":["online shopping","evidence-based tasks","answer problem","product-related platforms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.188","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1022","main.959","main.2228","main.2380","main.2117"],"title":"AnswerFact: Fact Checking in Product Question Answering","tldr":"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on tho...","track":"Question Answering"},"forum":"main.1004","id":"main.1004","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1022.png","content":{"abstract":"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases. To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task. With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases. We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.","authors":["Yeon Seonwoo","Ji-Hoon Kim","Jung-Woo Ha","Alice Oh"],"demo_url":"","keywords":["context prediction","context task","reading comprehension","extractive models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.189","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.449","main.2586","main.1788","main.3186","main.1837"],"title":"Context-Aware Answer Extraction in Question Answering","tldr":"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. Thi...","track":"Question Answering"},"forum":"main.1022","id":"main.1022","presentation_id":"38938829"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.108.png","content":{"abstract":"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.","authors":["Adam Roberts","Colin Raffel","Noam Shazeer"],"demo_url":"","keywords":["fine-tuning models","neural models","open-domain systems","model size"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.437","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1351","main.2931","main.3074","main.1159","main.1528"],"title":"How Much Knowledge Can You Pack Into the Parameters of a Language Model?","tldr":"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning p...","track":"Question Answering"},"forum":"main.108","id":"main.108","presentation_id":"38938651"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1191.png","content":{"abstract":"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step \"black box\" model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.","authors":["Mucheng Ren","Xiubo Geng","Tao Qin","Heyan Huang","Daxin Jiang"],"demo_url":"","keywords":["reasoning process","sequential approach","neural modules","reasoning modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.548","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.607","main.2253","main.2650","main.3470","main.1103"],"title":"Towards Interpretable Reasoning over Paragraph Effects in Situation","tldr":"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated ...","track":"Question Answering"},"forum":"main.1191","id":"main.1191","presentation_id":"38938859"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1262.png","content":{"abstract":"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.","authors":["Dayiheng Liu","Yeyun Gong","Jie Fu","Yu Yan","Jiusheng Chen","Jiancheng Lv","Nan Duan","Ming Zhou"],"demo_url":"","keywords":["machine comprehension","question generation","question-answering tasks","question task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.467","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3054","main.1485","main.319","main.1030","main.3140"],"title":"Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space","tldr":"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inferenc...","track":"Question Answering"},"forum":"main.1262","id":"main.1262","presentation_id":"38938874"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1580.png","content":{"abstract":"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.","authors":["Sewon Min","Julian Michael","Hannaneh Hajishirzi","Luke Zettlemoyer"],"demo_url":"","keywords":["open-domain answering","open-domain task","ambigqa","ambignq"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.466","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.319","main.2586","main.3186","main.2587","TACL.2049"],"title":"AmbigQA: Answering Ambiguous Open-domain Questions","tldr":"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task...","track":"Question Answering"},"forum":"main.1580","id":"main.1580","presentation_id":"38938941"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1622.png","content":{"abstract":"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \"Discern\", a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision \"yes/no/irrelevant\" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern.","authors":["Yifan Gao","Chien-Sheng Wu","Jingjing Li","Shafiq Joty","Steven C.H. Hoi","Caiming Xiong","Irwin King","Michael Lyu"],"demo_url":"","keywords":["document interpretation","dialog understanding","conversational reading","discern"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.191","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.485","main.2444","main.41","main.527","main.3010"],"title":"Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading","tldr":"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \"Discern\", a discourse-aware entailment reasoning network to strengthen the connection and enhance the understa...","track":"Question Answering"},"forum":"main.1622","id":"main.1622","presentation_id":"38938953"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1782.png","content":{"abstract":"We propose EXAMS \u2013 a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.","authors":["Momchil Hardalov","Todor Mihaylov","Dimitrina Zlatkova","Yoan Dinkov","Ivan Koychev","Preslav Nakov"],"demo_url":"","keywords":["cross-lingual answering","school answering","exams","fine-grained framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.438","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.871","main.2630","main.2278","main.1803","main.1379"],"title":"EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering","tldr":"We propose EXAMS \u2013 a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 ...","track":"Question Answering"},"forum":"main.1782","id":"main.1782","presentation_id":"38938985"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1788.png","content":{"abstract":"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.","authors":["Miyoung Ko","Jinhyuk Lee","Hyunjae Kim","Gangwoo Kim","Jaewoo Kang"],"demo_url":"","keywords":["extractive models","qa models","bidaf","de-biasing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.84","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1022","main.1837","main.2586","main.959","main.3183"],"title":"Look at the First Sentence: Position Bias in Question Answering","tldr":"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribu...","track":"Question Answering"},"forum":"main.1788","id":"main.1788","presentation_id":"38938988"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1837.png","content":{"abstract":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","authors":["Yuxiang Wu","Sebastian Riedel","Pasquale Minervini","Pontus Stenetorp"],"demo_url":"","keywords":["open-domain answering","adaptive computation","light-weight retriever","per-layer probability"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.244","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1022","main.3183","demo.54","main.3140","main.319"],"title":"Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering","tldr":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have show...","track":"Question Answering"},"forum":"main.1837","id":"main.1837","presentation_id":"38938996"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1877.png","content":{"abstract":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.","authors":["Siamak Shakeri","Cicero Nogueira dos Santos","Henghui Zhu","Patrick Ng","Feng Nan","Zhiguo Wang","Ramesh Nallapati","Bing Xiang"],"demo_url":"","keywords":["synthetic generation","end-to-end approach","transformer-based network","encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.439","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2635","main.3140","main.3054","main.2078","main.2586"],"title":"End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems","tldr":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the enco...","track":"Question Answering"},"forum":"main.1877","id":"main.1877","presentation_id":"38939002"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1975.png","content":{"abstract":"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.","authors":["Kunlong Chen","Weidi Xu","Xingyi Cheng","Zou Xiaochuan","Yuyu Zhang","Le Song","Taifeng Wang","Yuan Qi","Wei Chu"],"demo_url":"","keywords":["numerical reasoning","machine task","natural understanding","arithmetic computation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.549","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.574","main.2253","main.782","main.2761","main.151"],"title":"Question Directed Graph Attention Network for Numerical Reasoning over Text","tldr":"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we ...","track":"Question Answering"},"forum":"main.1975","id":"main.1975","presentation_id":"38939022"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2054.png","content":{"abstract":"Recent work by Clark et al. (2020) shows that transformers can act as \"soft theorem provers'' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers  (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for \"depth 5\", indicating significant scope for future work.","authors":["Swarnadeep Saha","Sayan Ghosh","Shashank Srivastava","Mohit Bansal"],"demo_url":"","keywords":["inference","qa generation","generalization","qa task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.9","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2415","TACL.2049","demo.86","main.607","main.3035"],"title":"PRover: Proof Generation for Interpretable Reasoning over Rules","tldr":"Recent work by Clark et al. (2020) shows that transformers can act as \"soft theorem provers'' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by pr...","track":"Question Answering"},"forum":"main.2054","id":"main.2054","presentation_id":"38939033"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2078.png","content":{"abstract":"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.","authors":["Rong Zhang","Revanth Gangi Reddy","Md Arafat Sultan","Vittorio Castelli","Anthony Ferritto","Radu Florian","Efsun Sarioglu Kayi","Salim Roukos","Avi Sil","Todd Ward"],"demo_url":"","keywords":["nlp tasks","fine-tuning","auxiliary tasks","lm transfer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.440","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2255","main.2476","main.2733","main.2087","main.1482"],"title":"Multi-Stage Pre-training for Low-Resource Domain Adaptation","tldr":"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning t...","track":"Question Answering"},"forum":"main.2078","id":"main.2078","presentation_id":"38939045"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2120.png","content":{"abstract":"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.","authors":["Jose Manuel Gomez-Perez","Ra\u00fal Ortega"],"demo_url":"","keywords":["textbook answering","machine comprehension","visual answering","transformer models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.441","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1030","main.319","main.449","TACL.2041","main.1022"],"title":"ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention","tldr":"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential ...","track":"Question Answering"},"forum":"main.2120","id":"main.2120","presentation_id":"38939054"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2228.png","content":{"abstract":"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC contains over 98K  explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: \"X is a Y\" AND \"Y has Z\" IMPLIES \"X has Z\"). We find that generalized chains maintain performance while also being more robust to certain perturbations.\\footnote{Code and datasets can be found at https://allenai.org/data/eqasc.","authors":["Harsh Jhamtani","Peter Clark"],"demo_url":"","keywords":["multihop qa","multihop","eqasc","qasc"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.10","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2258","main.959","main.3035","TACL.2049","main.2380"],"title":"Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering","tldr":"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in...","track":"Question Answering"},"forum":"main.2228","id":"main.2228","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2253.png","content":{"abstract":"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.","authors":["Jiangming Liu","Matt Gardner","Shay B. Cohen","Mirella Lapata"],"demo_url":"","keywords":["chained reasoning","black-box transformers","compositional model","neural networks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.245","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1975","demo.86","main.1191","main.531","main.607"],"title":"Multi-Step Inference for Reasoning Over Paragraphs","tldr":"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between thes...","track":"Question Answering"},"forum":"main.2253","id":"main.2253","presentation_id":"38939079"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2258.png","content":{"abstract":"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release  code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter","authors":["Priyanka Sen","Amir Saffari"],"demo_url":"","keywords":["question answering","reading comprehension","bert-based models","question variations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.190","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2586","main.2228","TACL.2049","main.319","main.2864"],"title":"What do Models Learn from Question Answering Datasets?","tldr":"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comp...","track":"Question Answering"},"forum":"main.2258","id":"main.2258","presentation_id":"38939080"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2337.png","content":{"abstract":"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.","authors":["Bernhard Kratzwald","Stefan Feuerriegel","Huan Sun"],"demo_url":"","keywords":["labeling","annotating questions","question qa","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.246","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2640","main.2739","main.1923","demo.54","main.1022"],"title":"Learning a Cost-Effective Annotation Policy for Question Answering","tldr":"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotat...","track":"Question Answering"},"forum":"main.2337","id":"main.2337","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2380.png","content":{"abstract":"Given questions regarding some prototypical situation --- such as Name something that people usually do before they leave the house for work? --- a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others.  This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show -- Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.","authors":["Michael Boratko","Xiang Li","Tim O'Gorman","Rajarshi Das","Dan Le","Andrew McCallum"],"demo_url":"","keywords":["generative task","artificial systems","common capabilities","model scores"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.85","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2228","TACL.2049","main.319","main.3186","main.2943"],"title":"ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning","tldr":"Given questions regarding some prototypical situation --- such as Name something that people usually do before they leave the house for work? --- a human can easily answer them via acquired experiences. There can be multiple right answers for such qu...","track":"Question Answering"},"forum":"main.2380","id":"main.2380","presentation_id":"38939102"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2586.png","content":{"abstract":"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.","authors":["Raul Puri","Ryan Spring","Mohammad Shoeybi","Mostofa Patwary","Bryan Catanzaro"],"demo_url":"","keywords":["question generation","squad task","em","data method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.468","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3140","main.2258","main.319","main.3186","main.2721"],"title":"Training Question Answering Models From Synthetic Data","tldr":"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer...","track":"Question Answering"},"forum":"main.2586","id":"main.2586","presentation_id":"38939150"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2587.png","content":{"abstract":"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.","authors":["Vladimir Karpukhin","Barlas Oguz","Sewon Min","Patrick Lewis","Ledell Wu","Sergey Edunov","Danqi Chen","Wen-tau Yih"],"demo_url":"","keywords":["open-domain answering","passage retrieval","retrieval","sparse models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.550","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.693","main.449","main.1022","demo.93"],"title":"Dense Passage Retrieval for Open-Domain Question Answering","tldr":"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically ...","track":"Question Answering"},"forum":"main.2587","id":"main.2587","presentation_id":"38939151"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2640.png","content":{"abstract":"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge.  We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.","authors":["Pratyay Banerjee","Chitta Baral"],"demo_url":"","keywords":["data annotation","knowledge learning","knowledge","self-supervised task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.11","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["demo.93","main.2635","main.3140","main.2337","main.2476"],"title":"Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering","tldr":"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on...","track":"Question Answering"},"forum":"main.2640","id":"main.2640","presentation_id":"38939163"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2721.png","content":{"abstract":"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.","authors":["Daniel Khashabi","Tushar Khot","Ashish Sabharwal"],"demo_url":"","keywords":["linguistic tasks","deep models","boolq","boolq models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.12","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2586","main.3054","main.2068","main.3140"],"title":"More Bang for Your Buck: Natural Perturbation for Robust Question Answering","tldr":"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so b...","track":"Question Answering"},"forum":"main.2721","id":"main.2721","presentation_id":"38939179"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2761.png","content":{"abstract":"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.","authors":["Yuwei Fang","Siqi Sun","Zhe Gan","Rohit Pillai","Shuohang Wang","Jingjing Liu"],"demo_url":"","keywords":["multi-hop answering","paragraph selection","supporting extraction","answer prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.710","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.782","main.574","main.1648","TACL.2121","main.158"],"title":"Hierarchical Graph Network for Multi-hop Question Answering","tldr":"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity ...","track":"Question Answering"},"forum":"main.2761","id":"main.2761","presentation_id":"38939187"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2943.png","content":{"abstract":"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named \\model, performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences \\emph{independently} of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.","authors":["Dirk Groeneveld","Tushar Khot","Mausam","Ashish Sabharwal"],"demo_url":"","keywords":["multi-hop answering","named recognition","graph-based reasoning","question decomposition"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.711","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.449","main.3517","demo.54","main.1032","main.210"],"title":"A Simple Yet Strong Pipeline for HotpotQA","tldr":"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. How...","track":"Question Answering"},"forum":"main.2943","id":"main.2943","presentation_id":"38939231"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2973.png","content":{"abstract":"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system's performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.","authors":["James Ferguson","Matt Gardner","Hannaneh Hajishirzi","Tushar Khot","Pradeep Dasigi"],"demo_url":"","keywords":["reading tasks","reading datasets","iirc","discrete reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.86","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.449","main.3186","main.928","main.2864","main.3529"],"title":"IIRC: A Dataset of Incomplete Information Reading Comprehension Questions","tldr":"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not eval...","track":"Question Answering"},"forum":"main.2973","id":"main.2973","presentation_id":"38939237"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3035.png","content":{"abstract":"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of \\emph{contrastive support sufficiency}, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments suggest that there hasn't been much progress in multi-hop QA in the reading comprehension setting. For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning (19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction.","authors":["Harsh Trivedi","Niranjan Balasubramanian","Tushar Khot","Ashish Sabharwal"],"demo_url":"","keywords":["multi-hop question-answering","automatic datasets","disconnected reasoning","multi-hop qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.712","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2049","main.2228","main.1648","main.2380","main.2943"],"title":"Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning","tldr":"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and def...","track":"Question Answering"},"forum":"main.3035","id":"main.3035","presentation_id":"38939255"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3140.png","content":{"abstract":"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown  to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.","authors":["Steven Rennie","Etienne Marcheret","Neil Mallinar","David Nahamoo","Vaibhava Goel"],"demo_url":"","keywords":["question-answering tasks","self-supervised tasks","word masking","sentence entailment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.87","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2586","main.319","main.3183","main.2635","main.1837"],"title":"Unsupervised Adaptation of Question Answering Systems via Generative Self-training","tldr":"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and senten...","track":"Question Answering"},"forum":"main.3140","id":"main.3140","presentation_id":"38939275"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.315.png","content":{"abstract":"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models,  CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.","authors":["Deming Ye","Yankai Lin","Jiaju Du","Zhenghao Liu","Peng Li","Maosong Sun","Zhiyuan Liu"],"demo_url":"","keywords":["downstream tasks","coreferential reasoning","common tasks","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.582","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1970","main.2476","main.1892","main.1130","main.3647"],"title":"Coreferential Reasoning Learning for Language Representation","tldr":"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most exist...","track":"Question Answering"},"forum":"main.315","id":"main.315","presentation_id":"38938680"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.319.png","content":{"abstract":"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.","authors":["Ethan Perez","Patrick Lewis","Wen-tau Yih","Kyunghyun Cho","Douwe Kiela"],"demo_url":"","keywords":["question qa","labeling questions","one-to-n transduction","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.713","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2586","main.449","main.3140","main.1580","main.2258"],"title":"Unsupervised Question Decomposition for Question Answering","tldr":"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to prod...","track":"Question Answering"},"forum":"main.319","id":"main.319","presentation_id":"38938683"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3672.png","content":{"abstract":"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.","authors":["Yuncheng Hua","Yuan-Fang Li","Gholamreza Haffari","Guilin Qi","Tongtong Wu"],"demo_url":"","keywords":["program induction","meta-training","cqa","neural approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.469","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3054","main.74","main.319","main.2838","main.2586"],"title":"Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning","tldr":"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, ha...","track":"Question Answering"},"forum":"main.3672","id":"main.3672","presentation_id":"38939385"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.390.png","content":{"abstract":"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.","authors":["Zhixing Tian","Yuanzhe Zhang","Kang Liu","Jun Zhao","Yantao Jia","Zhicheng Sheng"],"demo_url":"","keywords":["machine comprehension","graph network","graph gdin","gdin"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.247","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2758","main.2982","main.928","main.1129","main.605"],"title":"Scene Restoring for Narrative Machine Reading Comprehension","tldr":"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, whi...","track":"Question Answering"},"forum":"main.390","id":"main.390","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.449.png","content":{"abstract":"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.","authors":["Elad Segal","Avia Efrat","Mor Shoham","Amir Globerson","Jonathan Berant"],"demo_url":"","keywords":["reading comprehension","rc","learning problem","sequence problem"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.248","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.319","main.2973","main.1022","main.2943","main.2586"],"title":"A Simple and Effective Model for Answering Multi-span Questions","tldr":"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, fo...","track":"Question Answering"},"forum":"main.449","id":"main.449","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.531.png","content":{"abstract":"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.","authors":["Jin Dong","Marc-Antoine Rondeau","William L. Hamilton"],"demo_url":"","keywords":["relational reasoning","reasoning task","cross-modal transfer","text-based systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.551","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.666","main.1648","main.923","main.1159","main.1231"],"title":"Distilling Structured Knowledge for Text-Based Relational Reasoning","tldr":"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap be...","track":"Question Answering"},"forum":"main.531","id":"main.531","presentation_id":"38938727"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.574.png","content":{"abstract":"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.","authors":["Chen Zheng","Parisa Kordjamshidi"],"demo_url":"","keywords":["qa","graph network","semantic sub-graphs","graph encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.714","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1648","main.2761","main.782","main.158","main.237"],"title":"SRLGRN: Semantic Role Labeling Graph Reasoning Network","tldr":"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supportin...","track":"Question Answering"},"forum":"main.574","id":"main.574","presentation_id":"38938731"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.595.png","content":{"abstract":"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.","authors":["Johannes Bjerva","Nikita Bhutani","Behzad Golshan","Wang-Chiew Tan","Isabelle Augenstein"],"demo_url":"","keywords":["sentiment analysis","word-sense disambiguation","question qa","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.442","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1023","main.3186","main.1675","main.2973","main.1540"],"title":"SubjQA: A Dataset for Subjectivity and Review Comprehension","tldr":"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect...","track":"Question Answering"},"forum":"main.595","id":"main.595","presentation_id":"38938734"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.782.png","content":{"abstract":"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for textual multi-hop reasoning. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.","authors":["Nan Shao","Yiming Cui","Ting Liu","Shijin Wang","Guoping Hu"],"demo_url":"","keywords":["nlp areas","textual reasoning","graph networks","hotpotqa"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.583","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2761","main.574","main.1010","TACL.2121","main.1648"],"title":"Is Graph Structure Necessary for Multi-hop Question Answering?","tldr":"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop r...","track":"Question Answering"},"forum":"main.782","id":"main.782","presentation_id":"38938772"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.928.png","content":{"abstract":"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as ``what happened before/after [some event]?'' We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.","authors":["Qiang Ning","Hao Wu","Rujun Han","Nanyun Peng","Matt Gardner","Dan Roth"],"demo_url":"","keywords":["machine benchmarks","torque","roberta-large","temporal relationships"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.88","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2973","main.3186","main.607","demo.59","main.449"],"title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions","tldr":"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have p...","track":"Question Answering"},"forum":"main.928","id":"main.928","presentation_id":"38938807"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2041.png","content":{"abstract":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.","authors":["Alon Talmor","Yanai Elazar","Yoav Goldberg","Jonathan Berant"],"demo_url":"","keywords":["symbolic tasks","reasoning tasks","zero-shot evaluation","pre-training"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1130","main.74","TACL.2411","main.2838"],"title":"oLMpics - On what Language Model Pre-training Captures","tldr":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited an...","track":"Question Answering"},"forum":"TACL.2041","id":"TACL.2041","presentation_id":"38939400"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2049.png","content":{"abstract":"Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning\u2014two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of \u201chops\u201d in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.","authors":["Kyle Richardson","Ashish Sabharwal"],"demo_url":"","keywords":["knowledge challenges","benchmark tasks","diagnostic tasks","taxonomic reasoning"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3035","main.2258","main.41","main.2228","main.2380"],"title":"What Does My QA Model Know? Devising Controlled Probes using Expert","tldr":"Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing wheth...","track":"Question Answering"},"forum":"TACL.2049","id":"TACL.2049","presentation_id":"38939402"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2129.png","content":{"abstract":"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model. We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself (41.0F1).","authors":["Max Bartolo","Alastair Roberts","Johannes Welbl","Sebastian Riedel","Pontus Stenetorp"],"demo_url":"","keywords":["annotation methodology","annotation process","training","rc models"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2313","main.1923","main.359","main.3183","TACL.2389"],"title":"Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension","tldr":"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, suc...","track":"Question Answering"},"forum":"TACL.2129","id":"TACL.2129","presentation_id":"38939410"}]
