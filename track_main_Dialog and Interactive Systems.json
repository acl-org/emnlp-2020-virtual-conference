[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1006.png","content":{"abstract":"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping  response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.","authors":["Xueliang Zhao","Wei Wu","Can Xu","Chongyang Tao","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["knowledge-grounded generation","equipping","knowledge selection","response generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.272","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1201","main.689","main.1654","main.1846","main.128"],"title":"Knowledge-Grounded Dialogue Generation with Pre-trained Language Models","tldr":"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping  response generation defined by a pre-trained language model with a knowled...","track":"Dialog and Interactive Systems"},"forum":"main.1006","id":"main.1006","presentation_id":"38938823"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1009.png","content":{"abstract":"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.","authors":["Hung Le","Doyen Sahoo","Nancy Chen","Steven C.H. Hoi"],"demo_url":"","keywords":["video-grounded dialogues","high-resolution queries","video setting","bi-directional learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.145","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2839","main.2927","main.1113","main.355","main.1085"],"title":"BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues","tldr":"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over mu...","track":"Dialog and Interactive Systems"},"forum":"main.1009","id":"main.1009","presentation_id":"38938824"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1012.png","content":{"abstract":"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose \"UniConv\" - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.","authors":["Hung Le","Doyen Sahoo","Chenghao Liu","Nancy Chen","Steven C.H. Hoi"],"demo_url":"","keywords":["multi-domain dialogues","tracking states","end-to-end systems","dialogue tracking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.146","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1702","main.1201","main.1846","TACL.2143","main.2209"],"title":"UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues","tldr":"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states...","track":"Dialog and Interactive Systems"},"forum":"main.1012","id":"main.1012","presentation_id":"38938827"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1140.png","content":{"abstract":"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.","authors":["Shiquan Yang","Rui Zhang","Sarah Erfani"],"demo_url":"","keywords":["representation graphs","end-to-end systems","learning framework","recurrent architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.147","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2143","main.787","main.215","main.2141","main.1846"],"title":"GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems","tldr":"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; t...","track":"Dialog and Interactive Systems"},"forum":"main.1140","id":"main.1140","presentation_id":"38938852"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1179.png","content":{"abstract":"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which also set state-of-the-art in ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.","authors":["Armen Aghajanyan","Jean Maillard","Akshat Shrivastava","Keith Diedrick","Michael Haeger","Haoran Li","Yashar Mehdad","Veselin Stoyanov","Anuj Kumar","Mike Lewis","Sonal Gupta"],"demo_url":"","keywords":["semantic parsing","dialog challenges","session-based parsing","atis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.408","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1561","main.1957","TACL.2143","main.876","main.850"],"title":"Conversational Semantic Parsing","tldr":"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resoluti...","track":"Dialog and Interactive Systems"},"forum":"main.1179","id":"main.1179","presentation_id":"38938856"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1201.png","content":{"abstract":"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.","authors":["Chien-Sheng Wu","Steven C.H. Hoi","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["language modeling","pre-training","response task","task-oriented applications"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.66","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1654","main.215","main.1846","main.128","main.3393"],"title":"TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue","tldr":"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue dataset...","track":"Dialog and Interactive Systems"},"forum":"main.1201","id":"main.1201","presentation_id":"38938861"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1219.png","content":{"abstract":"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional'' BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.","authors":["Brielen Madureira","David Schlangen"],"demo_url":"","keywords":["nlp","interactive systems","language encoders","bidirectional lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.26","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2491","main.471","TACL.2041","main.1986","main.130"],"title":"Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU","tldr":"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and b...","track":"Dialog and Interactive Systems"},"forum":"main.1219","id":"main.1219","presentation_id":"38938866"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.128.png","content":{"abstract":"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.","authors":["Liang Qiu","Yizhou Zhao","Weiyan Shi","Yuan Liang","Feng Shi","Tao Yuan","Zhou Yu","Song-Chun Zhu"],"demo_url":"","keywords":["inducing representation","computational linguistics","dialogue design","discourse analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.148","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1654","main.3179","main.1201","main.2164","main.215"],"title":"Structured Attention for Unsupervised Dialogue Structure Induction","tldr":"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be...","track":"Dialog and Interactive Systems"},"forum":"main.128","id":"main.128","presentation_id":"38938654"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1389.png","content":{"abstract":"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.","authors":["Stefan Larson","Anthony Zheng","Anish Mahendran","Rishi Tekriwal","Adrian Cheung","Eric Guldan","Kevin Leach","Jonathan K. Kummerfeld"],"demo_url":"","keywords":["dialog tasks","intent classification","slot-filling","robust models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.650","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1923","main.345","main.2068","main.2739","main.2733"],"title":"Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness","tldr":"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively const...","track":"Dialog and Interactive Systems"},"forum":"main.1389","id":"main.1389","presentation_id":"38938895"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1522.png","content":{"abstract":"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances\u2019 logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.","authors":["Changzhen Ji","Xin Zhou","Yating Zhang","Xiaozhong Liu","Changlong Sun","Conghui Zhu","Tiejun Zhao"],"demo_url":"","keywords":["dialogue generation","model training","utterance generation","court debate"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.149","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2209","main.215","main.128","main.645","main.699"],"title":"Cross Copy Network for Dialogue Generation","tldr":"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accura...","track":"Dialog and Interactive Systems"},"forum":"main.1522","id":"main.1522","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1561.png","content":{"abstract":"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to ~20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.","authors":["Jianpeng Cheng","Devang Agrawal","H\u00e9ctor Mart\u00ednez Alonso","Shruti Bhargava","Joris Driesen","Federico Flego","Dain Kaplan","Dimitri Kartsaklis","Lin Li","Dhivya Piraviperumal","Jason D. Williams","Hong Yu","Diarmuid \u00d3 S\u00e9aghdha","Anders Johannsen"],"demo_url":"","keywords":["dialog tracking","semantic task","dst","hierarchical representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.651","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1179","main.689","TACL.2143","main.2209","main.977"],"title":"Conversational Semantic Parsing for Dialog State Tracking","tldr":"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositio...","track":"Dialog and Interactive Systems"},"forum":"main.1561","id":"main.1561","presentation_id":"38938934"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1606.png","content":{"abstract":"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.","authors":["Lishan Huang","Zheng Ye","Jinghui Qin","Liang Lin","Xiaodan Liang"],"demo_url":"","keywords":["automatically coherence","open-domain systems","automatic evaluation","grade"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.742","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1702","TACL.2143","main.128","main.1140","main.2141"],"title":"GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems","tldr":"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without ex...","track":"Dialog and Interactive Systems"},"forum":"main.1606","id":"main.1606","presentation_id":"38938945"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.165.png","content":{"abstract":"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.","authors":["Peixiang Zhong","Chen Zhang","Hao Wang","Yong Liu","Chunyan Miao"],"demo_url":"","keywords":["persona-based conversations","empathetic responding","empathetic models","cobert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.531","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.476","main.2707","main.3072","main.2561","main.3352"],"title":"Towards Persona-Based Empathetic Conversational Models","tldr":"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empi...","track":"Dialog and Interactive Systems"},"forum":"main.165","id":"main.165","presentation_id":"38938660"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1654.png","content":{"abstract":"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.","authors":["Chien-Sheng Wu","Caiming Xiong"],"demo_url":"","keywords":["task-oriented tasks","supervised probe","unsupervised probe","unsupervised aspect"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.409","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1201","main.128","main.215","main.2141","main.1006"],"title":"Probing Task-Oriented Dialogue Representation from Language Models","tldr":"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsuperv...","track":"Dialog and Interactive Systems"},"forum":"main.1654","id":"main.1654","presentation_id":"38938961"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1700.png","content":{"abstract":"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.","authors":["Xiang Gao","Yizhe Zhang","Michel Galley","Chris Brockett","Bill Dolan"],"demo_url":"","keywords":["feedback prediction","ranking problem","predicting feedback","open-domain models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.28","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3179","main.317","main.478","main.2410","main.2164"],"title":"Dialogue Response Ranking Training with Large-Scale Human Feedback Data","tldr":"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasin...","track":"Dialog and Interactive Systems"},"forum":"main.1700","id":"main.1700","presentation_id":"38938970"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1702.png","content":{"abstract":"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.","authors":["Jun Quan","Shian Zhang","Qian Cao","Zizhong Li","Deyi Xiong"],"demo_url":"","keywords":["task-oriented modeling","dialogue tasks","natural understanding","intent detection"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.67","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1012","main.1201","TACL.2143","main.1606","main.1846"],"title":"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling","tldr":"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contai...","track":"Dialog and Interactive Systems"},"forum":"main.1702","id":"main.1702","presentation_id":"38938971"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1797.png","content":{"abstract":"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.","authors":["Bodhisattwa Prasad Majumder","Harsh Jhamtani","Taylor Berg-Kirkpatrick","Julian McAuley"],"demo_url":"","keywords":["persona-consistent generation","persona-grounded models","dialog models","fine-grained personas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.739","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.419","main.2839","main.1972","main.787","main.128"],"title":"Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions","tldr":"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply l...","track":"Dialog and Interactive Systems"},"forum":"main.1797","id":"main.1797","presentation_id":"38938989"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1846.png","content":{"abstract":"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to \"carryover'' the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\\% training data, and 3) Lev greatly improves the inference efficiency.","authors":["Zhaojiang Lin","Andrea Madotto","Genta Indra Winata","Pascale Fung"],"demo_url":"","keywords":["dialogue tracking","dialogue generation","end-to-end generation","minimalist learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.273","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2143","main.1201","main.478","main.1012","main.215"],"title":"MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems","tldr":"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, w...","track":"Dialog and Interactive Systems"},"forum":"main.1846","id":"main.1846","presentation_id":"38938997"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2042.png","content":{"abstract":"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants. The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation. In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on  Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems. The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model. In those cases, a significant number of information leaking utterances can be detected by our models with high precision.","authors":["Qiongkai Xu","Lizhen Qu","Zeyu Gao","Gholamreza Haffari"],"demo_url":"","keywords":["leakage information","detection task","alignment problem","dataset persona-leakage"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.532","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.916","main.1863","main.2281","main.485","main.834"],"title":"Personal Information Leakage Detection in Conversations","tldr":"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy co...","track":"Dialog and Interactive Systems"},"forum":"main.2042","id":"main.2042","presentation_id":"38939030"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2050.png","content":{"abstract":"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.","authors":["Weishi Wang","Steven C.H. Hoi","Shafiq Joty"],"demo_url":"","keywords":["response selection","dynamic task","encoding","dynamic disentanglement"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.533","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1863","main.645","main.215","main.1201","main.916"],"title":"Response Selection for Multi-Party Conversations with Dynamic Topic Tracking","tldr":"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation ...","track":"Dialog and Interactive Systems"},"forum":"main.2050","id":"main.2050","presentation_id":"38939032"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2066.png","content":{"abstract":"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes. The classification is carried out by mapping text embeddings to the word graph embeddings of the classes. Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved. In particular, using the recently-released \\emph{Larson dataset}, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.","authors":["Paulo Cavalin","Victor Henrique Alves Ribeiro","Ana Appel","Claudio Pinhanez"],"demo_url":"","keywords":["intent classification","inverse task","classification","detection examples"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.324","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1611","main.1159","main.3292","main.1935"],"title":"Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes","tldr":"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The app...","track":"Dialog and Interactive Systems"},"forum":"main.2066","id":"main.2066","presentation_id":"38939039"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.214.png","content":{"abstract":"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.","authors":["Qian Liu","Bei Chen","Jian-Guang Lou","Bin Zhou","Dongmei Zhang"],"demo_url":"","keywords":["incomplete rewriting","machine task","semantic task","inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.227","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.891","main.1339","main.3257","main.2635","main.689"],"title":"Incomplete Utterance Rewriting as Semantic Segmentation","tldr":"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a no...","track":"Dialog and Interactive Systems"},"forum":"main.214","id":"main.214","presentation_id":"38938666"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2141.png","content":{"abstract":"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.","authors":["Song Feng","Hui Wan","Chulaka Gunasekara","Siva Patel","Sachindra Joshi","Luis Lastras"],"demo_url":"","keywords":["information-seeking conversations","dialogue tasks","dialogue flows","content elements"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.652","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1654","main.128","TACL.2143","main.1201","main.1702"],"title":"doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset","tldr":"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that c...","track":"Dialog and Interactive Systems"},"forum":"main.2141","id":"main.2141","presentation_id":"38939059"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.215.png","content":{"abstract":"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.","authors":["Qi Jia","Yizhu Liu","Siyu Ren","Kenny Zhu","Haifeng Tang"],"demo_url":"","keywords":["multi-turn selection","dialogue understanding","dialogue agents","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.150","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2209","main.1201","main.689","main.1522","main.128"],"title":"Multi-turn Response Selection using Dialogue Dependency Relations","tldr":"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the...","track":"Dialog and Interactive Systems"},"forum":"main.215","id":"main.215","presentation_id":"38938667"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2164.png","content":{"abstract":"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.","authors":["Reina Akama","Sho Yokoi","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["response generation","neural agents","scoring method","quality pairs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.68","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.128","main.478","main.1654","main.215","main.2141"],"title":"Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness","tldr":"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for sc...","track":"Dialog and Interactive Systems"},"forum":"main.2164","id":"main.2164","presentation_id":"38939062"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2209.png","content":{"abstract":"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.","authors":["Junfan Chen","Richong Zhang","Yongyi Mao","Jie Xu"],"demo_url":"","keywords":["multidomain models","mdst","parallel networks","pin"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.151","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.215","main.1522","main.689","main.1012","main.128"],"title":"Parallel Interactive Networks for Multi-Domain Dialogue State Generation","tldr":"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependenci...","track":"Dialog and Interactive Systems"},"forum":"main.2209","id":"main.2209","presentation_id":"38939069"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2212.png","content":{"abstract":"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and  their combination shows further improvement.","authors":["Xiuyi Chen","Fandong Meng","Peng Li","Feilong Chen","Shuang Xu","Bo Xu","Jie Zhou"],"demo_url":"","keywords":["knowledge selection","knowledge-grounded dialogue","dialogue generation","inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.275","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.787","main.1006","main.1846","main.3179","main.1522"],"title":"Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation","tldr":"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the dive...","track":"Dialog and Interactive Systems"},"forum":"main.2212","id":"main.2212","presentation_id":"38939070"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2281.png","content":{"abstract":"The lack  of  time  efficient  and  reliable  evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations  that  require  humans  to  converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and  tend  to  yield  low  quality  results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces  human-bot  conversations  with  conversations between bots.  Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot\u2019s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work  allows  for  frequent  evaluations  of  chatbots during their evaluation cycle.  We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work.  The framework is released asa ready-to-use tool.","authors":["Jan Deriu","Don Tuggener","Pius von D\u00e4niken","Jon Ander Campos","Alvaro Rodrigo","Thiziri Belkacem","Aitor Soroa","Eneko Agirre","Mark Cieliebak"],"demo_url":"","keywords":["evalu-ation methods","conversational systems","chat bots","spot bot"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.326","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1201","main.645","main.485","main.419"],"title":"Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems","tldr":"The lack  of  time  efficient  and  reliable  evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations  that  require  humans  to  converse with chat bots are time and cost intensive, put high cogni...","track":"Dialog and Interactive Systems"},"forum":"main.2281","id":"main.2281","presentation_id":"38939086"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.233.png","content":{"abstract":"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build  large-scale medical dialogue datasets -- MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System","authors":["Guangtao Zeng","Wenmian Yang","Zeqian Ju","Yue Yang","Sicheng Wang","Ruisi Zhang","Meng Zhou","Jiaqi Zeng","Xiangyu Dong","Ruoyu Zhang","Hongchao Fang","Penghui Zhu","Shu Chen","Pengtao Xie"],"demo_url":"","keywords":["telemedicine","transformer","low-resource tasks","medical tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.743","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.478","main.1702","main.1846","main.1201","main.1522"],"title":"MedDialog: Large-scale Medical Dialogue Datasets","tldr":"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we b...","track":"Dialog and Interactive Systems"},"forum":"main.233","id":"main.233","presentation_id":"38938668"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2410.png","content":{"abstract":"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.  A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.  We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.  We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.","authors":["Natasha Jaques","Judy Hanwen Shen","Asma Ghandeharioun","Craig Ferguson","Agata Lapedriza","Noah Jones","Shixiang Gu","Rosalind Picard"],"demo_url":"","keywords":["dialog model","rl policy","rl","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.327","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1700","main.954","main.2839","main.2583","main.1201"],"title":"Human-centric dialog training via offline reinforcement learning","tldr":"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended co...","track":"Dialog and Interactive Systems"},"forum":"main.2410","id":"main.2410","presentation_id":"38939109"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2444.png","content":{"abstract":"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and information elicitation in such settings, but has been limited to manual review of small corpora. We introduce **Interview**---a large-scale (105K conversations) media dialog dataset collected from news interview transcripts---which allows us to investigate such patterns at scale. We present a dialog model that leverages external knowledge as well as dialog acts via auxiliary losses and demonstrate that our model quantitatively and qualitatively outperforms strong discourse-agnostic baselines for dialog modeling---generating more specific and topical responses in interview-style conversations.","authors":["Bodhisattwa Prasad Majumder","Shuyang Li","Jianmo Ni","Julian McAuley"],"demo_url":"","keywords":["large-scale discourse","generative turns","interrogative patterns","modes persuasion"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.653","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.916","main.1622","main.527","main.128","main.2141"],"title":"Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding","tldr":"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understan...","track":"Dialog and Interactive Systems"},"forum":"main.2444","id":"main.2444","presentation_id":"38939122"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2583.png","content":{"abstract":"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of \\algo in the language-drift translation game.","authors":["Yuchen Lu","Soumye Singhal","Florian Strub","Olivier Pietquin","Aaron Courville"],"demo_url":"","keywords":["language drift","language-drift game","language models","word-based agents"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.325","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2389","main.74","main.128","main.1892","main.2851"],"title":"Supervised Seeded Iterated Learning for Interactive Language Learning","tldr":"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. ...","track":"Dialog and Interactive Systems"},"forum":"main.2583","id":"main.2583","presentation_id":"38939148"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2612.png","content":{"abstract":"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories.","authors":["Shirley Anugrah Hayati","Dongyeop Kang","Qingxiaoyang Zhu","Weiyan Shi","Zhou Yu"],"demo_url":"","keywords":["recommendation dialogs","movie recommendation","annotation scheme","automatic evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.654","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.1797","main.916","main.787","main.2444"],"title":"INSPIRED: Toward Sociable Recommendation Dialog Systems","tldr":"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with...","track":"Dialog and Interactive Systems"},"forum":"main.2612","id":"main.2612","presentation_id":"38939155"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2641.png","content":{"abstract":"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes  intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.","authors":["Weijia Xu","Batool Haider","Saab Mansour"],"demo_url":"","keywords":["natural understanding","natural","nlu","goal-oriented systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.410","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3046","main.870","main.1503","main.852"],"title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU","tldr":"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes  intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label p...","track":"Dialog and Interactive Systems"},"forum":"main.2641","id":"main.2641","presentation_id":"38939164"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2927.png","content":{"abstract":"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents' verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.","authors":["Hisashi Kamezawa","Noriki Nishida","Nobuyuki Shimizu","Takashi Miyazaki","Hideki Nakayama"],"demo_url":"","keywords":["real-world dialogue","social interactions","production responses","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.267","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1009","main.373","main.1201","main.647","main.128"],"title":"A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses","tldr":"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In th...","track":"Dialog and Interactive Systems"},"forum":"main.2927","id":"main.2927","presentation_id":"38939228"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2975.png","content":{"abstract":"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.","authors":["Qingfu Zhu","Wei-Nan Zhang","Ting Liu","William Yang Wang"],"demo_url":"","keywords":["open-domain generation","data problem","training","counterfactual reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.276","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.318","main.1196","main.664","main.3179","main.3495"],"title":"Counterfactual Off-Policy Training for Neural Dialogue Generation","tldr":"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfact...","track":"Dialog and Interactive Systems"},"forum":"main.2975","id":"main.2975","presentation_id":"38939239"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3157.png","content":{"abstract":"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.","authors":["Shaoxiong Feng","Xuancheng Ren","Hongshen Chen","Bin Sun","Kan Li","Xu Sun"],"demo_url":"","keywords":["inference","generative systems","imitation framework","dialogue model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.534","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1522","main.2209","main.2058","main.3179","main.1006"],"title":"Regularizing Dialogue Generation by Imitating Implicit Scenarios","tldr":"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialo...","track":"Dialog and Interactive Systems"},"forum":"main.3157","id":"main.3157","presentation_id":"38939278"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.317.png","content":{"abstract":"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.","authors":["Zibo Lin","Deng Cai","Yan Wang","Xiaojiang Liu","Haitao Zheng","Shuming Shi"],"demo_url":"","keywords":["response selection","retrieval-based systems","learning-to-rank problem","learning-to-rank"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.741","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.699","main.1700","main.471","main.3183","main.3179"],"title":"The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection","tldr":"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each ...","track":"Dialog and Interactive Systems"},"forum":"main.317","id":"main.317","presentation_id":"38938681"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.318.png","content":{"abstract":"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","authors":["Haochen Liu","Wentao Wang","Yiqi Wang","Hui Liu","Zitao Liu","Jiliang Tang"],"demo_url":"","keywords":["nlp tasks","word embedding","dialogue systems","debiasing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.64","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.834","main.1522","main.3179","main.3157","main.128"],"title":"Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning","tldr":"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect ...","track":"Dialog and Interactive Systems"},"forum":"main.318","id":"main.318","presentation_id":"38938682"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3217.png","content":{"abstract":"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.","authors":["Jianguo Zhang","Kazuma Hashimoto","Wenhao Liu","Chien-Sheng Wu","Yao Wan","Philip Yu","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["intent detection","detecting intents","oos detection","large-scale task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.411","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2799","main.148","main.1834","main.2793","main.1032"],"title":"Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference","tldr":"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detectio...","track":"Dialog and Interactive Systems"},"forum":"main.3217","id":"main.3217","presentation_id":"38939288"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3257.png","content":{"abstract":"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package  containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3)  A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.","authors":["Emanuele Bastianelli","Andrea Vanzo","Pawel Swietojanski","Verena Rieser"],"demo_url":"","keywords":["end-user applications","entity labelling","spoken understanding","slurp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.588","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.891","main.1892","main.143","main.3353","main.214"],"title":"SLURP: A Spoken Language Understanding Resource Package","tldr":"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we rel...","track":"Dialog and Interactive Systems"},"forum":"main.3257","id":"main.3257","presentation_id":"38939295"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3318.png","content":{"abstract":"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.","authors":["Yichi Zhang","Zhijian Ou","Min Hu","Junlan Feng"],"demo_url":"","keywords":["user tracking","database query","task-oriented systems","end-to-end systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.740","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2070","main.3393","main.1846","main.1797","main.148"],"title":"A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning","tldr":"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at allevia...","track":"Dialog and Interactive Systems"},"forum":"main.3318","id":"main.3318","presentation_id":"38939308"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3393.png","content":{"abstract":"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of ``request\" carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.","authors":["Semih Yavuz","Kazuma Hashimoto","Wenhao Liu","Nitish Shirish Keskar","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["da tagging","da","da taggers","maskaugment"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.412","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1201","main.478","main.128","main.1846","main.1702"],"title":"Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging","tldr":"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of ``request\" carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one do...","track":"Dialog and Interactive Systems"},"forum":"main.3393","id":"main.3393","presentation_id":"38939326"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3495.png","content":{"abstract":"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based program, built to exploit these patterns, had comparative performance to that of the neural models. In this paper we share our findings about the four types of patterns in the ShARC corpus and how the neural models exploit them. Motivated by the above findings, we create and share a modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.","authors":["Nikhil Verma","Abhishek Sharma","Dhiraj Madan","Danish Contractor","Harshit Kumar","Sachindra Joshi"],"demo_url":"","keywords":["neural tasks","sharc task","sharc","heuristic-based program"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.589","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3353","main.1960","main.2763","main.2389","main.3470"],"title":"Neural Conversational QA: Learning to Reason vs Exploiting Patterns","tldr":"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/pa...","track":"Dialog and Interactive Systems"},"forum":"main.3495","id":"main.3495","presentation_id":"38939347"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3580.png","content":{"abstract":"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work","authors":["Hui Su","Xiaoyu Shen","Zhou Xiao","Zheng Zhang","Ernie Chang","Cheng Zhang","Cheng Niu","Jie Zhou"],"demo_url":"","keywords":["in-depth chat","intent prediction","knowledge retrieval","neural approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.535","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2839","main.3179","main.317","main.1797","main.699"],"title":"MovieChats: Chat like Humans in a Closed Domain","tldr":"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-gra...","track":"Dialog and Interactive Systems"},"forum":"main.3580","id":"main.3580","presentation_id":"38939370"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3593.png","content":{"abstract":"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.","authors":["Jing Lu","Vincent Ng"],"demo_url":"","keywords":["entity resolution"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.536","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1621","main.3647","main.883","main.2416","main.315"],"title":"Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art","tldr":"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing ...","track":"Dialog and Interactive Systems"},"forum":"main.3593","id":"main.3593","presentation_id":"38939372"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3621.png","content":{"abstract":"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77). In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.","authors":["Di Wu","Liang Ding","Fan Lu","Jian Xie"],"demo_url":"","keywords":["slot filling","intent detection","spoken system","joint detection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.152","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3257","main.3051","main.1225","main.1432","main.557"],"title":"SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling","tldr":"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel tw...","track":"Dialog and Interactive Systems"},"forum":"main.3621","id":"main.3621","presentation_id":"38939377"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.407.png","content":{"abstract":"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\\% of the target language training data, achieves comparable performance to the supervised training with all the training data.","authors":["Zihan Liu","Genta Indra Winata","Peng Xu","Zhaojiang Lin","Pascale Fung"],"demo_url":"","keywords":["spoken systems","cross-lingual task","few-shot setting","cross-lingual models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.587","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.74","main.3046","main.1892","main.522","main.852"],"title":"Cross-lingual Spoken Language Understanding with Regularized Representation Alignment","tldr":"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub...","track":"Dialog and Interactive Systems"},"forum":"main.407","id":"main.407","presentation_id":"38938703"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.419.png","content":{"abstract":"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.","authors":["Hyunwoo Kim","Byeongchang Kim","Gunhee Kim"],"demo_url":"","keywords":["training","dialogue agents","generative agent","persona-based agents"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.65","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1797","main.1846","main.128","main.1522","main.699"],"title":"Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness","tldr":"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining co...","track":"Dialog and Interactive Systems"},"forum":"main.419","id":"main.419","presentation_id":"38938705"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.478.png","content":{"abstract":"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.","authors":["Rongsheng Zhang","Yinhe Zheng","Jianzhi Shao","Xiaoxi Mao","Yadong Xi","Minlie Huang"],"demo_url":"","keywords":["collecting data","automatic evaluation","open-domain systems","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.277","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1846","main.3393","main.2164","TACL.2143","main.1012"],"title":"Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data","tldr":"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we p...","track":"Dialog and Interactive Systems"},"forum":"main.478","id":"main.478","presentation_id":"38938718"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.527.png","content":{"abstract":"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as ``liking'' messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user's prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a \\textsc{bert} content model by 13 mean reciprocal rank points.","authors":["Pedro Rodriguez","Paul Crook","Seungwhan Moon","Zhiguang Wang"],"demo_url":"","keywords":["open-ended learning","wizard-of-oz task","digital assistants","crowd-sourcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.655","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2444","main.485","main.916","main.1622","main.41"],"title":"Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity","tldr":"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as ``liking'' m...","track":"Dialog and Interactive Systems"},"forum":"main.527","id":"main.527","presentation_id":"38938726"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.664.png","content":{"abstract":"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.","authors":["Sihan Wang","Kaijie Zhou","Kunfeng Lai","Jianping Shen"],"demo_url":"","keywords":["task-completion learning","decision-time planning","simulated task","monte search"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.278","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1522","main.1846","TACL.2143","main.215","main.699"],"title":"Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network","tldr":"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and...","track":"Dialog and Interactive Systems"},"forum":"main.664","id":"main.664","presentation_id":"38938752"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.689.png","content":{"abstract":"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.","authors":["Kun Xu","Haochen Tan","Linfeng Song","Han Wu","Haisong Zhang","Linqi Song","Dong Yu"],"demo_url":"","keywords":["multi-turn rewriting","attentive models","semantic labeling","semantic"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.537","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.215","main.2209","main.1561","main.1006","main.1201"],"title":"Semantic Role Labeling Guided Multi-turn Dialogue ReWriter","tldr":"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior foc...","track":"Dialog and Interactive Systems"},"forum":"main.689","id":"main.689","presentation_id":"38938757"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.699.png","content":{"abstract":"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems.  In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.","authors":["Yufan Zhao","Can Xu","Wei Wu"],"demo_url":"","keywords":["multi-turn generation","response generation","word recovery","utterance recovery"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.279","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3179","main.1846","main.1522","main.1201","main.215"],"title":"Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks","tldr":"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the ...","track":"Dialog and Interactive Systems"},"forum":"main.699","id":"main.699","presentation_id":"38938759"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.787.png","content":{"abstract":"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.","authors":["Jaehun Jung","Bokyung Son","Sungwon Lyu"],"demo_url":"","keywords":["dialogue systems","knowledge problem","path traversal","real-world systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.280","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1140","main.2212","main.128","main.485","main.689"],"title":"AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue","tldr":"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path tra...","track":"Dialog and Interactive Systems"},"forum":"main.787","id":"main.787","presentation_id":"38938773"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.834.png","content":{"abstract":"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods---including the quantity of gendered words, a dialogue safety classifier, and human assessments---all of which show that our models generate less gendered, but equally engaging chit-chat responses.","authors":["Emily Dinan","Angela Fan","Adina Williams","Jack Urbanek","Douwe Kiela","Jason Weston"],"demo_url":"","keywords":["counterfactual augmentation","targeted collection","bias training","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.656","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.318","main.2444","main.1700","main.916","main.2072"],"title":"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation","tldr":"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative ch...","track":"Dialog and Interactive Systems"},"forum":"main.834","id":"main.834","presentation_id":"38938780"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.850.png","content":{"abstract":"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user\u2019s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).  In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.","authors":["Xilun Chen","Asish Ghoshal","Yashar Mehdad","Luke Zettlemoyer","Sonal Gupta"],"demo_url":"","keywords":["task-oriented parsing","low-resource adaptation","generalization","virtual assistants"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.413","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.891","main.1179","main.1061","main.148"],"title":"Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing","tldr":"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user\u2019s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully...","track":"Dialog and Interactive Systems"},"forum":"main.850","id":"main.850","presentation_id":"38938783"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.876.png","content":{"abstract":"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user. For example, for queries like 'ask my wife if she can pick up the kids' or 'remind me to take my pills', we need to rephrase the content to 'can you pick up the kids' and  'take your pills'. In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset  of 3000 pairs of original query and rephrased query. We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it. We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model","authors":["Arash Einolghozati","Anchit Gupta","Keith Diedrick","Sonal Gupta"],"demo_url":"","keywords":["rephrasing","messaging","virtual assistants","intent-slot tagging"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.414","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1179","main.210","main.2382","demo.54"],"title":"Sound Natural: Content Rephrasing in Dialog Systems","tldr":"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in ...","track":"Dialog and Interactive Systems"},"forum":"main.876","id":"main.876","presentation_id":"38938795"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.954.png","content":{"abstract":"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a \u201cTwo-Teacher One-Student\u201d learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.","authors":["Wanwei He","Min Yang","Rui Yan","Chengming Li","Ying Shen","Ruifeng Xu"],"demo_url":"","keywords":["task completion","generating responses","task-oriented dialogue","task-oriented systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.281","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1201","main.1522","main.699","main.128"],"title":"Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training","tldr":"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a \u201cTwo-Teacher One-Student\u201d l...","track":"Dialog and Interactive Systems"},"forum":"main.954","id":"main.954","presentation_id":"38938810"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.999.png","content":{"abstract":"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.","authors":["Haoyu Song","Yan Wang","Wei-Nan Zhang","Zhengyu Zhao","Ting Liu","Xiaojiang Liu"],"demo_url":"","keywords":["attribute consistency","profile identification","dialogue agents","key-value model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.539","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.689","main.2141","main.1654","main.485","main.2209"],"title":"Profile Consistency Identification for Open-domain Dialogue Agents","tldr":"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few effort...","track":"Dialog and Interactive Systems"},"forum":"main.999","id":"main.999","presentation_id":"38938821"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2143.png","content":{"abstract":"We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset and code for replicating experiments are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines. ","authors":["Jacob Andreas","John Bufe","David Burkett","Charles Chen","Josh Clausman","Jean Crawford","Kate Crim","Jordan DeLoach","Leah Dorner","Jason Eisner","Hao Fang","Alan Guo","David Hall","Kristin Hayes","Kellie Hill","Diana Ho","Wendy Iwaszuk","Smriti Jha","Dan Klein","Jayant Krishnamurthy","Theo Lanman","Percy Liang","Christopher Lin","Ilya Lintsbakh","Andy McGovern","Alexander Nisnevich","Adam Pauls","Brent Read","Dan Roth","Subhro Roy","Beth Short","Div Slomin","Ben Snyder","Stephon Striplin","Yu Su","Zachary Tellman","Sam Thomson","Andrei Vorobev","Izabela Witoszko","Jason Wolfe","Abby Wray","Yuchen Zhang","Alexander Zotov","Jesse Rusak","Dmitrij Petters"],"demo_url":"","keywords":["task-oriented dialogue","revision","dialogue agent","metacomputation operators"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1702","main.1140","main.1201","main.1012"],"title":"Task-Oriented Dialogue as Dataflow Synthesis","tldr":"We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and...","track":"Dialog and Interactive Systems"},"forum":"TACL.2143","id":"TACL.2143","presentation_id":"38939413"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2389.png","content":{"abstract":"There is an increasing focus on model-based dialog evaluation metrics such as ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign a high score to all relevant responses and a low score to all irrelevant responses. Ideally, such models should be trained using multiple relevant and irrelevant responses for any given context. However, no such data is publicly available, and hence existing models are usually trained using a single relevant response and multiple randomly selected responses from other contexts (random negatives). To allow for better training and robust evaluation of model-based metrics, we introduce the DailyDialog++ dataset, consisting of (i) five relevant responses for each context and (ii) five \\textit{adversarially crafted} irrelevant responses for each context. Using this dataset, we first show that even in the presence of multiple correct references, n-gram based metrics and embedding based metrics do not perform well at separating relevant responses from even random negatives. While model-based metrics perform better than n-gram and embedding based metrics on random negatives, their performance drops substantially when evaluated on adversarial examples. To check if large scale pretraining could help, we propose a new BERT-based evaluation metric called DEB, which is pretrained on 727M Reddit conversations and then finetuned on our dataset. DEB significantly outperforms existing models, showing better correlation with human judgements and better performance on random negatives (88.27% accuracy). However, its performance again drops substantially, when evaluated on adversarial responses, thereby highlighting that even large-scale pretrained evaluation models are not robust to the adversarial examples in our dataset. The dataset and code are publicly available. (Dataset: https://iitmnlp.github.io/DailyDialog-plusplus/ and Code: https://github.com/iitmnlp/Dialogue-Evaluation-with-BERT).","authors":["Ananya Sai","Akash Mohan Kumar","Siddhartha Arora","Mitesh Khapra"],"demo_url":"","keywords":["large pretraining","embedding metrics","n-gram metrics","deb"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3434","main.1923","main.2313","main.2914","TACL.2129"],"title":"Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining","tldr":"There is an increasing focus on model-based dialog evaluation metrics such as ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign a high score to all relevant responses and a low score to all irrelevant responses. Ideally,...","track":"Dialog and Interactive Systems"},"forum":"TACL.2389","id":"TACL.2389","presentation_id":"38939417"}]
