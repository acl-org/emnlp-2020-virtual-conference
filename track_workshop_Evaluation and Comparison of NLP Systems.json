[{"content":{"abstract":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete picture of the model\u2019s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed metrics address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model\u2019s confidence score assignments have an impact on the system\u2019s behavior. We describe four key benefits of our proposed metrics as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics\u2019 definitions; the remaining benefits, generalization, and functional consistency are demonstrated empirically.","authors":["Reda Yacouby","Dustin Axman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models","tldr":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete p...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.13","presentation_id":"38939710","rocketchat_channel":"paper-eval4nlp-13","speakers":"Reda Yacouby|Dustin Axman","title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models"},{"content":{"abstract":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need for better summary evaluation methods that go beyond conventional overlap-based metrics. Our typology is structured into two dimensions. First, the Mapping Dimension describes surface-level errors and provides insight into word-sequence transformation issues. Second, the Meaning Dimension describes issues related to interpretation and provides insight into breakdowns in truth, i.e., factual faithfulness to the original text. Comparative analysis revealed that two neural summarization systems leveraging pre-trained models have an advantage in decreasing grammaticality errors, but not necessarily factual errors. We also discuss the importance of ensuring that summary length and abstractiveness do not interfere with evaluating summary quality.","authors":["Klaus-Michael Lux","Maya Sappelli","Martha Larson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries","tldr":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need fo...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.15","presentation_id":"38939711","rocketchat_channel":"paper-eval4nlp-15","speakers":"Klaus-Michael Lux|Maya Sappelli|Martha Larson","title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries"},{"content":{"abstract":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons: (1) it requires that word embeddings be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for model selection in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.","authors":["Nathan Stringham","Mike Izbicki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Word Embeddings on Low-Resource Languages","tldr":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reaso...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.16","presentation_id":"38939712","rocketchat_channel":"paper-eval4nlp-16","speakers":"Nathan Stringham|Mike Izbicki","title":"Evaluating Word Embeddings on Low-Resource Languages"},{"content":{"abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","authors":["Hila Gonen","Kellie Webster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.180","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","tldr":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Whil...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.1663","presentation_id":"38940033","rocketchat_channel":"paper-eval4nlp-1663","speakers":"Hila Gonen|Kellie Webster","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations"},{"content":{"abstract":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with human quality ratings. As a solution, we propose crowdsourcing as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of summarization by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, BLEU, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.","authors":["Neslihan Iskender","Tim Polzehl","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation","tldr":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.18","presentation_id":"38939713","rocketchat_channel":"paper-eval4nlp-18","speakers":"Neslihan Iskender|Tim Polzehl|Sebastian M\u00f6ller","title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation"},{"content":{"abstract":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.","authors":["Wanzheng Zhu","Suma Bhat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GRUEN for Evaluating Linguistic Quality of Generated Text","tldr":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge th...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.183","presentation_id":"38940645","rocketchat_channel":"paper-eval4nlp-183","speakers":"Wanzheng Zhu|Suma Bhat","title":"GRUEN for Evaluating Linguistic Quality of Generated Text"},{"content":{"abstract":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document\u2019s text. We present evidence that BLANC scores have as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.","authors":["Oleg Vasilyev","Vedant Dharnidharka","John Bohannon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fill in the BLANC: Human-free quality estimation of document summaries","tldr":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measur...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.21","presentation_id":"38939714","rocketchat_channel":"paper-eval4nlp-21","speakers":"Oleg Vasilyev|Vedant Dharnidharka|John Bohannon","title":"Fill in the BLANC: Human-free quality estimation of document summaries"},{"content":{"abstract":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model\u2019s behavior, and ignores linguistic properties of words that may allow some mis-predicted tokens to be useful in practice. Furthermore, statistics directly tied to prediction accuracy (including perplexity) may be confounded by the Zipfian nature of written language, as the majority of the prediction attempts will occur with frequently-occurring types. A model\u2019s performance may vary greatly between high- and low-frequency words, which in practice could lead to failure modes such as repetitive and dull generated text being produced by a downstream consumer of a language model. To address this, we propose two new intrinsic evaluation measures within the framework of a simple word prediction task that are designed to give a more holistic picture of a language model\u2019s performance. We evaluate several commonly-used large English language models using our proposed metrics, and demonstrate that our approach reveals functional differences in performance between the models that are obscured by more traditional metrics.","authors":["Shiran Dudy","Steven Bedrick"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Some Words Worth More than Others?","tldr":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.22","presentation_id":"38939715","rocketchat_channel":"paper-eval4nlp-22","speakers":"Shiran Dudy|Steven Bedrick","title":"Are Some Words Worth More than Others?"},{"content":{"abstract":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.","authors":["Adam Poliak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A survey on Recognizing Textual Entailment as an NLP Evaluation","tldr":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.23","presentation_id":"38939716","rocketchat_channel":"paper-eval4nlp-23","speakers":"Adam Poliak","title":"A survey on Recognizing Textual Entailment as an NLP Evaluation"},{"content":{"abstract":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedical data, and a range of evaluation measures suited to the task. The approach is applied to two recent DWSI systems, thus demonstrating its relevance and providing an in-depth analysis of the models.","authors":["Ashjan Alsulaimani","Erwan Moreau","Carl Vogel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.284","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Evaluation Method for Diachronic Word Sense Induction","tldr":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2311","presentation_id":"38940034","rocketchat_channel":"paper-eval4nlp-2311","speakers":"Ashjan Alsulaimani|Erwan Moreau|Carl Vogel","title":"An Evaluation Method for Diachronic Word Sense Induction"},{"content":{"abstract":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.","authors":["Zorik Gekhman","Roee Aharoni","Genady Beryozkin","Markus Freitag","Wolfgang Macherey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.287","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KoBE: Knowledge-Based Machine Translation Evaluation","tldr":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a la...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2378","presentation_id":"38940035","rocketchat_channel":"paper-eval4nlp-2378","speakers":"Zorik Gekhman|Roee Aharoni|Genady Beryozkin|Markus Freitag|Wolfgang Macherey","title":"KoBE: Knowledge-Based Machine Translation Evaluation"},{"content":{"abstract":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that still achieve good performance. We present several reproduction studies of intrinsic evaluation tasks that evaluate non-contextual word representations in multiple languages. Furthermore, we present 50-8-8, a new data set for the outlier identification task, which avoids limitations of the original data set, such as ambiguous words, infrequent words, and multi-word tokens, while increasing the number of test cases. The data set is expanded to contain semantic and syntactic tests and is multilingual (English, German, and Italian). We provide an in-depth analysis of word embedding models with a range of hyper-parameters. Our analysis shows the suitability of different models and hyper-parameters for different tasks and the greater difficulty of representing German and Italian languages.","authors":["Jesper Brink Andersen","Mikkel Bak Bertelsen","Mikkel H\u00f8rby Schou","Manuel R. Ciosici","Ira Assent"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations","tldr":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.25","presentation_id":"38939717","rocketchat_channel":"paper-eval4nlp-25","speakers":"Jesper Brink Andersen|Mikkel Bak Bertelsen|Mikkel H\u00f8rby Schou|Manuel R. Ciosici|Ira Assent","title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations"},{"content":{"abstract":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways (i.e. abstractive and extractive) on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in https://github.com/zide05/CDEvalSumm.","authors":["Yiran Chen","Pengfei Liu","Ming Zhong","Zi-Yi Dou","Danqing Wang","Xipeng Qiu","Xuanjing Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.329","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems","tldr":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2740","presentation_id":"38940036","rocketchat_channel":"paper-eval4nlp-2740","speakers":"Yiran Chen|Pengfei Liu|Ming Zhong|Zi-Yi Dou|Danqing Wang|Xipeng Qiu|Xuanjing Huang","title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems"},{"content":{"abstract":"","authors":["Guy Tevet","Jonathan Berant"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Evaluation of Diversity in Natural Language Generation","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.28","presentation_id":"38940785","rocketchat_channel":"paper-eval4nlp-28","speakers":"Guy Tevet|Jonathan Berant","title":"Evaluating the Evaluation of Diversity in Natural Language Generation"},{"content":{"abstract":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since it allows the assessment of both models and the prompts used to evaluate them. We use IRT to efficiently assess chatbots, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.","authors":["Jo\u00e3o Sedoc","Lyle Ungar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Item Response Theory for Efficient Human Evaluation of Chatbots","tldr":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, usin...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.29","presentation_id":"38939718","rocketchat_channel":"paper-eval4nlp-29","speakers":"Jo\u00e3o Sedoc|Lyle Ungar","title":"Item Response Theory for Efficient Human Evaluation of Chatbots"},{"content":{"abstract":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges don\u2019t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.","authors":["Rahul Jha","Keping Bi","Yang Li","Mahdi Pakdaman","Asli Celikyilmaz","Ivan Zhiboedov","Kieran McDonald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization","tldr":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarizatio...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3","presentation_id":"38939707","rocketchat_channel":"paper-eval4nlp-3","speakers":"Rahul Jha|Keping Bi|Yang Li|Mahdi Pakdaman|Asli Celikyilmaz|Ivan Zhiboedov|Kieran McDonald","title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization"},{"content":{"abstract":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the similarity score. Experimental results on three benchmark datasets show that our method correlates significantly better with human judgments than all existing metrics.","authors":["Hwanhee Lee","Seunghyun Yoon","Franck Dernoncourt","Doo Soon Kim","Trung Bui","Kyomin Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT","tldr":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic represent...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.30","presentation_id":"38939719","rocketchat_channel":"paper-eval4nlp-30","speakers":"Hwanhee Lee|Seunghyun Yoon|Franck Dernoncourt|Doo Soon Kim|Trung Bui|Kyomin Jung","title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT"},{"content":{"abstract":"","authors":["Asiye Tuba K\u00f6ksal","\u00d6zge Bozal","Emre Y\u00fcrekli","Gizem Gezici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3117","presentation_id":"38940037","rocketchat_channel":"paper-eval4nlp-3117","speakers":"Asiye Tuba K\u00f6ksal|\u00d6zge Bozal|Emre Y\u00fcrekli|Gizem Gezici","title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction"},{"content":{"abstract":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.","authors":["Harris Chan","Jamie Kiros","William Chan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.376","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels","tldr":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work,...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3148","presentation_id":"38940114","rocketchat_channel":"paper-eval4nlp-3148","speakers":"Harris Chan|Jamie Kiros|William Chan","title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels"},{"content":{"abstract":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and DBpedia facts having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in DBpedia. We in- vestigate whether\u2014and, if so, how\u2014a given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.","authors":["Kiril Gashteovski","Rainer Gemulla","Bhushan Kotnis","Sven Hertling","Christian Meilicke"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study","tldr":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and auto...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.34","presentation_id":"38939720","rocketchat_channel":"paper-eval4nlp-34","speakers":"Kiril Gashteovski|Rainer Gemulla|Bhushan Kotnis|Sven Hertling|Christian Meilicke","title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study"},{"content":{"abstract":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time, it has been argued that contextualized word representations exhibit sub-optimal statistical properties for encoding the true similarity between words or sentences. In this paper, we present two techniques for improving encoding representations for similarity metrics: a batch-mean centering strategy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERT-backbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks.","authors":["Xi Chen","Nan Ding","Tomer Levinboim","Radu Soricut"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance","tldr":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.35","presentation_id":"38939721","rocketchat_channel":"paper-eval4nlp-35","speakers":"Xi Chen|Nan Ding|Tomer Levinboim|Radu Soricut","title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance"},{"content":{"abstract":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper establishes a framework for addressing n-best evaluation by outlining three different questions one could consider when determining how one would define a \u2018good\u2019 n-best list and proposing evaluation measures for each question. The first and principal contribution is an evaluation measure that characterizes the translation quality of an entire n-best list by asking whether many of the valid translations are placed near the top of the list. The second is a measure that uses gold translations with preference annotations to ask to what degree systems can produce ranked lists in preference order. The third is a measure that rewards partial matches, evaluating the closeness of the many items in an n-best list to a set of many valid references. These three perspectives make clear that having access to many references can be useful when n-best evaluation is the goal.","authors":["Jacob Bremerman","Huda Khayrallah","Douglas Oard","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Evaluation of Machine Translation n-best Lists","tldr":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper es...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.36","presentation_id":"38939722","rocketchat_channel":"paper-eval4nlp-36","speakers":"Jacob Bremerman|Huda Khayrallah|Douglas Oard|Matt Post","title":"On the Evaluation of Machine Translation n-best Lists"},{"content":{"abstract":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as \u201cpsycholinguistic subjects\u201d and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA\u2019s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018a), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC.","authors":["Jingcheng Niu","Gerald Penn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Grammaticality and Language Modelling","tldr":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regard...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.37","presentation_id":"38939723","rocketchat_channel":"paper-eval4nlp-37","speakers":"Jingcheng Niu|Gerald Penn","title":"Grammaticality and Language Modelling"},{"content":{"abstract":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a Python-based tool for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a sentiment analysis and a patent classification task.","authors":["Hanna Wecker","Annemarie Friedrich","Heike Adel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation","tldr":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clu...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.5","presentation_id":"38939708","rocketchat_channel":"paper-eval4nlp-5","speakers":"Hanna Wecker|Annemarie Friedrich|Heike Adel","title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation"},{"content":{"abstract":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversity can be estimated using statistical measures such as perplexity, measuring language quality requires human evaluation. However, because human evaluation at scale is slow and expensive, it is used sparingly; it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for machine translation. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a kernel function. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that \u2013 on average \u2013 the quality estimation from a BLEU Neighbors model has a lower mean squared error and higher Spearman correlation with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art models on automatically grading essays, including models that have access to a gold-standard reference essay.","authors":["Kawin Ethayarajh","Dorsa Sadigh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation","tldr":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.7","presentation_id":"38939709","rocketchat_channel":"paper-eval4nlp-7","speakers":"Kawin Ethayarajh|Dorsa Sadigh","title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation"},{"content":{"abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","authors":["Daniel Deutsch","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","tldr":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the off...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.8","presentation_id":"38940784","rocketchat_channel":"paper-eval4nlp-8","speakers":"Daniel Deutsch|Dan Roth","title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics"},{"content":{"abstract":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better coverage of the space of valid translations and thereby improve its correlation with human judgments. Our experiments on the into-English language directions of the WMT19 metrics task (at both the system and sentence level) show that using paraphrased references does generally improve BLEU, and when it does, the more diverse the better. However, we also show that better results could be achieved if those paraphrases were to specifically target the parts of the space most relevant to the MT outputs being evaluated. Moreover, the gains remain slight even when human paraphrases are used, suggesting inherent limitations to BLEU\u2019s capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentence-level BLEU.","authors":["Rachel Bawden","Biao Zhang","Lisa Yankovskaya","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.82","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing","tldr":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better cove...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.815","presentation_id":"38940032","rocketchat_channel":"paper-eval4nlp-815","speakers":"Rachel Bawden|Biao Zhang|Lisa Yankovskaya|Andre T\u00e4ttar|Matt Post","title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing"}]
