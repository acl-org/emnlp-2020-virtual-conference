[{"content":{"abstract":"","authors":["Sayali Kulkarni","Shailee Jain","Mohammad Javad Hosseini","Jason Baldridge","Eugene Ie","Li Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Geocoding with multi-level loss for spatial language representation","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.11","presentation_id":"38940083","rocketchat_channel":"paper-splu2020-11","speakers":"Sayali Kulkarni|Shailee Jain|Mohammad Javad Hosseini|Jason Baldridge|Eugene Ie|Li Zhang","title":"Geocoding with multi-level loss for spatial language representation"},{"content":{"abstract":"","authors":["Roshanak Mirzaee","Hossein Rajaby Faghihi","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.12","presentation_id":"38940084","rocketchat_channel":"paper-splu2020-12","speakers":"Roshanak Mirzaee|Hossein Rajaby Faghihi|Parisa Kordjamshidi","title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning"},{"content":{"abstract":"","authors":["Yue Zhang","Quan Guo","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.13","presentation_id":"38940085","rocketchat_channel":"paper-splu2020-13","speakers":"Yue Zhang|Quan Guo|Parisa Kordjamshidi","title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations"},{"content":{"abstract":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent RMM where robots need to both ask and answer questions.","authors":["Homero Roman Roman","Yonatan Bisk","Jesse Thomason","Asli Celikyilmaz","Jianfeng Gao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.157","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RMM: A Recursive Mental Model for Dialogue Navigation","tldr":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and ask...","track":"Spatial Language Understanding"},"id":"WS-10.1453","presentation_id":"38940095","rocketchat_channel":"paper-splu2020-1453","speakers":"Homero Roman Roman|Yonatan Bisk|Jesse Thomason|Asli Celikyilmaz|Jianfeng Gao","title":"RMM: A Recursive Mental Model for Dialogue Navigation"},{"content":{"abstract":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","authors":["Tsung-Yen Yang","Andrew Lan","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.172","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Robust and Interpretable Grounding of Spatial References with Relation Networks","tldr":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for...","track":"Spatial Language Understanding"},"id":"WS-10.1595","presentation_id":"38940094","rocketchat_channel":"paper-splu2020-1595","speakers":"Tsung-Yen Yang|Andrew Lan|Karthik Narasimhan","title":"Robust and Interpretable Grounding of Spatial References with Relation Networks"},{"content":{"abstract":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.","authors":["Alberto Testoni","Claudio Greco","Tobias Bianchi","Mauricio Mazuecos","Agata Marcante","Luciana Benotti","Raffaella Bernardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"They are not all alike: answering different spatial questions requires different grounding strategies","tldr":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We b...","track":"Spatial Language Understanding"},"id":"WS-10.2","presentation_id":"38940076","rocketchat_channel":"paper-splu2020-2","speakers":"Alberto Testoni|Claudio Greco|Tobias Bianchi|Mauricio Mazuecos|Agata Marcante|Luciana Benotti|Raffaella Bernardi","title":"They are not all alike: answering different spatial questions requires different grounding strategies"},{"content":{"abstract":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ARRAMON. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language (English) instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work.","authors":["Hyounghun Kim","Abhaysinh Zala","Graham Burri","Hao Tan","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.348","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments","tldr":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We ...","track":"Spatial Language Understanding"},"id":"WS-10.2904","presentation_id":"38940093","rocketchat_channel":"paper-splu2020-2904","speakers":"Hyounghun Kim|Abhaysinh Zala|Graham Burri|Hao Tan|Mohit Bansal","title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments"},{"content":{"abstract":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while other features are more salient when assessing typicality. In this paper we explore the extent to which this is the case for English spatial prepositions and discuss the implications for pragmatic strategies and semantic models. We hypothesise that object-specific features \u2014 related to object properties and affordances \u2014 are more salient in categorisation, while geometric and physical relationships between objects are more salient in typicality judgements. In order to test this hypothesis we conducted a study using virtual environments to collect both category and typicality judgements in 3D scenes. Based on the collected data we cannot verify the hypothesis and conclude that object-specific features appear to be salient in both category and typicality judgements, further evidencing the need to include these types of features in semantic models.","authors":["Adam Richard-Bollans","Anthony Cohn","Luc\u00eda G\u00f3mez \u00c1lvarez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions","tldr":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while ot...","track":"Spatial Language Understanding"},"id":"WS-10.3","presentation_id":"38940077","rocketchat_channel":"paper-splu2020-3","speakers":"Adam Richard-Bollans|Anthony Cohn|Luc\u00eda G\u00f3mez \u00c1lvarez","title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions"},{"content":{"abstract":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Currently, the best-performing models are able to complete less than 1% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information, the starting location in the virtual environment, is incorporated, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases, suggesting contextualized language models may provide strong planning modules for grounded virtual agents.","authors":["Peter Jansen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.395","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions","tldr":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Curre...","track":"Spatial Language Understanding"},"id":"WS-10.3302","presentation_id":"38940098","rocketchat_channel":"paper-splu2020-3302","speakers":"Peter Jansen","title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions"},{"content":{"abstract":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our method with a user study, validating that our generated spatial arrangements align with human expectation.","authors":["Gorjan Radevski","Guillem Collell","Marie-Francine Moens","Tinne Tuytelaars"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.408","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Decoding Language Spatial Relations to 2D Spatial Arrangements","tldr":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-...","track":"Spatial Language Understanding"},"id":"WS-10.3382","presentation_id":"38940092","rocketchat_channel":"paper-splu2020-3382","speakers":"Gorjan Radevski|Guillem Collell|Marie-Francine Moens|Tinne Tuytelaars","title":"Decoding Language Spatial Relations to 2D Spatial Arrangements"},{"content":{"abstract":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with multiple visual features with different sizes of receptive fields, though the proper size of the receptive field of visual features intuitively varies depending on expressions. In this paper, we introduce a neural network architecture that modulates visual features with varying sizes of receptive field by linguistic features. We evaluate our architecture on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our architecture. Source code is available at https://github.com/Alab-NII/lcfp .","authors":["Taichi Iki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.420","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks","tldr":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models cons...","track":"Spatial Language Understanding"},"id":"WS-10.3466","presentation_id":"38940091","rocketchat_channel":"paper-splu2020-3466","speakers":"Taichi Iki|Akiko Aizawa","title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks"},{"content":{"abstract":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).","authors":["Hyeong Jin Shin","Jeong Yeon Park","Dae Bum Yuk","Jae Sung Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-based Spatial Information Extraction","tldr":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018)...","track":"Spatial Language Understanding"},"id":"WS-10.5","presentation_id":"38940078","rocketchat_channel":"paper-splu2020-5","speakers":"Hyeong Jin Shin|Jeong Yeon Park|Dae Bum Yuk|Jae Sung Lee","title":"BERT-based Spatial Information Extraction"},{"content":{"abstract":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.","authors":["Chao Xu","Emmanuelle-Anna Dietz Saldanha","Dagmar Gromann","Beihai Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Cognitively Motivated Approach to Spatial Information Extraction","tldr":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing s...","track":"Spatial Language Understanding"},"id":"WS-10.6","presentation_id":"38940079","rocketchat_channel":"paper-splu2020-6","speakers":"Chao Xu|Emmanuelle-Anna Dietz Saldanha|Dagmar Gromann|Beihai Zhou","title":"A Cognitively Motivated Approach to Spatial Information Extraction"},{"content":{"abstract":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model\u2019s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues.","authors":["Takuma Udagawa","Takato Yamazaki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.67","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions","tldr":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize th...","track":"Spatial Language Understanding"},"id":"WS-10.676","presentation_id":"38940097","rocketchat_channel":"paper-splu2020-676","speakers":"Takuma Udagawa|Takato Yamazaki|Akiko Aizawa","title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions"},{"content":{"abstract":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with respect to some anatomical structures. As the expressions result from the mental visualization of the radiologist\u2019s interpretations, they are varied and complex. The focus of this work is to automatically identify the spatial expression terms from three different radiology sub-domains. We propose a hybrid deep learning-based NLP method that includes \u2013 1) generating a set of candidate spatial triggers by exact match with the known trigger terms from the training data, 2) applying domain-specific constraints to filter the candidate triggers, and 3) utilizing a BERT-based classifier to predict whether a candidate trigger is a true spatial trigger or not. The results are promising, with an improvement of 24 points in the average F1 measure compared to a standard BERT-based sequence labeler.","authors":["Surabhi Datta","Kirk Roberts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports","tldr":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with r...","track":"Spatial Language Understanding"},"id":"WS-10.7","presentation_id":"38940080","rocketchat_channel":"paper-splu2020-7","speakers":"Surabhi Datta|Kirk Roberts","title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports"},{"content":{"abstract":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an image retrieval engine, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed method achieves a higher F1 value (89.67%) than a baseline method, demonstrating the effectiveness of using element-wise visual information.","authors":["Takuya Komada","Takashi Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition","tldr":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NE...","track":"Spatial Language Understanding"},"id":"WS-10.8","presentation_id":"38940081","rocketchat_channel":"paper-splu2020-8","speakers":"Takuya Komada|Takashi Inui","title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition"},{"content":{"abstract":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the LiMiT dataset and share it publicly as a new resource for the research community.","authors":["Irene Manotas","Ngoc Phuoc An Vo","Vadim Sheinin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.88","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LiMiT: The Literal Motion in Text Dataset","tldr":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) datase...","track":"Spatial Language Understanding"},"id":"WS-10.857","presentation_id":"38940096","rocketchat_channel":"paper-splu2020-857","speakers":"Irene Manotas|Ngoc Phuoc An Vo|Vadim Sheinin","title":"LiMiT: The Literal Motion in Text Dataset"},{"content":{"abstract":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.","authors":["Harsh Mehta","Yoav Artzi","Jason Baldridge","Eugene Ie","Piotr Mirowski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View","tldr":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively wi...","track":"Spatial Language Understanding"},"id":"WS-10.9","presentation_id":"38940082","rocketchat_channel":"paper-splu2020-9","speakers":"Harsh Mehta|Yoav Artzi|Jason Baldridge|Eugene Ie|Piotr Mirowski","title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View"}]
