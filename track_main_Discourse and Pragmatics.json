[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1141.png","content":{"abstract":"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences. Extensive evaluations are conducted on six public datasets. The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.","authors":["Baiyun Cui","Yingming Li","Zhongfei Zhang"],"demo_url":"","keywords":["bert-enhanced network","brson","bert","coherence modeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.511","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.2476","main.861","main.3437","main.574"],"title":"BERT-enhanced Relational Sentence Ordering Network","tldr":"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. ...","track":"Discourse and Pragmatics"},"forum":"main.1141","id":"main.1141","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1518.png","content":{"abstract":"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy","authors":["Shubham Toshniwal","Sam Wiseman","Allyson Ettinger","Karen Livescu","Kevin Gimpel"],"demo_url":"","keywords":["long resolution","incremental resolution","memory-augmented network","memory strategy"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.685","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.883","main.3647","main.315","main.1621","main.2661"],"title":"Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks","tldr":"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical ...","track":"Discourse and Pragmatics"},"forum":"main.1518","id":"main.1518","presentation_id":"38938926"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1621.png","content":{"abstract":"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.","authors":["Liyan Xu","Jinho D. Choi"],"demo_url":"","keywords":["coreference resolution","entity equalization","span clustering","higher-order inference"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.686","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3647","main.883","main.3593","main.1518","main.2271"],"title":"Revealing the Myth of Higher-Order Inference in Coreference Resolution","tldr":"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representat...","track":"Discourse and Pragmatics"},"forum":"main.1621","id":"main.1621","presentation_id":"38938952"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1863.png","content":{"abstract":"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.","authors":["Tao Yu","Shafiq Joty"],"demo_url":"","keywords":["conversation disentanglement","generalization","time-consuming engineering","disentanglement"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.512","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.916","main.645","main.2050","main.527","main.2839"],"title":"Online Conversation Disentanglement with Pointer Networks","tldr":"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information ...","track":"Discourse and Pragmatics"},"forum":"main.1863","id":"main.1863","presentation_id":"38939000"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1972.png","content":{"abstract":"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret 'I\u2019m starving.\u2019 in response to \u2018Hungry?\u2019, even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today\u2019s systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus 'Circa\u2019 with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes.","authors":["Annie Louis","Dan Roth","Filip Radlinski"],"demo_url":"","keywords":["pragmatic problem","dialog","dialog systems","language model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.601","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.41","main.1622","main.32","main.527","main.1797"],"title":"\u201cI'd rather just go to bed\u201d: Understanding Indirect Answers","tldr":"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret 'I\u2019m starving.\u2019 in response to \u2018Hungry?\u2019, even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natura...","track":"Discourse and Pragmatics"},"forum":"main.1972","id":"main.1972","presentation_id":"38939020"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2072.png","content":{"abstract":"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (\"_She daydreams about being a doctor_\") while a man is portrayed as more proactive and powerful (\"_He pursues his dream of being a doctor_\").  We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models.  Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","authors":["Xinyao Ma","Maarten Sap","Hannah Rashkin","Yejin Choi"],"demo_url":"","keywords":["bias correction","controllable debiasing","revision task","powertransformer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.602","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.32","main.84","demo.118","TACL.2411","main.1581"],"title":"PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction","tldr":"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (\"_She daydreams about b...","track":"Discourse and Pragmatics"},"forum":"main.2072","id":"main.2072","presentation_id":"38939042"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2075.png","content":{"abstract":"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4-way classification is \\textless  60\\%. This is a dramatic drop of 35\\% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines will be made freely available.","authors":["Wanqiu Long","Bonnie Webber","Deyi Xiong"],"demo_url":"","keywords":["-way classification","same-language transfer","same-domain transfer","ted-cdb"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.223","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3566","main.1702","main.2205","main.1379","main.2641"],"title":"TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks","tldr":"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in ...","track":"Discourse and Pragmatics"},"forum":"main.2075","id":"main.2075","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2205.png","content":{"abstract":"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":"","keywords":["rst-style parsing","data-driven approaches","deep-learning","scalable methodology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.603","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1892","TACL.2411","main.2075","main.3257","main.128"],"title":"MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision","tldr":"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse tree...","track":"Discourse and Pragmatics"},"forum":"main.2205","id":"main.2205","presentation_id":"38939067"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2377.png","content":{"abstract":"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims.","authors":["Sungho Jeon","Michael Strube"],"demo_url":"","keywords":["automated scoring","neural models","coherence model","linguistic coherence"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.604","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1159","main.3010","main.2650","main.1892","main.128"],"title":"Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments","tldr":"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse struct...","track":"Discourse and Pragmatics"},"forum":"main.2377","id":"main.2377","presentation_id":"38939101"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2512.png","content":{"abstract":"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding.  However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs,  which in turn allows us to crowd-source wide-coverage data annotated with discourse relations,  via an intuitively appealing interface for composing such questions and answers. Based on our proposed  representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.","authors":["Valentina Pyatkin","Ayal Klein","Reut Tsarfaty","Ido Dagan"],"demo_url":"","keywords":["natural understanding","predicting relations","discourse relations","question-and-answer pairs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.224","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2922","main.2141","main.2476","main.574","main.2640"],"title":"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","tldr":"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding.  However, annotating discourse relations typically requires expert annotators. Recently...","track":"Discourse and Pragmatics"},"forum":"main.2512","id":"main.2512","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3072.png","content":{"abstract":"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.","authors":["Ritam Dutt","Rishabh Joshi","Carolyn Rose"],"demo_url":"","keywords":["face utilization","politeness theory","computational models","notion face"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.605","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2561","main.3352","main.2707","main.851","main.165"],"title":"Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions","tldr":"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critic...","track":"Discourse and Pragmatics"},"forum":"main.3072","id":"main.3072","presentation_id":"38939264"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3292.png","content":{"abstract":"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.","authors":["Machel Reid","Edison Marrese-Taylor","Yutaka Matsuo"],"demo_url":"","keywords":["definition modeling","estimation","generative model","variational inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.513","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1130","main.2349","main.143","main.3635"],"title":"VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling","tldr":"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rat...","track":"Discourse and Pragmatics"},"forum":"main.3292","id":"main.3292","presentation_id":"38939304"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.955.png","content":{"abstract":"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii)  a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.","authors":["Wei Song","Ziyao Song","Ruiji Fu","Lizhen Liu","Miaomiao Cheng","Ting Liu"],"demo_url":"","keywords":["self-attention","structural encodings","inter-sentence attentions","sentence representation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.225","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.128","main.2141","main.2377","main.2512","main.1750"],"title":"Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays","tldr":"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent s...","track":"Discourse and Pragmatics"},"forum":"main.955","id":"main.955","presentation_id":"38938811"}]
