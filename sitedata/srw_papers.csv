UID,title,authors,abstract,keywords,track,paper_type,pdf_url
srw.135,Logical Inferences with Comparatives and Generalized Quantifiers,Izumi Haruta|Koji Mineshima|Daisuke Bekki,"Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models.",Logical Inferences|Comparative constructions|NLI task|automated proving,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.35.pdf
srw.109,Understanding Points of Correspondence between Sentences for Abstractive Summarization,Logan Lebanoff|John Muchovej|Franck Dernoncourt|Doo Soon Kim|Lidan Wang|Walter Chang|Fei Liu,"Fusing sentences containing disparate content is a remarkable human ability that helps create informative and succinct summaries. Such a simple task for humans has remained challenging for modern abstractive summarizers, substantially restricting their applicability in real-world scenarios. In this paper, we present an investigation into fusing sentences drawn from a document by introducing the notion of points of correspondence, which are cohesive devices that tie any two sentences together into a coherent text. The types of points of correspondence are delineated by text cohesion theory, covering pronominal and nominal referencing, repetition and beyond. We create a dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. Our dataset bridges the gap between coreference resolution and summarization. It is publicly shared to serve as a basis for future work to measure the success of sentence fusion systems.",Abstractive Summarization|coreference resolution|summarization|cohesive devices,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.26.pdf
srw.2,Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems,Vinay Pandramish|Dipti Misra Sharma,"In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the N-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder's ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well.",Neural Systems|Checkpoint Reranking|Neural systems|decoding process,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.38.pdf
srw.122,Exploring the Role of Context to Distinguish Rhetorical and Information-Seeking Questions,Yuan Zhuang|Ellen Riloff,"Social media posts often contain questions, but many of the questions are rhetorical and do not seek information. Our work studies the problem of distinguishing rhetorical and information-seeking questions on Twitter. Most work has focused on features of the question itself, but we hypothesize that the prior context plays a role too. This paper introduces a new dataset containing questions in tweets paired with their prior tweets to provide context. We create classification models to assess the difficulty of distinguishing rhetorical and information-seeking questions, and experiment with different properties of the prior context. Our results show that the prior tweet and topic features can improve performance on this task.",distinguishing questions|classification models|Rhetorical Questions|features,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.41.pdf
srw.48,Pre-training via Leveraging Assisting Languages for Neural Machine Translation,Haiyue Song|Raj Dabre|Zhuoyuan Mao|Fei Cheng|Sadao Kurohashi|Eiichiro Sumita,"Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios.",Neural Translation|S2S tasks|LOI|low-resource translation,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.37.pdf
srw.49,SCAR: Sentence Compression using Autoencoders for Reconstruction,Chanakya Malireddy|Tirth Maniar|Manish Shrivastava,"Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is primarily composed of two encoder-decoder pairs: a compressor and a reconstructor. The compressor masks the input, and the reconstructor tries to regenerate it. The model is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. SCAR’s merit lies in the novel Linkage Loss function, which correlates the compressor and its effect on reconstruction, guiding it to drop inferable tokens. SCAR achieves higher ROUGE scores on benchmark datasets than the existing state-of-the-art methods and baselines. We also conduct a user study to demonstrate the application of our model as a text highlighting system. Using our model to underscore salient information facilitates speed-reading and reduces the time required to skim a document.",Sentence Compression|Reconstruction|deletion-based compression|reconstructor,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.13.pdf
srw.123,Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,Takuma Kato|Kaori Abe|Hiroki Ouchi|Shumpei Miyawaki|Jun Suzuki|Kentaro Inui,"In general, the labels used in sequence labeling consist of different types of elements.For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person).However, while most sequence labeling models do not consider such label components, the shared components across labels, such as Person, can be beneficial for label prediction.In this work, we propose to integrate label component information as embeddings into models.Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels.",Sequence Labeling|Fine-grained Recognition|label prediction|English recognition,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.30.pdf
srw.137,Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining,Ivana Kvapilíková|Mikel Artetxe|Gorka Labaka|Eneko Agirre|Ondřej Bojar,"Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.",Unsupervised Embeddings|Parallel Mining|multilingual embeddings|parallel tasks,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.34.pdf
srw.127,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,Gantavya Bhatt|Hritik Bansal|Rishubh Singh|Sumeet Agarwal,"Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully.",linguistic tasks|unsupervised setting|sentence grammaticality|language tasks,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.33.pdf
srw.5,Topic Balancing with Additive Regularization of Topic Models,Eugeniia Veselova|Konstantin Vorontsov,"This article proposes a new approach for building topic models on unbalanced collections in topic modelling, based on the existing methods and our experiments with such methods. Real-world data collections contain topics in various proportions, and often documents of the relatively small theme become distributed all over the larger topics instead of being grouped into one topic. To address this issue, we design a new regularizer for Theta and Phi matrices in probabilistic Latent Semantic Analysis (pLSA) model. We make sure this regularizer increases the quality of topic models, trained on unbalanced collections. Besides, we conceptually support this regularizer by our experiments.",Topic Balancing|unbalanced modelling|probabilistic model|Additive Models,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.9.pdf
srw.58,uBLEU: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems,Tsuta Yuma|Naoki Yoshinaga|Masashi Toyoda,"Because open-domain dialogues allow diverse responses, basic reference-based metrics such as BLEU do not work well unless we prepare a massive reference set of high-quality responses for input utterances. To reduce this burden, a human-aided, uncertainty-aware metric, ΔBLEU, has been proposed; it embeds human judgment on the quality of reference outputs into the computation of multiple-reference BLEU. In this study, we instead propose a fully automatic, uncertainty-aware evaluation method for open-domain dialogue systems, υBLEU. This method first collects diverse reference responses from massive dialogue data and then annotates their quality judgments by using a neural network trained on automatically collected training data. Experimental results on massive Twitter data confirmed that υBLEU is comparable to ΔBLEU in terms of its correlation with human judgment and that the state of the art automatic evaluation method, RUBER, is improved by integrating υBLEU.",Open-Domain Systems|uBLEU|Uncertainty-Aware Method|ΔBLEU,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.27.pdf
srw.99,Research Replication Prediction Using Weakly Supervised Learning,Tianyi Luo|Xingyu Li|Hainan Wang|Yang Liu,"Knowing whether a published research result can be replicated or not is important. Carrying out direct replication of published research incurs high cost. It is therefore desirable to have a machine learning aided automatic prediction of a result's replicability. Such predictions can provide a confidence score for each article which can further provide guidelines for spot-checks.Since we will only have access to a small size of annotated dataset to train a machine predictor, we explore the possibility of using weakly supervised learning approaches to improve the prediction accuracy of research replication using both labelled and unlabelled datasets based on text information of research papers. Our experiments over real-world datasets show that much better prediction performance can be obtained compared to the supervised models utilizing only a small size of labelled dataset.",Research Prediction|Replication Prediction|automatic prediction|prediction replication,SRW,SRW,
srw.98,AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss,Mahmoud Daif|Shunsuke Kitada|Hitoshi Iyatomi,"Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis, word segmentation, and hand-crafted feature engineering.These could be eliminated by using character-level features.We propose a novel end-to-end Arabic document classification framework, Arabic document image-based classifier (AraDIC), inspired by the work on image-based character embeddings.AraDIC consists of an image-based character encoder and a classifier.They are trained in an end-to-end fashion using the class balanced loss to deal with the long-tailed data distribution problem.To evaluate the effectiveness of AraDIC, we created and published two datasets, the Arabic Wikipedia title (AWT) dataset and the Arabic poetry (AraP) dataset.To the best of our knowledge, this is the first image-based character embedding framework addressing the problem of Arabic text classification. We also present the first deep learning-based text classifier widely evaluated on modern standard Arabic, colloquial Arabic, and Classical Arabic.AraDIC shows performance improvement over classical and deep learning baselines by 12.29% and 23.05% for the micro and macro F-score, respectively.",Arabic Classification|Class-Balanced Loss|word segmentation|long-tailed problem,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.29.pdf
srw.131,Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation,Hiroaki Funayama|Shota Sasaki|Yuichiroh Matsubayashi|Tomoya Mizumoto|Jun Suzuki|Masato Mita|Kentaro Inui,"Many recent Short Answer Scoring (SAS) systems have employed Quadratic Weighted Kappa (QWK) as the evaluation measure of their systems. However, we hypothesize that QWK is unsatisfactory for the evaluation of the SAS systems when we consider measuring their effectiveness in actual usage. We introduce a new task formulation of SAS that matches the actual usage. In our formulation, the SAS systems should extract as many scoring predictions that are not critical scoring errors (CSEs). We conduct the experiments in our new task formulation and demonstrate that a typical SAS system can predict scores with zero CSE for approximately 50% of test data at maximum by ﬁltering out low-reliablility predictions on the basis of a certain conﬁdence estimation. This result directly indicates the possibility of reducing half the scoring cost of human raters, which is more preferable for the evaluation of SAS systems.",Short Scoring|evaluation systems|Confidence Estimation|Short systems,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.32.pdf
srw.28,Compositional Generalization by Factorizing Alignment and Translation,Jacob Russin|Jason Jo|Randall O'Reilly|Yoshua Bengio,"Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in cognitive science suggesting a functional distinction between systems for syntactic and semantic processing, we implement a modification to an existing approach in neural machine translation, imposing an analogous separation between alignment and translation. The resulting architecture substantially outperforms standard recurrent networks on the SCAN dataset, a compositional generalization task, without any additional supervision. Our work suggests that learning to align and to translate in separate modules may be a useful heuristic for capturing compositional structure.",Compositional Generalization|Translation|natural processing|cognitive science,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.42.pdf
srw.14,Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition,Hwichan Kim|Tosho Hirasawa|Mamoru Komachi,"The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes.We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline.",English Translation|Character Tokenization|Phoneme Decomposition|zero-shot approach,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.11.pdf
srw.15,Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner's Error Tendency,Yujin Takahashi|Satoru Katsumata|Mamoru Komachi,"Recently, several studies have focused on improving the performance of grammatical error correction (GEC) tasks using pseudo data. However, a large amount of pseudo data are required to train an accurate GEC model. To address the limitations of language and computational resources, we assume that introducing pseudo errors into sentences similar to those written by the language learners is more efficient, rather than incorporating random pseudo errors into monolingual data. In this regard, we study the effect of pseudo data on GEC task performance using two approaches. First, we extract sentences that are similar to the learners' sentences from monolingual data. Second, we generate realistic pseudo errors by considering error types that learners often make. Based on our comparative results, we observe that F0.5 scores for the Russian GEC task are significantly improved.",Grammatical Correction|grammatical tasks|GEC task|Russian task,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.5.pdf
srw.17,Story-level Text Style Transfer: A Proposal,Yusu Qian,"Text style transfer aims to change the style of the input text to the target style while preserving the content to some extent. Previous works on this task are on the sentence level. We aim to work on story-level text style transfer to generate stories that preserve the plot of the input story while exhibiting a strong target style. The challenge in this task compared to previous work is that the structure of the input story, consisting of named entities and their relations with each other, needs to be preserved, and that the generated story needs to be consistent after adding flavors. We plan to explore three methods including the BERT-based method, the Story Realization method, and the Graph-based method.",Story-level Transfer|Text transfer|BERT-based method|Story method,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.2.pdf
srw.16,Adaptive Transformers for Learning Multimodal Representations,Prajjwal Bhargava,"The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.",Multimodal Representations|vision tasks|Adaptive Transformers|transformers,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.1.pdf
srw.144,Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources,Magdalena Biesialska|Bardia Rafieian|Marta R. Costa-jussà,"In this work, we present an effective method for semantic specialization of word vector representations. To this end, we use traditional word embeddings and apply specialization methods to better capture semantic relations between words. In our approach, we leverage external knowledge from rich lexical resources such as BabelNet. We also show that our proposed post-specialization method based on an adversarial neural network with the Wasserstein distance allows to gain improvements over state-of-the-art methods on two tasks: word similarity and dialog state tracking.",semantic representations|dialog tracking|word embeddings|specialization methods,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.36.pdf
srw.39,RPD: A Distance Function Between Word Embeddings,Xuhui Zhou|Shujian Huang|Zaixiang Zheng,"It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of em-beddings deviate from each other. In this paper, we propose a novel metric called Relative Pairwise Inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This unitary-invariant metric has a unified scale for comparing different sets of word embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding space.",RPD|Word Embeddings|training processes|Relative Distance,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.7.pdf
srw.35,HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing,Miaomiao Yu|Yujiu Yang|Chenhui Li,"Recently deep learning has been used in Medical subject headings (MeSH) indexing to reduce the time and monetary cost by manual annotation, including DeepMeSH, TextCNN, etc. However, these models still suffer from failing to capture the complex correlations between MeSH terms. To this end, we introduce Graph Convolution Network (GCN) to learn the relationship between these terms, and present a novel Hybrid Graph Convolution Net for MeSH index (HGCN4MeSH). Basically, we utilize two BiGRUs to learn the embedding representation of the abstract and the title of the MeSH index text respectively. At the same time, we establish the adjacency matrix of MeSH terms based on the co-occurrence relationships in Corpus, which is easy to apply for GCN representation learning. On the basis of learning the mixed representation, the prediction problem of the MeSH index keywords is transformed into an extreme multi-label classification problem after the attention layer operation. Experimental results on two datasets show that HGCN4MeSH is competitive compared with the state-of-the-art methods.",MeSH Indexing|Medical indexing|manual annotation|prediction problem,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.4.pdf
srw.36,Research on Task Discovery for Transfer Learning in Deep Neural Networks,Arda Akdemir,"Deep neural network based machine learning models are shown to perform poorly on unseen or out-of-domain examples by numerous recent studies. Transfer learning aims to avoid overfitting and to improve generalizability by leveraging the information obtained from multiple tasks. Yet, the benefits of transfer learning depend largely on task selection and finding the right method of sharing. In this thesis, we hypothesize that current deep neural network based transfer learning models do not achieve their fullest potential for various tasks and there are still many task combinations that will benefit from transfer learning that are not considered by the current models. To this end, we started our research by implementing a novel multi-task learner with relaxed annotated data requirements and obtained a performance improvement on two NLP tasks. We will further devise models to tackle tasks from multiple areas of machine learning, such as Bioinformatics and Computer Vision, in addition to NLP.",Task Discovery|Transfer Learning|task selection|NLP tasks,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.6.pdf
srw.22,Non-Topical Coherence in Social Talk: A Call for Dialogue Model Enrichment,Alex Luu|Sophia A. Malamud,"Current models of dialogue mainly focus on utterances within a topically coherent discourse segment, rather than new-topic utterances (NTUs), which begin a new topic not correlating with the content of prior discourse. As a result, these models may sufficiently account for discourse context of task-oriented but not social conversations. We conduct a pilot annotation study of NTUs as a first step towards a model capable of rationalizing conversational coherence in social talk. We start with the naturally occurring social dialogues in the Disco-SPICE corpus, annotated with discourse relations in the Penn Discourse Treebank and Cognitive approach to Coherence Relations frameworks. We first annotate content-based coherence relations that are not available in Disco-SPICE, and then heuristically identify NTUs, which lack a coherence relation to prior discourse. Based on the interaction between NTUs and their discourse context, we construct a classification for NTUs that actually convey certain non-topical coherence in social talk. This classification introduces new sequence-based social intents that traditional taxonomies of speech acts do not capture. The new findings advocates the development of a Bayesian game-theoretic model for social talk.",Dialogue Enrichment|Coherence frameworks|classification|NTUs,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.17.pdf
srw.18,"Media Bias, the Social Sciences, and NLP: Automating Frame Analyses to Identify Bias by Word Choice and Labeling",Felix Hamborg,"Media bias can strongly impact the public perception of topics reported in the news. A difficult to detect, yet powerful form of slanted news coverage is called bias by word choice and labeling (WCL). WCL bias can occur, for example, when journalists refer to the same semantic concept by using different terms that frame the concept differently and consequently may lead to different assessments by readers, such as the terms ""freedom fighters"" and ""terrorists,"" or ""gun rights"" and ""gun control."" In this research project, I aim to devise methods that identify instances of WCL bias and estimate the frames they induce, e.g., not only is ""terrorists"" of negative polarity but also ascribes to aggression and fear. To achieve this, I plan to research methods using natural language processing and deep learning while employing models and using analysis concepts from the social sciences, where researchers have studied media bias for decades. The first results indicate the effectiveness of this interdisciplinary research approach. My vision is to devise a system that helps news readers to become aware of the differences in media coverage caused by bias.",NLP|labeling|natural processing|Frame Analyses,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.12.pdf
srw.19,Unsupervised Paraphasia Classification in Aphasic Speech,Sharan Pai|Nikhil Sachdeva|Prince Sachdeva|Rajiv Ratn Shah,"Aphasia is a speech and language disorder which results from brain damage, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia).Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods cant be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel unsupervised method which can be implemented without the need for labeled paraphasia data. Our evaluations show that our method outperforms previous work based on supervised learning and transfer learning approaches for English. We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from aphasia in any language.",Unsupervised Classification|speech disorder|naming detection|treatment,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.3.pdf
srw.114,Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition,Yuhui Zhang|Allen Nie,"The principle of compositionality has deep roots in linguistics: the meaning of an expression is determined by its structure and the meanings of its constituents. However, modern neural network models such as long short-term memory network process expressions in a linear fashion and do not seem to incorporate more complex compositional patterns. In this work, we show that we can explicitly induce grammar by tracing the computational process of a long short-term memory network. We show: (i) the multiplicative nature of long short-term memory network allows complex interaction beyond sequential linear combination; (ii) we can generate compositional trees from the network without external linguistic knowledge; (iii) we evaluate the syntactic difference between the generated trees, randomly generated trees and gold reference trees produced by constituency parsers; (iv) we evaluate whether the generated trees contain the rich semantic information.",Inducing Grammar|Long Networks|Shapley Decomposition|neural models,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.40.pdf
srw.128,Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages,Vikrant Goyal|Sourav Kumar|Dipti Misra Sharma,"A large percentage of the world’s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines.",Neural Translation|machine systems|translation|NMT,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.22.pdf
srw.95,Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup,Jishnu Ray Chowdhury|Cornelia Caragea|Doina Caragea,"Distinguishing informative and actionable messages from a social media platform like Twitter is critical for facilitating disaster management. For this purpose, we compile a multilingual dataset of over 130K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labelled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main model is based on Multilingual BERT, which we further improve with Manifold Mixup. We show that our model generalizes to unseen disasters in the test set. Furthermore, we analyze the capability of our model for zero-shot generalization to new languages. Our code, dataset, and other resources are available on Github.",Cross-Lingual Classification|Distinguishing messages|disaster management|multi-label tweets,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.39.pdf
srw.42,Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder,Zheng Tang|Gus Hahn-Powell|Mihai Surdeanu,"We propose an interpretable approach for event extraction that mitigates the tension between generalization and interpretability by jointly training for the two goals. Our approach uses an encoder-decoder architecture, which jointly trains a classifier for event extraction, and a rule decoder that generates syntactico-semantic rules that explain the decisions of the event classifier. We evaluate the proposed approach on three biomedical events and show that the decoder generates interpretable rules that serve as accurate explanations for the event classifier's decisions, and, importantly, that the joint training generally improves the performance of the event classifier. Lastly, we show that our approach can be used for semi-supervised learning, and that its performance improves when trained on automatically-labeled data generated by a rule-based system.",Interpretability Extraction|Event Extraction|semi-supervised learning|Multitask Learning,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.23.pdf
srw.129,Building a Japanese Typo Dataset from Wikipedia's Revision History,Yu Tanaka|Yugo Murawaki|Daisuke Kawahara|Sadao Kurohashi,"User generated texts contain many typos for which correction is necessary for NLP systems to work. Although a large number of typo–correction pairs are needed to develop a data-driven typo correction system, no such dataset is available for Japanese. In this paper, we extract over half a million Japanese typo–correction pairs from Wikipedia’s revision history. Unlike other languages, Japanese poses unique challenges: (1) Japanese texts are unsegmented so that we cannot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction.",NLP systems|typo correction|data-driven system|spelling checker,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.31.pdf
srw.115,Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms,Goro Kobayashi|Tatsuki Kuribayashi|Sho Yokoi|Kentaro Inui,"Self-attention modules are essential building blocks of Transformer-based language models and hence are the subject of a large number of studies aiming to discover which linguistic capabilities these models possess (Rogers et al., 2020). Such studies are commonly conducted by analyzing correlations of attention weights with specific linguistic phenomena. In this paper, we show that attention weights alone are only one of two factors determining the output of self-attention modules and propose to incorporate the other factor, namely the norm of the transformed input vectors, into the analysis, as well. Our analysis of self-attention modules in BERT (Devlin et al., 2019) shows that the proposed method produces insights that better agree with linguistic intuitions than an analysis based on attention-weights alone. Our analysis further reveals that BERT controls the amount of the contribution from frequent informative and less informative tokens not by attention weights but via vector norms.",BERT|Self-attention modules|Transformer-based models|output modules,SRW,SRW,
srw.9,Combining Subword Representations into Word-level Representations in the Transformer Architecture,Noe Casas|Marta R. Costa-jussà|José A. R. Fonollosa,"In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies.We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information.Our experiments show that this approach maintains the translation quality with respect to the normal Transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies.",Neural Translation|Subword Representations|Word-level Representations|Transformer Architecture,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.10.pdf
srw.82,A Simple and Effective Dependency Parser for Telugu,Sneha Nallani|Manish Shrivastava|Dipti Sharma,"We present a simple and effective dependency parser for Telugu, a morphologically rich, free word order language. We propose to replace the rich linguistic feature templates used in the past approaches with a minimal feature function using contextual vector representations. We train a BERT model on the Telugu Wikipedia data and use vector representations from this model to train the parser. Each sentence token is associated with a vector representing the token in the context of that sentence and the feature vectors are constructed by concatenating two token representations from the stack and one from the buffer. We put the feature representations through a feedforward network and train with a greedy transition based approach. The resulting parser has a very simple architecture with minimal feature engineering and achieves state-of-the-art results for Telugu.",Telugu|Dependency Parser|contextual representations|BERT model,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.19.pdf
srw.69,Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models,Pia Sommerauer,"What do powerful models of word mean- ing created from distributional data (e.g. Word2vec (Mikolov et al., 2013) BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018)) represent? What causes words to be similar in the semantic space? What type of information is lacking? This thesis proposal presents a framework for investigating the information encoded in distributional semantic models. Several analysis methods have been suggested, but they have been shown to be limited and are not well understood. This approach pairs observations made on actual corpora with insights obtained from data manipulation experiments. The expected outcome is a better understanding of (1) the semantic information we can infer purely based on linguistic co-occurrence patterns and (2) the potential of distributional semantic models to pick up linguistic evidence.",word ing|distributional models|BERT|ELMO,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.18.pdf
srw.55,Considering Likelihood in NLP Classiﬁcation Explanations with Occlusion and Language Modeling,David Harbecke|Christoph Alt,"Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model's decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models. Furthermore, gradient-based explanation methods disregard the discrete distribution of data in NLP. Thus, we propose OLM: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood, given the context of the original input. We lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in NLP and provide results that underline the importance of considering data likelihood in occlusion-based explanation.",NLP|NLP Explanations|Language Modeling|NLP models,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.16.pdf
srw.54,Multi-Task Neural Model for Agglutinative Language Translation,Yirong Pan|Xiao Li|Yating Yang|Rui Dong,"Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data.",Agglutinative Translation|agglutinative task|bi-directional translation|agglutinative stemming,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.15.pdf
srw.116,Noise-Based Augmentation Techniques for Emotion Datasets: What do we Recommend?,Mimansa Jaiswal|Emily Mower Provost,"Emotion recognition systems are widely used for many downstream applications such as mental health monitoring, educational problems diagnosis, hate speech classification and targeted advertising. Yet, these systems are generally trained on audio or multimodal datasets collected in a laboratory environment.While acoustically different, they are generally free of major environmental noises. The result is that systems trained on these datasets falter when presented with noisy data, even when that noise doesn’t affect the human perception of emotions. In this work, we use multiple categories of environmental and synthetic noises to generate black box adversarial examples and use these noises to modify the samples in the IEMOCAP dataset. We evaluate how both human and machine emotion perception changes when noise is introduced. We find that the trained state-of-the-art models fail to classify even moderately noisy samples that humans usually have no trouble comprehend-ing, demonstrating the brittleness of these systems in real world conditions.",mental monitoring|educational diagnosis|hate classification|targeted advertising,SRW,SRW,
srw.106,Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions,Steinþór Steingrímsson|Hrafn Loftsson|Andy Way,"Parallel corpora are key to developing good machine translation systems. However, abundant parallel data are hard to come by, especially for languages with a low number of speakers. When rich morphology exacerbates the data sparsity problem, it is imperative to have accurate alignment and filtering methods that can help make the most of what is available by maximising the number of correctly translated segments in a corpus and minimising noise by removing incorrect translations and segments containing extraneous data.This paper sets out a research plan for improving alignment and filtering methods for parallel texts in low-resource settings. We propose an effective unsupervised alignment method to tackle the alignment problem. Moreover, we propose a strategy to supplement state-of-the-art models with automatically extracted information using basic NLP tools to effectively handle rich morphology.",Aligning Corpora|machine systems|data problem|alignment problem,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.25.pdf
srw.79,Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya,Abrhalei Frezghi Tela|Abraham Woubie Zewoudie|Ville Hautamäki,"In recent years, transformer models have achieved great success in natural language processing tasks. Most of the current state-of-the-art NLP results are achieved by using monolingual transformer models, where the model is pre-trained using a single language unlabelled text corpus. Then, the model is fine-tuned to the specific downstream task. However, the cost of pre-training a new transformer model is high for most languages. In this work, we propose a novel transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language. Thus, using XLNet language model, we demonstrate competitive performance with mBERT and a pre-trained target language model on the Cross-lingual Sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya. With only 10k examples of the given Tigrinya sentiment analysis dataset, English XLNet has achieved 78.88% F1-Score outperforming BERT and mBERT by 10% and 7%, respectively. More interestingly, fine-tuning (English) XLNet model on the CLS dataset has promising results compared to mBERT and even outperformed mBERT for one dataset of the Japanese language.",natural tasks|NLP|downstream task|pre-training,SRW,SRW,
srw.105,A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples,Zhao Meng|Roger Wattenhofer,"Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of deep neural networks. Experiments on two datasets with two different models show that our attack fools the models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.",Generating Examples|Human evaluation|Geometry-Inspired Attack|decision networks,SRW,SRW,
srw.84,Pointwise Paraphrase Appraisal is Potentially Problematic,Hannah Chen|Yangfeng Ji|David Evans,"The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models. As a first step towards that goal, we show that although the standard way of fine-tuning BERT for paraphrase identification by pairing two sentences as one sequence results in a model with state-of-the-art performance, that model may perform poorly on simple tasks like identifying pairs with two identical sentences. Moreover, we show that these models may even predict a pair of randomly-selected sentences with higher paraphrase score than a pair of identical ones.",Pointwise Appraisal|binary problem|paraphrase identification|paraphrase models,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.20.pdf
srw.90,To compress or not to compress? A Finite-State approach to Nen verbal morphology,Saliha Muradoglu|Nicholas Evans|Hanna Suominen,"This paper describes the development of a verbal morphological parser for an under-resourced Papuan language, Nen. Nen verbal morphology is particularly complex, with a transitive verb taking up to 1,740 unique features. The structural properties exhibited by Nen verbs raises interesting choices for analysis. Here we compare two possible methods of analysis: ‘Chunking’ and decomposition. ‘Chunking’ refers to the concept of collating morphological segments into one, whereas the decomposition model follows a more classical linguistic approach. Both models are built using the Finite-State Transducer toolkit foma. The resultant architecture shows differences in size and structural clarity. While the ‘Chunking’ model is under half the size of the full de-composed counterpart, the decomposition displays higher structural order. In this paper, we describe the challenges encountered when modelling a language exhibiting distributed exponence and present the first morphological analyser for Nen, with an overall accuracy of 80.3%.",Finite-State approach|verbal parser|Chunking|decomposition model,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.28.pdf
srw.53,Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information,Hyeryun Park|Kyungmo Kim|Jooyoung Yoon|Seongkeun Park|Jinwook Choi,"Medical image captioning can reduce the workload of physicians and save time and expense by automatically generating reports. However, current datasets are small and limited, creating additional challenges for researchers. In this study, we propose a feature difference and tag information combined long short-term memory (LSTM) model for chest x-ray report generation. A feature vector extracted from the image conveys visual information, but its ability to describe the image is limited. Other image captioning studies exhibited improved performance by exploiting feature differences, so the proposed model also utilizes them. First, we propose a difference and tag (DiTag) model containing the difference between the patient and normal images. Then, we propose a multi-difference and tag (mDiTag) model that also contains information about low-level differences, such as contrast, texture, and localized area. Evaluation of the proposed models demonstrates that the mDiTag model provides more information to generate captions and outperforms all other models.",chest generation|image studies|medical model|difference model,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.14.pdf
srw.52,Reflection-based Word Attribute Transfer,Yoichi Ishibashi|Katsuhito Sudoh|Koichiro Yoshino|Satoshi Nakamura,"Word embeddings, which often represent such analogic relations as king - man + woman ~ queen, can be used to change a word's attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes.",Reflection-based Transfer|word transfer|Word embeddings|analogy-based manner,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.8.pdf
srw.46,Dominance as an Indicator of Rapport and Learning in Human-Agent Communication,Amanda Buddemeyer|Xiaoyi Tian|Erin Walker,"Power dynamics in human-human communication can impact rapport-building and learning gains, but little is known about how power impacts human-agent communication. In this paper, we examine dominance behavior in utterances between middle-school students and a teachable robot as they work through math problems, as coded by Rogers and Farace's Relational Communication Control Coding Scheme (RCCCS). We hypothesize that relatively dominant students will show increased learning gains, as will students with greater dominance agreement with the robot. We also hypothesize that gender could be an indicator of differences in dominance behavior. We present a preliminary analysis of dominance characteristics in some of the transactions between robot and student. Ultimately, we hope to determine if manipulating the dominance behavior of a learning robot could support learning.",rapport-building gains|learning|Relational Scheme|RCCCS,SRW,SRW,
srw.85,#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media,Ashima Suvarna|Grusha Bhalla,"The recent surge in online forums and movements supporting sexual assault survivors has led to the emergence of a `virtual bubble' where survivors can recount their stories. However, this also makes the survivors vulnerable to bullying, trolling and victim blaming. Specifically, victim blaming has been shown to have acute psychological effects on the survivors and further discourage formal reporting of such crimes. Therefore, it is important to devise computationally relevant methods to identify and prevent victim blaming to protect the victims. In our work, we discuss the drastic effects of victim blaming through a short case study and then propose a single step transfer-learning based classification method to identify victim blaming language on Twitter. Finally, we compare the performance of our proposed model against various deep learning and machine learning models on a manually annotated domain-specific dataset.",sexual survivors|victim blaming|computationally methods|transfer-learning method,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.43.pdf
srw.104,Crossing the Line: Where do Demographic Variables Fit into Humor Detection?,J. A. Meaney,"Recent humor classification shared tasks have struggled with two issues: either the data comprises a highly constrained genre of humor which does not broadly represent humor, or the data is so indiscriminate that the inter-annotator agreement on its humor content is drastically low. These tasks typically average over all annotators' judgments, in spite of the fact that humor is a highly subjective phenomenon. We argue that demographic factors influence whether a text is perceived as humorous or not. We propose the addition of demographic information about the humor annotators in order to bin ratings more sensibly. We also suggest the addition of an 'offensive' label to distinguish between different generations, in terms of humor. This would allow for more nuanced shared tasks and could lead to better performance on downstream tasks, such as content moderation.",Humor Detection|humor tasks|Demographic Variables|humor,SRW,SRW,https://www.aclweb.org/anthology/2020.acl-srw.24.pdf
