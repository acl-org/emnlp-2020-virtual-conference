[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1010.png","content":{"abstract":"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on local features while neglecting global information. To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph. Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations. Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information. Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.","authors":["Qinzhuo Wu","Qi Zhang","Jinlan Fu","Xuanjing Huang"],"demo_url":"","keywords":["natural tasks","math solving","generation","knowledge-aware representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.579","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.782","main.666","TACL.2121","main.179","main.1648"],"title":"A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving","tldr":"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the proble...","track":"NLP Applications"},"forum":"main.1010","id":"main.1010","presentation_id":"38938825"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1032.png","content":{"abstract":"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.","authors":["Andreas R\u00fcckl\u00e9","Jonas Pfeiffer","Iryna Gurevych"],"demo_url":"","keywords":["answer tasks","zero-shot transfer","text models","self-supervised training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.194","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.858","main.74","main.1803","main.2167","main.2943"],"title":"MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale","tldr":"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks o...","track":"NLP Applications"},"forum":"main.1032","id":"main.1032","presentation_id":"38938833"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.110.png","content":{"abstract":"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.","authors":["Dongfang Li","Baotian Hu","Qingcai Chen","Weihua Peng","Anqi Wang"],"demo_url":"","keywords":["national examination","medical examination","large-scale models","reading model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.111","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1631","main.1923","main.2630","main.748","main.1159"],"title":"Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text","tldr":"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the ...","track":"NLP Applications"},"forum":"main.110","id":"main.110","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1129.png","content":{"abstract":"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.","authors":["Charuta Pethe","Allen Kim","Steve Skiena"],"demo_url":"","keywords":["predicting boundaries","segmenting texts","chapter segmentation","exact prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.672","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2367","main.3389","main.3010","main.1049","main.2650"],"title":"Chapter Captor: Text Segmentation in Novels","tldr":"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenber...","track":"NLP Applications"},"forum":"main.1129","id":"main.1129","presentation_id":"38938849"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1135.png","content":{"abstract":"Recognizing the flow of time in a story is a crucial aspect of understanding it.  Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hour. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.","authors":["Allen Kim","Charuta Pethe","Steve Skiena"],"demo_url":"","keywords":["time-of-day model","dynamic breakpoints","temporal expressions","relative events"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.730","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.928","main.605","main.3617","main.1421","main.3486"],"title":"What time is it? Temporal Analysis of Novels","tldr":"Recognizing the flow of time in a story is a crucial aspect of understanding it.  Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating ea...","track":"NLP Applications"},"forum":"main.1135","id":"main.1135","presentation_id":"38938851"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1180.png","content":{"abstract":"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.","authors":["Wenyue Zhang","Xiaoli Li","Yang Li","Suge Wang","Deyu Li","Jian Liao","Jianxing Zheng"],"demo_url":"","keywords":["detecting drift","distribution learning","classification model","hierarchical model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.307","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1575","main.1675","main.1289","main.2068","main.2430"],"title":"Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder","tldr":"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this pa...","track":"NLP Applications"},"forum":"main.1180","id":"main.1180","presentation_id":"38938857"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1231.png","content":{"abstract":"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.","authors":["Martin Schmitt","Sahand Sharifzadeh","Volker Tresp","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["graph-to-text generation","text-to-graph extraction","semantic parsing","unsupervised generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.577","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.666","main.1706","TACL.2121","main.531","main.143"],"title":"An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing","tldr":"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific paral...","track":"NLP Applications"},"forum":"main.1231","id":"main.1231","presentation_id":"38938870"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.125.png","content":{"abstract":"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the \u201cauthor\u201d of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T1 and T2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/ 302zWdz","authors":["Adaku Uchendu","Thai Le","Kai Shu","Dongwon Lee"],"demo_url":"","keywords":["generating texts","authorship problem","xlm","pplm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.673","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["demo.126","main.648","main.2650","main.1187","main.870"],"title":"Authorship Attribution for Neural Text Generation","tldr":"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts...","track":"NLP Applications"},"forum":"main.125","id":"main.125","presentation_id":"38938653"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1250.png","content":{"abstract":"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","authors":["Huilin Zhong","Junsheng Zhou","Weiguang Qu","Yunfei Long","Yanhui Gu"],"demo_url":"","keywords":["legal prediction","classification","law","lemm"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.540","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1952","main.3651","main.2630","main.911"],"title":"An Element-aware Multi-representation Model for Law Article Prediction","tldr":"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label sample...","track":"NLP Applications"},"forum":"main.1250","id":"main.1250","presentation_id":"38938872"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1263.png","content":{"abstract":"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and \\yoruba on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.","authors":["Michael A. Hedderich","David Adelani","Dawei Zhu","Jesujoba Alabi","Udia Markus","Dietrich Klakow"],"demo_url":"","keywords":["nlp tasks","ner classification","low-resource learning","multilingual models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.204","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.74","main.1680","main.2500","main.858","main.1379"],"title":"Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages","tldr":"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to r...","track":"NLP Applications"},"forum":"main.1263","id":"main.1263","presentation_id":"38938875"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1271.png","content":{"abstract":"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.","authors":["Zhihong Chen","Yan Song","Tsung-Hui Chang","Xiang Wan"],"demo_url":"","keywords":["medical imaging","writing reports","automatically reports","clinical automation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.112","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.110","main.55","main.1923","main.1942","main.1219"],"title":"Generating Radiology Reports via Memory-driven Transformer","tldr":"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is ...","track":"NLP Applications"},"forum":"main.1271","id":"main.1271","presentation_id":"38938877"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1322.png","content":{"abstract":"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.","authors":["Bhanu Prakash Reddy Guda","Sasi Bhushan Seelaboyina","Soumya Sarkar","Animesh Mukherjee"],"demo_url":"","keywords":["manual designation","wikipedia representation","ablation studies","editors"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.674","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["demo.58","main.1952","main.1485","main.450","main.2931"],"title":"NwQM: A neural quality assessment framework for Wikipedia","tldr":"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several qu...","track":"NLP Applications"},"forum":"main.1322","id":"main.1322","presentation_id":"38938887"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1390.png","content":{"abstract":"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users' needs. We study the task of predicting the documents that customer care agents can use to facilitate users' needs. We also introduce a new public dataset which supports the aforementioned problem. Using this dataset and two others,  we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task.  Additionally, we analyze the practicality of such systems in terms of inference time complexity.  Our show that an hybrid IR+DL approach provides the best of both worlds.","authors":["Jatin Ganhotra","Haggai Roitman","Doron Cohen","Nathaniel Mills","Chulaka Gunasekara","Yosi Mass","Sachindra Joshi","Luis Lastras","David Konopnicki"],"demo_url":"","keywords":["customer conversations","predicting documents","customer agents","information models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.25","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.116","main.3495","main.3327","main.527","main.916"],"title":"Conversational Document Prediction to Assist Customer Care Agents","tldr":"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users' needs. We study the task of predicting the documents that customer care agents can use to facilitate users' needs. We also in...","track":"NLP Applications"},"forum":"main.1390","id":"main.1390","presentation_id":"38938896"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1446.png","content":{"abstract":"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing  sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.","authors":["Chunyuan Li","Xiang Gao","Yuan Li","Baolin Peng","Xiujun Li","Yizhe Zhang","Jianfeng Gao"],"demo_url":"","keywords":["language tasks","guided generation","low-resource tasks","variational autoencoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.378","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.1892","main.1130","main.2491","main.3483"],"title":"Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space","tldr":"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (O...","track":"NLP Applications"},"forum":"main.1446","id":"main.1446","presentation_id":"38938906"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1465.png","content":{"abstract":"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs.  Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.","authors":["Woojeong Jin","Meng Qu","Xisen Jin","Xiang Ren"],"demo_url":"","keywords":["knowledge reasoning","natural processing","predicting interactions","link prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.541","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3084","main.3617","main.2972","main.237","main.1116"],"title":"Recurrent Event Network: Autoregressive Structure Inferenceover Temporal Knowledge Graphs","tldr":"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps a...","track":"NLP Applications"},"forum":"main.1465","id":"main.1465","presentation_id":"38938911"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1484.png","content":{"abstract":"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards -- in their current form -- can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model's utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","authors":["Kawin Ethayarajh","Dan Jurafsky"],"demo_url":"","keywords":["nlp","performance-based evaluation","leaderboard paradigm","microeconomic theory"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.393","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2238","main.527","main.2215","main.2221","main.1219"],"title":"Utility is in the Eye of the User: A Critique of NLP Leaderboards","tldr":"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expens...","track":"NLP Applications"},"forum":"main.1484","id":"main.1484","presentation_id":"38938914"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1594.png","content":{"abstract":"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing.  We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].","authors":["Hoo-Chang Shin","Yang Zhang","Evelina Bakhturina","Raul Puri","Mostofa Patwary","Mohammad Shoeybi","Raghav Mani"],"demo_url":"","keywords":["domain applications","domain transfer","question answering","named recognition"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.379","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1631","main.1428","main.748","main.16","main.110"],"title":"BioMegatron: Larger Biomedical Domain Language Model","tldr":"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Book...","track":"NLP Applications"},"forum":"main.1594","id":"main.1594","presentation_id":"38938943"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1634.png","content":{"abstract":"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies. In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments. We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection. Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.","authors":["Jingfeng Yang","Diyi Yang","Zhaoran Ma"],"demo_url":"","keywords":["disfluency detection","pretraining","data methods","augmentation approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.113","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3353","main.2590","main.2511","main.648","main.1049"],"title":"Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection","tldr":"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion...","track":"NLP Applications"},"forum":"main.1634","id":"main.1634","presentation_id":"38938957"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1739.png","content":{"abstract":"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) 'Expression' token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.","authors":["Bugeun Kim","Kyung Seo Ki","Donggeon Lee","Gahgene Gweon"],"demo_url":"","keywords":["solving problems","natural task","algebraic problems","expression fragmentation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.308","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.179","TACL.2141","main.2448","demo.86","main.2990"],"title":"Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model","tldr":"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens a...","track":"NLP Applications"},"forum":"main.1739","id":"main.1739","presentation_id":"38938978"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.179.png","content":{"abstract":"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols\u2019 semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.","authors":["Jinghui Qin","Lihui Lin","Xiaodan Liang","Rumin Zhang","Liang Lin"],"demo_url":"","keywords":["one-unknown mwps","universal","uet","semantically-aligned solver"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.309","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1010","main.1061","main.2342","main.1739","main.1503"],"title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems","tldr":"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expressio...","track":"NLP Applications"},"forum":"main.179","id":"main.179","presentation_id":"38938662"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1906.png","content":{"abstract":"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.","authors":["Irshad Bhat","Talita Anthonio","Michael Roth"],"demo_url":"","keywords":["revision experiments","wikihow","edits","textual edits"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.675","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3507","main.607","main.928","main.2922","main.2112"],"title":"Towards Modeling Revision Requirements in wikiHow Instructions","tldr":"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this wo...","track":"NLP Applications"},"forum":"main.1906","id":"main.1906","presentation_id":"38939007"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1908.png","content":{"abstract":"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.","authors":["Qiao Jin","Chuanqi Tan","Mosha Chen","Xiaozhong Liu","Songfang Huang"],"demo_url":"","keywords":["evidence-based medicine","clinical task","manual collection","ctrp framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.114","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1942","main.1631","main.1923","main.2238","main.3486"],"title":"Predicting Clinical Trial Results by Implicit Evidence Integration","tldr":"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) tas...","track":"NLP Applications"},"forum":"main.1908","id":"main.1908","presentation_id":"38939008"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1942.png","content":{"abstract":"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention  as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.","authors":["Jinyue Feng","Chantal Shaib","Frank Rudzicz"],"demo_url":"","keywords":["sepsis prediction","clinical models","hierarchical model","multi-task model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.115","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1908","main.110","main.2989","main.3648","main.2307"],"title":"Explainable Clinical Decision Support from Text","tldr":"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a ...","track":"NLP Applications"},"forum":"main.1942","id":"main.1942","presentation_id":"38939013"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2068.png","content":{"abstract":"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.","authors":["Husam Quteineh","Spyridon Samothrakis","Richard Sutcliffe"],"demo_url":"","keywords":["data task","optimization problem","non-guided process","trec-"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.600","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2733","main.1923","main.493","main.471","main.1351"],"title":"Textual Data Augmentation for Efficient Active Learning on Tiny Datasets","tldr":"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data gener...","track":"NLP Applications"},"forum":"main.2068","id":"main.2068","presentation_id":"38939040"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2117.png","content":{"abstract":"Fact checking at scale is difficult---while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs --- in particular QABriefs --- increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.","authors":["Angela Fan","Aleksandra Piktus","Fabio Petroni","Guillaume Wenzek","Marzieh Saeidi","Andreas Vlachos","Antoine Bordes","Sebastian Riedel"],"demo_url":"","keywords":["fact checking","claim detection","media ecosystem","qabriefer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.580","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.872","main.3151","main.2570","main.1004","main.2506"],"title":"Generating Fact Checking Briefs","tldr":"Fact checking at scale is difficult---while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often erro...","track":"NLP Applications"},"forum":"main.2117","id":"main.2117","presentation_id":"38939053"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2167.png","content":{"abstract":"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of \\lmtc datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.","authors":["Ilias Chalkidis","Manos Fergadiotis","Sotiris Kotitsas","Prodromos Malakasiotis","Nikolaos Aletras","Ion Androutsopoulos"],"demo_url":"","keywords":["flat classification","hierarchical approaches","zero-shot learning","few learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.607","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1682","main.1611","main.1032","main.2289","main.148"],"title":"An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels","tldr":"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set...","track":"NLP Applications"},"forum":"main.2167","id":"main.2167","presentation_id":"38939063"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2225.png","content":{"abstract":"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S\\&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.","authors":["Ramit Sawhney","Shivam Agarwal","Arnav Wadhwa","Rajiv Ratn Shah"],"demo_url":"","keywords":["risk modeling","profit generation","stock task","stock forecasting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.676","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2520","main.1180","main.3486","main.2784","main.1287"],"title":"Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations","tldr":"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock ...","track":"NLP Applications"},"forum":"main.2225","id":"main.2225","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2261.png","content":{"abstract":"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.","authors":["Weicheng Ma","Ruibo Liu","Lili Wang","Soroush Vosoughi"],"demo_url":"","keywords":["natural tasks","emojis","linguistic components","multi-class setting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.542","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3329","main.3437","main.1675","main.1766","main.3013"],"title":"Multi-resolution Annotations for Emoji Prediction","tldr":"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary ta...","track":"NLP Applications"},"forum":"main.2261","id":"main.2261","presentation_id":"38939081"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2367.png","content":{"abstract":"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.","authors":["Michal Lukasik","Boris Dadachev","Kishore Papineni","Gon\u00e7alo Sim\u00f5es"],"demo_url":"","keywords":["document segmentation","nlp tasks","downstream tasks","information retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.380","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1129","main.151","main.2476","main.825","main.693"],"title":"Text Segmentation by Cross Segment Attention","tldr":"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three t...","track":"NLP Applications"},"forum":"main.2367","id":"main.2367","presentation_id":"38939099"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2396.png","content":{"abstract":"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.  However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning.  We study GigaBERT's effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.","authors":["Wuwei Lan","Yang Chen","Wei Xu","Alan Ritter"],"demo_url":"","keywords":["cross-lingual transfer","arabic tasks","arabic nlp","zero-short transfer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.382","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.871","TACL.2107","main.1803","main.143","main.1263"],"title":"An Empirical Study of Pre-trained Transformers for Arabic Information Extraction","tldr":"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.  However, their performance on Arabic information extraction (IE...","track":"NLP Applications"},"forum":"main.2396","id":"main.2396","presentation_id":"38939107"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2515.png","content":{"abstract":"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.","authors":["Fei Tan","Yifan Hu","Changwei Hu","Keqian Li","Kevin Yen"],"demo_url":"","keywords":["content moderation","text manipulation","masked recovery","hate task"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.383","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2396","main.1898","main.3337","main.2389","main.247"],"title":"TNT: Text Normalization based Pre-training of Transformers for Content Moderation","tldr":"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation ...","track":"NLP Applications"},"forum":"main.2515","id":"main.2515","presentation_id":"38939136"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2579.png","content":{"abstract":"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.","authors":["Patrick Xia","Shijie Wu","Benjamin Van Durme"],"demo_url":"","keywords":["language learning","pretrained encoders","model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.608","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2476","main.2931","main.883","main.2122","main.2078"],"title":"Which *BERT? A Survey Organizing Contextualized Encoders","tldr":"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While signific...","track":"NLP Applications"},"forum":"main.2579","id":"main.2579","presentation_id":"38939146"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2764.png","content":{"abstract":"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.","authors":["Deyu Zhou","Xuemeng Hu","Rui Wang"],"demo_url":"","keywords":["natural community","graph networks","gnns","message passing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.310","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.782","main.1488","TACL.2121","main.2761","main.151"],"title":"Neural Topic Modeling by Incorporating Document Relationship Graph","tldr":"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural...","track":"NLP Applications"},"forum":"main.2764","id":"main.2764","presentation_id":"38939189"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2779.png","content":{"abstract":"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.","authors":["Zhiwei Yu","Hongyu Zang","Xiaojun Wan"],"demo_url":"","keywords":["recipe generation","content selection","recipe task","routing method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.311","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3327","main.2943","demo.119","main.3541","main.2382"],"title":"Routing Enforced Generative Model for Recipe Generation","tldr":"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much inform...","track":"NLP Applications"},"forum":"main.2779","id":"main.2779","presentation_id":"38939193"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2962.png","content":{"abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.","authors":["David Wadden","Shanchuan Lin","Kyle Lo","Lucy Lu Wang","Madeleine van Zuylen","Arman Cohan","Hannaneh Hajishirzi"],"demo_url":"","keywords":["scientific verification","scifact","domain techniques","covid-"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.609","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3151","main.2570","main.2117","TACL.2049","main.2506"],"title":"Fact or Fiction: Verifying Scientific Claims","tldr":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we...","track":"NLP Applications"},"forum":"main.2962","id":"main.2962","presentation_id":"38939235"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2990.png","content":{"abstract":"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5~ outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.","authors":["Colin Clement","Dawn Drain","Jonathan Timcheck","Alexey Svyatkovskiy","Neel Sundaresan"],"demo_url":"","keywords":["automated understanding","docstring generation","method generation","docstring summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.728","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2590","main.2382","main.246","main.648","main.852"],"title":"PyMT5: multi-mode translation of natural language and Python code with transformers","tldr":"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transforme...","track":"NLP Applications"},"forum":"main.2990","id":"main.2990","presentation_id":"38939242"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3013.png","content":{"abstract":"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.","authors":["Qianli Ma","Zhenxi Lin","Jiangyue Yan","Zipeng Chen","Liuhong Yu"],"demo_url":"","keywords":["sentence classification","extracting features","generalization","cnn models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.544","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2430","main.989","main.471","main.930","main.3656"],"title":"MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification","tldr":"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redun...","track":"NLP Applications"},"forum":"main.3013","id":"main.3013","presentation_id":"38939250"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3049.png","content":{"abstract":"In recent years, there has been an increasing interest in the application of Artificial Intelligence \u2013 and especially Machine Learning \u2013 to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","authors":["Costanza Conforti","Stephanie Hirmer","Dai Morgan","Marco Basaldella","Yau Ben Or"],"demo_url":"","keywords":["sustainable development","sd","project sustainability","community profiling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.677","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.748","main.1669","main.1923","main.2068","main.387"],"title":"Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling","tldr":"In recent years, there has been an increasing interest in the application of Artificial Intelligence \u2013 and especially Machine Learning \u2013 to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this...","track":"NLP Applications"},"forum":"main.3049","id":"main.3049","presentation_id":"38939257"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3054.png","content":{"abstract":"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.\\footnote{Our code is available at \\url{https://github.com/WangsyGit/PathQG}.}","authors":["Siyuan Wang","Zhongyu Wei","Zhihao Fan","Zengfeng Huang","Weijian Sun","Qi Zhang","Xuanjing Huang"],"demo_url":"","keywords":["question generation","query learning","query-based generation","sequence problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.729","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.3186","main.2650","main.2586","main.3327","main.1022"],"title":"PathQG: Neural Question Generation from Facts","tldr":"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate fact...","track":"NLP Applications"},"forum":"main.3054","id":"main.3054","presentation_id":"38939259"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3065.png","content":{"abstract":"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.","authors":["Ruibo Liu","Guangxuan Xu","Chenyan Jia","Weicheng Ma","Lili Wang","Soroush Vosoughi"],"demo_url":"","keywords":["data augmentation","nlu tasks","data boost","text tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.726","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2068","main.2733","main.1923","main.1356","main.493"],"title":"Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation","tldr":"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforceme...","track":"NLP Applications"},"forum":"main.3065","id":"main.3065","presentation_id":"38939262"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3093.png","content":{"abstract":"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.","authors":["Dhanasekar Sundararaman","Shijing Si","Vivek Subramanian","Guoyin Wang","Devamanyu Hazarika","Lawrence Carin"],"demo_url":"","keywords":["numerical reasoning","question answering","list maximum","decoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.384","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3292","main.3635","main.1305","main.2251","main.2596"],"title":"Methods for Numeracy-Preserving Word Embeddings","tldr":"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving t...","track":"NLP Applications"},"forum":"main.3093","id":"main.3093","presentation_id":"38939268"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3126.png","content":{"abstract":"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT's architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.","authors":["Thanh Tran","Yifan Hu","Changwei Hu","Kevin Yen","Fei Tan","Kyumin Lee","Se Rim Park"],"demo_url":"","keywords":["downstream task","hatespeech classification","habertor model","bert model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.606","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.426","main.3434","main.47","main.2914","TACL.2389"],"title":"HABERTOR: An Efficient and Effective Deep Hatespeech Detector","tldr":"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classific...","track":"NLP Applications"},"forum":"main.3126","id":"main.3126","presentation_id":"38939273"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3151.png","content":{"abstract":"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.","authors":["Neema Kotonya","Francesca Toni"],"demo_url":"","keywords":["fact-checking","fact-checking studies","explainable fact-checking","public health"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.623","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2962","main.2117","main.2570","TACL.2049","main.2506"],"title":"Explainable Automated Fact-Checking for Public Health Claims","tldr":"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for o...","track":"NLP Applications"},"forum":"main.3151","id":"main.3151","presentation_id":"38939277"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3174.png","content":{"abstract":"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.","authors":["Wenshuo Yang","Jiyi Li","Fumiyo Fukumoto","Yanming Ye"],"demo_url":"","keywords":["data problem","multi-label classification","hybrid solution","general networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.545","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3013","main.1575","main.493","main.426","main.1217"],"title":"HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification","tldr":"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases...","track":"NLP Applications"},"forum":"main.3174","id":"main.3174","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.328.png","content":{"abstract":"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.","authors":["Jiaming Shen","Heng Ji","Jiawei Han"],"demo_url":"","keywords":["linguistic steganography","syntactical modification","neural models","neural lms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.22","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.371","main.2914","main.3391","main.60","main.125"],"title":"Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding","tldr":"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neur...","track":"NLP Applications"},"forum":"main.328","id":"main.328","presentation_id":"38938685"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3299.png","content":{"abstract":"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.","authors":["Mengyun Chen","Tao Ge","Xingxing Zhang","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["erroneous detection","erroneous correction","inference","language-independent approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.581","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2448","main.767","main.2357","TACL.2047","main.639"],"title":"Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction","tldr":"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically...","track":"NLP Applications"},"forum":"main.3299","id":"main.3299","presentation_id":"38939306"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3321.png","content":{"abstract":"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.","authors":["Weijie Yu","Chen Xu","Jun Xu","Liang Pang","Xiaopeng Gao","Xiaozhao Wang","Ji-Rong Wen"],"demo_url":"","keywords":["real-world practices","text matching","matching models","match method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.239","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3609","demo.79","main.2790","main.2076","main.973"],"title":"Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains","tldr":"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is of...","track":"NLP Applications"},"forum":"main.3321","id":"main.3321","presentation_id":"38939309"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3344.png","content":{"abstract":"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small  vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners\u2019 performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent\u2019s performance. To enable the agent to behave like a learner, we leverage entailment modeling\u2019s capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning.","authors":["Yun-Hsuan Jen","Chieh-Yang Huang","MeiHua Chen","Ting-Hao Huang","Lun-Wei Ku"],"demo_url":"","keywords":["sentence tasks","classroom study","english-as-a-second learners","inference-based agent"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.312","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2630","main.76","main.471","main.41","TACL.2041"],"title":"Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent","tldr":"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small  vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafte...","track":"NLP Applications"},"forum":"main.3344","id":"main.3344","presentation_id":"38939314"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3431.png","content":{"abstract":"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic. A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products. Hence, multi-product AD post generation is meaningful and important. We propose a novel end-to-end model named S-MG Net to generate the AD post. Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network). (2) generate a post including selected products via the MGenNet (Multi-Generator Network). Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products. Then, MGenNet generates the description copywriting of each product. Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.","authors":["Zhangming Chan","Yuchi Zhang","Xiuying Chen","Shen Gao","Zhiqiang Zhang","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["multi-product generation","real-world problem","ad task","human evaluations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.313","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.702","main.3088","main.3136","main.317","main.471"],"title":"Selection and Generation: Learning towards Multi-Product Advertisement Post Generation","tldr":"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the cust...","track":"NLP Applications"},"forum":"main.3431","id":"main.3431","presentation_id":"38939333"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3438.png","content":{"abstract":"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event's life cycle. Such coarse-grained methods failed to capture the event's unique features in different states. To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation. Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events. For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction. This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection. Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems. We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.","authors":["Rui Xia","Kaizhou Xuan","Jianfei Yu"],"demo_url":"","keywords":["automatic detection","rumor detection","rumor prediction","early detection"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.727","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1749","main.1465","main.2508","main.1135","main.1116"],"title":"A State-independent and Time-evolving Network for Early Rumor Detection in Social Media","tldr":"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However,...","track":"NLP Applications"},"forum":"main.3438","id":"main.3438","presentation_id":"38939336"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3486.png","content":{"abstract":"We conduct a large scale empirical investigation  of  contextualized  number  prediction  in running  text.    Specifically,  we  consider  two tasks: (1)masked number prediction\u2013 predict-ing  a  missing  numerical  value  within  a  sentence, and (2)numerical anomaly detection\u2013detecting  an  errorful  numeric  value  within  a sentence.   We  experiment  with  novel  combinations of contextual encoders and output distributions over the real number line.   Specifically,  we introduce a suite of output distribution  parameterizations  that  incorporate  latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent  and  transformer-based  encoder  architectures.   We  evaluate  these  models  on  two  numeric  datasets  in  the  financial  and  scientific domain.   Our  findings  show  that  output  distributions that incorporate discrete latent variables  and  allow  for  multiple  modes  outperform  simple  flow-based  counterparts  on  all datasets, yielding more accurate numerical pre-diction and anomaly detection.  We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.","authors":["Taylor Berg-Kirkpatrick","Daniel Spokoyny"],"demo_url":"","keywords":["contextualized prediction","prediction","detecting","numerical pre-diction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.385","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2650","main.1159","TACL.2411","main.2307"],"title":"An Empirical Investigation of Contextualized Number Prediction","tldr":"We conduct a large scale empirical investigation  of  contextualized  number  prediction  in running  text.    Specifically,  we  consider  two tasks: (1)masked number prediction\u2013 predict-ing  a  missing  numerical  value  within  a  sentence, and (2...","track":"NLP Applications"},"forum":"main.3486","id":"main.3486","presentation_id":"38939346"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3496.png","content":{"abstract":"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures. We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms. To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task. We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation. Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines. Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.","authors":["Milan Aggarwal","Hiresh Gupta","Mausoom Sarkar","Balaji Krishnamurthy"],"demo_url":"","keywords":["document extraction","semantic task","image resolution","structure extraction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.314","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2367","main.652","main.1755","main.1129","main.1159"],"title":"Form2Seq : A Framework for Higher-Order Form Structure Extraction","tldr":"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to whi...","track":"NLP Applications"},"forum":"main.3496","id":"main.3496","presentation_id":"38939348"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3567.png","content":{"abstract":"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..","authors":["Wei Song","Kai Zhang","Ruiji Fu","Lizhen Liu","Ting Liu","Miaomiao Cheng"],"demo_url":"","keywords":["supervised fine-tuning","pre-training method","weakly pre-training","essay scorer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.546","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1299","main.2635","main.2947","main.2793","main.1032"],"title":"Multi-Stage Pre-training for Automated Chinese Essay Scoring","tldr":"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is ...","track":"NLP Applications"},"forum":"main.3567","id":"main.3567","presentation_id":"38939367"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3573.png","content":{"abstract":"State of the art research for date-time\\footnote{We use date-time entities, date entities, time entities and temporal entities interchangeably to denote entities associated with dates and/ or times.} entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don\u2019t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.","authors":["Barun Patra","Chala Fufa","Pamela Bhattacharya","Charles Lee"],"demo_url":"","keywords":["entity extraction","generic extraction","date-time extraction","identifying constraints"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.678","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3617","main.1135","main.1116","main.3497","main.2849"],"title":"To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints","tldr":"State of the art research for date-time\\footnote{We use date-time entities, date entities, time entities and temporal entities interchangeably to denote entities associated with dates and/ or times.} entity extraction from text is task agnostic. Cons...","track":"NLP Applications"},"forum":"main.3573","id":"main.3573","presentation_id":"38939368"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3651.png","content":{"abstract":"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.","authors":["Jinghui Yan","Yining Wang","Lu Xiang","Yu Zhou","Chengqing Zong"],"demo_url":"","keywords":["medical normalization","medical processing","chinese normalization","multi-implication procedures"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.116","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.911","main.3216","main.3298","main.2947","main.1952"],"title":"A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization","tldr":"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. H...","track":"NLP Applications"},"forum":"main.3651","id":"main.3651","presentation_id":"38939383"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3676.png","content":{"abstract":"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.","authors":["Xiao Li","Guanyi Chen","Chenghua Lin","Ruizhe Li"],"demo_url":"","keywords":["text transfer","dgst","dual-generator architecture","discriminators"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.578","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2476","main.2367","main.2412","main.2422","main.1581"],"title":"DGST: a Dual-Generator Network for Text Style Transfer","tldr":"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experim...","track":"NLP Applications"},"forum":"main.3676","id":"main.3676","presentation_id":"38939386"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.470.png","content":{"abstract":"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC). Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines. As a result, a high Kappa score of 61% is achieved for inter-annotator agreement. Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2). Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.","authors":["Changmao Li","Elaine Fisher","Rebecca Thomas","Steve Pittard","Vicki Hertzberg","Jinho D. Choi"],"demo_url":"","keywords":["resume classification","job applications","real platforms","triple annotation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.679","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.471","main.2724","main.977","main.3617","main.143"],"title":"Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models","tldr":"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are ...","track":"NLP Applications"},"forum":"main.470","id":"main.470","presentation_id":"38938715"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.55.png","content":{"abstract":"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.","authors":["Akshay Smit","Saahil Jain","Pranav Rajpurkar","Anuj Pareek","Andrew Ng","Matthew Lungren"],"demo_url":"","keywords":["extraction labels","large-scale models","labeling","medical labeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.117","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1271","main.1942","demo.72","main.1611","main.2337"],"title":"Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT","tldr":"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual...","track":"NLP Applications"},"forum":"main.55","id":"main.55","presentation_id":"38938643"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.767.png","content":{"abstract":"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.","authors":["Simon Flachs","Oph\u00e9lie Lacroix","Helen Yannakoudakis","Marek Rei","Anders S\u00f8gaard"],"demo_url":"","keywords":["gec applications","gec","gec systems","internal model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.680","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2047","TACL.2013","main.3181","main.2271","main.3115"],"title":"Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses","tldr":"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and re...","track":"NLP Applications"},"forum":"main.767","id":"main.767","presentation_id":"38938771"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.802.png","content":{"abstract":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark.  Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","authors":["Emily Dinan","Angela Fan","Ledell Wu","Jason Weston","Douwe Kiela","Adina Williams"],"demo_url":"","keywords":["detecting bias","machine models","nlp models","fine-grained framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.23","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2886","main.838","main.834","main.1611","TACL.2011"],"title":"Multi-Dimensional Gender Bias Classification","tldr":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in tex...","track":"NLP Applications"},"forum":"main.802","id":"main.802","presentation_id":"38938775"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.809.png","content":{"abstract":"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.","authors":["Elena V. Epure","Guillaume Salha","Manuel Moussallam","Romain Hennequin"],"demo_url":"","keywords":["music perception","unsupervised annotation","musicology","music retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.386","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["CL.2","main.1613","main.2718","main.3116","main.2596"],"title":"Modeling the Music Genre Perception across Language-Bound Cultures","tldr":"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in...","track":"NLP Applications"},"forum":"main.809","id":"main.809","presentation_id":"38938776"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.821.png","content":{"abstract":"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.","authors":["Matthew Khoury","Rumen Dangovski","Longwu Ou","Preslav Nakov","Yichen Shen","Li Jing"],"demo_url":"","keywords":["natural applications","neural translation","neural nmt","neural"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.640","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1485","main.2491","main.1960","main.3337","main.522"],"title":"Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications","tldr":"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size re...","track":"NLP Applications"},"forum":"main.821","id":"main.821","presentation_id":"38938778"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.851.png","content":{"abstract":"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character\u2019s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.","authors":["Victor Martinez","Krishna Somandepalli","Yalda Tehranian-Uhls","Shrikanth Narayanan"],"demo_url":"","keywords":["creative production","movie representations","multi-task approach","violent content"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.387","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3352","main.3072","main.2561","main.838","main.789"],"title":"Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts","tldr":"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited i...","track":"NLP Applications"},"forum":"main.851","id":"main.851","presentation_id":"38938784"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.956.png","content":{"abstract":"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover's Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model's performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression","authors":["Jianquan Li","Xiaokang Liu","Honghong Zhao","Ruifeng Xu","Min Yang","Yaohong Jin"],"demo_url":"","keywords":["natural tasks","nlp tasks","matching","many-to-many mapping"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.242","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1485","main.3394","main.2783","main.1707","TACL.2041"],"title":"BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance","tldr":"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-c...","track":"NLP Applications"},"forum":"main.956","id":"main.956","presentation_id":"38938812"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/CL.3.png","content":{"abstract":"Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, very few attempts to incorporate out-of-game signals have been made. Specifically, it was previously unclear whether linguistic signals gathered from players\u2019 interviews can add information that does not appear in performance metrics. To bridge that gap, we define text classification tasks of predicting deviations from mean in NBA players\u2019 in-game actions, which are associated with strategic choices, player behavior, and risk, using their choice of language prior to the game. We collected a data set of transcripts from key NBA players\u2019 pre-game interviews and their in-game performance metrics, totalling 5,226 interview-metric pairs. We design neural models for players\u2019 action prediction based on increasingly more complex aspects of the language signals in their openended interviews. Our models can make their predictions based on the textual signal alone, or on a combination of that signal with signals from past-performance metrics. Our text-based models outperform strong baselines trained on performance metrics only, demonstrating the importance of language usage for action prediction. Moreover, the models that utilize both textual input and past-performance metrics produced the best results. Finally, as neural networks are notoriously difficult to interpret, we propose a method for gaining further insight into what our models have learned. Particularly, we present a latent Dirichlet allocation\u2013based analysis, where we interpretmodel predictions in terms of correlated topics. We find that our best performing textual modelis most associated with topics that are intuitively related to each prediction task and that bettermodels yield higher correlation with more informative topics.","authors":["Nadav Oved","Amir Feder","Roi Reichart"],"demo_url":"","keywords":["computer science","player prediction","text tasks","players prediction"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1578","main.2574","main.527","main.2410","main.851"],"title":"Predicting In-game Actions from Interviews of NBA Players","tldr":"Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, v...","track":"NLP Applications"},"forum":"CL.3","id":"CL.3","presentation_id":"38939391"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2047.png","content":{"abstract":"Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning data in the BEA-2019 shared task. Building upon recent work in Neural Machine Translation (NMT), we make use of both kinds of data by deriving example-level scores on our large pretraining data based on a smaller, higher-quality dataset. In this work, we perform an empirical study to discover how to best incorporate delta-log-perplexity, a type of example scoring, into a training schedule for GEC. In doing so, we perform experiments that shed light on the function and applicability of delta-log-perplexity. Models trained on scored data achieve state-of-the-art results on common GEC test sets.","authors":["Jared Lichtarge","Chris Alberti","Shankar Kumar"],"demo_url":"","keywords":["neural nmt","neural","example scoring","gec"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.767","main.1351","main.3227","main.1960","main.2491"],"title":"Data Weighted Training Strategies for Grammatical Error Correction","tldr":"Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning...","track":"NLP Applications"},"forum":"TACL.2047","id":"TACL.2047","presentation_id":"38939401"}]
