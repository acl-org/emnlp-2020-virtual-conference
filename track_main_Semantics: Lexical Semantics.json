[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1305.png","content":{"abstract":"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.","authors":["Kian Kenyon-Dean","Edward Newell","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["nlp applications","nlp tasks","word embeddings","feature words"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.681","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1428","main.585","main.2596","main.2251"],"title":"Deconstructing word embedding algorithms","tldr":"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memo...","track":"Semantics: Lexical Semantics"},"forum":"main.1305","id":"main.1305","presentation_id":"38938885"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1393.png","content":{"abstract":"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.","authors":["Adam Tsakalidis","Maria Liakata"],"demo_url":"","keywords":["semantic detection","detecting words","neural embeddings","vector representation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.682","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1938","main.3093","main.1957","main.1135","main.1935"],"title":"Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection","tldr":"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representat...","track":"Semantics: Lexical Semantics"},"forum":"main.1393","id":"main.1393","presentation_id":"38938897"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1395.png","content":{"abstract":"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors. Our method requires only another pre-trained model and no labeled data is needed. We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning  both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.","authors":["Qianchu Liu","Diana McCarthy","Anna Korhonen"],"demo_url":"","keywords":["transformation","contextualized models","dynamic embeddings","post-processing technique"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.333","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2251","main.2363","main.3292","main.298","main.143"],"title":"Towards Better Context-aware Lexical Semantics:Adjusting Contextualized Representations through Static Anchors","tldr":"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that e...","track":"Semantics: Lexical Semantics"},"forum":"main.1395","id":"main.1395","presentation_id":"38938898"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.143.png","content":{"abstract":"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.","authors":["Alessandro Raganato","Tommaso Pasini","Jose Camacho-Collados","Mohammad Taher Pilehvar"],"demo_url":"","keywords":["disambiguation task","binary problem","evaluation scenarios","zero-shot transfer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.584","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1379","main.623","main.2630","main.1061","main.2251"],"title":"XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization","tldr":"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNe...","track":"Semantics: Lexical Semantics"},"forum":"main.143","id":"main.143","presentation_id":"38938656"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1857.png","content":{"abstract":"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains. Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem. In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings. We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.","authors":["Xin Wu","Yi Cai","Yang Kai","Tao Wang","Qing Li"],"demo_url":"","keywords":["natural tasks","downstream tasks","meta-embedding learning","meta-embedding methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.282","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1952","main.151","main.2078","TACL.2255","main.3298"],"title":"Task-oriented Domain-specific Meta-Embedding for Text Classification","tldr":"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-...","track":"Semantics: Lexical Semantics"},"forum":"main.1857","id":"main.1857","presentation_id":"38938998"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1935.png","content":{"abstract":"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.","authors":["Daniel Loureiro","Jose Camacho-Collados"],"demo_url":"","keywords":["word disambiguation","word","wsd","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.283","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3224","main.2251","main.143","main.644","main.2349"],"title":"Don't Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation","tldr":"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotate...","track":"Semantics: Lexical Semantics"},"forum":"main.1935","id":"main.1935","presentation_id":"38939011"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2076.png","content":{"abstract":"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the  help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved. However,  they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark  datasets, our framework demonstrates improvements that are both competitive and explainable.","authors":["Changlong Yu","Jialong Han","Peifeng Wang","Yangqiu Song","Hongming Zhang","Wilfred Ng","Shuming Shi"],"demo_url":"","keywords":["hypernymy detection","pattern-based ones","distributional methods","pattern-based model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.502","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2122","main.210","main.143","main.84","main.1997"],"title":"When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models","tldr":"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the  help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Rec...","track":"Semantics: Lexical Semantics"},"forum":"main.2076","id":"main.2076","presentation_id":"38939044"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2094.png","content":{"abstract":"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.","authors":["Aina Gar\u00ed Soler","Marianna Apidianaki"],"demo_url":"","keywords":["natural reasoning","intensity detection","indirect task","bert-based approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.598","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3457","main.210","main.2076","main.2122","main.1970"],"title":"BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations","tldr":"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approac...","track":"Semantics: Lexical Semantics"},"forum":"main.2094","id":"main.2094","presentation_id":"38939048"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2112.png","content":{"abstract":"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century') within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.","authors":["Marius Pasca"],"demo_url":"","keywords":["automatically constituents","open-domain method","modifier constituents","constituent modifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.503","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["demo.89","main.2512","main.3507","main.3453","main.3216"],"title":"Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs","tldr":"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century') within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored unders...","track":"Semantics: Lexical Semantics"},"forum":"main.2112","id":"main.2112","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2151.png","content":{"abstract":"We propose the novel \\emph{Within-Between} Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.","authors":["Oren Barkan","Avi Caciularu","Ido Dagan"],"demo_url":"","keywords":["recognizing relations","sub-space representation","lexical-semantic relations","relational signals"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.284","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.3593","main.2112","main.447","main.2419"],"title":"Within-Between Lexical Relation Classification","tldr":"We propose the novel \\emph{Within-Between} Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show t...","track":"Semantics: Lexical Semantics"},"forum":"main.2151","id":"main.2151","presentation_id":"38939060"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2251.png","content":{"abstract":"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.","authors":["Bianca Scarlini","Tommaso Pasini","Roberto Navigli"],"demo_url":"","keywords":["natural processing","english task","word-in-context task","contextualized embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.285","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.298","main.143","main.3224","main.1935","main.3093"],"title":"With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation","tldr":"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In...","track":"Semantics: Lexical Semantics"},"forum":"main.2251","id":"main.2251","presentation_id":"38939078"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2349.png","content":{"abstract":"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.","authors":["Michele Bevilacqua","Marco Maru","Roberto Navigli"],"demo_url":"","keywords":["generative modeling","definition modeling","discriminative tasks","word disambiguation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.585","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.143","main.3093","main.1935","main.2363","main.2251"],"title":"Generationary or \u201cHow We Went beyond Word Sense Inventories and Learned to Gloss\u201d","tldr":"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce ...","track":"Semantics: Lexical Semantics"},"forum":"main.2349","id":"main.2349","presentation_id":"38939096"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2363.png","content":{"abstract":"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.","authors":["Ivan Vuli\u0107","Edoardo Maria Ponti","Robert Litschko","Goran Glava\u0161","Anna Korhonen"],"demo_url":"","keywords":["lexical tasks","pretrained models","lms","lexical strategies"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.586","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2630","main.1970","main.1130","main.143","TACL.2411"],"title":"Probing Pretrained Language Models for Lexical Semantics","tldr":"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic,...","track":"Semantics: Lexical Semantics"},"forum":"main.2363","id":"main.2363","presentation_id":"38939098"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2596.png","content":{"abstract":"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.","authors":["Charles Welch","Jonathan K. Kummerfeld","Ver\u00f3nica P\u00e9rez-Rosas","Rada Mihalcea"],"demo_url":"","keywords":["language tasks","language modeling","word associations","word embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.334","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2792","main.1305","TACL.2093","main.585"],"title":"Compositional Demographic Word Embeddings","tldr":"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve lang...","track":"Semantics: Lexical Semantics"},"forum":"main.2596","id":"main.2596","presentation_id":"38939153"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2891.png","content":{"abstract":"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation. Since our approach is language independent, we perform WSD experiments on several languages. The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD. To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.","authors":["Yixing Luan","Bradley Hauer","Lili Mou","Grzegorz Kondrak"],"demo_url":"","keywords":["monolingual disambiguation","monolingual wsd","english wsd","wsd systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.332","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3224","main.1935","main.1503","main.2718","main.639"],"title":"Improving Word Sense Disambiguation with Translations","tldr":"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generat...","track":"Semantics: Lexical Semantics"},"forum":"main.2891","id":"main.2891","presentation_id":"38939218"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2920.png","content":{"abstract":"In politics, neologisms are frequently invented for partisan objectives. For example, ``undocumented workers\u201d and ``illegal aliens\u201d refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g.,  \u201cimmigrants\" vs. \u201caliens\", \u201cestate tax\" vs. \u201cdeath tax\") move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.","authors":["Albert Webson","Zhizhong Chen","Carsten Eickhoff","Ellie Pavlick"],"demo_url":"","keywords":["reference-based theories","nlp","intrinsic interpretability","extrinsic application"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.335","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2072","main.3450","main.32","TACL.2011","main.353"],"title":"Do \u201cUndocumented Workers\u201d == \u201cIllegal Aliens\u201d? Differentiating Denotation and Connotation in Vector Spaces","tldr":"In politics, neologisms are frequently invented for partisan objectives. For example, ``undocumented workers\u201d and ``illegal aliens\u201d refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations...","track":"Semantics: Lexical Semantics"},"forum":"main.2920","id":"main.2920","presentation_id":"38939226"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.298.png","content":{"abstract":"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.","authors":["Ming Wang","Yinglin Wang"],"demo_url":"","keywords":["word disambiguation","word","sense enhancement","contextual embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.504","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2251","main.3224","main.1935","main.2891","main.1395"],"title":"A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation","tldr":"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this ...","track":"Semantics: Lexical Semantics"},"forum":"main.298","id":"main.298","presentation_id":"38938678"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3224.png","content":{"abstract":"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations. We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets. We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks. Our results indicate a significant improvement over the application of the dense word representations.","authors":["G\u00e1bor Berend"],"demo_url":"","keywords":["fine-grained disambiguation","part-of-speech tagging","sparse representations","task-specific models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.683","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1935","main.2251","main.298","main.2891","main.644"],"title":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","tldr":"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relie...","track":"Semantics: Lexical Semantics"},"forum":"main.3224","id":"main.3224","presentation_id":"38939289"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.598.png","content":{"abstract":"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.","authors":["Jie Huang","Zilong Wang","Kevin Chang","Wen-mei Hwu","JinJun Xiong"],"demo_url":"","keywords":["natural processing","artificial intelligence","linear regression","semantic capacity"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.684","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2179","main.315","main.1938","main.1970","demo.89"],"title":"Exploring Semantic Capacity of Terms","tldr":"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity...","track":"Semantics: Lexical Semantics"},"forum":"main.598","id":"main.598","presentation_id":"38938735"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/TACL.2011.png","content":{"abstract":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology which not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighbouring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric - Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02%. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","authors":["Vaibhav Kumar","Tenzin Bhotia","Vaibhav Kumar","Tanmoy Chakraborty"],"demo_url":"","keywords":["word embeddings","semantic words","coreference resolution","post-processing methods"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1018","main.3093","main.838","main.1399","main.2596"],"title":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings","tldr":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-proces...","track":"Semantics: Lexical Semantics"},"forum":"TACL.2011","id":"TACL.2011","presentation_id":"38939398"}]
