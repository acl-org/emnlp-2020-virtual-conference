[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1100.png","content":{"abstract":"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.","authors":["Hao-Ran Wei","Zhirui Zhang","Boxing Chen","Weihua Luo"],"demo_url":"","keywords":["domain-specific translation","domain adaptation","back-translation method","out-of-domain systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.474","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1572","main.2661","main.3227","main.856","main.888"],"title":"Iterative Domain-Repaired Back-Translation","tldr":"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translat...","track":"Machine Translation and Multilinguality"},"forum":"main.1100","id":"main.1100","presentation_id":"38938843"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1146.png","content":{"abstract":"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.","authors":["Dana Ruiter","Josef van Genabith","Cristina Espa\u00f1a-Bonet"],"demo_url":"","keywords":["training","self-supervised","ssnmt","ssnmt model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.202","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.26","TACL.2107","main.701","main.130","main.2893"],"title":"Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation","tldr":"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In t...","track":"Machine Translation and Multilinguality"},"forum":"main.1146","id":"main.1146","presentation_id":"38938854"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1248.png","content":{"abstract":"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.","authors":["Jind\u0159ich Libovick\u00fd","Alexander Fraser"],"demo_url":"","keywords":["transformer architecture","segmentation","subword model","neural model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.203","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3337","main.1733","main.1960","main.2675","main.1798"],"title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems","tldr":"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show tha...","track":"Machine Translation and Multilinguality"},"forum":"main.1248","id":"main.1248","presentation_id":"38938871"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.130.png","content":{"abstract":"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT\u201916 English-German and WMT\u201914 English-French translation tasks show that it is 1:4 \u0002 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/ SDT-Training.","authors":["Bei Li","Ziyang Wang","Hui Liu","Yufan Jiang","Quan Du","Tong Xiao","Huizhen Wang","Jingbo Zhu"],"demo_url":"","keywords":["neural systems","wmt tasks","deep encoders","deep encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.72","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.3227","main.1485","main.522","main.1680"],"title":"Shallow-to-Deep Training for Neural Machine Translation","tldr":"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the ...","track":"Machine Translation and Multilinguality"},"forum":"main.130","id":"main.130","presentation_id":"38938655"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1383.png","content":{"abstract":"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.","authors":["Philipp Dufter","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["zero-shot transfer","training","unsupervised alignment","xnli"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.358","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2490","main.1986","main.517","main.3116","TACL.2107"],"title":"Identifying Elements Essential for BERT's Multilinguality","tldr":"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literat...","track":"Machine Translation and Multilinguality"},"forum":"main.1383","id":"main.1383","presentation_id":"38938893"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1432.png","content":{"abstract":"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT\u201914 En\u2192De, WMT\u201916 Ro\u2192En and IWSLT\u201916 De\u2192En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT\u201914 En\u2192De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).","authors":["Jason Lee","Raphael Shu","Kyunghyun Cho"],"demo_url":"","keywords":["non-autoregressive translation","translation","machine translation","inference procedure"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.73","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.453","main.2430","main.2661","main.3348","main.891"],"title":"Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation","tldr":"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train ...","track":"Machine Translation and Multilinguality"},"forum":"main.1432","id":"main.1432","presentation_id":"38938904"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1445.png","content":{"abstract":"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers' generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.","authors":["Zirui Wang","Zachary C. Lipton","Yulia Tsvetkov"],"demo_url":"","keywords":["multilingual models","meta-learning algorithm","multilingual representations","negative interference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.359","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3116","main.852","main.522","main.2630","main.3688"],"title":"On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment","tldr":"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that...","track":"Machine Translation and Multilinguality"},"forum":"main.1445","id":"main.1445","presentation_id":"38938905"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1456.png","content":{"abstract":"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We release these dictionaries to the research community. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60%. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.","authors":["Naoki Otani","Satoru Ozaki","Xingyuan Zhao","Yucen Li","Micaelah St Johns","Lori Levin"],"demo_url":"","keywords":["word-translation task","mwe translation","single-word translations","cross-lingual algorithms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.360","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1503","main.3046","main.1935","main.1061","main.852"],"title":"Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings","tldr":"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separa...","track":"Machine Translation and Multilinguality"},"forum":"main.1456","id":"main.1456","presentation_id":"38938908"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1485.png","content":{"abstract":"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese\u2192English, Turkish\u2192English, and  English\u2192German directions. Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.","authors":["Yimeng Wu","Peyman Passban","Mehdi Rezagholizadeh","Qun Liu"],"demo_url":"","keywords":["knowledge distillation","neural models","knowledge kd","kd"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.74","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.130","main.618","main.3394","main.1130","main.1986"],"title":"Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers","tldr":"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is ...","track":"Machine Translation and Multilinguality"},"forum":"main.1485","id":"main.1485","presentation_id":"38938915"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1503.png","content":{"abstract":"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token's context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.","authors":["Masaaki Nagata","Katsuki Chousa","Masaaki Nishino"],"demo_url":"","keywords":["cross-language prediction","word problem","squad task","alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.41","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1061","main.639","main.2641","main.1694","main.143"],"title":"A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT","tldr":"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. ...","track":"Machine Translation and Multilinguality"},"forum":"main.1503","id":"main.1503","presentation_id":"38938923"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1504.png","content":{"abstract":"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.","authors":["Xiaomian Kang","Yang Zhao","Jiajun Zhang","Chengqing Zong"],"demo_url":"","keywords":["document-level translation","translations","document-level model","selection module"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.175","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1572","main.3227","main.246","main.1770","main.471"],"title":"Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning","tldr":"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of con...","track":"Machine Translation and Multilinguality"},"forum":"main.1504","id":"main.1504","presentation_id":"38938924"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1552.png","content":{"abstract":"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially\u2014for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage\u2019s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance","authors":["Liyuan Liu","Xiaodong Liu","Jianfeng Gao","Weizhu Chen","Jiawei Han"],"demo_url":"","keywords":["nlp tasks","training","transformer training","transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.463","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1485","demo.71","main.3543","main.2615","main.618"],"title":"Understanding the Difficulty of Training Transformers","tldr":"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectiv...","track":"Machine Translation and Multilinguality"},"forum":"main.1552","id":"main.1552","presentation_id":"38938933"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1572.png","content":{"abstract":"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to~1.8 BLEU points over competitive baselines.","authors":["Zi-Yi Dou","Antonios Anastasopoulos","Graham Neubig"],"demo_url":"","keywords":["neural translation","neural nmt","nmt","domain adaptation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.475","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1100","main.3227","main.3688","main.1680","main.1504"],"title":"Dynamic Data Selection and Weighting for Iterative Back-Translation","tldr":"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-tra...","track":"Machine Translation and Multilinguality"},"forum":"main.1572","id":"main.1572","presentation_id":"38938937"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1680.png","content":{"abstract":"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.","authors":["Yiren Wang","ChengXiang Zhai","Hany Hassan"],"demo_url":"","keywords":["bilingual nmt","bilingual","multilingual systems","translation task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.75","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3688","TACL.2107","main.522","main.852","main.1986"],"title":"Multi-task Learning for Multilingual Neural Machine Translation","tldr":"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose ...","track":"Machine Translation and Multilinguality"},"forum":"main.1680","id":"main.1680","presentation_id":"38938966"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1694.png","content":{"abstract":"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","authors":["Yun Chen","Yang Liu","Guanhua Chen","Xin Jiang","Qun Liu"],"demo_url":"","keywords":["transformer","attention mechanism","word methods","shift-att"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.42","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3688","main.1503","main.1061","main.522","main.3227"],"title":"Accurate Word Alignment Induction from Neural Machine Translation","tldr":"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignme...","track":"Machine Translation and Multilinguality"},"forum":"main.1694","id":"main.1694","presentation_id":"38938969"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1770.png","content":{"abstract":"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.","authors":["Shuhao Gu","Jinchao Zhang","Fandong Meng","Yang Feng","Wanying Xie","Jie Zhou","Dong Yu"],"demo_url":"","keywords":["nmt","vanilla model","golden distribution","token phenomenon"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.76","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.334","main.522","main.3688","main.1504","main.1694"],"title":"Token-level Adaptive Training for Neural Machine Translation","tldr":"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts t...","track":"Machine Translation and Multilinguality"},"forum":"main.1770","id":"main.1770","presentation_id":"38938984"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1798.png","content":{"abstract":"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1\\%). In addition, our methods also surpass the Transformer-Big model, with only 54\\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.","authors":["Jianhao Yan","Fandong Meng","Jie Zhou"],"demo_url":"","keywords":["neural translation","transformer","machine tasks","inference process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.77","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.618","main.130","main.3337","main.701","main.1618"],"title":"Multi-Unit Transformers for Neural Machine Translation","tldr":"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investiga...","track":"Machine Translation and Multilinguality"},"forum":"main.1798","id":"main.1798","presentation_id":"38938990"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1803.png","content":{"abstract":"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.","authors":["Jonas Pfeiffer","Ivan Vuli\u0107","Iryna Gurevych","Sebastian Ruder"],"demo_url":"","keywords":["transfer","pre-training","cross transfer","named recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.617","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.74","main.1379","main.3688","main.871","main.2500"],"title":"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer","tldr":"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to l...","track":"Machine Translation and Multilinguality"},"forum":"main.1803","id":"main.1803","presentation_id":"38938991"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1960.png","content":{"abstract":"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.","authors":["Yong Wang","Longyue Wang","Victor Li","Zhaopeng Tu"],"demo_url":"","keywords":["nmt architectures","over-parameterization","underutilization resources","redundant parameters"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.78","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.1943","main.522","main.835","main.1351","main.2661"],"title":"On the Sparsity of Neural Machine Translation Models","tldr":"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investi...","track":"Machine Translation and Multilinguality"},"forum":"main.1960","id":"main.1960","presentation_id":"38939018"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1986.png","content":{"abstract":"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.","authors":["Sungwon Lyu","Bokyung Son","Kichang Yang","Jaekyoung Bae"],"demo_url":"","keywords":["multilingual translation","multilingual industries","-","multilingual model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.476","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1680","main.3688","main.522","main.852","main.1263"],"title":"Revisiting Modularized Multilingual NMT to Meet Industrial Demands","tldr":"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industr...","track":"Machine Translation and Multilinguality"},"forum":"main.1986","id":"main.1986","presentation_id":"38939024"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2055.png","content":{"abstract":"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.","authors":["Mikel Artetxe","Gorka Labaka","Eneko Agirre"],"demo_url":"","keywords":["human translation","cross-lingual learning","natural inference","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.618","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.522","main.3688","main.1572","main.701","main.888"],"title":"Translation Artifacts in Cross-lingual Transfer Learning","tldr":"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the t...","track":"Machine Translation and Multilinguality"},"forum":"main.2055","id":"main.2055","presentation_id":"38939034"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2061.png","content":{"abstract":"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.","authors":["Jingyi Zhang","Josef van Genabith"],"demo_url":"","keywords":["qe task","qe","translation","supervised learning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.205","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.888","TACL.1997","main.1572","main.835","main.856"],"title":"Translation Quality Estimation by Jointly Learning to Score and Rank","tldr":"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of th...","track":"Machine Translation and Multilinguality"},"forum":"main.2061","id":"main.2061","presentation_id":"38939037"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2100.png","content":{"abstract":"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad  et  al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.","authors":["Xiang Kong","Zhisong Zhang","Eduard Hovy"],"demo_url":"","keywords":["translation tasks","local mechanism","non-autoregressive models","merging algorithm"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.79","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.453","TACL.2221","main.1402","main.1694","main.1432"],"title":"Incorporating a Local Translation Mechanism into Non-autoregressive Translation","tldr":"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of...","track":"Machine Translation and Multilinguality"},"forum":"main.2100","id":"main.2100","presentation_id":"38939050"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2114.png","content":{"abstract":"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.","authors":["Denis Emelin","Ivan Titov","Rico Sennrich"],"demo_url":"","keywords":["word disambiguation","nmt","prediction errors","adversarial strategy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.616","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.457","main.1935","main.2891","main.3224","main.2363"],"title":"Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks","tldr":"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-o...","track":"Machine Translation and Multilinguality"},"forum":"main.2114","id":"main.2114","presentation_id":"38939052"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2131.png","content":{"abstract":"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI,  parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.","authors":["Haim Dubossarsky","Ivan Vuli\u0107","Roi Reichart","Anna Korhonen"],"demo_url":"","keywords":["cross-lingual tasks","large-scale study","bli","parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.186","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.865","main.2278","main.2363","CL.2","main.1901"],"title":"The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures","tldr":"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approxima...","track":"Machine Translation and Multilinguality"},"forum":"main.2131","id":"main.2131","presentation_id":"38939057"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2163.png","content":{"abstract":"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account.  This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk.  An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.","authors":["Javier Iranzo-S\u00e1nchez","Adri\u00e0 Gim\u00e9nez Pastor","Joan Albert Silvestre-Cerd\u00e0","Pau Baquero-Arnal","Jorge Civera Saiz","Alfons Juan"],"demo_url":"","keywords":["st","streaming st","pipeline","automatic system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.206","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2915","TACL.2221","TACL.2107","main.106","main.888"],"title":"Direct Segmentation Models for Streaming Speech Translation","tldr":"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the AS...","track":"Machine Translation and Multilinguality"},"forum":"main.2163","id":"main.2163","presentation_id":"38939061"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2278.png","content":{"abstract":"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ``strong'' cross-lingual alignment, requiring semantically related \\textit{cross}-language pairs to be closer in representation space than unrelated \\textit{same}-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target ``weak\" alignment is not predictive of performance on LAReQA\\@. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at \\url{https://github.com/google-research-datasets/lareqa}.","authors":["Uma Roy","Noah Constant","Rami Al-Rfou","Aditya Barua","Aaron Phillips","Yinfei Yang"],"demo_url":"","keywords":["language-agnostic retrieval","cross-lingual tasks","cross-lingual retrieval","alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.477","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2630","main.1803","main.143","main.3216","main.1061"],"title":"LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool","tldr":"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ``strong'' cross-lingual alignment, requiring semantically related \\textit...","track":"Machine Translation and Multilinguality"},"forum":"main.2278","id":"main.2278","presentation_id":"38939085"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2298.png","content":{"abstract":"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.","authors":["Tahmid Hasan","Abhik Bhattacharjee","Kazi Samin","Masum Hasan","Madhusudan Basak","M. Sohel Rahman","Rifat Shahriyar"],"demo_url":"","keywords":["machine literature","erroneous segmentation","parallel creation","aligner ensembling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.207","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.522","main.1379","TACL.2107","main.888","main.852"],"title":"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation","tldr":"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; a...","track":"Machine Translation and Multilinguality"},"forum":"main.2298","id":"main.2298","presentation_id":"38939088"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2412.png","content":{"abstract":"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.","authors":["Jerin Philip","Alexandre Berard","Matthias Gall\u00e9","Laurent Besacier"],"demo_url":"","keywords":["adapter formalism","multilingual models","bilingual adapters","-language baseline"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.361","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1339","main.1803","main.835","main.1263","main.3688"],"title":"Monolingual Adapters for Zero-Shot Neural Machine Translation","tldr":"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingua...","track":"Machine Translation and Multilinguality"},"forum":"main.2412","id":"main.2412","presentation_id":"38939110"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.246.png","content":{"abstract":"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus. Additionally,  we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].  To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.","authors":["Zhen Yang","Bojie Hu","Ambyera Han","Shen Huang","Qi Ju"],"demo_url":"","keywords":["neural nmt","lexicon induction","unsupervised nmt","pre-training method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.208","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.852","main.3688","main.888","main.522"],"title":"CSP:Code-Switching Pre-training for Neural Machine Translation","tldr":"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CS...","track":"Machine Translation and Multilinguality"},"forum":"main.246","id":"main.246","presentation_id":"38938670"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2490.png","content":{"abstract":"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.","authors":["Shijie Wu","Mark Dredze"],"demo_url":"","keywords":["multilingual","unsupervised encoders","cross-lingual representation","contrastive objective"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.362","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1379","main.2278","main.143","main.1680","main.3688"],"title":"Do Explicit Alignments Robustly Improve Multilingual Encoders?","tldr":"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve ...","track":"Machine Translation and Multilinguality"},"forum":"main.2490","id":"main.2490","presentation_id":"38939127"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2493.png","content":{"abstract":"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.","authors":["Shruti Rijhwani","Antonios Anastasopoulos","Graham Neubig"],"demo_url":"","keywords":["natural models","general-purpose tools","ocr method","recognition rate"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.478","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2777","main.2847","main.517","main.870","main.1997"],"title":"OCR Post Correction for Endangered Language Texts","tldr":"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In...","track":"Machine Translation and Multilinguality"},"forum":"main.2493","id":"main.2493","presentation_id":"38939129"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2500.png","content":{"abstract":"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.","authors":["Anne Lauscher","Vinit Ravishankar","Ivan Vuli\u0107","Goran Glava\u0161"],"demo_url":"","keywords":["zero-shot transfer","downstream transfer","resource-lean scenarios","pos tagging"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.363","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.858","main.74","main.1803","main.1263","main.852"],"title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers","tldr":"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify thei...","track":"Machine Translation and Multilinguality"},"forum":"main.2500","id":"main.2500","presentation_id":"38939130"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2581.png","content":{"abstract":"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT'14 English $\\to$ German and WMT'18 English $\\to$ Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English $\\to$ German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.","authors":["William Chan","Mitchell Stern","Jamie Kiros","Jakob Uszkoreit"],"demo_url":"","keywords":["machine translation","insertion-based modeling","soft framework","transformer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.464","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.648","main.1707","main.2590","main.2382","main.870"],"title":"An Empirical Study of Generation Order for Machine Translation","tldr":"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary ora...","track":"Machine Translation and Multilinguality"},"forum":"main.2581","id":"main.2581","presentation_id":"38939147"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.26.png","content":{"abstract":"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.","authors":["Yu Wan","Baosong Yang","Derek F. Wong","Yikai Zhou","Lidia S. Chao","Haibo Zhang","Boxing Chen"],"demo_url":"","keywords":["neural","curriculum learning","translation tasks","nmt"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.80","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1146","main.701","main.2389","main.1960","main.835"],"title":"Self-Paced Learning for Neural Machine Translation","tldr":"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule...","track":"Machine Translation and Multilinguality"},"forum":"main.26","id":"main.26","presentation_id":"38938638"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2630.png","content":{"abstract":"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for \\langnum typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.","authors":["Zhengbao Jiang","Antonios Anastasopoulos","Jun Araki","Haibo Ding","Graham Neubig"],"demo_url":"","keywords":["factual retrieval","language models","lms","probing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.479","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2363","main.143","main.2278","main.3453","main.623"],"title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models","tldr":"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many language...","track":"Machine Translation and Multilinguality"},"forum":"main.2630","id":"main.2630","presentation_id":"38939158"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2661.png","content":{"abstract":"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.","authors":["Anna Currey","Prashant Mathur","Georgiana Dinu"],"demo_url":"","keywords":["translation","neural translation","multi-domain model","high-resource conditions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.364","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.856","main.1960","main.891","main.3337","main.1100"],"title":"Distilling Multiple Domains for Neural Machine Translation","tldr":"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale wel...","track":"Machine Translation and Multilinguality"},"forum":"main.2661","id":"main.2661","presentation_id":"38939168"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.267.png","content":{"abstract":"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.","authors":["Nils Reimers","Iryna Gurevych"],"demo_url":"","keywords":["training","sentence models","monolingual models","monolingual model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.365","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.410","main.3688","main.852","main.870","main.3116"],"title":"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation","tldr":"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should...","track":"Machine Translation and Multilinguality"},"forum":"main.267","id":"main.267","presentation_id":"38938673"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2674.png","content":{"abstract":"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.","authors":["Wenxiang Jiao","Xing Wang","Shilin He","Irwin King","Michael Lyu","Zhaopeng Tu"],"demo_url":"","keywords":["data rejuvenation","neural models","nmt models","identification model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.176","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3227","TACL.2047","main.894","main.522","main.1960"],"title":"Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation","tldr":"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to...","track":"Machine Translation and Multilinguality"},"forum":"main.2674","id":"main.2674","presentation_id":"38939169"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2718.png","content":{"abstract":"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.","authors":["Arturo Oncevay","Barry Haddow","Alexandra Birch"],"demo_url":"","keywords":["multilingual translation","language clustering","multilingual transfer","singular analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.187","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["CL.2","main.3116","main.750","main.870","main.2891"],"title":"Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations","tldr":"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisati...","track":"Machine Translation and Multilinguality"},"forum":"main.2718","id":"main.2718","presentation_id":"38939178"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2746.png","content":{"abstract":"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.","authors":["Ahmed El-Kishky","Vishrav Chaudhary","Francisco Guzm\u00e1n","Philipp Koehn"],"demo_url":"","keywords":["cross-lingual alignment","mining sentences","cross-lingual nlp","cross-lingual representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.480","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1298","main.1061","main.3453","main.2131","main.3046"],"title":"CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs","tldr":"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with a...","track":"Machine Translation and Multilinguality"},"forum":"main.2746","id":"main.2746","presentation_id":"38939183"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2767.png","content":{"abstract":"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically \"refills\" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.","authors":["Kevin Yang","Violet Yao","John DeNero","Dan Klein"],"demo_url":"","keywords":["variable-length decoding","decoding","variable-width search","semantic parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.366","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2169","main.3236","main.1960","main.3074","TACL.1943"],"title":"A Streaming Approach For Efficient Batched Beam Search","tldr":"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically \"refills\" the batch before proceeding w...","track":"Machine Translation and Multilinguality"},"forum":"main.2767","id":"main.2767","presentation_id":"38939191"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3051.png","content":{"abstract":"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard ``mask-predict'' algorithm, and provide analyses of its behavior on machine translation tasks.","authors":["Julia Kreutzer","George Foster","Colin Cherry"],"demo_url":"","keywords":["non-autoregressive tasks","machine translation","masked inference","machine tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.465","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2389","main.2198","main.247","main.3483","main.1356"],"title":"Inference Strategies for Machine Translation with Conditional Masking","tldr":"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference stra...","track":"Machine Translation and Multilinguality"},"forum":"main.3051","id":"main.3051","presentation_id":"38939258"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3116.png","content":{"abstract":"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.","authors":["Hyung Won Chung","Dan Garrette","Kiat Chuan Tan","Jason Riesa"],"demo_url":"","keywords":["massively applications","multilingual generation","cross-lingual sharing","multilingual models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.367","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.870","main.852","main.143","main.1445","main.410"],"title":"Improving Multilingual Models with Language-Clustered Vocabularies","tldr":"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applicatio...","track":"Machine Translation and Multilinguality"},"forum":"main.3116","id":"main.3116","presentation_id":"38939272"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3227.png","content":{"abstract":"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.","authors":["Prathyusha Jwalapuram","Shafiq Joty","Youlin Shen"],"demo_url":"","keywords":["pronoun translations","pronoun translation","neural training","backtranslation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.177","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.888","main.1572","main.852","main.3688","TACL.2107"],"title":"Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses","tldr":"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that w...","track":"Machine Translation and Multilinguality"},"forum":"main.3227","id":"main.3227","presentation_id":"38939290"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3236.png","content":{"abstract":"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.","authors":["Ruiqing Zhang","Chuanqiang Zhang","Zhongjun He","Hua Wu","Haifeng Wang"],"demo_url":"","keywords":["simultaneous translation","translation","segmentation","chinese-english translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.178","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2661","main.1402","demo.111","main.1572","main.2100"],"title":"Learning Adaptive Segmentation Policy for Simultaneous Translation","tldr":"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency w...","track":"Machine Translation and Multilinguality"},"forum":"main.3236","id":"main.3236","presentation_id":"38939292"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3337.png","content":{"abstract":"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.","authors":["Pei Zhang","Boxing Chen","Niyu Ge","Kai Fan"],"demo_url":"","keywords":["document-level translation","document-level systems","context-aware architecture","transformer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.81","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1960","main.2661","main.1618","main.835","main.891"],"title":"Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation","tldr":"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline mo...","track":"Machine Translation and Multilinguality"},"forum":"main.3337","id":"main.3337","presentation_id":"38939313"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3370.png","content":{"abstract":"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.","authors":["Shiyue Zhang","Benjamin Frey","Mohit Bansal"],"demo_url":"","keywords":["machine research","in-domain evaluation","semi-supervised learning","multilingual training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.43","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2493","main.2777","main.852","main.2298","main.1379"],"title":"ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization","tldr":"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the worl...","track":"Machine Translation and Multilinguality"},"forum":"main.3370","id":"main.3370","presentation_id":"38939321"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3441.png","content":{"abstract":"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.","authors":["Xuanfu Wu","Yang Feng","Chenze Shao"],"demo_url":"","keywords":["neural","inference","chinese-english tasks","nmt"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.82","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.891","main.2661","main.1960","main.3227","main.2389"],"title":"Generating Diverse Translation from Model Distribution with Dropout","tldr":"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with...","track":"Machine Translation and Multilinguality"},"forum":"main.3441","id":"main.3441","presentation_id":"38939337"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3597.png","content":{"abstract":"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.  We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.","authors":["Mehrad Moradshahi","Giovanni Campagna","Sina Semnani","Silei Xu","Monica Lam"],"demo_url":"","keywords":["english answering","english qa","machine translation","semantic localizer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.481","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3506","main.143","main.852","main.2777"],"title":"Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation","tldr":"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by a...","track":"Machine Translation and Multilinguality"},"forum":"main.3597","id":"main.3597","presentation_id":"38939374"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3688.png","content":{"abstract":"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then \ufb01ne-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves signi\ufb01cant performance improvement compared to directly training on those target pairs. It is the \ufb01rst time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.","authors":["Zehui Lin","Xiao Pan","Mingxuan Wang","Xipeng Qiu","Jiangtao Feng","Hao Zhou","Lei Li"],"demo_url":"","keywords":["machine mt","mt","rich mt","universal model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.210","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1680","main.522","main.852","main.267","TACL.2107"],"title":"Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information","tldr":"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-tra...","track":"Machine Translation and Multilinguality"},"forum":"main.3688","id":"main.3688","presentation_id":"38939388"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.400.png","content":{"abstract":"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks. To address the issues, we propose a meta graph learning (MGL) method. Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages. Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer. Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.","authors":["Zheng Li","Mukul Kumar","William Headden","Bing Yin","Ying Wei","Yu Zhang","Qiang Yang"],"demo_url":"","keywords":["cross-lingual transfer","clt task","multilingual","mplm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.179","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.74","main.1803","main.871","main.3688","main.2630"],"title":"Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages","tldr":"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt gen...","track":"Machine Translation and Multilinguality"},"forum":"main.400","id":"main.400","presentation_id":"38938702"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.453.png","content":{"abstract":"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates. In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline. Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model. On the competitive WMT'14 En$\\rightarrow$De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.","authors":["Chitwan Saharia","William Chan","Saurabh Saxena","Mohammad Norouzi"],"demo_url":"","keywords":["non-autoregressive translation","machine translation","single-step translation","re-scoring"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.83","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2100","main.1432","TACL.2107","main.1694","main.522"],"title":"Non-Autoregressive Machine Translation with Latent Alignments","tldr":"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve stat...","track":"Machine Translation and Multilinguality"},"forum":"main.453","id":"main.453","presentation_id":"38938713"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.522.png","content":{"abstract":"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM \"disagrees\" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.","authors":["Christos Baziotis","Barry Haddow","Alexandra Birch"],"demo_url":"","keywords":["neural translation","neural tm","knowledge distillation","training time"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.615","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.852","main.3688","main.1680","main.74","main.1960"],"title":"Language Model Prior for Low-Resource Neural Machine Translation","tldr":"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to i...","track":"Machine Translation and Multilinguality"},"forum":"main.522","id":"main.522","presentation_id":"38938725"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.585.png","content":{"abstract":"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.","authors":["Michelle Yuan","Mozhi Zhang","Benjamin Van Durme","Leah Findlater","Jordan Boyd-Graber"],"demo_url":"","keywords":["classification problem","cross-lingual embeddings","clime","interactive system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.482","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3093","main.1305","main.1130","main.2718"],"title":"Interactive Refinement of Cross-Lingual Word Embeddings","tldr":"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given...","track":"Machine Translation and Multilinguality"},"forum":"main.585","id":"main.585","presentation_id":"38938732"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.618.png","content":{"abstract":"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish\u2192English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English\u2192German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.","authors":["Maximiliana Behnke","Kenneth Heafield"],"demo_url":"","keywords":["machine translation","inference","attention mechanism","transformer architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.211","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1485","main.1798","main.130","main.3337","main.1960"],"title":"Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation","tldr":"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower ...","track":"Machine Translation and Multilinguality"},"forum":"main.618","id":"main.618","presentation_id":"38938739"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.619.png","content":{"abstract":"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.","authors":["Yvette Graham","Barry Haddow","Philipp Koehn"],"demo_url":"","keywords":["machine evaluation","human-parity mt","human translation","significance tests"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.6","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.84","main.894","main.888","main.2055","TACL.2221"],"title":"Statistical Power and Translationese in Machine Translation Evaluation","tldr":"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclus...","track":"Machine Translation and Multilinguality"},"forum":"main.619","id":"main.619","presentation_id":"38938740"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.623.png","content":{"abstract":"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur\u00edmac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.","authors":["Edoardo Maria Ponti","Goran Glava\u0161","Olga Majewska","Qianchu Liu","Ivan Vuli\u0107","Anna Korhonen"],"demo_url":"","keywords":["machine reasoning","cross-lingual transfer","causal reasoning","multilingual pretraining"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.185","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.143","main.2630","main.1803","main.1130","main.1379"],"title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning","tldr":"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired ...","track":"Machine Translation and Multilinguality"},"forum":"main.623","id":"main.623","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.639.png","content":{"abstract":"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala\u2013English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.","authors":["Brian Thompson","Philipp Koehn"],"demo_url":"","keywords":["candidate generation","candidate re-scoring","wmt task","mt"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.483","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1503","main.754","main.825","main.2891","main.1061"],"title":"Exploiting Sentence Order in Document Alignment","tldr":"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result o...","track":"Machine Translation and Multilinguality"},"forum":"main.639","id":"main.639","presentation_id":"38938745"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.701.png","content":{"abstract":"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.  Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named \\textsc{FEnmt}). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our \\textsc{FEnmt} could improve translation quality by effectively reducing unfaithful translations.","authors":["Rongxiang Weng","Heng Yu","Xiangpeng Wei","Weihua Luo"],"demo_url":"","keywords":["neural nmt","neural","nmt","training strategy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.212","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.26","main.894","main.888","main.891","main.2055"],"title":"Towards Enhancing Faithfulness for Neural Machine Translation","tldr":"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.  Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g....","track":"Machine Translation and Multilinguality"},"forum":"main.701","id":"main.701","presentation_id":"38938760"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.74.png","content":{"abstract":"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset).  A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.","authors":["Farhad Nooralahzadeh","Giannis Bekoulis","Johannes Bjerva","Isabelle Augenstein"],"demo_url":"","keywords":["strategic knowledge","downstream task","multilingual applications","natural tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.368","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.858","main.1803","main.407","main.400","main.2500"],"title":"Zero-Shot Cross-Lingual Transfer with Meta Learning","tldr":"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in t...","track":"Machine Translation and Multilinguality"},"forum":"main.74","id":"main.74","presentation_id":"38938645"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.835.png","content":{"abstract":"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.","authors":["Ricardo Rei","Craig Stewart","Ana C Farinha","Alon Lavie"],"demo_url":"","keywords":["cross-lingual modeling","wmt task","high-performing systems","comet"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.213","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3688","main.1960","main.1803","main.143","main.870"],"title":"COMET: A Neural Framework for MT Evaluation","tldr":"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrai...","track":"Machine Translation and Multilinguality"},"forum":"main.835","id":"main.835","presentation_id":"38938781"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.84.png","content":{"abstract":"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.","authors":["Markus Freitag","David Grangier","Isaac Caswell"],"demo_url":"","keywords":["machine translation","automated evaluation","paraphrasing task","human evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.5","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.888","main.619","main.210","main.2076","main.143"],"title":"BLEU might be Guilty but References are not Innocent","tldr":"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical...","track":"Machine Translation and Multilinguality"},"forum":"main.84","id":"main.84","presentation_id":"38938647"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.852.png","content":{"abstract":"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.","authors":["Alexandra Chronopoulou","Dario Stojanovski","Alexander Fraser"],"demo_url":"","keywords":["language model","lm","unsupervised system","unmt model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.214","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.522","main.3688","TACL.2107","main.1680","main.267"],"title":"Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT","tldr":"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, howe...","track":"Machine Translation and Multilinguality"},"forum":"main.852","id":"main.852","presentation_id":"38938785"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.856.png","content":{"abstract":"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser\u2019s distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation.","authors":["Huda Khayrallah","Brian Thompson","Matt Post","Philipp Koehn"],"demo_url":"","keywords":["machine mt","mt","simulated training","simulated"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.7","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.888","main.2661","main.891","main.522","main.1572"],"title":"Simulated multiple reference training improves low-resource machine translation","tldr":"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel M...","track":"Machine Translation and Multilinguality"},"forum":"main.856","id":"main.856","presentation_id":"38938786"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.858.png","content":{"abstract":"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.","authors":["Phillip Keung","Yichao Lu","Julian Salazar","Vikas Bhardwaj"],"demo_url":"","keywords":["zero-shot learning","mldoc task","model selection","mldoc tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.40","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.74","main.2500","main.1803","main.1032","main.407"],"title":"Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings","tldr":"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, publis...","track":"Machine Translation and Multilinguality"},"forum":"main.858","id":"main.858","presentation_id":"38938787"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.865.png","content":{"abstract":"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages.  In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.","authors":["Tasnim Mohiuddin","M Saiful Bari","Shafiq Joty"],"demo_url":"","keywords":["bilingual induction","bilingual","bli","semi-supervised method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.215","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.410","main.2131","main.3205","main.3358","main.852"],"title":"LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space","tldr":"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric str...","track":"Machine Translation and Multilinguality"},"forum":"main.865","id":"main.865","presentation_id":"38938789"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.871.png","content":{"abstract":"In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.","authors":["Yaobo Liang","Nan Duan","Yeyun Gong","Ning Wu","Fenfei Guo","Weizhen Qi","Ming Gong","Linjun Shou","Daxin Jiang","Guihong Cao","Xiaodong Fan","Ruofei Zhang","Rahul Agrawal","Edward Cui","Sining Wei","Taroon Bharti","Ying Qiao","Jiun-Hung Chen","Winnie Wu","Shuguang Liu","Fan Yang","Daniel Campos","Rangan Majumder","Ming Zhou"],"demo_url":"","keywords":["large-scale models","cross-lingual tasks","natural tasks","cross-lingual pre-training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.484","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1803","main.852","main.143","main.1379","main.1680"],"title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation","tldr":"In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (...","track":"Machine Translation and Multilinguality"},"forum":"main.871","id":"main.871","presentation_id":"38938792"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.875.png","content":{"abstract":"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.","authors":["Phillip Keung","Yichao Lu","Gy\u00f6rgy Szarvas","Noah A. Smith"],"demo_url":"","keywords":["multilingual classification","supervised classification","zero-shot learning","marc"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.369","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.504","main.2746","main.1298","main.1023","main.3116"],"title":"The Multilingual Amazon Reviews Corpus","tldr":"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected be...","track":"Machine Translation and Multilinguality"},"forum":"main.875","id":"main.875","presentation_id":"38938794"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.888.png","content":{"abstract":"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser\u2019s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric\u2014conditioning on the source instead of the reference\u2014and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.","authors":["Brian Thompson","Matt Post"],"demo_url":"","keywords":["machine evaluation","zero-shot task","wmt task","quality estimation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.8","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.856","main.3688","main.522","main.3227","TACL.2107"],"title":"Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing","tldr":"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating par...","track":"Machine Translation and Multilinguality"},"forum":"main.888","id":"main.888","presentation_id":"38938798"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.891.png","content":{"abstract":"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.","authors":["Xiangpeng Wei","Heng Yu","Yue Hu","Rongxiang Weng","Luxi Xing","Weihua Luo"],"demo_url":"","keywords":["sequence-to-sequence task","nmt","inference","translation tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.216","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2635","main.522","main.856","main.214","main.1960"],"title":"Uncertainty-Aware Semantic Augmentation for Neural Machine Translation","tldr":"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only obs...","track":"Machine Translation and Multilinguality"},"forum":"main.891","id":"main.891","presentation_id":"38938799"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.894.png","content":{"abstract":"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.","authors":["Shamil Chollampatt","Raymond Hendy Susanto","Liling Tan","Ewa Szymanska"],"demo_url":"","keywords":["automatic post-editing","automatic","machine translations","ape task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.217","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.701","main.888","main.522","TACL.2107","main.3227"],"title":"Can Automatic Post-Editing Improve NMT?","tldr":"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine...","track":"Machine Translation and Multilinguality"},"forum":"main.894","id":"main.894","presentation_id":"38938800"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.2.png","content":{"abstract":"Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a methodology for comparing languages based on their organization of semantic concepts. We propose to conduct an adapted version of representational similarity analysis of a selected set of concepts in computational multilingual representations. Using this analysis method, we can reconstruct a phylogenetic tree that closely resembles those assumed by linguistic experts. These results indicate that multilingual distributional representations that are only trained on monolingual text and bilingual dictionaries preserve relations between languages without the need for any etymological information. In addition, we propose a measure to identify semantic drift between language families.We perform experiments on word-based and sentence-based multilingual models and provide both quantitative results and qualitative examples. Analyses of semantic drift in multilingual representations can serve two purposes: They can indicate unwanted characteristics of the computational models and they provide a quantitative means to study linguistic phenomena across languages.","authors":["Lisa Beinborn","Rochelle Choenni"],"demo_url":"","keywords":["multilingual representations","computational representations","representational analysis","analysis method"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2718","main.143","main.3116","main.410","main.3181"],"title":"Semantic Drift in Multilingual Representations","tldr":"Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a metho...","track":"Machine Translation and Multilinguality"},"forum":"CL.2","id":"CL.2","presentation_id":"38939390"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1943.png","content":{"abstract":"Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model's architecture or training recipe can mean the difference between a positive and negative research result or between a state-of-the-art and under-performing system. While recent literature has proposed methods for automatic hyperparameter optimization (HPO), there has been limited work on applying these methods to NMT, due in part to the high costs associated with experiments that train large numbers of model variants. To facilitate research in this space, we introduce a lookup-based approach that uses a library of pre-trained models for fast, low cost HPO experimentation. Our contributions include (1) the release of a large collection of trained NMT models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for evaluating HPO methods on NMT, and (3) a reproducible benchmark of several HPO methods against our model library, including novel graph-based and multi-objective methods.","authors":["Xuan Zhang","Kevin Duh"],"demo_url":"","keywords":["hyperparameter selection","neural systems","automatic optimization","nmt"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1960","main.522","main.3227","TACL.1997","main.2661"],"title":"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems","tldr":"Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model's architecture or training recipe can mean the difference between a positive and ne...","track":"Machine Translation and Multilinguality"},"forum":"TACL.1943","id":"TACL.1943","presentation_id":"38939395"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1997.png","content":{"abstract":"Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By employing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivalling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE.","authors":["Marina Fomicheva","Shuo Sun","Lisa Yankovskaya","Fr\u00e9d\u00e9ric Blain","Francisco Guzm\u00e1n","Mark Fishel","Nikolaos Aletras","Vishrav Chaudhary","Lucia Specia"],"demo_url":"","keywords":["machine mt","real-world applications","qe","uncertainty quantification"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2061","TACL.1943","main.888","main.894","main.522"],"title":"Unsupervised Quality Estimation for Neural Machine Translation","tldr":"Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of exper...","track":"Machine Translation and Multilinguality"},"forum":"TACL.1997","id":"TACL.1997","presentation_id":"38939397"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2107.png","content":{"abstract":"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.","authors":["Jiatao Gu","Yinhan Liu","Naman Goyal","Xian Li","Sergey Edunov","Marjan Ghazvininejad","Mike Lewis","Luke Zettlemoyer"],"demo_url":"","keywords":["machine tasks","pre-training","multilingual pre-training","mbart"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1680","main.852","main.3688","main.522","main.888"],"title":"Multilingual Denoising Pre-training for Neural Machine Translation","tldr":"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-sc...","track":"Machine Translation and Multilinguality"},"forum":"TACL.2107","id":"TACL.2107","presentation_id":"38939408"}]
