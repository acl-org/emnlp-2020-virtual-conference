[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1070.png","content":{"abstract":"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have a similar likelihood of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities' infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.","authors":["Jiaming Shen","Wenda Qiu","Jingbo Shang","Michelle Vanni","Xiang Ren","Jiawei Han"],"demo_url":"","keywords":["entity expansion","synonym discovery","nlp tasks","synonym tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.666","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3216","main.2849","main.1787","main.298","main.1706"],"title":"SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery","tldr":"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymou...","track":"Information Retrieval and Text Mining"},"forum":"main.1070","id":"main.1070","presentation_id":"38938838"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1298.png","content":{"abstract":"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url]. We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.","authors":["Shuo Sun","Kevin Duh"],"demo_url":"","keywords":["cross-lingual retrieval","end-to-end retrieval","single-language retrieval","clirmatrix"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.340","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2746","main.2278","main.1379","main.871","main.3116"],"title":"CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval","tldr":"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language...","track":"Information Retrieval and Text Mining"},"forum":"main.1298","id":"main.1298","presentation_id":"38938883"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1460.png","content":{"abstract":"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner's perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.","authors":["Tara Safavi","Danai Koutra","Edgar Meij"],"demo_url":"","keywords":["human-ai collaboration","calibrated predictions","knowledge task","knowledge models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.667","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1493","main.2406","main.1508","main.684"],"title":"Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction","tldr":"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output conf...","track":"Information Retrieval and Text Mining"},"forum":"main.1460","id":"main.1460","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1488.png","content":{"abstract":"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.","authors":["Haopeng Zhang","Jiawei Zhang"],"demo_url":"","keywords":["text classification","natural processing","text task","graph techniques"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.668","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.151","main.782","main.2764","TACL.2121","main.3437"],"title":"Text Graph Transformer for Document Classification","tldr":"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus...","track":"Information Retrieval and Text Mining"},"forum":"main.1488","id":"main.1488","presentation_id":"38938916"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1493.png","content":{"abstract":"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.","authors":["Tara Safavi","Danai Koutra"],"demo_url":"","keywords":["baseline prediction","triple classification","link benchmark","codex"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.669","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1706","demo.58","main.666","main.1787","demo.119"],"title":"CoDEx: A Comprehensive Knowledge Graph Completion Benchmark","tldr":"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge...","track":"Information Retrieval and Text Mining"},"forum":"main.1493","id":"main.1493","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.16.png","content":{"abstract":"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT,  the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.","authors":["Chengyu Wang","Minghui Qiu","Jun Huang","Xiaofeng He"],"demo_url":"","keywords":["nlp tasks","fine-tuning","learning process","multi-domain tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.250","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2793","main.1631","main.3023","main.2491","main.400"],"title":"Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining","tldr":"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how t...","track":"Information Retrieval and Text Mining"},"forum":"main.16","id":"main.16","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1611.png","content":{"abstract":"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.","authors":["Yu Meng","Yunyi Zhang","Jiaxin Huang","Chenyan Xiong","Heng Ji","Chao Zhang","Jiawei Han"],"demo_url":"","keywords":["classification","category understanding","document classification","topic classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.724","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2167","main.76","main.1159","main.989","main.2893"],"title":"Text Classification Using Label Names Only: A Language Model Self-Training Approach","tldr":"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples b...","track":"Information Retrieval and Text Mining"},"forum":"main.1611","id":"main.1611","presentation_id":"38938946"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1949.png","content":{"abstract":"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.","authors":["Cicero Nogueira dos Santos","Xiaofei Ma","Ramesh Nallapati","Zhiheng Huang","Bing Xiang"],"demo_url":"","keywords":["information retrieval","ranking documents","ir tasks","discriminative functions"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.134","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3327","main.3240","main.1023","main.2931","main.693"],"title":"Beyond [CLS] through Ranking by Generation","tldr":"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural...","track":"Information Retrieval and Text Mining"},"forum":"main.1949","id":"main.1949","presentation_id":"38939015"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2289.png","content":{"abstract":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as \u201cseed motifs\u201d, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","authors":["Dheeraj Mekala","Xinyang Zhang","Jingbo Shang"],"demo_url":"","keywords":["weakly learning","systematic modeling","meta","bootstrapping manner"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.670","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2167","main.1023","main.1159","main.693","main.3298"],"title":"META: Metadata-Empowered Weak Supervision for Text Classification","tldr":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata informa...","track":"Information Retrieval and Text Mining"},"forum":"main.2289","id":"main.2289","presentation_id":"38939087"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2756.png","content":{"abstract":"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.","authors":["Xuemeng Hu","Rui Wang","Deyu Zhou","Yuxuan Xiong"],"demo_url":"","keywords":["neural modeling","deep models","adversarial-neural model","adversarially network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.725","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.30","TACL.2093","TACL.2083","main.2430","main.2931"],"title":"Neural Topic Modeling with Cycle-Consistent Adversarial Training","tldr":"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior t...","track":"Information Retrieval and Text Mining"},"forum":"main.2756","id":"main.2756","presentation_id":"38939185"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2792.png","content":{"abstract":"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.","authors":["Suzanna Sia","Ayush Dalmia","Sabrina J. Mielke"],"demo_url":"","keywords":["weighted clustering","reranking words","dimensionality reduction","topic models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.135","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2931","main.2596","main.30","main.3093"],"title":"Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!","tldr":"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain ...","track":"Information Retrieval and Text Mining"},"forum":"main.2792","id":"main.2792","presentation_id":"38939197"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2825.png","content":{"abstract":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.","authors":["Sean MacAvaney","Arman Cohan","Nazli Goharian"],"demo_url":"","keywords":["zero-shot algorithm","neural model","zero-shot methods","covid- search"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.341","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.124","demo.109","main.748","main.1631","main.2943"],"title":"SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search","tldr":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these ar...","track":"Information Retrieval and Text Mining"},"forum":"main.2825","id":"main.2825","presentation_id":"38939204"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.286.png","content":{"abstract":"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.","authors":["Yuning Mao","Yanru Qu","Yiqing Xie","Xiang Ren","Jiawei Han"],"demo_url":"","keywords":["single-document summarization","single-document sds","multi-document summarization","multi-document mds"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.136","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.965","main.471","main.714","main.1023","main.1504"],"title":"Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning","tldr":"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS...","track":"Information Retrieval and Text Mining"},"forum":"main.286","id":"main.286","presentation_id":"38938676"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2931.png","content":{"abstract":"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.","authors":["Alexander Miserlis Hoyle","Pranav Goel","Philip Resnik"],"demo_url":"","keywords":["topic models","knowledge distillation","probabilistic models","pretrained transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.137","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2792","TACL.2093","main.1952","main.2476","main.3581"],"title":"Improving Neural Topic Models using Knowledge Distillation","tldr":"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular m...","track":"Information Retrieval and Text Mining"},"forum":"main.2931","id":"main.2931","presentation_id":"38939229"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.30.png","content":{"abstract":"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts. Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics. We observe that our model can highly improve short text topic modeling performance. Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.","authors":["Xiaobao Wu","Chunping Li","Yan Zhu","Yishu Miao"],"demo_url":"","keywords":["decoding","short modeling","topic models","neural model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.138","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2756","main.2792","TACL.2083","main.2430"],"title":"Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder","tldr":"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield rep...","track":"Information Retrieval and Text Mining"},"forum":"main.30","id":"main.30","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3240.png","content":{"abstract":"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.","authors":["Yosi Mass","Haggai Roitman"],"demo_url":"","keywords":["ad-hoc retrieval","manually data","weakly-supervised method","deep models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.343","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1949","main.2733","main.2083","main.693","main.471"],"title":"Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2","tldr":"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the cor...","track":"Information Retrieval and Text Mining"},"forum":"main.3240","id":"main.3240","presentation_id":"38939294"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3327.png","content":{"abstract":"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.","authors":["Ruey-Cheng Chen","Chia-Jung Lee"],"demo_url":"","keywords":["query suggestion","conditional problem","search session","querying"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.251","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1949","main.3054","demo.54","main.2931","main.693"],"title":"Incorporating Behavioral Hypotheses for Query Generation","tldr":"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a ...","track":"Information Retrieval and Text Mining"},"forum":"main.3327","id":"main.3327","presentation_id":"38939310"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3419.png","content":{"abstract":"We present a query-based biomedical information retrieval task across two vastly different genres -- newswire and research literature -- where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.","authors":["Chaoyuan Zuo","Narayan Acharya","Ritwik Banerjee"],"demo_url":"","keywords":["query-based task","cross-genre ir","incorporating knowledge","ir approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.139","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.748","main.693","main.574","demo.109","main.2962"],"title":"Querying Across Genres for Medical Claims in News","tldr":"We present a query-based biomedical information retrieval task across two vastly different genres -- newswire and research literature -- where the goal is to find the research publication that supports the primary claim made in a health-related news ...","track":"Information Retrieval and Text Mining"},"forum":"main.3419","id":"main.3419","presentation_id":"38939331"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3541.png","content":{"abstract":"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.","authors":["Luyu Gao","Zhuyun Dai","Jamie Callan"],"demo_url":"","keywords":["information retrieval","ranking process","text representation","interaction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.342","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1949","main.2943","main.2931","main.204","main.3517"],"title":"Modularized Transfomer-based Ranking Framework","tldr":"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking proce...","track":"Information Retrieval and Text Mining"},"forum":"main.3541","id":"main.3541","presentation_id":"38939359"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.450.png","content":{"abstract":"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as ``mix-up\", ``self-ensembling\", ``distinctiveness score\",  is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.","authors":["Jianfeng He","Xuchao Zhang","Shuo Lei","Zhiqian Chen","Fanglan Chen","Abdulaziz Alhamadani","Bei Xiao","ChangTien Lu"],"demo_url":"","keywords":["uncertainty classified","rectification","text classification","mix-up"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.671","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.1997","main.958","main.493","main.345","main.1159"],"title":"Towards More Accurate Uncertainty Estimation In Text Classification","tldr":"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether addit...","track":"Information Retrieval and Text Mining"},"forum":"main.450","id":"main.450","presentation_id":"38938712"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.548.png","content":{"abstract":"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.","authors":["Xinhong Chen","Qing Li","Jianping Wang"],"demo_url":"","keywords":["extraction clauses","prediction","manual annotation","negative sampling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.252","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3329","main.1159","main.387","main.3532","main.1289"],"title":"Conditional Causal Relationships between Emotions and Causes in Texts","tldr":"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the...","track":"Information Retrieval and Text Mining"},"forum":"main.548","id":"main.548","presentation_id":"38938729"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.652.png","content":{"abstract":"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding.  Effective designs encode within the layout and formatting signals that point to where the important information can be found.  In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection.  Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models.  A qualitative post-hoc analysis illustrates how these features function within the model.","authors":["Yansen Wang","Zhen Fan","Carolyn Rose"],"demo_url":"","keywords":["kpe","nlp task","information retrieval","navigation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.140","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3646","demo.48","main.3496","main.3453","main.2688"],"title":"Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction","tldr":"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy...","track":"Information Retrieval and Text Mining"},"forum":"main.652","id":"main.652","presentation_id":"38938750"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.666.png","content":{"abstract":"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.  Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.","authors":["Tao Shen","Yi Mao","Pengcheng He","Guodong Long","Adam Trischler","Weizhu Chen"],"demo_url":"","keywords":["self-supervised tasks","pre-training","entity linking","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.722","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.531","main.1231","main.1159","TACL.2121","main.923"],"title":"Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning","tldr":"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.  Building upon entity-level masked language models, our firs...","track":"Information Retrieval and Text Mining"},"forum":"main.666","id":"main.666","presentation_id":"38938753"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.668.png","content":{"abstract":"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events). Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses. Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision. We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.","authors":["Sudipta Kar","Gustavo Aguilar","Mirella Lapata","Thamar Solorio"],"demo_url":"","keywords":["characterizing stories","multi-view model","theme","style"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.454","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9C","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.645","main.1129","main.2650","main.1928","main.1952"],"title":"Multi-view Story Characterization from Movie Plot Synopses and Reviews","tldr":"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attr...","track":"Information Retrieval and Text Mining"},"forum":"main.668","id":"main.668","presentation_id":"38938754"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.748.png","content":{"abstract":"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.","authors":["Marco Basaldella","Fangyu Liu","Ehsan Shareghi","Nigel Collier"],"demo_url":"","keywords":["entity linking","el","string- models","to models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.253","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["demo.124","main.3049","main.1540","main.387","main.1631"],"title":"COMETA: A Corpus for Medical Entity Linking in the Social Media","tldr":"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understa...","track":"Information Retrieval and Text Mining"},"forum":"main.748","id":"main.748","presentation_id":"38938767"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.989.png","content":{"abstract":"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.  However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags),  while it is another challenge to obtain such effective resources.  In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector  based  on  reinforcement  learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks.  Extensive experiments on two  CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using   any annotated lexicon or corpus.","authors":["Ying Luo","Hai Zhao","Junlang Zhan"],"demo_url":"","keywords":["named recognition","entity detection","type prediction","deep models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.723","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2851","main.2799","main.911","main.3013"],"title":"Named Entity Recognition Only from Word Embeddings","tldr":"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.  However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human anno...","track":"Information Retrieval and Text Mining"},"forum":"main.989","id":"main.989","presentation_id":"38938819"}]
