[{"content":{"abstract":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good performance on a dataset might not indicate performance that correlates well with human\u2019s expectations from models that \u201cunderstand\u201d language. In this work we consider a top performing model on several Multiple Choice Question Answering (MCQA) datasets, and evaluate it against a set of expectations one might have from such a model, using a series of zero-information perturbations of the model\u2019s inputs. Our results show that the model clearly falls short of our expectations, and motivates a modified training approach that forces the model to better attend to the inputs. We show that the new training paradigm leads to a model that performs on par with the original model while better satisfying our expectations.","authors":["Krunal Shah","Nitish Gupta","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.317","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What do we expect from Multiple-choice QA Systems?","tldr":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good perf...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2575","presentation_id":"38940132","rocketchat_channel":"paper-insights-2575","speakers":"Krunal Shah|Nitish Gupta|Dan Roth","title":"What do we expect from Multiple-choice QA Systems?"},{"content":{"abstract":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several research challenges which are not tackled well for domain specific corpus found in industries. In this paper, we have highlighted these problems through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model. Our experiments have lead to interesting findings that showed: 1) Largest substring from the left that is found in the vocabulary (in-vocab) is always chosen at every sub-word unit that can lead to suboptimal tokenization choices, 2) Semantic meaning of a vocabulary word deteriorates when found as a substring in an Out-Of-Vocabulary (OOV) word, and 3) Minor misspellings in words are inadequately handled. We believe that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.","authors":["Anmol Nayak","Hariprasad Timmapathini","Karthikeyan Ponnalagu","Vijendran Gopalan Venkoparao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words","tldr":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several rese...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.1","presentation_id":"38940788","rocketchat_channel":"paper-insights-1","speakers":"Anmol Nayak|Hariprasad Timmapathini|Karthikeyan Ponnalagu|Vijendran Gopalan Venkoparao","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"content":{"abstract":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.","authors":["Prasanna Parthasarathi","Sharan Narang","Arvind Neelakantan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Task-Level Dialogue Composition of Generative Transformer Model","tldr":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.12","presentation_id":"38940793","rocketchat_channel":"paper-insights-12","speakers":"Prasanna Parthasarathi|Sharan Narang|Arvind Neelakantan","title":"On Task-Level Dialogue Composition of Generative Transformer Model"},{"content":{"abstract":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to determine the veracity of news. Our experiments find that the success of these detectors can be limited since they are rarely sensitive to semantic perturbations and are very sensitive to syntactic perturbations. Also, we would like to open-source our code and believe it could be a useful diagnostic tool for evaluating models aimed at fighting machine-generated fake news.","authors":["Meghana Moorthy Bhat","Srinivasan Parthasarathy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study","tldr":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to de...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.19","presentation_id":"38940794","rocketchat_channel":"paper-insights-19","speakers":"Meghana Moorthy Bhat|Srinivasan Parthasarathy","title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study"},{"content":{"abstract":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.","authors":["Ashwin Geet D\u2019Sa","Irina Illina","Dominique Fohr","Dietrich Klakow","Dana Ruiter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification","tldr":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of lab...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.20","presentation_id":"38940795","rocketchat_channel":"paper-insights-20","speakers":"Ashwin Geet D\u2019Sa|Irina Illina|Dominique Fohr|Dietrich Klakow|Dana Ruiter","title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification"},{"content":{"abstract":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p <0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.","authors":["Catherine Finegan-Dollak","Ashish Verma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layout-Aware Text Representations Harm Clustering Documents by Type","tldr":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document lay...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.22","presentation_id":"38940796","rocketchat_channel":"paper-insights-22","speakers":"Catherine Finegan-Dollak|Ashish Verma","title":"Layout-Aware Text Representations Harm Clustering Documents by Type"},{"content":{"abstract":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Networks (CapsNets) are a relatively new architecture in NLP. Due to their novelty, CapsNets are being used more and more in NLP tasks. However, their usefulness is still mostly untested.In this paper, we compare three neural network architectures\u2014LSTM, CNN, and CapsNet\u2014on a part of speech tagging task. We compare these architectures in both high- and low-resource training conditions and find that no architecture consistently performs the best. Our analysis shows that our CapsNet performs nearly as well as a more complex LSTM under certain training conditions, but not others, and that our CapsNet almost always outperforms our CNN. We also find that our CapsNet implementation shows faster prediction times than the LSTM for Scottish Gaelic but not for Spanish, highlighting the effect that the choice of languages can have on the models.","authors":["Andrew Zupon","Faiz Rafique","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios","tldr":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Network...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.23","presentation_id":"38940797","rocketchat_channel":"paper-insights-23","speakers":"Andrew Zupon|Faiz Rafique|Mihai Surdeanu","title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios"},{"content":{"abstract":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve around claims made by people about a given topic of interest. Fact-checking portals offer partially structured information that can assist such analysis. However, exploiting the network structure of such online discourse data is as of yet under-explored. We study the effectiveness of using neural-graph embedding features for claim topic prediction and their complementarity with text embeddings. We show that graph embeddings are modestly complementary with text embeddings, but the low performance of graph embedding features alone indicate that the model fails to capture topological features pertinent of the topic prediction task.","authors":["Valentina Beretta","S\u00e9bastien Harispe","Katarina Boland","Luke Lo Seen","Konstantin Todorov","Andon Tchechmedjiev"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?","tldr":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve aroun...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.24","presentation_id":"38940798","rocketchat_channel":"paper-insights-24","speakers":"Valentina Beretta|S\u00e9bastien Harispe|Katarina Boland|Luke Lo Seen|Konstantin Todorov|Andon Tchechmedjiev","title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?"},{"content":{"abstract":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB\u201905 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.","authors":["Piotr Szyma\u0144ski","Piotr \u017belasko","Mikolaj Morzy","Adrian Szymczak","Marzena \u017by\u0142a-Hoppe","Joanna Banaszczak","Lukasz Augustyniak","Jan Mizgajski","Yishay Carmiel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.295","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WER we are and WER we think we are","tldr":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Re...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2436","presentation_id":"38940634","rocketchat_channel":"paper-insights-2436","speakers":"Piotr Szyma\u0144ski|Piotr \u017belasko|Mikolaj Morzy|Adrian Szymczak|Marzena \u017by\u0142a-Hoppe|Joanna Banaszczak|Lukasz Augustyniak|Jan Mizgajski|Yishay Carmiel","title":"WER we are and WER we think we are"},{"content":{"abstract":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge.","authors":["Zhengzhong Liang","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?","tldr":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.26","presentation_id":"38940799","rocketchat_channel":"paper-insights-26","speakers":"Zhengzhong Liang|Mihai Surdeanu","title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?"},{"content":{"abstract":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data\u2014data built by minimally editing a set of seed examples to yield counterfactual labels\u2014to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.","authors":["William Huang","Haokun Liu","Samuel R. Bowman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data","tldr":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain e...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.27","presentation_id":"38940800","rocketchat_channel":"paper-insights-27","speakers":"William Huang|Haokun Liu|Samuel R. Bowman","title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data"},{"content":{"abstract":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive empirical investigation indicates otherwise. In this paper, we establish that ensemble summary for single document using NMF is no better than the best base model summary.","authors":["Alka Khurana","Vasudha Bhatnagar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMF Ensembles? Not for Text Summarization!","tldr":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive emp...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.29","presentation_id":"38940801","rocketchat_channel":"paper-insights-29","speakers":"Alka Khurana|Vasudha Bhatnagar","title":"NMF Ensembles? Not for Text Summarization!"},{"content":{"abstract":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two scorers differ in their handling of them, finding that one approach produces F1 scores approximately 0.5 points higher on the CoNLL 2003 English development and test sets. We propose best practices to increase the replicability of NER evaluations by increasing transparency regarding the handling of improper label sequences.","authors":["Constantine Lignos","Marjan Kamyab"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come","tldr":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.30","presentation_id":"38940802","rocketchat_channel":"paper-insights-30","speakers":"Constantine Lignos|Marjan Kamyab","title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come"},{"content":{"abstract":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.","authors":["Jatin Ganhotra","Robert Moore","Sachindra Joshi","Kahini Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.358","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effects of Naturalistic Variation in Goal-Oriented Dialog","tldr":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fix...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3004","presentation_id":"38940807","rocketchat_channel":"paper-insights-3004","speakers":"Jatin Ganhotra|Robert Moore|Sachindra Joshi|Kahini Wadhawan","title":"Effects of Naturalistic Variation in Goal-Oriented Dialog"},{"content":{"abstract":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.","authors":["Gaurav Arora","Chirag Jain","Manas Chaturvedi","Krupal Modi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HINT3: Raising the bar for Intent Detection in the Wild","tldr":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-wor...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.31","presentation_id":"38940803","rocketchat_channel":"paper-insights-31","speakers":"Gaurav Arora|Chirag Jain|Manas Chaturvedi|Krupal Modi","title":"HINT3: Raising the bar for Intent Detection in the Wild"},{"content":{"abstract":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier\u2019s predictions can be steered to the poison target class with success rates of >80\\% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.","authors":["Alvin Chan","Yi Tay","Yew-Soon Ong","Aston Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.373","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder","tldr":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regula...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3106","presentation_id":"38940808","rocketchat_channel":"paper-insights-3106","speakers":"Alvin Chan|Yi Tay|Yew-Soon Ong|Aston Zhang","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"content":{"abstract":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei andZou, 2019) and Back-Translation (Sennrichet al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XLNet, and RoBERTa. We observe a negative result, finding that techniques which previously reported strong improvements for non-pretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.","authors":["Shayne Longpre","Yu Wang","Chris DuBois"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.394","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?","tldr":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3296","presentation_id":"38940806","rocketchat_channel":"paper-insights-3296","speakers":"Shayne Longpre|Yu Wang|Chris DuBois","title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?"},{"content":{"abstract":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action \u2014 e.g., \u201cI started a new book I bought last week\u201d, where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods.","authors":["Yanai Elazar","Victoria Basmov","Shauli Ravfogel","Yoav Goldberg","Reut Tsarfaty"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Extraordinary Failure of Complement Coercion Crowdsourcing","tldr":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied acti...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.33","presentation_id":"38940804","rocketchat_channel":"paper-insights-33","speakers":"Yanai Elazar|Victoria Basmov|Shauli Ravfogel|Yoav Goldberg|Reut Tsarfaty","title":"The Extraordinary Failure of Complement Coercion Crowdsourcing"},{"content":{"abstract":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment with a multi-task learning approach for explicitly incorporating the structured elements of dictionary entries, such as user-assigned tags and usage examples, when learning embeddings for dictionary headwords. Our work generalizes several existing models for learning word embeddings from dictionaries. However, we find that the most effective representations overall are learned by simply training with a skip-gram objective over the concatenated text of all entries in the dictionary, giving no particular focus to the structure of the entries.","authors":["Steven Wilson","Walid Magdy","Barbara McGillivray","Gareth Tyson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Embedding Structured Dictionary Entries","tldr":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.34","presentation_id":"38940805","rocketchat_channel":"paper-insights-34","speakers":"Steven Wilson|Walid Magdy|Barbara McGillivray|Gareth Tyson","title":"Embedding Structured Dictionary Entries"},{"content":{"abstract":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as unstable model behaviour and some noise within the dataset itself. We then experiment with two approaches for integrating information from knowledge graphs: (i) concatenating knowledge graph triples to text passages and (ii) encoding knowledge with a Graph Neural Network. Neither of these approaches show a clear improvement and we hypothesize that this may be due to a combination of inaccuracies in the knowledge graph, imprecision in entity linking, and the models\u2019 inability to capture additional information from knowledge graphs.","authors":["Daria Dzendzik","Carl Vogel","Jennifer Foster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!","tldr":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as uns...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.4","presentation_id":"38940789","rocketchat_channel":"paper-insights-4","speakers":"Daria Dzendzik|Carl Vogel|Jennifer Foster","title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!"},{"content":{"abstract":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied. We propose a systematic approach aimed at evaluating data selection in scenarios of increasing complexity. Specifically, we compare the case in which source and target tasks are the same while source and target domains are different, against the more challenging scenario where both tasks and domains are different. We run a number of experiments on semantic sequence tagging tasks, which are relatively less investigated in data selection, and conclude that data selection has more benefit on the scenario when the tasks are the same, while in case of different (although related) tasks from distant domains, a combination of data selection and multi-task learning is ineffective for most cases.","authors":["Samuel Louvan","Bernardo Magnini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks","tldr":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied....","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.6","presentation_id":"38940790","rocketchat_channel":"paper-insights-6","speakers":"Samuel Louvan|Bernardo Magnini","title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks"},{"content":{"abstract":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search (ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS to NLP tasks, our results are mixed \u2013 we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.","authors":["Ansel MacLaughlin","Jwala Dhamala","Anoop Kumar","Sriram Venkatapathy","Ragav Venkatesan","Rahul Gupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks","tldr":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and c...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.7","presentation_id":"38940791","rocketchat_channel":"paper-insights-7","speakers":"Ansel MacLaughlin|Jwala Dhamala|Anoop Kumar|Sriram Venkatapathy|Ragav Venkatesan|Rahul Gupta","title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks"},{"content":{"abstract":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.","authors":["Silvia Terragni","Debora Nozza","Elisabetta Fersini","Messina Enza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models","tldr":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. Whil...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.8","presentation_id":"38940792","rocketchat_channel":"paper-insights-8","speakers":"Silvia Terragni|Debora Nozza|Elisabetta Fersini|Messina Enza","title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models"}]
