[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1061.png","content":{"abstract":"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings.  The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence.  It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language.  Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.","authors":["Rui Cai","Mirella Lapata"],"demo_url":"","keywords":["cross-lingual labeling","predicting roles","srl","machine engines"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.319","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1379","main.3597","main.143","main.2641","main.3046"],"title":"Alignment-free Cross-lingual Semantic Role Labeling","tldr":"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1061","id":"main.1061","presentation_id":"38938837"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1086.png","content":{"abstract":"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.","authors":["Hongzhi Zhang","Yingyao Wang","Sirui Wang","Xuezhi Cao","Fuzheng Zhang","Zhongyuan Wang"],"demo_url":"","keywords":["symbolic reasoning","pre-trained models","pre-trained transformers","table representation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.126","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2724","main.1618","main.204","main.3398","main.2253"],"title":"Table Fact Verification with Structure-Aware Transformer","tldr":"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, becau...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1086","id":"main.1086","presentation_id":"38938841"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1103.png","content":{"abstract":"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states,  motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of ~670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.","authors":["Nasrin Mostafazadeh","Aditya Kalyanpur","Lori Moon","David Buchanan","Lauren Berkowitz","Or Biran","Jennifer Chu-Carroll"],"demo_url":"","keywords":["ai systems","mental models","causal explanation","pretrained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.370","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3291","main.1972","main.2758","main.3450","main.1191"],"title":"GLUCOSE: GeneraLized and COntextualized Story Explanations","tldr":"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit c...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1103","id":"main.1103","presentation_id":"38938844"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1107.png","content":{"abstract":"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unveri\ufb01ed examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are veri\ufb01ed through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.","authors":["Victor Zhong","Mike Lewis","Sida I. Wang","Luke Zettlemoyer"],"demo_url":"","keywords":["adaptation","grounded adaptation","semantic parser","database schemas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.558","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.850","main.1495","main.1179","main.891","main.1130"],"title":"Grounded Adaptation for Zero-shot Executable Semantic Parsing","tldr":"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthe...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1107","id":"main.1107","presentation_id":"38938845"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1187.png","content":{"abstract":"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.","authors":["Wanjun Zhong","Duyu Tang","Zenan Xu","Ruize Wang","Nan Duan","Ming Zhou","Jiahai Wang","Jian Yin"],"demo_url":"","keywords":["deepfake detection","automatically text","deepfake text","natural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.193","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1892","main.989","main.1159","main.1488"],"title":"Neural Deepfake Detection with Factual Structure of Text","tldr":"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coa...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1187","id":"main.1187","presentation_id":"38938858"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1267.png","content":{"abstract":"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.","authors":["Ruize Wang","Duyu Tang","Nan Duan","Wanjun Zhong","Zhongyu Wei","Xuanjing Huang","Daxin Jiang","Ming Zhou"],"demo_url":"","keywords":["detection fragments","training process","fine-grained detection","declarative techniques"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.320","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2763","demo.97","main.2553","main.1970","main.2739"],"title":"Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection","tldr":"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specif...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1267","id":"main.1267","presentation_id":"38938876"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1275.png","content":{"abstract":"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work's performance is often not comprehensively evaluated due to the lack of readily-available execution engines.  Upon identifying these gaps, we propose~\\benchmarkname{}, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets $\\times$ four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.","authors":["Jiaqi Guo","Qian Liu","Jian-Guang Lou","Zhenwen Li","Xueqing Liu","Tao Xie","Ting Liu"],"demo_url":"","keywords":["meaning representation","semantic parsing","unimer","meaning representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.118","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2179","main.1754","CL.2","main.1957","main.143"],"title":"Benchmarking Meaning Representations in Neural Semantic Parsing","tldr":"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is le...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1275","id":"main.1275","presentation_id":"38938878"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1379.png","content":{"abstract":"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable  across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.","authors":["Angel Daza","Anette Frank"],"demo_url":"","keywords":["generalization learning","multilingual learning","high-quality translation","srl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.321","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1061","main.143","main.1803","main.2777","main.871"],"title":"X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset","tldr":"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1379","id":"main.1379","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1408.png","content":{"abstract":"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.  In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.","authors":["Prasetya Ajie Utama","Nafise Sadat Moosavi","Iryna Gurevych"],"demo_url":"","keywords":["nlu tasks","nlu models","debiasing methods","self-debiasing framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.613","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1399","main.1923","main.345","main.2886","TACL.2055"],"title":"Towards Debiasing NLU Models from Unknown Biases","tldr":"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a majo...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1408","id":"main.1408","presentation_id":"38938901"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1421.png","content":{"abstract":"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as ``buying a car'' can be used in the context of a new but analogous process such as ``buying a house''. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.","authors":["Hongming Zhang","Muhao Chen","Haoyu Wang","Yangqiu Song","Dan Roth"],"demo_url":"","keywords":["event understanding","nlp","structured representations","process framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.119","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1116","main.2508","main.237","main.96","main.607"],"title":"Analogous Process Structure Induction for Sub-event Sequence Prediction","tldr":"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (so...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1421","id":"main.1421","presentation_id":"38938902"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1495.png","content":{"abstract":"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at {https://github.com/sunlab-osu/MISP}.","authors":["Ziyu Yao","Yiqi Tang","Wen-tau Yih","Huan Sun","Yu Su"],"demo_url":"","keywords":["bootstrapping","fine-tuning parsers","theoretical analysis","text-to-sql problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.559","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.850","TACL.2411","main.1107","main.3327","main.471"],"title":"An Imitation Game for Learning Semantic Parsers from User Interaction","tldr":"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop metho...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1495","id":"main.1495","presentation_id":"38938921"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1540.png","content":{"abstract":"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in \u03b1NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.","authors":["Yixin Nie","Xiang Zhou","Mohit Bansal"],"demo_url":"","keywords":["nlp tasks","nlu evaluations","snli","majority label"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.734","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1923","main.1023","main.1159","main.3183","main.748"],"title":"What Can We Learn from Collective Human Opinions on Natural Language Inference Data?","tldr":"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1540","id":"main.1540","presentation_id":"38938929"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1649.png","content":{"abstract":"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.","authors":["Rexhina Blloshmi","Rocco Tripodi","Roberto Navigli"],"demo_url":"","keywords":["encoding semantics","cross-lingual parsing","english parsing","amr"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.195","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.870","main.2098","main.1061","main.357","main.852"],"title":"XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques","tldr":"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the e...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1649","id":"main.1649","presentation_id":"38938960"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1754.png","content":{"abstract":"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by `composing' word representations of the first and last words in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are `decomposed' back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.","authors":["Diego Marcheggiani","Ivan Titov"],"demo_url":"","keywords":["semantic labeling","identifying predicates","labeling spans","syntax-aware srl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.322","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1957","main.1061","main.2179","main.1179","main.574"],"title":"Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling","tldr":"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as argumen...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1754","id":"main.1754","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1784.png","content":{"abstract":"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.","authors":["Yitao Cai","Xiaojun Wan"],"demo_url":"","keywords":["context-dependent task","context-dependent drawn","decoding phase","encoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.560","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.1706","TACL.2121","main.3682","main.1488"],"title":"IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation","tldr":"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic inf...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1784","id":"main.1784","presentation_id":"38938986"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1834.png","content":{"abstract":"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference ({NLI}). We propose {G}en{NLI}, a generative classifier for {NLI} tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like {BERT}. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the ``infinilog loss''. Our experiments show that {GenNLI} outperforms both discriminative and pretrained baselines across several challenging {NLI} experimental settings, including small training sets, imbalanced label distributions, and label noise.","authors":["Xiaoan Ding","Tianyu Liu","Baobao Chang","Zhifang Sui","Kevin Gimpel"],"demo_url":"","keywords":["natural inference","nli tasks","discriminative fine-tuning","discriminative classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.657","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.3348","main.989","main.3023","main.3227"],"title":"Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference","tldr":"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference ({NLI}). We propose {...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1834","id":"main.1834","presentation_id":"38938994"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1866.png","content":{"abstract":"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users' natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.","authors":["Yuntao Li","Bei Chen","Qian Liu","Yan Gao","Jian-Guang Lou","Yan Zhang","Dongmei Zhang"],"demo_url":"","keywords":["databases systems","text-to-sql technique","parsers","parser-independent approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.561","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.2590","TACL.1983","demo.54","main.3682"],"title":"\u201cWhat Do You Mean by That?\u201d A Parser-Independent Interactive Approach for Enhancing Text-to-SQL","tldr":"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1866","id":"main.1866","presentation_id":"38939001"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1892.png","content":{"abstract":"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.","authors":["Haejun Lee","Drew A. Hudson","Kangwook Lee","Christopher D. Manning"],"demo_url":"","keywords":["nlp","sentence-level modeling","discourse representation","pre-training methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.120","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2851","main.407","main.2635","main.1130","main.852"],"title":"SLM: Learning a Discourse Language Representation with Sentence Unshuffling","tldr":"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language r...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1892","id":"main.1892","presentation_id":"38939003"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1923.png","content":{"abstract":"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit *pre-filled* text boxes, reduce previously observed issues with annotation artifacts.","authors":["Samuel R. Bowman","Jennimaria Palomaki","Livio Baldini Soares","Emily Pitler"],"demo_url":"","keywords":["benchmarking","language understanding","transfer applications","crowdsourcing protocol"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.658","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.74","main.2739","main.2721","main.2491","main.1540"],"title":"New Protocols and Negative Results for Textual Entailment Data Collection","tldr":"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was n...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1923","id":"main.1923","presentation_id":"38939009"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1938.png","content":{"abstract":"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.","authors":["Rik van Noord","Antonio Toral","Johan Bos"],"demo_url":"","keywords":["discourse parsing","analysis","character-level representations","character representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.371","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1892","main.1957","main.2890","main.891","main.315"],"title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT","tldr":"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1938","id":"main.1938","presentation_id":"38939012"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1957.png","content":{"abstract":"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our \ufb01ndings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.","authors":["Tianze Shi","Igor Malioutov","Ozan Irsoy"],"demo_url":"","keywords":["span-based labeling","syntactic parsing","semantic labeling","conversion scheme"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.610","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1754","main.1061","main.1179","main.2419","TACL.2141"],"title":"Semantic Role Labeling as Syntactic Dependency Parsing","tldr":"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1957","id":"main.1957","presentation_id":"38939017"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2098.png","content":{"abstract":"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.","authors":["Dongqin Xu","Junhui Li","Muhua Zhu","Min Zhang","Guodong Zhou"],"demo_url":"","keywords":["abstract parsing","amr parsing","sequence-to-sequence parsing","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.196","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1649","main.2635","main.3227","main.2491","main.130"],"title":"Improving AMR Parsing with Sequence-to-Sequence Pre-training","tldr":"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trai...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2098","id":"main.2098","presentation_id":"38939049"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.210.png","content":{"abstract":"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable syntactic cues caused by its automatic generation process. Taking advantage of such cues, we show that even a simple rule-based model can perform on par with the state-of-the-art model. To remedy such limitations, we collect and release two crowdsourced benchmark datasets. We not only make sure that they contain significantly more diverse syntax, but also carefully control for their quality according to a well-defined set of criteria. While no satisfactory automatic metric exists, we apply fine-grained manual evaluation based on these criteria using crowdsourcing, showing that our datasets better represent the task and are significantly more challenging for the models.","authors":["Li Zhang","Huaiyu Zhu","Siddhartha Brahma","Yunyao Li"],"demo_url":"","keywords":["text task","fine-grained evaluation","automatic process","rule-based model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.91","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2076","main.2943","main.84","main.876"],"title":"Small but Mighty: New Benchmarks for Split and Rephrase","tldr":"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark datas...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.210","id":"main.210","presentation_id":"38938665"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2268.png","content":{"abstract":"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.","authors":["Xiang Zhou","Yixin Nie","Hao Tan","Mohit Bansal"],"demo_url":"","keywords":["nli","reading sets","model-selection routine","instability"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.659","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.84","main.2238","main.1159","demo.102","main.2739"],"title":"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions","tldr":"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability o...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2268","id":"main.2268","presentation_id":"38939082"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2342.png","content":{"abstract":"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and  (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations.  Universal NLP can be probably achieved through different routines. In this work, we introduce  Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on  new entailment domains in a few-shot setting, and show its effectiveness  as a unified solver for several downstream NLP tasks such as question answering and  coreference resolution  when the end-task annotations are limited.","authors":["Wenpeng Yin","Nazneen Fatema Rajani","Dragomir Radev","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["nlp problems","textual entailment","nlp task","downstream tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.660","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3470","demo.54","main.3010","main.1923","demo.48"],"title":"Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start","tldr":"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new pro...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2342","id":"main.2342","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2415.png","content":{"abstract":"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions (\"and\", \"or\", \"but\", \"nor\") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions.","authors":["Swarnadeep Saha","Yixin Nie","Mohit Bansal"],"demo_url":"","keywords":["natural inference","conjnli","large-scale models","roberta"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.661","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2054","demo.86","TACL.2013","main.2470","main.1622"],"title":"ConjNLI: Natural Language Inference Over Conjunctive Sentences","tldr":"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not cons...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2415","id":"main.2415","presentation_id":"38939112"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2416.png","content":{"abstract":"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.","authors":["Md Mosharaf Hossain","Venelin Kovatchev","Pranoy Dutta","Tiffany Kao","Elizabeth Wei","Eduardo Blanco"],"demo_url":"","keywords":["natural inference","inference judgments","transformers","negation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.732","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2415","main.2253","TACL.2013","main.1970","main.2040"],"title":"An Analysis of Natural Language Inference Benchmarks through the Lens of Negation","tldr":"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for na...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2416","id":"main.2416","presentation_id":"38939113"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2419.png","content":{"abstract":"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.","authors":["Matthias Lindemann","Jonas Groschwitz","Alexander Koller"],"demo_url":"","keywords":["am parsing","neural parsing","linguistically method","parsers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.323","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.1957","main.447","main.2890","TACL.2141"],"title":"Fast semantic parsing with well-typedness guarantees","tldr":"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser a...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2419","id":"main.2419","presentation_id":"38939114"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2533.png","content":{"abstract":"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.","authors":["Yun He","Zhuoer Wang","Yin Zhang","Ruihong Huang","James Caverlee"],"demo_url":"","keywords":["paraphrase identification","neural models","bert fine-tuning","fine-tuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.611","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3506","main.210","main.1970","main.3470","main.1159"],"title":"PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge","tldr":"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based o...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2533","id":"main.2533","presentation_id":"38939139"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2553.png","content":{"abstract":"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.","authors":["Eleftheria Briakou","Marine Carpuat"],"demo_url":"","keywords":["detecting content","cross-lingual nlp","machine problem","annotation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.121","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.1379","main.1970","main.143","main.2890"],"title":"Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank","tldr":"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2553","id":"main.2553","presentation_id":"38939142"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2570.png","content":{"abstract":"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification. Our source code is available at https://github.com/ShyamSubramanian/HESM.","authors":["Shyam Subramanian","Kyumin Lee"],"demo_url":"","keywords":["automated verification","claim verification","fact extraction","fact verification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.627","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2506","main.2962","main.2117","main.1159","main.3151"],"title":"Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification","tldr":"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leadin...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2570","id":"main.2570","presentation_id":"38939144"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2635.png","content":{"abstract":"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.","authors":["Shuohang Wang","Yuwei Fang","Siqi Sun","Zhe Gan","Yu Cheng","Jingjing Liu","Jing Jiang"],"demo_url":"","keywords":["pre-training encoder","large-scale tasks","question answering","predicting words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.30","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1339","main.891","main.1892","main.2491","main.148"],"title":"Cross-Thought for Sentence Encoder Pre-training","tldr":"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2635","id":"main.2635","presentation_id":"38939160"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2724.png","content":{"abstract":"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.","authors":["Xiaoyu Yang","Feng Nie","Yufei Feng","Quan Liu","Zhigang Chen","Xiaodan Zhu"],"demo_url":"","keywords":["fact verification","real-life applications","symbolic operations","programs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.628","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.782","main.1086","main.1010","main.2506","main.574"],"title":"Program Enhanced Fact Verification with Verbalization and Graph Attention Network","tldr":"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. I...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2724","id":"main.2724","presentation_id":"38939180"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2777.png","content":{"abstract":"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.","authors":["Emrah Budur","R\u0131za \u00d6z\u00e7elik","Tunga Gungor","Christopher Potts"],"demo_url":"","keywords":["nli","morphological parsing","commercial systems","in-language embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.662","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1379","main.870","main.3597","main.1061","main.522"],"title":"Data and Representation for Turkish Natural Language Inference","tldr":"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, com...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2777","id":"main.2777","presentation_id":"38939192"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2890.png","content":{"abstract":"We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in  labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.","authors":["Maryam Aminian","Mohammad Sadegh Rasooli","Mona Diab"],"demo_url":"","keywords":["syntactic parsing","auxiliary task","multitask setup","annotation projection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.663","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1061","main.891","main.1957","main.750","main.1379"],"title":"Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies","tldr":"We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic pa...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2890","id":"main.2890","presentation_id":"38939217"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2989.png","content":{"abstract":"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.","authors":["Yun He","Ziwei Zhu","Yin Zhang","Qin Chen","James Caverlee"],"demo_url":"","keywords":["health-related tasks","consumer answering","medical inference","disease recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.372","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.748","main.110","main.1528","TACL.2049","main.3151"],"title":"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","tldr":"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question an...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2989","id":"main.2989","presentation_id":"38939241"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2999.png","content":{"abstract":"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalization\u2014the combination of input specification, loss function, and reuse of pretrained parameters\u2014by users of the dataset, rather than improvements in the pretrained model\u2019s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model\u2019s extreme sensitivity to hyperparameters.  We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.","authors":["Haokun Liu","William Huang","Dhara Mungra","Samuel R. Bowman"],"demo_url":"","keywords":["task formalization","input specification","ablation","formalization decisions"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.664","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.210","main.607","TACL.2041","main.3470","TACL.2411"],"title":"Precise Task Formalization Matters in Winograd Schema Evaluations","tldr":"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly la...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2999","id":"main.2999","presentation_id":"38939247"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.300.png","content":{"abstract":"Few-shot Knowledge Graph\u00a0(KG) completion is a focus of current research, where each task aims at querying\u00a0unseen facts\u00a0of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at https://github.com/JiaweiSheng/FAAN.","authors":["Jiawei Sheng","Shu Guo","Zhenyu Chen","Juwei Yue","Lihong Wang","Tingwen Liu","Hongbo Xu"],"demo_url":"","keywords":["few-shot completion","knowledge acquisition","link prediction","adaptive network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.131","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2974","main.2761","main.1528","main.666","main.2426"],"title":"Adaptive Attentional Network for Few-Shot Knowledge Graph Completion","tldr":"Few-shot Knowledge Graph\u00a0(KG) completion is a focus of current research, where each task aims at querying\u00a0unseen facts\u00a0of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of e...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.300","id":"main.300","presentation_id":"38938679"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3032.png","content":{"abstract":"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI, a high-quality (91.5% coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel model architectures.","authors":["Niket Tandon","Keisuke Sakaguchi","Bhavana Dalvi","Dheeraj Rajagopal","Peter Clark","Michal Guerquin","Kyle Richardson","Eduard Hovy"],"demo_url":"","keywords":["tracking text","fog removal","task formulation","crowdsourcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.520","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.605","main.2849","demo.72","main.2974","main.652"],"title":"A Dataset for Tracking Entities in Open Domain Procedural Text","tldr":"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being fogg...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3032","id":"main.3032","presentation_id":"38939254"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3057.png","content":{"abstract":"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a high-performance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.","authors":["Kazumasa Omura","Daisuke Kawahara","Sadao Kurohashi"],"demo_url":"","keywords":["automatic extraction","language research","crowdsourcing","transfer model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.192","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2922","main.210","main.76","main.3470"],"title":"A Method for Building a Commonsense Inference Dataset based on Basic Events","tldr":"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic event...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3057","id":"main.3057","presentation_id":"38939260"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3291.png","content":{"abstract":"When does a sequence of events define an everyday scenario  and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.","authors":["Noah Weber","Rachel Rudinger","Benjamin Van Durme"],"demo_url":"","keywords":["script induction","correlation-based approach","causal events","interventions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.612","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1103","main.607","main.928","main.1191","main.1455"],"title":"Causal Inference of Script Knowledge","tldr":"When does a sequence of events define an everyday scenario  and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus....","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3291","id":"main.3291","presentation_id":"38939303"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3358.png","content":{"abstract":"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks -- contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.","authors":["John Wieting","Graham Neubig","Taylor Berg-Kirkpatrick"],"demo_url":"","keywords":["source separation","semantic encoding","data distributions","unsupervised evaluations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.122","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.267","main.865","main.2851","main.410"],"title":"A Bilingual Generative Transformer for Semantic Sentence Embedding","tldr":"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3358","id":"main.3358","presentation_id":"38939319"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.349.png","content":{"abstract":"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.","authors":["Lijie Wang","Ao Zhang","Kun Wu","Ke Sun","Zhenghua Li","Hua Wu","Min Zhang","Haifeng Wang"],"demo_url":"","keywords":["text-to-sql parsing","cross-domain task","manually questions","sql queries"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.562","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1866","main.3682","main.3544","main.3507","main.1784"],"title":"DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset","tldr":"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the c...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.349","id":"main.349","presentation_id":"38938688"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3506.png","content":{"abstract":"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.  We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers.  To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.","authors":["Silei Xu","Sina Semnani","Giovanni Campagna","Monica Lam"],"demo_url":"","keywords":["database operations","automatic paraphrasing","autoqa","semantic parsers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.31","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3597","main.2763","main.888","main.2586","main.2590"],"title":"AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data","tldr":"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for traini...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3506","id":"main.3506","presentation_id":"38939351"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3507.png","content":{"abstract":"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.","authors":["Jianqiang Ma","Zeyu Yan","Shuai Pang","Yang Zhang","Jianping Shen"],"demo_url":"","keywords":["text-to-sql systems","slot- approach","dedicated models","modularized systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.563","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3682","main.3517","main.349","main.3216","main.1706"],"title":"Mention Extraction and Linking for SQL Query Generation","tldr":"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturi...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3507","id":"main.3507","presentation_id":"38939352"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3544.png","content":{"abstract":"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.","authors":["Ruiqi Zhong","Tao Yu","Dan Klein"],"demo_url":"","keywords":["text-to-sql models","distilled suite","spider board","test accuracy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.29","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.349","main.3507","main.1866","main.3682","main.84"],"title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suites","tldr":"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At ev...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3544","id":"main.3544","presentation_id":"38939361"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.357.png","content":{"abstract":"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.","authors":["Rafael Anchi\u00eata","Thiago Pardo"],"demo_url":"","keywords":["abstract representation","abstract","amr","graph-based formalism"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.123","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1649","main.2891","main.1957","CL.2","main.1061"],"title":"Semantically Inspired AMR Alignment for the Portuguese Language","tldr":"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. How...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.357","id":"main.357","presentation_id":"38938691"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3609.png","content":{"abstract":"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.","authors":["Yan Zhang","Ruidan He","Zuozhu Liu","Kwan Hui Lim","Lidong Bing"],"demo_url":"","keywords":["sentence-pair tasks","clustering","semantic search","downstream tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.124","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.2083","main.471","main.3205","main.2790"],"title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization","tldr":"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantical...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3609","id":"main.3609","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3635.png","content":{"abstract":"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \\url{https://github.com/bohanli/BERT-flow}.","authors":["Bohan Li","Hao Zhou","Junxian He","Mingxuan Wang","Yiming Yang","Lei Li"],"demo_url":"","keywords":["natural processing","semantic task","semantic tasks","pre-trained representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.733","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.3358","main.3609","TACL.2411","main.3093","main.2851"],"title":"On the Sentence Embeddings from Pre-trained Language Models","tldr":"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3635","id":"main.3635","presentation_id":"38939378"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3647.png","content":{"abstract":"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.","authors":["Yuval Varkel","Amir Globerson"],"demo_url":"","keywords":["collecting data","coreference resolution","self-supervised tasks","mention representation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.687","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.883","main.1621","main.315","main.1518","main.2890"],"title":"Pre-training Mention Representations in Coreference Models","tldr":"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerf...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3647","id":"main.3647","presentation_id":"38939381"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3682.png","content":{"abstract":"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking.  We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider  despite its structural simplicity.  Many remaining errors are attributable to corpus noise.  This suggests schema linking is the crux for the current text-to-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks.","authors":["Wenqiang Lei","Weixin Wang","Zhixin Ma","Tian Gan","Wei Lu","Min-Yen Kan","Tat-Seng Chua"],"demo_url":"","keywords":["schema linking","data-driven study","text-to-sql task","text-to-sql tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.564","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.3507","main.1866","main.1784","main.3544"],"title":"Re-examining the Role of Schema Linking in Text-to-SQL","tldr":"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of s...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3682","id":"main.3682","presentation_id":"38939387"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.41.png","content":{"abstract":"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as \"what is the definition of...\" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.","authors":["Vered Shwartz","Peter West","Ronan Le Bras","Chandra Bhagavatula","Yejin Choi"],"demo_url":"","keywords":["natural understanding","multiple-choice tasks","commonsense reasoning","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.373","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2763","main.3186","TACL.2049","main.2630","main.1052"],"title":"Unsupervised Commonsense Question Answering with Self-Talk","tldr":"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KB...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.41","id":"main.41","presentation_id":"38938641"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.607.png","content":{"abstract":"We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (\u201clearn poses\u201d is a step in the larger goal of \u201cdoing yoga\u201d) and step-step temporal relations (\u201cbuy a yoga mat\u201d typically precedes \u201clearn poses\u201d). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.","authors":["Li Zhang","Qing Lyu","Chris Callison-Burch"],"demo_url":"","keywords":["reasoning tasks","common-sense inference","out-of-domain tasks","swag"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.374","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.928","main.3470","main.1191","demo.86","main.2054"],"title":"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow","tldr":"We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (\u201clearn poses\u201d is a step in the larger goal of \u201cdoing yoga\u201d) and step-step temporal relations (\u201cbuy a yoga mat\u201d typically precedes \u201clearn p...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.607","id":"main.607","presentation_id":"38938737"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.754.png","content":{"abstract":"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.","authors":["Yuki Arase","Jun'ichi Tsujii"],"demo_url":"","keywords":["phrase alignment","modelling interactions","textual recognition","phrase problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.125","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.639","main.1957","main.1938","main.825","main.1503"],"title":"Compositional Phrase Alignment and Beyond","tldr":"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alig...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.754","id":"main.754","presentation_id":"38938769"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.877.png","content":{"abstract":"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each node of a syntax tree is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation. We design a tree-parallel mini-batch strategy for efficient training and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.","authors":["Haiyan Wu","Ying Liu","Shaoyun Shi"],"demo_url":"","keywords":["sentence task","training predicting","tree-based modeling","modularized network"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.222","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3375","main.3013","main.1952","main.1957","main.2040"],"title":"Modularized Syntactic Neural Networks for Sentence Classification","tldr":"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modulari...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.877","id":"main.877","presentation_id":"38938796"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.911.png","content":{"abstract":"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.","authors":["Ikuya Yamada","Akari Asai","Hiroyuki Shindo","Hideaki Takeda","Yuji Matsumoto"],"demo_url":"","keywords":["natural tasks","pretraining task","transformer","entity-related tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.523","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1528","main.1159","main.2849","main.989","main.327"],"title":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","tldr":"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.911","id":"main.911","presentation_id":"38938803"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2055.png","content":{"abstract":"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.","authors":["Lifu Tu","Garima Lalwani","Spandana Gella","He He"],"demo_url":"","keywords":["generalization","natural inference","paraphrase identification","pre-trained models"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2491","TACL.2041","TACL.2047","main.1631","main.1196"],"title":"An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models","tldr":"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"TACL.2055","id":"TACL.2055","presentation_id":"38939403"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2135.png","content":{"abstract":"Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to describe with brief sentences, and sometimes require examples to fully convey the user\u2019s intent. We present a framework for regex synthesis in this setting where both natural language (NL) and examples are available. First, a semantic parser (either grammar-based or neural) maps the natural language description into an intermediate sketch, which is an incomplete regex containing holes to denote missing components. Then a program synthesizer searches over the regex space defined by the sketch and finds a regex that is consistent with the given string examples. Our semantic parser can be trained purely from weak supervision based on correctness of the synthesized regex, or it can leverage heuristically-derived sketches. We evaluate on two prior datasets (Kushman and Barzilay, 2013; Locascio et al., 2016) and a real-world dataset from Stack Overflow. Our system achieves state-of-the-art performance on the prior datasets and solves 57% of the real-world dataset, which existing neural systems completely fail on.","authors":["Xi Ye","Qiaochu Chen","Xinyu Wang","Isil Dillig","Greg Durrett"],"demo_url":"","keywords":["regex synthesis","semantic parser","program synthesizer","neural systems"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1130","main.1647","main.2342","main.648"],"title":"Sketch-Driven Regular Expression Generation from Natural Language and Examples","tldr":"Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to descr...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"TACL.2135","id":"TACL.2135","presentation_id":"38939411"}]
