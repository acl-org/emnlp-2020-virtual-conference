[{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1004.png","content":{"abstract":"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.","authors":["Wenxuan Zhang","Yang Deng","Jing Ma","Wai Lam"],"demo_url":"","keywords":["online shopping","evidence-based tasks","answer problem","product-related platforms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.188","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1022","main.959","main.2228","main.2380","main.2117"],"title":"AnswerFact: Fact Checking in Product Question Answering","tldr":"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on tho...","track":"Question Answering"},"forum":"main.1004","id":"main.1004","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1006.png","content":{"abstract":"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping  response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.","authors":["Xueliang Zhao","Wei Wu","Can Xu","Chongyang Tao","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["knowledge-grounded generation","equipping","knowledge selection","response generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.272","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1201","main.689","main.1654","main.1846","main.128"],"title":"Knowledge-Grounded Dialogue Generation with Pre-trained Language Models","tldr":"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping  response generation defined by a pre-trained language model with a knowled...","track":"Dialog and Interactive Systems"},"forum":"main.1006","id":"main.1006","presentation_id":"38938823"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1009.png","content":{"abstract":"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.","authors":["Hung Le","Doyen Sahoo","Nancy Chen","Steven C.H. Hoi"],"demo_url":"","keywords":["video-grounded dialogues","high-resolution queries","video setting","bi-directional learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.145","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2839","main.2927","main.1113","main.355","main.1085"],"title":"BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues","tldr":"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over mu...","track":"Dialog and Interactive Systems"},"forum":"main.1009","id":"main.1009","presentation_id":"38938824"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1010.png","content":{"abstract":"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on local features while neglecting global information. To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph. Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations. Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information. Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.","authors":["Qinzhuo Wu","Qi Zhang","Jinlan Fu","Xuanjing Huang"],"demo_url":"","keywords":["natural tasks","math solving","generation","knowledge-aware representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.579","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.782","main.666","TACL.2121","main.179","main.1648"],"title":"A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving","tldr":"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the proble...","track":"NLP Applications"},"forum":"main.1010","id":"main.1010","presentation_id":"38938825"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1011.png","content":{"abstract":"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.","authors":["Zequn Sun","Muhao Chen","Wei Hu","Chengming Wang","Jian Dai","Wei Zhang"],"demo_url":"","keywords":["capturing associations","entity alignment","entity inference","nlp applications"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.460","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2877","main.2406","main.1787","main.1460"],"title":"Knowledge Association with Hyperbolic Knowledge Graph Embeddings","tldr":"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are...","track":"Information Extraction"},"forum":"main.1011","id":"main.1011","presentation_id":"38938826"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1012.png","content":{"abstract":"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose \"UniConv\" - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.","authors":["Hung Le","Doyen Sahoo","Chenghao Liu","Nancy Chen","Steven C.H. Hoi"],"demo_url":"","keywords":["multi-domain dialogues","tracking states","end-to-end systems","dialogue tracking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.146","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1702","main.1201","main.1846","TACL.2143","main.2209"],"title":"UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues","tldr":"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states...","track":"Dialog and Interactive Systems"},"forum":"main.1012","id":"main.1012","presentation_id":"38938827"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1018.png","content":{"abstract":"Bolukbasi et al. (2016) presents one of the first gender  bias  mitigation  techniques  for  word embeddings.   Their  method  takes  pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings.  However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version.  We take inspiration from kernel principal component analysis and derive a  non-linear bias isolation technique.  We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","authors":["Francisco Vargas","Ryan Cotterell"],"demo_url":"","keywords":["word embeddings","analogical task","non-linear mitigation","gender"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.232","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2011","main.3093","main.865","main.802","main.3143"],"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","tldr":"Bolukbasi et al. (2016) presents one of the first gender  bias  mitigation  techniques  for  word embeddings.   Their  method  takes  pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias...","track":"Machine Learning for NLP"},"forum":"main.1018","id":"main.1018","presentation_id":"38938828"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1022.png","content":{"abstract":"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases. To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task. With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases. We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.","authors":["Yeon Seonwoo","Ji-Hoon Kim","Jung-Woo Ha","Alice Oh"],"demo_url":"","keywords":["context prediction","context task","reading comprehension","extractive models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.189","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.449","main.2586","main.1788","main.3186","main.1837"],"title":"Context-Aware Answer Extraction in Question Answering","tldr":"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. Thi...","track":"Question Answering"},"forum":"main.1022","id":"main.1022","presentation_id":"38938829"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1023.png","content":{"abstract":"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.","authors":["Arthur Bra\u017einskas","Mirella Lapata","Ivan Titov"],"demo_url":"","keywords":["opinion summarization","automatic text","summary production","summarization mode"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.337","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.965","main.471","main.3012","main.3581","main.2506"],"title":"Few-Shot Learning for Opinion Summarization","tldr":"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the ...","track":"Summarization"},"forum":"main.1023","id":"main.1023","presentation_id":"38938830"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1024.png","content":{"abstract":"While being an essential component of spoken language, fillers (e.g. \"um\" or \"uh\") often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two downstream tasks --- predicting a speaker's stance and expressed confidence.","authors":["Tanvi Dinkar","Pierre Colombo","Matthieu Labeau","Chlo\u00e9 Clavel"],"demo_url":"","keywords":["spoken tasks","modelling language","deep embeddings","fillers"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.641","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3257","main.3353","main.1006","main.1455","main.1938"],"title":"The importance of fillers for text representations of speech transcripts","tldr":"While being an essential component of spoken language, fillers (e.g. \"um\" or \"uh\") often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing impr...","track":"Speech and Multimodality"},"forum":"main.1024","id":"main.1024","presentation_id":"38938831"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1030.png","content":{"abstract":"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.","authors":["Hrituraj Singh","Sumit Shekhar"],"demo_url":"","keywords":["answering questions","natural questions","sequential localization","question encoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.264","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2120","main.319","main.3398","main.3054","main.1123"],"title":"STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering","tldr":"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1030","id":"main.1030","presentation_id":"38938832"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1032.png","content":{"abstract":"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.","authors":["Andreas R\u00fcckl\u00e9","Jonas Pfeiffer","Iryna Gurevych"],"demo_url":"","keywords":["answer tasks","zero-shot transfer","text models","self-supervised training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.194","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.858","main.74","main.1803","main.2167","main.2943"],"title":"MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale","tldr":"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks o...","track":"NLP Applications"},"forum":"main.1032","id":"main.1032","presentation_id":"38938833"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1046.png","content":{"abstract":"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA","authors":["Jiaao Chen","Zhenghui Wang","Ran Tian","Zichao Yang","Diyi Yang"],"demo_url":"","keywords":["named recognition","deep understanding","semi-supervised ner","entity learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.95","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2733","main.989","main.426","main.1458"],"title":"Local Additivity Based Data Augmentation for Semi-supervised NER","tldr":"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data ...","track":"Machine Learning for NLP"},"forum":"main.1046","id":"main.1046","presentation_id":"38938834"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1049.png","content":{"abstract":"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as  results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.","authors":["Ankur Parikh","Xuezhi Wang","Sebastian Gehrmann","Manaal Faruqui","Bhuwan Dhingra","Diyi Yang","Dipanjan Das"],"demo_url":"","keywords":["controlled task","high-precision generation","totto","dataset process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.89","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.870","main.835","main.2476","main.2590","main.2635"],"title":"ToTTo: A Controlled Table-To-Text Generation Dataset","tldr":"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain...","track":"Language Generation"},"forum":"main.1049","id":"main.1049","presentation_id":"38938835"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1052.png","content":{"abstract":"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.","authors":["Julian Michael","Jan A. Botha","Ian Tenney"],"demo_url":"","keywords":["pretrained encoders","elmo","bert","latent learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.552","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2122","main.1159","main.41","main.1130","main.2363"],"title":"Asking without Telling: Exploring Latent Ontologies in Contextual Representations","tldr":"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is th...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1052","id":"main.1052","presentation_id":"38938836"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.106.png","content":{"abstract":"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.","authors":["Elizabeth Nielsen","Mark Steedman","Sharon Goldwater"],"demo_url":"","keywords":["pitch detection","cnn-based model","phenomena","contrast"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.642","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.2349","main.920","main.1613","main.699"],"title":"The role of context in neural pitch accent detection in English","tldr":"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model f...","track":"Speech and Multimodality"},"forum":"main.106","id":"main.106","presentation_id":"38938650"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1061.png","content":{"abstract":"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings.  The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence.  It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language.  Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.","authors":["Rui Cai","Mirella Lapata"],"demo_url":"","keywords":["cross-lingual labeling","predicting roles","srl","machine engines"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.319","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1379","main.3597","main.143","main.2641","main.3046"],"title":"Alignment-free Cross-lingual Semantic Role Labeling","tldr":"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1061","id":"main.1061","presentation_id":"38938837"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1070.png","content":{"abstract":"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have a similar likelihood of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities' infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.","authors":["Jiaming Shen","Wenda Qiu","Jingbo Shang","Michelle Vanni","Xiang Ren","Jiawei Han"],"demo_url":"","keywords":["entity expansion","synonym discovery","nlp tasks","synonym tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.666","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3216","main.2849","main.1787","main.298","main.1706"],"title":"SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery","tldr":"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymou...","track":"Information Retrieval and Text Mining"},"forum":"main.1070","id":"main.1070","presentation_id":"38938838"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1071.png","content":{"abstract":"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images.  In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and  text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.","authors":["Bowen Zhang","Hexiang Hu","Vihan Jain","Eugene Ie","Fei Sha"],"demo_url":"","keywords":["cross-modal retrieval","referring expression","compositional recognition","pre-training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.60","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2702","main.666","main.531","main.1052","main.1231"],"title":"Learning to Represent Image and Text with Denotation Graph","tldr":"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers t...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1071","id":"main.1071","presentation_id":"38938839"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.108.png","content":{"abstract":"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.","authors":["Adam Roberts","Colin Raffel","Noam Shazeer"],"demo_url":"","keywords":["fine-tuning models","neural models","open-domain systems","model size"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.437","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1351","main.2931","main.3074","main.1159","main.1528"],"title":"How Much Knowledge Can You Pack Into the Parameters of a Language Model?","tldr":"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning p...","track":"Question Answering"},"forum":"main.108","id":"main.108","presentation_id":"38938651"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1085.png","content":{"abstract":"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating \\textit{commonsense} captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset ``Video-to-Commonsense (V2C)\" that contains $\\sim9k$ videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.","authors":["Zhiyuan Fang","Tejas Gokhale","Pratyay Banerjee","Chitta Baral","Yezhou Yang"],"demo_url":"","keywords":["captioning","video understanding","video captioning","generating captions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.61","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2839","main.2322","main.1009","main.284","main.702"],"title":"Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning","tldr":"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and trans...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1085","id":"main.1085","presentation_id":"38938840"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1086.png","content":{"abstract":"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.","authors":["Hongzhi Zhang","Yingyao Wang","Sirui Wang","Xuezhi Cao","Fuzheng Zhang","Zhongyuan Wang"],"demo_url":"","keywords":["symbolic reasoning","pre-trained models","pre-trained transformers","table representation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.126","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2724","main.1618","main.204","main.3398","main.2253"],"title":"Table Fact Verification with Structure-Aware Transformer","tldr":"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, becau...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1086","id":"main.1086","presentation_id":"38938841"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1091.png","content":{"abstract":"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.","authors":["Dayiheng Liu","Yeyun Gong","Yu Yan","Jie Fu","Bo Shao","Daxin Jiang","Jiancheng Lv","Nan Duan"],"demo_url":"","keywords":["news generation","single generation","multi-source decoder","keyphrases"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.505","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3088","main.652","main.714","main.471","main.1023"],"title":"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation","tldr":"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. Howeve...","track":"Summarization"},"forum":"main.1091","id":"main.1091","presentation_id":"38938842"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.110.png","content":{"abstract":"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.","authors":["Dongfang Li","Baotian Hu","Qingcai Chen","Weihua Peng","Anqi Wang"],"demo_url":"","keywords":["national examination","medical examination","large-scale models","reading model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.111","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1631","main.1923","main.2630","main.748","main.1159"],"title":"Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text","tldr":"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the ...","track":"NLP Applications"},"forum":"main.110","id":"main.110","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1100.png","content":{"abstract":"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.","authors":["Hao-Ran Wei","Zhirui Zhang","Boxing Chen","Weihua Luo"],"demo_url":"","keywords":["domain-specific translation","domain adaptation","back-translation method","out-of-domain systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.474","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1572","main.2661","main.3227","main.856","main.888"],"title":"Iterative Domain-Repaired Back-Translation","tldr":"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translat...","track":"Machine Translation and Multilinguality"},"forum":"main.1100","id":"main.1100","presentation_id":"38938843"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1103.png","content":{"abstract":"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states,  motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of ~670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.","authors":["Nasrin Mostafazadeh","Aditya Kalyanpur","Lori Moon","David Buchanan","Lauren Berkowitz","Or Biran","Jennifer Chu-Carroll"],"demo_url":"","keywords":["ai systems","mental models","causal explanation","pretrained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.370","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3291","main.1972","main.2758","main.3450","main.1191"],"title":"GLUCOSE: GeneraLized and COntextualized Story Explanations","tldr":"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit c...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1103","id":"main.1103","presentation_id":"38938844"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1107.png","content":{"abstract":"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unveri\ufb01ed examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are veri\ufb01ed through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.","authors":["Victor Zhong","Mike Lewis","Sida I. Wang","Luke Zettlemoyer"],"demo_url":"","keywords":["adaptation","grounded adaptation","semantic parser","database schemas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.558","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.850","main.1495","main.1179","main.891","main.1130"],"title":"Grounded Adaptation for Zero-shot Executable Semantic Parsing","tldr":"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthe...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1107","id":"main.1107","presentation_id":"38938845"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1113.png","content":{"abstract":"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments.","authors":["Alexander Ku","Peter Anderson","Roma Patel","Eugene Ie","Jason Baldridge"],"demo_url":"","keywords":["multitask learning","embodied agents","vln","rxr"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.356","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.995","main.647","main.355","main.1402","main.3360"],"title":"Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding","tldr":"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by a...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1113","id":"main.1113","presentation_id":"38938846"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1116.png","content":{"abstract":"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.","authors":["Haoyu Wang","Muhao Chen","Hongming Zhang","Dan Roth"],"demo_url":"","keywords":["understanding language","temporal extraction","event construction","inducing complexes"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.51","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2972","main.237","main.1421","main.2508","main.1569"],"title":"Joint Constrained Learning for Event-Event Relation Extraction","tldr":"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membersh...","track":"Information Extraction"},"forum":"main.1116","id":"main.1116","presentation_id":"38938847"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1123.png","content":{"abstract":"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.","authors":["Yang Deng","Wenxuan Zhang","Wai Lam"],"demo_url":"","keywords":["question-driven summarization","question-driven method","multi-hop generator","multi-hop"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.547","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2125","main.1648","main.2761","main.693","main.3054"],"title":"Multi-hop Inference for Question-driven Summarization","tldr":"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive sum...","track":"Summarization"},"forum":"main.1123","id":"main.1123","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1129.png","content":{"abstract":"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.","authors":["Charuta Pethe","Allen Kim","Steve Skiena"],"demo_url":"","keywords":["predicting boundaries","segmenting texts","chapter segmentation","exact prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.672","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2367","main.3389","main.3010","main.1049","main.2650"],"title":"Chapter Captor: Text Segmentation in Novels","tldr":"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenber...","track":"NLP Applications"},"forum":"main.1129","id":"main.1129","presentation_id":"38938849"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1130.png","content":{"abstract":"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's \\emph{vocabulary}---typically selected before training and permanently fixed later---affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.","authors":["Nikolaos Pappas","Phoebe Mulcaire","Noah A. Smith"],"demo_url":"","keywords":["language modeling","cross-domain settings","language models","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.96","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2363","TACL.2041","main.1892","main.74","main.852"],"title":"Grounded Compositional Outputs for Adaptive Language Modeling","tldr":"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's \\emph{vocabulary}---typically selected b...","track":"Machine Learning for NLP"},"forum":"main.1130","id":"main.1130","presentation_id":"38938850"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1135.png","content":{"abstract":"Recognizing the flow of time in a story is a crucial aspect of understanding it.  Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hour. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.","authors":["Allen Kim","Charuta Pethe","Steve Skiena"],"demo_url":"","keywords":["time-of-day model","dynamic breakpoints","temporal expressions","relative events"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.730","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.928","main.605","main.3617","main.1421","main.3486"],"title":"What time is it? Temporal Analysis of Novels","tldr":"Recognizing the flow of time in a story is a crucial aspect of understanding it.  Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating ea...","track":"NLP Applications"},"forum":"main.1135","id":"main.1135","presentation_id":"38938851"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1140.png","content":{"abstract":"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.","authors":["Shiquan Yang","Rui Zhang","Sarah Erfani"],"demo_url":"","keywords":["representation graphs","end-to-end systems","learning framework","recurrent architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.147","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2143","main.787","main.215","main.2141","main.1846"],"title":"GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems","tldr":"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; t...","track":"Dialog and Interactive Systems"},"forum":"main.1140","id":"main.1140","presentation_id":"38938852"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1141.png","content":{"abstract":"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences. Extensive evaluations are conducted on six public datasets. The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.","authors":["Baiyun Cui","Yingming Li","Zhongfei Zhang"],"demo_url":"","keywords":["bert-enhanced network","brson","bert","coherence modeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.511","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.2476","main.861","main.3437","main.574"],"title":"BERT-enhanced Relational Sentence Ordering Network","tldr":"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. ...","track":"Discourse and Pragmatics"},"forum":"main.1141","id":"main.1141","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1146.png","content":{"abstract":"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.","authors":["Dana Ruiter","Josef van Genabith","Cristina Espa\u00f1a-Bonet"],"demo_url":"","keywords":["training","self-supervised","ssnmt","ssnmt model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.202","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.26","TACL.2107","main.701","main.130","main.2893"],"title":"Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation","tldr":"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In t...","track":"Machine Translation and Multilinguality"},"forum":"main.1146","id":"main.1146","presentation_id":"38938854"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1159.png","content":{"abstract":"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.","authors":["Hao Peng","Tianyu Gao","Xu Han","Yankai Lin","Peng Li","Zhiyuan Liu","Maosong Sun","Jie Zhou"],"demo_url":"","keywords":["relation benchmarks","re scenarios","neural models","re models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.298","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2739","main.1669","main.2307","main.2363","main.1528"],"title":"Learning from Context or Names? An Empirical Study on Neural Relation Extraction","tldr":"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these ...","track":"Information Extraction"},"forum":"main.1159","id":"main.1159","presentation_id":"38938855"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1179.png","content":{"abstract":"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which also set state-of-the-art in ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.","authors":["Armen Aghajanyan","Jean Maillard","Akshat Shrivastava","Keith Diedrick","Michael Haeger","Haoran Li","Yashar Mehdad","Veselin Stoyanov","Anuj Kumar","Mike Lewis","Sonal Gupta"],"demo_url":"","keywords":["semantic parsing","dialog challenges","session-based parsing","atis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.408","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1561","main.1957","TACL.2143","main.876","main.850"],"title":"Conversational Semantic Parsing","tldr":"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resoluti...","track":"Dialog and Interactive Systems"},"forum":"main.1179","id":"main.1179","presentation_id":"38938856"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1180.png","content":{"abstract":"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.","authors":["Wenyue Zhang","Xiaoli Li","Yang Li","Suge Wang","Deyu Li","Jian Liao","Jianxing Zheng"],"demo_url":"","keywords":["detecting drift","distribution learning","classification model","hierarchical model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.307","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1575","main.1675","main.1289","main.2068","main.2430"],"title":"Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder","tldr":"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this pa...","track":"NLP Applications"},"forum":"main.1180","id":"main.1180","presentation_id":"38938857"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1187.png","content":{"abstract":"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.","authors":["Wanjun Zhong","Duyu Tang","Zenan Xu","Ruize Wang","Nan Duan","Ming Zhou","Jiahai Wang","Jian Yin"],"demo_url":"","keywords":["deepfake detection","automatically text","deepfake text","natural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.193","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1892","main.989","main.1159","main.1488"],"title":"Neural Deepfake Detection with Factual Structure of Text","tldr":"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coa...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1187","id":"main.1187","presentation_id":"38938858"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1191.png","content":{"abstract":"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step \"black box\" model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.","authors":["Mucheng Ren","Xiubo Geng","Tao Qin","Heyan Huang","Daxin Jiang"],"demo_url":"","keywords":["reasoning process","sequential approach","neural modules","reasoning modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.548","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.607","main.2253","main.2650","main.3470","main.1103"],"title":"Towards Interpretable Reasoning over Paragraph Effects in Situation","tldr":"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated ...","track":"Question Answering"},"forum":"main.1191","id":"main.1191","presentation_id":"38938859"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1196.png","content":{"abstract":"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model's robustness.","authors":["Zujie Liang","Weitao Jiang","Haifeng Hu","Jiaying Zhu"],"demo_url":"","keywords":["visual","generating samples","augmentation","self-supervised mechanism"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.265","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3183","TACL.2055","main.387","TACL.2129","main.2258"],"title":"Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering","tldr":"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1196","id":"main.1196","presentation_id":"38938860"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1201.png","content":{"abstract":"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.","authors":["Chien-Sheng Wu","Steven C.H. Hoi","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["language modeling","pre-training","response task","task-oriented applications"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.66","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1654","main.215","main.1846","main.128","main.3393"],"title":"TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue","tldr":"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue dataset...","track":"Dialog and Interactive Systems"},"forum":"main.1201","id":"main.1201","presentation_id":"38938861"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1205.png","content":{"abstract":"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.","authors":["Pierangelo Lombardo","Alessio Boiardi","Luca Colombo","Angelo Schiavone","Nicol\u00f2 Tamagnone"],"demo_url":"","keywords":["content-based recommenders","construction","top-rank evaluation","semantic models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.249","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1787","TACL.2095","main.693","main.300","main.1949"],"title":"Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models","tldr":"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime exam...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1205","id":"main.1205","presentation_id":"38938862"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1208.png","content":{"abstract":"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.","authors":["Yung-Sung Chuang","Shang-Yu Su","Yun-Nung Chen"],"demo_url":"","keywords":["lll tasks","sequence generation","text tasks","lifelong"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.233","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2838","TACL.2041","main.74","main.1734","main.1485"],"title":"Lifelong Language Knowledge Distillation","tldr":"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2K...","track":"Machine Learning for NLP"},"forum":"main.1208","id":"main.1208","presentation_id":"38938863"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1210.png","content":{"abstract":"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks","authors":["Fabio Massimo Zanzotto","Andrea Santilli","Leonardo Ranaldi","Dario Onorati","Pierfrancesco Tommasino","Francesca Fallucchi"],"demo_url":"","keywords":["natural understanding","inference","syntactic parsers","large-scale learners"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.18","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2411","main.2040","TACL.2141","main.2179","main.1957"],"title":"KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations","tldr":"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose K...","track":"Machine Learning for NLP"},"forum":"main.1210","id":"main.1210","presentation_id":"38938864"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1217.png","content":{"abstract":"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.","authors":["Zehui Dai","Cheng Peng","Huajie Chen","Yadong Ding"],"demo_url":"","keywords":["tacsa tasks","aspect-category analysis","targeted analysis","multi-task learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.565","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3579","main.3013","main.3185","main.1675","main.3064"],"title":"A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis","tldr":"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)AC...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1217","id":"main.1217","presentation_id":"38938865"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1219.png","content":{"abstract":"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional'' BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.","authors":["Brielen Madureira","David Schlangen"],"demo_url":"","keywords":["nlp","interactive systems","language encoders","bidirectional lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.26","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2491","main.471","TACL.2041","main.1986","main.130"],"title":"Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU","tldr":"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and b...","track":"Dialog and Interactive Systems"},"forum":"main.1219","id":"main.1219","presentation_id":"38938866"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1220.png","content":{"abstract":"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.","authors":["Mengjie Zhao","Tao Lin","Fei Mi","Martin Jaggi","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["masking bert","nlp tasks","downstream tasks","masking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.174","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3337","main.3074","main.247","demo.118","main.1299"],"title":"Masking as an Efficient Alternative to Finetuning for Pretrained Language Models","tldr":"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleve...","track":"Machine Learning for NLP"},"forum":"main.1220","id":"main.1220","presentation_id":"38938867"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1225.png","content":{"abstract":"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.","authors":["Tao Gui","Jiacheng Ye","Qi Zhang","Zhengyan Li","Zichu Fei","Yeyun Gong","Xuanjing Huang"],"demo_url":"","keywords":["label decoding","sequence tasks","parallel decoding","faster prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.181","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1615","main.2636","main.2790","main.148","main.1720"],"title":"Uncertainty-Aware Label Refinement for Sequence Labeling","tldr":"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel tw...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1225","id":"main.1225","presentation_id":"38938868"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1227.png","content":{"abstract":"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.","authors":["Sangwhan Moon","Naoaki Okazaki"],"demo_url":"","keywords":["natural processing","downstream tasks","mitigation","large models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.631","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.2635","main.1960","main.1351","main.858"],"title":"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching","tldr":"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on do...","track":"Machine Learning for NLP"},"forum":"main.1227","id":"main.1227","presentation_id":"38938869"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1231.png","content":{"abstract":"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.","authors":["Martin Schmitt","Sahand Sharifzadeh","Volker Tresp","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["graph-to-text generation","text-to-graph extraction","semantic parsing","unsupervised generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.577","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.666","main.1706","TACL.2121","main.531","main.143"],"title":"An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing","tldr":"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific paral...","track":"NLP Applications"},"forum":"main.1231","id":"main.1231","presentation_id":"38938870"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1248.png","content":{"abstract":"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.","authors":["Jind\u0159ich Libovick\u00fd","Alexander Fraser"],"demo_url":"","keywords":["transformer architecture","segmentation","subword model","neural model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.203","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3337","main.1733","main.1960","main.2675","main.1798"],"title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems","tldr":"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show tha...","track":"Machine Translation and Multilinguality"},"forum":"main.1248","id":"main.1248","presentation_id":"38938871"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.125.png","content":{"abstract":"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the \u201cauthor\u201d of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T1 and T2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/ 302zWdz","authors":["Adaku Uchendu","Thai Le","Kai Shu","Dongwon Lee"],"demo_url":"","keywords":["generating texts","authorship problem","xlm","pplm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.673","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["demo.126","main.648","main.2650","main.1187","main.870"],"title":"Authorship Attribution for Neural Text Generation","tldr":"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts...","track":"NLP Applications"},"forum":"main.125","id":"main.125","presentation_id":"38938653"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1250.png","content":{"abstract":"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","authors":["Huilin Zhong","Junsheng Zhou","Weiguang Qu","Yunfei Long","Yanhui Gu"],"demo_url":"","keywords":["legal prediction","classification","law","lemm"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.540","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1952","main.3651","main.2630","main.911"],"title":"An Element-aware Multi-representation Model for Law Article Prediction","tldr":"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label sample...","track":"NLP Applications"},"forum":"main.1250","id":"main.1250","presentation_id":"38938872"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1258.png","content":{"abstract":"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework  (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework's default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.","authors":["Amrith Krishna","Ashim Gupta","Deepak Garasangi","Pavankumar Satuluri","Pawan Goyal"],"demo_url":"","keywords":["joint parsing","dependency parsing","structured tasks","morphological parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.388","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1957","main.1943","TACL.2141","main.447"],"title":"Keep it Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit","tldr":"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend t...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1258","id":"main.1258","presentation_id":"38938873"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1262.png","content":{"abstract":"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.","authors":["Dayiheng Liu","Yeyun Gong","Jie Fu","Yu Yan","Jiusheng Chen","Jiancheng Lv","Nan Duan","Ming Zhou"],"demo_url":"","keywords":["machine comprehension","question generation","question-answering tasks","question task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.467","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3054","main.1485","main.319","main.1030","main.3140"],"title":"Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space","tldr":"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inferenc...","track":"Question Answering"},"forum":"main.1262","id":"main.1262","presentation_id":"38938874"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1263.png","content":{"abstract":"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and \\yoruba on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.","authors":["Michael A. Hedderich","David Adelani","Dawei Zhu","Jesujoba Alabi","Udia Markus","Dietrich Klakow"],"demo_url":"","keywords":["nlp tasks","ner classification","low-resource learning","multilingual models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.204","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.74","main.1680","main.2500","main.858","main.1379"],"title":"Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages","tldr":"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to r...","track":"NLP Applications"},"forum":"main.1263","id":"main.1263","presentation_id":"38938875"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1267.png","content":{"abstract":"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.","authors":["Ruize Wang","Duyu Tang","Nan Duan","Wanjun Zhong","Zhongyu Wei","Xuanjing Huang","Daxin Jiang","Ming Zhou"],"demo_url":"","keywords":["detection fragments","training process","fine-grained detection","declarative techniques"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.320","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2763","demo.97","main.2553","main.1970","main.2739"],"title":"Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection","tldr":"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specif...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1267","id":"main.1267","presentation_id":"38938876"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1271.png","content":{"abstract":"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.","authors":["Zhihong Chen","Yan Song","Tsung-Hui Chang","Xiang Wan"],"demo_url":"","keywords":["medical imaging","writing reports","automatically reports","clinical automation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.112","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.110","main.55","main.1923","main.1942","main.1219"],"title":"Generating Radiology Reports via Memory-driven Transformer","tldr":"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is ...","track":"NLP Applications"},"forum":"main.1271","id":"main.1271","presentation_id":"38938877"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1275.png","content":{"abstract":"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work's performance is often not comprehensively evaluated due to the lack of readily-available execution engines.  Upon identifying these gaps, we propose~\\benchmarkname{}, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets $\\times$ four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.","authors":["Jiaqi Guo","Qian Liu","Jian-Guang Lou","Zhenwen Li","Xueqing Liu","Tao Xie","Ting Liu"],"demo_url":"","keywords":["meaning representation","semantic parsing","unimer","meaning representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.118","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2179","main.1754","CL.2","main.1957","main.143"],"title":"Benchmarking Meaning Representations in Neural Semantic Parsing","tldr":"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is le...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1275","id":"main.1275","presentation_id":"38938878"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.128.png","content":{"abstract":"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.","authors":["Liang Qiu","Yizhou Zhao","Weiyan Shi","Yuan Liang","Feng Shi","Tao Yuan","Zhou Yu","Song-Chun Zhu"],"demo_url":"","keywords":["inducing representation","computational linguistics","dialogue design","discourse analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.148","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1654","main.3179","main.1201","main.2164","main.215"],"title":"Structured Attention for Unsupervised Dialogue Structure Induction","tldr":"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be...","track":"Dialog and Interactive Systems"},"forum":"main.128","id":"main.128","presentation_id":"38938654"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1280.png","content":{"abstract":"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the \\textit{embryology} of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at \\url{https://github.com/d223302/albert-embryology}.","authors":["Cheng-Han Chiang","Sung-Feng Huang","Hung-yi Lee"],"demo_url":"","keywords":["pretraining","developmental process","pretrained models","lms"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.553","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2424","TACL.2041","main.2363","main.2414","main.852"],"title":"Pretrained Language Model Embryology: The Birth of ALBERT","tldr":"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent languag...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1280","id":"main.1280","presentation_id":"38938879"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1282.png","content":{"abstract":"Across languages, multiple consecutive adjectives modifying a noun (e.g.~``the big red dog'') follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.","authors":["Jun Yen Leung","Guy Emerson","Ryan Cotterell"],"demo_url":"","keywords":["multi-lingual ordering","corpus-driven model","latent-variable model","statistical model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.329","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7B","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3181","main.3115","main.457","TACL.2013","main.820"],"title":"Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model","tldr":"Across languages, multiple consecutive adjectives modifying a noun (e.g.~``the big red dog'') follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the int...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.1282","id":"main.1282","presentation_id":"38938880"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1287.png","content":{"abstract":"This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers\u2019 responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chinese microblog Sina Weibo with over 13 thousand trending topics, emotion votes in 24 fine-grained types from massive participants, and user comments to allow context understanding. In experiments, we examine baseline performance to predict a topic\u2019s possible social emotions in a multilabel classification setting. The results show that a seq2seq model with user comment modeling performs the best, even surpassing human prediction. More analyses shed light on the effects of emotion types, topic description lengths, contexts from user comments, and the limited capacity of the existing models.","authors":["Keyang Ding","Jing Li","Yuji Zhang"],"demo_url":"","keywords":["online topics","context understanding","seqseq model","user modeling"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.106","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.916","TACL.2093","main.1675","main.2261","main.2996"],"title":"Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics","tldr":"This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers\u2019 responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chines...","track":"Computational Social Science and Social Media"},"forum":"main.1287","id":"main.1287","presentation_id":"38938881"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1289.png","content":{"abstract":"Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our model can complement supervised syntactic features with latent semantic dependencies. Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.","authors":["Chenhua Chen","Zhiyang Teng","Yue Zhang"],"demo_url":"","keywords":["aspect-level analysis","graph networks","induced graphs","gating mechanisms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.451","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9C","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3286","main.1675","main.1766","main.3375","main.1550"],"title":"Inducing Target-Specific Latent Structures for Aspect Sentiment Classification","tldr":"Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsi...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1289","id":"main.1289","presentation_id":"38938882"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1298.png","content":{"abstract":"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url]. We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.","authors":["Shuo Sun","Kevin Duh"],"demo_url":"","keywords":["cross-lingual retrieval","end-to-end retrieval","single-language retrieval","clirmatrix"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.340","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2746","main.2278","main.1379","main.871","main.3116"],"title":"CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval","tldr":"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language...","track":"Information Retrieval and Text Mining"},"forum":"main.1298","id":"main.1298","presentation_id":"38938883"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1299.png","content":{"abstract":"Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50\\% of computation cost, which indicates our method is both effective and efficient. The source code of this paper can be obtained from \\url{https://github.com/thunlp/SelectiveMasking}.","authors":["Yuxian Gu","Zhengyan Zhang","Xiaozhi Wang","Zhiyuan Liu","Maosong Sun"],"demo_url":"","keywords":["pre-training stage","fine-tuning stage","general pre-training","sentiment tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.566","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1734","main.2491","main.247","main.16","main.2793"],"title":"Train No Evil: Selective Masking for Task-Guided Pre-Training","tldr":"Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage us...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1299","id":"main.1299","presentation_id":"38938884"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.130.png","content":{"abstract":"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT\u201916 English-German and WMT\u201914 English-French translation tasks show that it is 1:4 \u0002 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/ SDT-Training.","authors":["Bei Li","Ziyang Wang","Hui Liu","Yufan Jiang","Quan Du","Tong Xiao","Huizhen Wang","Jingbo Zhu"],"demo_url":"","keywords":["neural systems","wmt tasks","deep encoders","deep encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.72","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.3227","main.1485","main.522","main.1680"],"title":"Shallow-to-Deep Training for Neural Machine Translation","tldr":"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the ...","track":"Machine Translation and Multilinguality"},"forum":"main.130","id":"main.130","presentation_id":"38938655"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1305.png","content":{"abstract":"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.","authors":["Kian Kenyon-Dean","Edward Newell","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["nlp applications","nlp tasks","word embeddings","feature words"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.681","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1428","main.585","main.2596","main.2251"],"title":"Deconstructing word embedding algorithms","tldr":"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memo...","track":"Semantics: Lexical Semantics"},"forum":"main.1305","id":"main.1305","presentation_id":"38938885"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1320.png","content":{"abstract":"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.","authors":["Samson Tan","Shafiq Joty","Lav Varshney","Min-Yen Kan"],"demo_url":"","keywords":["comprehension","fine-tuning models","downstream tasks","nlp systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.455","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3566","main.2847","main.870","main.701","main.2349"],"title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding","tldr":"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.1320","id":"main.1320","presentation_id":"38938886"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1322.png","content":{"abstract":"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.","authors":["Bhanu Prakash Reddy Guda","Sasi Bhushan Seelaboyina","Soumya Sarkar","Animesh Mukherjee"],"demo_url":"","keywords":["manual designation","wikipedia representation","ablation studies","editors"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.674","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["demo.58","main.1952","main.1485","main.450","main.2931"],"title":"NwQM: A neural quality assessment framework for Wikipedia","tldr":"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several qu...","track":"NLP Applications"},"forum":"main.1322","id":"main.1322","presentation_id":"38938887"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1339.png","content":{"abstract":"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.  We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks.  A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.","authors":["Jonathan Mallinson","Rico Sennrich","Mirella Lapata"],"demo_url":"","keywords":["sentence simplification","translation","simplification","encoder-decoder models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.415","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2635","main.2412","main.891","main.214","main.2915"],"title":"Zero-Shot Crosslingual Sentence Simplification","tldr":"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.  We propose a zero-...","track":"Language Generation"},"forum":"main.1339","id":"main.1339","presentation_id":"38938888"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1351.png","content":{"abstract":"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.","authors":["Vincent Micheli","Martin d'Hoffschmidt","Fran\u00e7ois Fleuret"],"demo_url":"","keywords":["language modeling","sustainable practices","compact models","multiple models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.632","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1960","TACL.2047","main.1631","main.2078"],"title":"On the importance of pre-training data volume for compact language models","tldr":"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multipl...","track":"Machine Learning for NLP"},"forum":"main.1351","id":"main.1351","presentation_id":"38938889"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1356.png","content":{"abstract":"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.","authors":["Demi Guo","Yoon Kim","Alexander Rush"],"demo_url":"","keywords":["sequence-to-sequence problems","scan","semantic parsing","neural networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.447","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.910","main.2430","main.891","main.1960","main.148"],"title":"Sequence-Level Mixed Sample Data Augmentation","tldr":"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-se...","track":"Machine Learning for NLP"},"forum":"main.1356","id":"main.1356","presentation_id":"38938890"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1377.png","content":{"abstract":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science.  We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","authors":["Clara Meister","Ryan Cotterell","Tim Vieira"],"demo_url":"","keywords":["language tasks","beam search","decoding","maximum decoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.170","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["TACL.2169","main.2198","main.648","main.3183","main.2307"],"title":"If beam search is the answer, what was the question?","tldr":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhe...","track":"Language Generation"},"forum":"main.1377","id":"main.1377","presentation_id":"38938891"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1379.png","content":{"abstract":"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable  across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.","authors":["Angel Daza","Anette Frank"],"demo_url":"","keywords":["generalization learning","multilingual learning","high-quality translation","srl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.321","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1061","main.143","main.1803","main.2777","main.871"],"title":"X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset","tldr":"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1379","id":"main.1379","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1383.png","content":{"abstract":"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.","authors":["Philipp Dufter","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["zero-shot transfer","training","unsupervised alignment","xnli"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.358","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2490","main.1986","main.517","main.3116","TACL.2107"],"title":"Identifying Elements Essential for BERT's Multilinguality","tldr":"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literat...","track":"Machine Translation and Multilinguality"},"forum":"main.1383","id":"main.1383","presentation_id":"38938893"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1388.png","content":{"abstract":"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work.","authors":["Xisen Jin","Junyi Du","Arka Sadhu","Ram Nevatia","Xiang Ren"],"demo_url":"","keywords":["visually task","continual phrases","visually-grounded task","compositional generalization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.158","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2702","main.3360","TACL.2041","main.3470","main.2838"],"title":"Visually Grounded Continual Learning of Compositional Phrases","tldr":"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning tas...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1388","id":"main.1388","presentation_id":"38938894"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1389.png","content":{"abstract":"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.","authors":["Stefan Larson","Anthony Zheng","Anish Mahendran","Rishi Tekriwal","Adrian Cheung","Eric Guldan","Kevin Leach","Jonathan K. Kummerfeld"],"demo_url":"","keywords":["dialog tasks","intent classification","slot-filling","robust models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.650","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1923","main.345","main.2068","main.2739","main.2733"],"title":"Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness","tldr":"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively const...","track":"Dialog and Interactive Systems"},"forum":"main.1389","id":"main.1389","presentation_id":"38938895"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1390.png","content":{"abstract":"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users' needs. We study the task of predicting the documents that customer care agents can use to facilitate users' needs. We also introduce a new public dataset which supports the aforementioned problem. Using this dataset and two others,  we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task.  Additionally, we analyze the practicality of such systems in terms of inference time complexity.  Our show that an hybrid IR+DL approach provides the best of both worlds.","authors":["Jatin Ganhotra","Haggai Roitman","Doron Cohen","Nathaniel Mills","Chulaka Gunasekara","Yosi Mass","Sachindra Joshi","Luis Lastras","David Konopnicki"],"demo_url":"","keywords":["customer conversations","predicting documents","customer agents","information models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.25","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.116","main.3495","main.3327","main.527","main.916"],"title":"Conversational Document Prediction to Assist Customer Care Agents","tldr":"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users' needs. We study the task of predicting the documents that customer care agents can use to facilitate users' needs. We also in...","track":"NLP Applications"},"forum":"main.1390","id":"main.1390","presentation_id":"38938896"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1393.png","content":{"abstract":"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.","authors":["Adam Tsakalidis","Maria Liakata"],"demo_url":"","keywords":["semantic detection","detecting words","neural embeddings","vector representation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.682","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1938","main.3093","main.1957","main.1135","main.1935"],"title":"Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection","tldr":"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representat...","track":"Semantics: Lexical Semantics"},"forum":"main.1393","id":"main.1393","presentation_id":"38938897"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1395.png","content":{"abstract":"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors. Our method requires only another pre-trained model and no labeled data is needed. We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning  both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.","authors":["Qianchu Liu","Diana McCarthy","Anna Korhonen"],"demo_url":"","keywords":["transformation","contextualized models","dynamic embeddings","post-processing technique"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.333","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2251","main.2363","main.3292","main.298","main.143"],"title":"Towards Better Context-aware Lexical Semantics:Adjusting Contextualized Representations through Static Anchors","tldr":"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that e...","track":"Semantics: Lexical Semantics"},"forum":"main.1395","id":"main.1395","presentation_id":"38938898"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1399.png","content":{"abstract":"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.","authors":["Joseph Fisher","Arpit Mittal","Dave Palfrey","Christos Christodoulopoulos"],"demo_url":"","keywords":["nlp pipelines","triple task","knowledge embeddings","graph embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.595","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1508","main.2721","main.1923","main.1305","main.426"],"title":"Debiasing knowledge graph embeddings","tldr":"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pi...","track":"Machine Learning for NLP"},"forum":"main.1399","id":"main.1399","presentation_id":"38938899"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1402.png","content":{"abstract":"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.","authors":["Ozan Caglayan","Julia Ive","Veneta Haralampieva","Pranava Madhyastha","Lo\u00efc Barrault","Lucia Specia"],"demo_url":"","keywords":["simt","multimodal approaches","simt frameworks","visually-grounded models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.184","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["demo.111","main.888","TACL.2221","main.1572","main.2702"],"title":"Simultaneous Machine Translation with Visual Context","tldr":"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progr...","track":"Speech and Multimodality"},"forum":"main.1402","id":"main.1402","presentation_id":"38938900"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1408.png","content":{"abstract":"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.  In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.","authors":["Prasetya Ajie Utama","Nafise Sadat Moosavi","Iryna Gurevych"],"demo_url":"","keywords":["nlu tasks","nlu models","debiasing methods","self-debiasing framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.613","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1399","main.1923","main.345","main.2886","TACL.2055"],"title":"Towards Debiasing NLU Models from Unknown Biases","tldr":"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a majo...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1408","id":"main.1408","presentation_id":"38938901"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1421.png","content":{"abstract":"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as ``buying a car'' can be used in the context of a new but analogous process such as ``buying a house''. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.","authors":["Hongming Zhang","Muhao Chen","Haoyu Wang","Yangqiu Song","Dan Roth"],"demo_url":"","keywords":["event understanding","nlp","structured representations","process framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.119","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1116","main.2508","main.237","main.96","main.607"],"title":"Analogous Process Structure Induction for Sub-event Sequence Prediction","tldr":"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (so...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1421","id":"main.1421","presentation_id":"38938902"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1428.png","content":{"abstract":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","authors":["Charles Welch","Rada Mihalcea","Jonathan K. Kummerfeld"],"demo_url":"","keywords":["nlp applications","language model","language research","byte-pair encoding"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.696","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1130","main.1631","main.1351","main.2078","main.2777"],"title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation","tldr":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1428","id":"main.1428","presentation_id":"38938903"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.143.png","content":{"abstract":"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.","authors":["Alessandro Raganato","Tommaso Pasini","Jose Camacho-Collados","Mohammad Taher Pilehvar"],"demo_url":"","keywords":["disambiguation task","binary problem","evaluation scenarios","zero-shot transfer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.584","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1379","main.623","main.2630","main.1061","main.2251"],"title":"XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization","tldr":"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNe...","track":"Semantics: Lexical Semantics"},"forum":"main.143","id":"main.143","presentation_id":"38938656"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1432.png","content":{"abstract":"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT\u201914 En\u2192De, WMT\u201916 Ro\u2192En and IWSLT\u201916 De\u2192En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT\u201914 En\u2192De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).","authors":["Jason Lee","Raphael Shu","Kyunghyun Cho"],"demo_url":"","keywords":["non-autoregressive translation","translation","machine translation","inference procedure"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.73","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.453","main.2430","main.2661","main.3348","main.891"],"title":"Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation","tldr":"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train ...","track":"Machine Translation and Multilinguality"},"forum":"main.1432","id":"main.1432","presentation_id":"38938904"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1445.png","content":{"abstract":"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers' generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.","authors":["Zirui Wang","Zachary C. Lipton","Yulia Tsvetkov"],"demo_url":"","keywords":["multilingual models","meta-learning algorithm","multilingual representations","negative interference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.359","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3116","main.852","main.522","main.2630","main.3688"],"title":"On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment","tldr":"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that...","track":"Machine Translation and Multilinguality"},"forum":"main.1445","id":"main.1445","presentation_id":"38938905"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1446.png","content":{"abstract":"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing  sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.","authors":["Chunyuan Li","Xiang Gao","Yuan Li","Baolin Peng","Xiujun Li","Yizhe Zhang","Jianfeng Gao"],"demo_url":"","keywords":["language tasks","guided generation","low-resource tasks","variational autoencoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.378","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.1892","main.1130","main.2491","main.3483"],"title":"Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space","tldr":"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (O...","track":"NLP Applications"},"forum":"main.1446","id":"main.1446","presentation_id":"38938906"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1455.png","content":{"abstract":"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle  tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.  Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.","authors":["Yonatan Bisk","Ari Holtzman","Jesse Thomason","Jacob Andreas","Yoshua Bengio","Joyce Chai","Mirella Lapata","Angeliki Lazaridou","Jonathan May","Aleksandr Nisnevich","Nicolas Pinto","Joseph Turian"],"demo_url":"","keywords":["language research","tasks","linguistic communication","natural processing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.703","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2638","main.3115","main.2363","main.1130","main.1613"],"title":"Experience Grounds Language","tldr":"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle  tasks after b...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1455","id":"main.1455","presentation_id":"38938907"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1456.png","content":{"abstract":"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We release these dictionaries to the research community. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60%. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.","authors":["Naoki Otani","Satoru Ozaki","Xingyuan Zhao","Yucen Li","Micaelah St Johns","Lori Levin"],"demo_url":"","keywords":["word-translation task","mwe translation","single-word translations","cross-lingual algorithms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.360","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1503","main.3046","main.1935","main.1061","main.852"],"title":"Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings","tldr":"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separa...","track":"Machine Translation and Multilinguality"},"forum":"main.1456","id":"main.1456","presentation_id":"38938908"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1458.png","content":{"abstract":"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.","authors":["Nathan Ng","Kyunghyun Cho","Marzyeh Ghassemi"],"demo_url":"","keywords":["data augmentation","ood generalization","robustness benchmarks","ssmba"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.97","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2959","main.745","main.1046","main.3434","main.426"],"title":"SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness","tldr":"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to ...","track":"Machine Learning for NLP"},"forum":"main.1458","id":"main.1458","presentation_id":"38938909"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1460.png","content":{"abstract":"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner's perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.","authors":["Tara Safavi","Danai Koutra","Edgar Meij"],"demo_url":"","keywords":["human-ai collaboration","calibrated predictions","knowledge task","knowledge models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.667","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1493","main.2406","main.1508","main.684"],"title":"Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction","tldr":"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output conf...","track":"Information Retrieval and Text Mining"},"forum":"main.1460","id":"main.1460","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1465.png","content":{"abstract":"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs.  Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.","authors":["Woojeong Jin","Meng Qu","Xisen Jin","Xiang Ren"],"demo_url":"","keywords":["knowledge reasoning","natural processing","predicting interactions","link prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.541","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3084","main.3617","main.2972","main.237","main.1116"],"title":"Recurrent Event Network: Autoregressive Structure Inferenceover Temporal Knowledge Graphs","tldr":"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps a...","track":"NLP Applications"},"forum":"main.1465","id":"main.1465","presentation_id":"38938911"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1466.png","content":{"abstract":"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to guide a sophisticated walk-based reinforcement learning (RL) model. An alternate approach is to use traditional symbolic methods (e.g., rule induction), which achieve good performance but can be hard to generalize due to the limitation of symbolic representation. In this paper, we propose RuleGuider, which leverages high-quality rules generated by symbolic-based methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets shows that RuleGuider clearly improves the performance of walk-based models without losing interpretability.","authors":["Deren Lei","Gangrong Jiang","Xiaotao Gu","Kexuan Sun","Yuning Mao","Xiang Ren"],"demo_url":"","keywords":["knowledge reasoning","walk-based models","kg","walk-based model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.688","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.684","main.1648","main.607","main.574","main.787"],"title":"Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning","tldr":"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to...","track":"Information Extraction"},"forum":"main.1466","id":"main.1466","presentation_id":"38938912"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.148.png","content":{"abstract":"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results. Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.","authors":["Ben Athiwaratkun","Cicero Nogueira dos Santos","Jason Krone","Bing Xiang"],"demo_url":"","keywords":["joint labeling","sentence-level classification","sequence tasks","few-shot learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.27","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2635","main.1720","main.74","main.3216","main.2790"],"title":"Augmented Natural Language for Generative Sequence Labeling","tldr":"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, ou...","track":"Language Generation"},"forum":"main.148","id":"main.148","presentation_id":"38938657"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1482.png","content":{"abstract":"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework\\footnote{\\url{https://github.com/wenhuchen/KGPT}}.","authors":["Wenhu Chen","Yu Su","Xifeng Yan","William Yang Wang"],"demo_url":"","keywords":["data-to-text generation","data-to-text tasks","fully-supervised setting","pre-training learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.697","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3184","main.1631","main.74","main.714","main.1647"],"title":"KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation","tldr":"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, whic...","track":"Language Generation"},"forum":"main.1482","id":"main.1482","presentation_id":"38938913"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1484.png","content":{"abstract":"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards -- in their current form -- can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model's utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","authors":["Kawin Ethayarajh","Dan Jurafsky"],"demo_url":"","keywords":["nlp","performance-based evaluation","leaderboard paradigm","microeconomic theory"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.393","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2238","main.527","main.2215","main.2221","main.1219"],"title":"Utility is in the Eye of the User: A Critique of NLP Leaderboards","tldr":"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expens...","track":"NLP Applications"},"forum":"main.1484","id":"main.1484","presentation_id":"38938914"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1485.png","content":{"abstract":"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese\u2192English, Turkish\u2192English, and  English\u2192German directions. Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.","authors":["Yimeng Wu","Peyman Passban","Mehdi Rezagholizadeh","Qun Liu"],"demo_url":"","keywords":["knowledge distillation","neural models","knowledge kd","kd"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.74","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.130","main.618","main.3394","main.1130","main.1986"],"title":"Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers","tldr":"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is ...","track":"Machine Translation and Multilinguality"},"forum":"main.1485","id":"main.1485","presentation_id":"38938915"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1488.png","content":{"abstract":"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.","authors":["Haopeng Zhang","Jiawei Zhang"],"demo_url":"","keywords":["text classification","natural processing","text task","graph techniques"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.668","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.151","main.782","main.2764","TACL.2121","main.3437"],"title":"Text Graph Transformer for Document Classification","tldr":"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus...","track":"Information Retrieval and Text Mining"},"forum":"main.1488","id":"main.1488","presentation_id":"38938916"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1490.png","content":{"abstract":"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.","authors":["Florian Mai","Nikolaos Pappas","Ivan Montero","Noah A. Smith","James Henderson"],"demo_url":"","keywords":["conditional tasks","style transfer","style tasks","text autoencoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.491","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2430","main.1446","main.3483","main.2635","main.910"],"title":"Plug and Play Autoencoders for Conditional Text Generation","tldr":"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedd...","track":"Machine Learning for NLP"},"forum":"main.1490","id":"main.1490","presentation_id":"38938917"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1492.png","content":{"abstract":"Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese.  The model includes known control variables together with word surprisal and word entropy. We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors. We also found sentence length to be a significant predictor, which has been related to sentence complexity. We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language. We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.","authors":["Jes\u00fas Calvillo","Le Fang","Jeremy Cole","David Reitter"],"demo_url":"","keywords":["code-switching","inhibition language","computational model","surprisal"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.330","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7B","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3181","main.246","main.1613","main.767","main.3115"],"title":"Surprisal Predicts Code-Switching in Chinese-English Bilingual Text","tldr":"Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text w...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.1492","id":"main.1492","presentation_id":"38938918"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1493.png","content":{"abstract":"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.","authors":["Tara Safavi","Danai Koutra"],"demo_url":"","keywords":["baseline prediction","triple classification","link benchmark","codex"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.669","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1706","demo.58","main.666","main.1787","demo.119"],"title":"CoDEx: A Comprehensive Knowledge Graph Completion Benchmark","tldr":"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge...","track":"Information Retrieval and Text Mining"},"forum":"main.1493","id":"main.1493","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1494.png","content":{"abstract":"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.","authors":["Steven Cao","Nikita Kitaev","Dan Klein"],"demo_url":"","keywords":["unsupervised parsing","constituency test","grammaticality decisions","unsupervised parser"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.389","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.2684","main.3181","TACL.2141","main.2064"],"title":"Unsupervised Parsing via Constituency Tests","tldr":"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the re...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1494","id":"main.1494","presentation_id":"38938920"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1495.png","content":{"abstract":"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at {https://github.com/sunlab-osu/MISP}.","authors":["Ziyu Yao","Yiqi Tang","Wen-tau Yih","Huan Sun","Yu Su"],"demo_url":"","keywords":["bootstrapping","fine-tuning parsers","theoretical analysis","text-to-sql problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.559","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.850","TACL.2411","main.1107","main.3327","main.471"],"title":"An Imitation Game for Learning Semantic Parsers from User Interaction","tldr":"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop metho...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1495","id":"main.1495","presentation_id":"38938921"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1498.png","content":{"abstract":"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.","authors":["Alexander Terenin","M\u00e5ns Magnusson","Leif Jonsson"],"demo_url":"","keywords":["data-parallel training","computation","probabilistic models","parallel systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.234","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2792","main.30","main.3348","main.2865"],"title":"Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models","tldr":"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hiera...","track":"Machine Learning for NLP"},"forum":"main.1498","id":"main.1498","presentation_id":"38938922"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1503.png","content":{"abstract":"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token's context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.","authors":["Masaaki Nagata","Katsuki Chousa","Masaaki Nishino"],"demo_url":"","keywords":["cross-language prediction","word problem","squad task","alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.41","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1061","main.639","main.2641","main.1694","main.143"],"title":"A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT","tldr":"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. ...","track":"Machine Translation and Multilinguality"},"forum":"main.1503","id":"main.1503","presentation_id":"38938923"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1504.png","content":{"abstract":"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.","authors":["Xiaomian Kang","Yang Zhao","Jiajun Zhang","Chengqing Zong"],"demo_url":"","keywords":["document-level translation","translations","document-level model","selection module"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.175","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1572","main.3227","main.246","main.1770","main.471"],"title":"Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning","tldr":"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of con...","track":"Machine Translation and Multilinguality"},"forum":"main.1504","id":"main.1504","presentation_id":"38938924"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1508.png","content":{"abstract":"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node's $k$-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.","authors":["Kian Ahrabian","Aarash Feizi","Yasmin Salehi","William L. Hamilton","Avishek Joey Bose"],"demo_url":"","keywords":["inferring patterns","learning representations","contrastive estimation","contrastive approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.492","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1399","main.2873","main.3434","main.426","main.1460"],"title":"Structure Aware Negative Sampling in Knowledge Graphs","tldr":"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches i...","track":"Machine Learning for NLP"},"forum":"main.1508","id":"main.1508","presentation_id":"38938925"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.151.png","content":{"abstract":"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.","authors":["Kaize Ding","Jianling Wang","Jundong Li","Dingcheng Li","Huan Liu"],"demo_url":"","keywords":["text classification","natural processing","text learning","text task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.399","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1488","main.782","main.2367","main.2995","main.1669"],"title":"Be More with Less: Hypergraph Attention Networks for Inductive Text Classification","tldr":"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on t...","track":"Machine Learning for NLP"},"forum":"main.151","id":"main.151","presentation_id":"38938658"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1518.png","content":{"abstract":"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy","authors":["Shubham Toshniwal","Sam Wiseman","Allyson Ettinger","Karen Livescu","Kevin Gimpel"],"demo_url":"","keywords":["long resolution","incremental resolution","memory-augmented network","memory strategy"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.685","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.883","main.3647","main.315","main.1621","main.2661"],"title":"Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks","tldr":"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical ...","track":"Discourse and Pragmatics"},"forum":"main.1518","id":"main.1518","presentation_id":"38938926"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1522.png","content":{"abstract":"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances\u2019 logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.","authors":["Changzhen Ji","Xin Zhou","Yating Zhang","Xiaozhong Liu","Changlong Sun","Conghui Zhu","Tiejun Zhao"],"demo_url":"","keywords":["dialogue generation","model training","utterance generation","court debate"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.149","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2209","main.215","main.128","main.645","main.699"],"title":"Cross Copy Network for Dialogue Generation","tldr":"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accura...","track":"Dialog and Interactive Systems"},"forum":"main.1522","id":"main.1522","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1528.png","content":{"abstract":"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model---Entities as Experts (EaE)---that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EaE's entity representations are learned directly from text. We show that EaE's learned representations capture sufficient knowledge to answer TriviaQA questions such as \"Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?'', outperforming an encoder-generator Transformer model with 10x the parameters on this task.  According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE's performance.","authors":["Thibault F\u00e9vry","Livio Baldini Soares","Nicholas FitzGerald","Eunsol Choi","Tom Kwiatkowski"],"demo_url":"","keywords":["capturing knowledge","eae","identification entities","language model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.400","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.911","main.1159","main.666","main.605","main.2630"],"title":"Entities as Experts: Sparse Memory Access with Entity Supervision","tldr":"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model---Entities as Experts (EaE)---that can access distinct memories of the entities mentioned in a piece of ...","track":"Machine Learning for NLP"},"forum":"main.1528","id":"main.1528","presentation_id":"38938928"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1540.png","content":{"abstract":"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in \u03b1NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.","authors":["Yixin Nie","Xiang Zhou","Mohit Bansal"],"demo_url":"","keywords":["nlp tasks","nlu evaluations","snli","majority label"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.734","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1923","main.1023","main.1159","main.3183","main.748"],"title":"What Can We Learn from Collective Human Opinions on Natural Language Inference Data?","tldr":"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1540","id":"main.1540","presentation_id":"38938929"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1547.png","content":{"abstract":"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.","authors":["Xuming Hu","Lijie Wen","Yusong Xu","Chenwei Zhang","Philip Yu"],"demo_url":"","keywords":["open extraction","extracting facts","adaptive clustering","relation classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.299","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3540","main.1159","main.2640","main.2793","demo.48"],"title":"SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction","tldr":"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or ...","track":"Information Extraction"},"forum":"main.1547","id":"main.1547","presentation_id":"38938930"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1550.png","content":{"abstract":"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.","authors":["Pei Ke","Haozhe Ji","Siyang Liu","Xiaoyan Zhu","Minlie Huang"],"demo_url":"","keywords":["language understanding","language tasks","nlp tasks","sentiment analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.567","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3375","main.1675","main.1766","main.1289","main.1952"],"title":"SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge","tldr":"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel la...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1550","id":"main.1550","presentation_id":"38938931"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1551.png","content":{"abstract":"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe's performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.","authors":["Tiago Pimentel","Naomi Saphra","Adina Williams","Ryan Cotterell"],"demo_url":"","keywords":["simplistic tasks","pos labeling","dependency labeling","full parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.254","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2122","main.498","main.3093","main.947","main.1970"],"title":"Pareto Probing: Trading Off Accuracy for Complexity","tldr":"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1551","id":"main.1551","presentation_id":"38938932"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1552.png","content":{"abstract":"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially\u2014for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage\u2019s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance","authors":["Liyuan Liu","Xiaodong Liu","Jianfeng Gao","Weizhu Chen","Jiawei Han"],"demo_url":"","keywords":["nlp tasks","training","transformer training","transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.463","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1485","demo.71","main.3543","main.2615","main.618"],"title":"Understanding the Difficulty of Training Transformers","tldr":"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectiv...","track":"Machine Translation and Multilinguality"},"forum":"main.1552","id":"main.1552","presentation_id":"38938933"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1561.png","content":{"abstract":"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to ~20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.","authors":["Jianpeng Cheng","Devang Agrawal","H\u00e9ctor Mart\u00ednez Alonso","Shruti Bhargava","Joris Driesen","Federico Flego","Dain Kaplan","Dimitri Kartsaklis","Lin Li","Dhivya Piraviperumal","Jason D. Williams","Hong Yu","Diarmuid \u00d3 S\u00e9aghdha","Anders Johannsen"],"demo_url":"","keywords":["dialog tracking","semantic task","dst","hierarchical representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.651","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1179","main.689","TACL.2143","main.2209","main.977"],"title":"Conversational Semantic Parsing for Dialog State Tracking","tldr":"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositio...","track":"Dialog and Interactive Systems"},"forum":"main.1561","id":"main.1561","presentation_id":"38938934"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1566.png","content":{"abstract":"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.","authors":["Chaojun Xiao","Yuan Yao","Ruobing Xie","Xu Han","Zhiyuan Liu","Maosong Sun","Fen Lin","Leyu Lin"],"demo_url":"","keywords":["sentence-level extraction","document-level extraction","docre","re"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.300","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2367","main.2078","main.2476","main.1339","main.983"],"title":"Denoising Relation Extraction from Document-level Distant Supervision","tldr":"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-lev...","track":"Information Extraction"},"forum":"main.1566","id":"main.1566","presentation_id":"38938935"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1569.png","content":{"abstract":"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.","authors":["Hayley Ross","Jonathon Cai","Bonan Min"],"demo_url":"","keywords":["extracting relations","constructing timelines","time-related answering","temporal parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.689","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1957","main.2972","main.315","main.1561"],"title":"Exploring Contextualized Neural Language Models for Temporal Dependency Parsing","tldr":"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentenc...","track":"Information Extraction"},"forum":"main.1569","id":"main.1569","presentation_id":"38938936"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1572.png","content":{"abstract":"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to~1.8 BLEU points over competitive baselines.","authors":["Zi-Yi Dou","Antonios Anastasopoulos","Graham Neubig"],"demo_url":"","keywords":["neural translation","neural nmt","nmt","domain adaptation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.475","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1100","main.3227","main.3688","main.1680","main.1504"],"title":"Dynamic Data Selection and Weighting for Iterative Back-Translation","tldr":"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-tra...","track":"Machine Translation and Multilinguality"},"forum":"main.1572","id":"main.1572","presentation_id":"38938937"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1574.png","content":{"abstract":"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.","authors":["Canwen Xu","Wangchunshu Zhou","Tao Ge","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["bert compression","model compression","model approach","progressive replacing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.633","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2783","main.956","main.3394","main.2779","main.1219"],"title":"BERT-of-Theseus: Compressing BERT by Progressive Module Replacing","tldr":"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly repla...","track":"Machine Learning for NLP"},"forum":"main.1574","id":"main.1574","presentation_id":"38938938"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1575.png","content":{"abstract":"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.","authors":["Yang Gao","Yi-Fan Li","Yu Lin","Charu Aggarwal","Latifur Khan"],"demo_url":"","keywords":["real-world problems","sentiment classification","ir","machine methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.98","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.493","main.3174","main.1180","main.2068","main.2958"],"title":"SetConv: A New Approach for Learning from Imbalanced Data","tldr":"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (Se...","track":"Machine Learning for NLP"},"forum":"main.1575","id":"main.1575","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1578.png","content":{"abstract":"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame.","authors":["Shunyu Yao","Rohan Rao","Matthew Hausknecht","Karthik Narasimhan"],"demo_url":"","keywords":["autonomous agents","contextual model","calm","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.704","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.763","main.2574","main.699","TACL.2143","main.2410"],"title":"Keep CALM and Explore: Language Models for Action Generation in Text-based Games","tldr":"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates a...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1578","id":"main.1578","presentation_id":"38938940"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.158.png","content":{"abstract":"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.","authors":["Shuang Zeng","Runxin Xu","Baobao Chang","Lei Li"],"demo_url":"","keywords":["document-level extraction","sentence-level extraction","graph network","graph"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.127","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.327","main.574","main.2761","main.1706","main.1648"],"title":"Double Graph Based Reasoning for Document-level Relation Extraction","tldr":"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggrega...","track":"Information Extraction"},"forum":"main.158","id":"main.158","presentation_id":"38938659"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1580.png","content":{"abstract":"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.","authors":["Sewon Min","Julian Michael","Hannaneh Hajishirzi","Luke Zettlemoyer"],"demo_url":"","keywords":["open-domain answering","open-domain task","ambigqa","ambignq"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.466","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.319","main.2586","main.3186","main.2587","TACL.2049"],"title":"AmbigQA: Answering Ambiguous Open-domain Questions","tldr":"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task...","track":"Question Answering"},"forum":"main.1580","id":"main.1580","presentation_id":"38938941"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1581.png","content":{"abstract":"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the  input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.","authors":["Kalpesh Krishna","John Wieting","Mohit Iyyer"],"demo_url":"","keywords":["style transfer","attribute transfer","unsupervised transfer","paraphrase problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.55","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.888","main.2382","main.1898","main.3227","main.2349"],"title":"Reformulating Unsupervised Style Transfer as Paraphrase Generation","tldr":"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existin...","track":"Language Generation"},"forum":"main.1581","id":"main.1581","presentation_id":"38938942"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1594.png","content":{"abstract":"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing.  We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].","authors":["Hoo-Chang Shin","Yang Zhang","Evelina Bakhturina","Raul Puri","Mostofa Patwary","Mohammad Shoeybi","Raghav Mani"],"demo_url":"","keywords":["domain applications","domain transfer","question answering","named recognition"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.379","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1631","main.1428","main.748","main.16","main.110"],"title":"BioMegatron: Larger Biomedical Domain Language Model","tldr":"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Book...","track":"NLP Applications"},"forum":"main.1594","id":"main.1594","presentation_id":"38938943"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.16.png","content":{"abstract":"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT,  the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.","authors":["Chengyu Wang","Minghui Qiu","Jun Huang","Xiaofeng He"],"demo_url":"","keywords":["nlp tasks","fine-tuning","learning process","multi-domain tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.250","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2793","main.1631","main.3023","main.2491","main.400"],"title":"Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining","tldr":"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how t...","track":"Information Retrieval and Text Mining"},"forum":"main.16","id":"main.16","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1603.png","content":{"abstract":"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion  detection, as it can provide medical staff  or caregivers with essential information about patients. However, progress on this task has been hampered  by  the  absence of large labeled datasets. To this end, we introduce CancerEmo, an emotion dataset created from an online  health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.","authors":["Tiberiu Sosea","Cornelia Caragea"],"demo_url":"","keywords":["emotion","emotion detection","detection","canceremo"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.715","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2261","main.392","main.376","main.3532","main.1322"],"title":"CancerEmo: A Dataset for Fine-Grained Emotion Detection","tldr":"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion  detection, as it can provide medical staff  or careg...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1603","id":"main.1603","presentation_id":"38938944"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1606.png","content":{"abstract":"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.","authors":["Lishan Huang","Zheng Ye","Jinghui Qin","Liang Lin","Xiaodan Liang"],"demo_url":"","keywords":["automatically coherence","open-domain systems","automatic evaluation","grade"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.742","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1702","TACL.2143","main.128","main.1140","main.2141"],"title":"GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems","tldr":"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without ex...","track":"Dialog and Interactive Systems"},"forum":"main.1606","id":"main.1606","presentation_id":"38938945"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1611.png","content":{"abstract":"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.","authors":["Yu Meng","Yunyi Zhang","Jiaxin Huang","Chenyan Xiong","Heng Ji","Chao Zhang","Jiawei Han"],"demo_url":"","keywords":["classification","category understanding","document classification","topic classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.724","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2167","main.76","main.1159","main.989","main.2893"],"title":"Text Classification Using Label Names Only: A Language Model Self-Training Approach","tldr":"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples b...","track":"Information Retrieval and Text Mining"},"forum":"main.1611","id":"main.1611","presentation_id":"38938946"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1612.png","content":{"abstract":"To demystify the ``black box\" property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.","authors":["Siwon Kim","Jihun Yi","Eunji Kim","Sungroh Yoon"],"demo_url":"","keywords":["nlp","out-of-distribution problem","sentiment analysis","natural inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.255","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1960","main.2585","main.2650","main.2389","main.3348"],"title":"Interpretation of NLP models through input marginalization","tldr":"To demystify the ``black box\" property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an i...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1612","id":"main.1612","presentation_id":"38938947"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1613.png","content":{"abstract":"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language.  We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.","authors":["Isabel Papadimitriou","Dan Jurafsky"],"demo_url":"","keywords":["analyzing structure","encoding structure","natural acquisition","transfer learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.554","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2851","main.3181","main.1892","main.3115","main.852"],"title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models","tldr":"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce gene...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1613","id":"main.1613","presentation_id":"38938948"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1614.png","content":{"abstract":"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.","authors":["Pepa Atanasova","Dustin Wright","Isabelle Augenstein"],"demo_url":"","keywords":["inference tasks","fact checking","universal generation","adversarial attacks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.256","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2895","main.60","main.47","main.2313"],"title":"Generating Label Cohesive and Well-Formed Adversarial Claims","tldr":"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model i...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1614","id":"main.1614","presentation_id":"38938949"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1615.png","content":{"abstract":"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.","authors":["Xinyu Wang","Yong Jiang","Nguyen Bach","Tao Wang","Zhongqiang Huang","Fei Huang","Kewei Tu"],"demo_url":"","keywords":["parallelization","faster prediction","linear-chain model","neural approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.485","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1225","main.3348","main.2636","main.1432","main.2198"],"title":"AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network","tldr":"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training a...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1615","id":"main.1615","presentation_id":"38938950"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1618.png","content":{"abstract":"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, \"Extended Transformer Construction\" (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a \"Contrastive Predictive Coding\" (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.","authors":["Joshua Ainslie","Santiago Ontanon","Chris Alberti","Vaclav Cvicek","Zachary Fisher","Philip Pham","Anirudh Ravula","Sumit Sanghai","Qifan Wang","Li Yang"],"demo_url":"","keywords":["natural tasks","encoding inputs","transformer models","transformer architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.19","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.60","main.3337","main.2635","main.3278","main.3398"],"title":"ETC: Encoding Long and Structured Inputs in Transformers","tldr":"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, \"Extended Transformer Construction\" (ETC), that addresses two key challenges of standard ...","track":"Machine Learning for NLP"},"forum":"main.1618","id":"main.1618","presentation_id":"38938951"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1621.png","content":{"abstract":"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.","authors":["Liyan Xu","Jinho D. Choi"],"demo_url":"","keywords":["coreference resolution","entity equalization","span clustering","higher-order inference"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.686","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3647","main.883","main.3593","main.1518","main.2271"],"title":"Revealing the Myth of Higher-Order Inference in Coreference Resolution","tldr":"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representat...","track":"Discourse and Pragmatics"},"forum":"main.1621","id":"main.1621","presentation_id":"38938952"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1622.png","content":{"abstract":"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \"Discern\", a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision \"yes/no/irrelevant\" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern.","authors":["Yifan Gao","Chien-Sheng Wu","Jingjing Li","Shafiq Joty","Steven C.H. Hoi","Caiming Xiong","Irwin King","Michael Lyu"],"demo_url":"","keywords":["document interpretation","dialog understanding","conversational reading","discern"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.191","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.485","main.2444","main.41","main.527","main.3010"],"title":"Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading","tldr":"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \"Discern\", a discourse-aware entailment reasoning network to strengthen the connection and enhance the understa...","track":"Question Answering"},"forum":"main.1622","id":"main.1622","presentation_id":"38938953"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1625.png","content":{"abstract":"A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.","authors":["Yoshihide Kato","Shigeki Matsubara"],"demo_url":"","keywords":["gapping construction","coordinated structure","redundant elements","elided elements"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.218","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2419","demo.89","main.1957","main.486","main.447"],"title":"Parsing Gapping Constructions Based on Grammatical and Semantic Roles","tldr":"A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on cons...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1625","id":"main.1625","presentation_id":"38938954"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1626.png","content":{"abstract":"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.","authors":["Hammad Rizwan","Muhammad Haroon Shakeel","Asim Karim"],"demo_url":"","keywords":["automatic detection","hate-speech detection","language models","transfer learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.197","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1997","main.2396","main.3566","main.106","main.3101"],"title":"Hate-Speech and Offensive Language Detection in Roman Urdu","tldr":"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainl...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1626","id":"main.1626","presentation_id":"38938955"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1631.png","content":{"abstract":"Pre-training large language models has become a standard in the natural language processing community.  Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain.  However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required.  In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE.  In this paper we conduct an empirical investigation into known methods to mitigate CF.  We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks.  Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.","authors":["Kristjan Arumae","Qing Sun","Parminder Bhatia"],"demo_url":"","keywords":["pre-training models","natural community","out tasks","clinical recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.394","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.16","main.2491","main.1482","main.1351"],"title":"An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training","tldr":"Pre-training large language models has become a standard in the natural language processing community.  Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain.  However, in...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1631","id":"main.1631","presentation_id":"38938956"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1634.png","content":{"abstract":"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies. In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments. We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection. Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.","authors":["Jingfeng Yang","Diyi Yang","Zhaoran Ma"],"demo_url":"","keywords":["disfluency detection","pretraining","data methods","augmentation approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.113","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3353","main.2590","main.2511","main.648","main.1049"],"title":"Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection","tldr":"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion...","track":"NLP Applications"},"forum":"main.1634","id":"main.1634","presentation_id":"38938957"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1647.png","content":{"abstract":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","authors":["Peng Xu","Mostofa Patwary","Mohammad Shoeybi","Raul Puri","Pascale Fung","Anima Anandkumar","Bryan Catanzaro"],"demo_url":"","keywords":["text generation","pre-trained models","megatron-cntrl","large-scale models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.226","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1928","main.1130","main.1482","main.2650","main.3054"],"title":"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models","tldr":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text...","track":"Language Generation"},"forum":"main.1647","id":"main.1647","presentation_id":"38938958"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1648.png","content":{"abstract":"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.","authors":["Yanlin Feng","Xinyue Chen","Bill Yuchen Lin","Peifeng Wang","Jun Yan","Xiang Ren"],"demo_url":"","keywords":["multi-hop reasoning","question models","knowledge-aware approach","multi-hop module"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.99","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.923","main.574","main.2761","main.531","main.1123"],"title":"Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering","tldr":"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propos...","track":"Machine Learning for NLP"},"forum":"main.1648","id":"main.1648","presentation_id":"38938959"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1649.png","content":{"abstract":"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.","authors":["Rexhina Blloshmi","Rocco Tripodi","Roberto Navigli"],"demo_url":"","keywords":["encoding semantics","cross-lingual parsing","english parsing","amr"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.195","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.870","main.2098","main.1061","main.357","main.852"],"title":"XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques","tldr":"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the e...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1649","id":"main.1649","presentation_id":"38938960"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.165.png","content":{"abstract":"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.","authors":["Peixiang Zhong","Chen Zhang","Hao Wang","Yong Liu","Chunyan Miao"],"demo_url":"","keywords":["persona-based conversations","empathetic responding","empathetic models","cobert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.531","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.476","main.2707","main.3072","main.2561","main.3352"],"title":"Towards Persona-Based Empathetic Conversational Models","tldr":"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empi...","track":"Dialog and Interactive Systems"},"forum":"main.165","id":"main.165","presentation_id":"38938660"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1654.png","content":{"abstract":"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.","authors":["Chien-Sheng Wu","Caiming Xiong"],"demo_url":"","keywords":["task-oriented tasks","supervised probe","unsupervised probe","unsupervised aspect"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.409","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1201","main.128","main.215","main.2141","main.1006"],"title":"Probing Task-Oriented Dialogue Representation from Language Models","tldr":"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsuperv...","track":"Dialog and Interactive Systems"},"forum":"main.1654","id":"main.1654","presentation_id":"38938961"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1658.png","content":{"abstract":"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88% novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.","authors":["Tuhin Chakrabarty","Smaranda Muresan","Nanyun Peng"],"demo_url":"","keywords":["human imagination","simile generation","mapping properties","sequence model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.524","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.920","main.2758","main.1581","main.2349","main.701"],"title":"Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation","tldr":"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of s...","track":"Language Generation"},"forum":"main.1658","id":"main.1658","presentation_id":"38938962"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1669.png","content":{"abstract":"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence  \"Beethoven composed the Ode to Joy.\", we are expected to extract the triple <Beethoven, composed, Ode to Joy>. In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e.,  by more than 200% in both cases). Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.","authors":["Patrick Hohenecker","Frank Mtumbuka","Vid Kocijan","Thomas Lukasiewicz"],"demo_url":"","keywords":["open extraction","oie","neural architectures","training approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.690","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1159","demo.48","main.471","main.2476","main.2739"],"title":"Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction","tldr":"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence  \"Beethoven composed the Ode to Joy.\", ...","track":"Information Extraction"},"forum":"main.1669","id":"main.1669","presentation_id":"38938963"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1670.png","content":{"abstract":"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.","authors":["Jack Hessel","Lillian Lee"],"demo_url":"","keywords":["modeling interactions","multimodal tasks","visual answering","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.62","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3183","main.284","main.376","TACL.2041","main.317"],"title":"Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!","tldr":"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.1670","id":"main.1670","presentation_id":"38938964"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1675.png","content":{"abstract":"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.","authors":["Jiaxin Huang","Yu Meng","Fang Guo","Heng Ji","Jiawei Han"],"demo_url":"","keywords":["extracting aspects","classifying reviews","aspect-based analysis","aspect classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.568","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3286","main.3185","main.3064","main.1289","main.3375"],"title":"Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding","tldr":"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentime...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1675","id":"main.1675","presentation_id":"38938965"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.168.png","content":{"abstract":"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.  This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances.  In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance.  As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance.  We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker's intentions and the listener's perceptions in both cases.","authors":["Liye Fu","Susan Fussell","Cristian Danescu-Niculescu-Mizil"],"demo_url":"","keywords":["communication approaches","paraphrases","speaker intentions","technology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.416","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3072","main.2766","main.2561","main.3352","main.2058"],"title":"Facilitating the Communication of Politeness through Fine-Grained Paraphrasing","tldr":"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.  This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse...","track":"Language Generation"},"forum":"main.168","id":"main.168","presentation_id":"38938661"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1680.png","content":{"abstract":"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.","authors":["Yiren Wang","ChengXiang Zhai","Hany Hassan"],"demo_url":"","keywords":["bilingual nmt","bilingual","multilingual systems","translation task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.75","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3688","TACL.2107","main.522","main.852","main.1986"],"title":"Multi-task Learning for Multilingual Neural Machine Translation","tldr":"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose ...","track":"Machine Translation and Multilinguality"},"forum":"main.1680","id":"main.1680","presentation_id":"38938966"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1682.png","content":{"abstract":"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations.  Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.","authors":["Jueqing Lu","Lan Du","Ming Liu","Joanna Dipnall"],"demo_url":"","keywords":["fewzero-shot learning","classifications tasks","multi-label classification","classifier"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.235","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2167","main.2793","main.1032","main.148","main.1611"],"title":"Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs","tldr":"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where e...","track":"Machine Learning for NLP"},"forum":"main.1682","id":"main.1682","presentation_id":"38938967"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1687.png","content":{"abstract":"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.","authors":["Hannah Rashkin","Asli Celikyilmaz","Yejin Choi","Jianfeng Gao"],"demo_url":"","keywords":["outline-conditioned generation","plotmachines","neural model","large-scale models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.349","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1928","main.2758","main.2982","main.605","main.2382"],"title":"PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking","tldr":"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline....","track":"Language Generation"},"forum":"main.1687","id":"main.1687","presentation_id":"38938968"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1694.png","content":{"abstract":"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","authors":["Yun Chen","Yang Liu","Guanhua Chen","Xin Jiang","Qun Liu"],"demo_url":"","keywords":["transformer","attention mechanism","word methods","shift-att"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.42","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3688","main.1503","main.1061","main.522","main.3227"],"title":"Accurate Word Alignment Induction from Neural Machine Translation","tldr":"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignme...","track":"Machine Translation and Multilinguality"},"forum":"main.1694","id":"main.1694","presentation_id":"38938969"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1700.png","content":{"abstract":"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.","authors":["Xiang Gao","Yizhe Zhang","Michel Galley","Chris Brockett","Bill Dolan"],"demo_url":"","keywords":["feedback prediction","ranking problem","predicting feedback","open-domain models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.28","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3179","main.317","main.478","main.2410","main.2164"],"title":"Dialogue Response Ranking Training with Large-Scale Human Feedback Data","tldr":"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasin...","track":"Dialog and Interactive Systems"},"forum":"main.1700","id":"main.1700","presentation_id":"38938970"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1702.png","content":{"abstract":"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.","authors":["Jun Quan","Shian Zhang","Qian Cao","Zizhong Li","Deyi Xiong"],"demo_url":"","keywords":["task-oriented modeling","dialogue tasks","natural understanding","intent detection"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.67","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1012","main.1201","TACL.2143","main.1606","main.1846"],"title":"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling","tldr":"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contai...","track":"Dialog and Interactive Systems"},"forum":"main.1702","id":"main.1702","presentation_id":"38938971"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1706.png","content":{"abstract":"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.","authors":["Liying Cheng","Dekun Wu","Lidong Bing","Yan Zhang","Zhanming Jie","Wei Lu","Luo Si"],"demo_url":"","keywords":["knowledge-to-text generation","information loss","kg","graph-to-sequence models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.90","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1231","main.158","main.666","main.531"],"title":"ENT-DESC: Entity Description Generation by Exploring Knowledge Graph","tldr":"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have...","track":"Language Generation"},"forum":"main.1706","id":"main.1706","presentation_id":"38938972"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1707.png","content":{"abstract":"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research.","authors":["Yizhe Zhang","Guoyin Wang","Chunyuan Li","Zhe Gan","Chris Brockett","Bill Dolan"],"demo_url":"","keywords":["language learning","free-form generation","hard-constrained generation","hard-constrained tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.698","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.3483","main.730","main.2382","main.2511"],"title":"POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training","tldr":"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified...","track":"Language Generation"},"forum":"main.1707","id":"main.1707","presentation_id":"38938973"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1720.png","content":{"abstract":"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by $2.27\\%$--$3.75\\%$ in terms of $F_1$ scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.","authors":["Rongzhi Zhang","Yue Yu","Chao Zhang"],"demo_url":"","keywords":["low-resource tasks","active labeling","mixup","sequence mixup"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.691","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.2790","main.2636","main.1356","main.1225"],"title":"SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup","tldr":"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We pro...","track":"Information Extraction"},"forum":"main.1720","id":"main.1720","presentation_id":"38938974"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1733.png","content":{"abstract":"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as \"black boxes''. We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.","authors":["Peerat Limkonchotiwat","Wannaphong Phatthiyaphaibun","Raheem Sarwar","Ekapol Chuangsuwanich","Sarana Nutanong"],"demo_url":"","keywords":["natural tasks","thai segmentation","transfer learning","filter-and-refine solution"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.315","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","TACL.2255","main.2389","main.2078","main.930"],"title":"Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble","tldr":"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can inter...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.1733","id":"main.1733","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1734.png","content":{"abstract":"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.","authors":["Sanyuan Chen","Yutai Hou","Yiming Cui","Wanxiang Che","Ting Liu","Xiangzhan Yu"],"demo_url":"","keywords":["pretraining","pretraining tasks","learning tasks","fine-tuning bert-large"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.634","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2838","main.1299","TACL.2041","main.2491","main.74"],"title":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting","tldr":"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performanc...","track":"Machine Learning for NLP"},"forum":"main.1734","id":"main.1734","presentation_id":"38938976"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1738.png","content":{"abstract":"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches  such  as  BERT.  However,  currentpre-training techniques focus on building lan-guage  modeling  objectives  to  learn  a  gen-eral representation, ignoring the named entity-related  knowledge.   To  this  end,  we  proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge  into  pre-trained  models.   Specifi-cally,  we  first  warm-up  the  model  via  an  en-tity span identification task by training it withWikipedia  anchors,  which  can  be  deemed  asgeneral-typed entities.   Then we leverage thegazetteer-based distant supervision strategy totrain  the  model  extract  coarse-grained  typedentities.   Finally,  we devise a self-supervisedauxiliary task to mine the fine-grained namedentity  knowledge  via  clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks.   Besides,  weshow that our framework gains promising re-sults  without  using  human-labeled  trainingdata,  demonstrating its effectiveness in label-few and low-resource scenarios.","authors":["Xue Mengge","Bowen Yu","Zhenyu Zhang","Tingwen Liu","Yue Zhang","Bin Wang"],"demo_url":"","keywords":["named recognition","bert","en-tity task","pre-trainingapproaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.514","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2947","main.3216","main.1482","main.2799","main.989"],"title":"Coarse-to-Fine Pre-training for Named Entity Recognition","tldr":"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches  such  as  BERT.  However,  currentpre-training techniques focus on building lan-guage  modeling  objectives  to  learn  a  gen-eral representation, ig...","track":"Information Extraction"},"forum":"main.1738","id":"main.1738","presentation_id":"38938977"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1739.png","content":{"abstract":"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) 'Expression' token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.","authors":["Bugeun Kim","Kyung Seo Ki","Donggeon Lee","Gahgene Gweon"],"demo_url":"","keywords":["solving problems","natural task","algebraic problems","expression fragmentation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.308","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.179","TACL.2141","main.2448","demo.86","main.2990"],"title":"Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model","tldr":"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens a...","track":"NLP Applications"},"forum":"main.1739","id":"main.1739","presentation_id":"38938978"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1749.png","content":{"abstract":"Conventional approaches to event detection usually require  a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.","authors":["Pengfei Cao","Yubo Chen","Jun Zhao","Taifeng Wang"],"demo_url":"","keywords":["event detection","real-world applications","incremental detection","training problems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.52","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2048","main.3438","main.1116","main.2427","main.3617"],"title":"Incremental Event Detection via Knowledge Consolidation Networks","tldr":"Conventional approaches to event detection usually require  a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it...","track":"Information Extraction"},"forum":"main.1749","id":"main.1749","presentation_id":"38938979"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1750.png","content":{"abstract":"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the best predictive performance in assessing the persuasiveness of the arguments on online debates.","authors":["Jialu Li","Esin Durmus","Claire Cardie"],"demo_url":"","keywords":["natural processing","nlp","persuasion studies","online debates"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.716","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2996","main.916","main.440","main.955","main.2688"],"title":"Exploring the Role of Argument Structure in Online Debate Persuasion","tldr":"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extr...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1750","id":"main.1750","presentation_id":"38938980"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1754.png","content":{"abstract":"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by `composing' word representations of the first and last words in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are `decomposed' back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.","authors":["Diego Marcheggiani","Ivan Titov"],"demo_url":"","keywords":["semantic labeling","identifying predicates","labeling spans","syntax-aware srl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.322","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1957","main.1061","main.2179","main.1179","main.574"],"title":"Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling","tldr":"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as argumen...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1754","id":"main.1754","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1755.png","content":{"abstract":"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity. To address this issue, we present a novel nested NER model named HIT. Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary. Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary. Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.","authors":["Yu Wang","Yun Li","Hanghang Tong","Ziye Zhu"],"demo_url":"","keywords":["named recognition","ner","natural processing","sequence approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.486","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2103","main.2974","main.911","main.989","main.2799"],"title":"HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction","tldr":"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approa...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1755","id":"main.1755","presentation_id":"38938982"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1766.png","content":{"abstract":"The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like \u201cnothing special\u201d. Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation \u201cfood-was\u201d is treated equally as an adjectival complement relation \u201cwas-okay\u201d in \u201cfood was okay\u201d.  To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs. Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information. Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs. Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs. Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.","authors":["Mi Zhang","Tieyun Qian"],"demo_url":"","keywords":["aspect-level classification","graph models","bi-level network","syntactic structure"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.286","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1289","main.3375","main.1675","main.1550","main.2363"],"title":"Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis","tldr":"The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence informat...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.1766","id":"main.1766","presentation_id":"38938983"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1770.png","content":{"abstract":"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.","authors":["Shuhao Gu","Jinchao Zhang","Fandong Meng","Yang Feng","Wanying Xie","Jie Zhou","Dong Yu"],"demo_url":"","keywords":["nmt","vanilla model","golden distribution","token phenomenon"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.76","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.334","main.522","main.3688","main.1504","main.1694"],"title":"Token-level Adaptive Training for Neural Machine Translation","tldr":"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts t...","track":"Machine Translation and Multilinguality"},"forum":"main.1770","id":"main.1770","presentation_id":"38938984"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1782.png","content":{"abstract":"We propose EXAMS \u2013 a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.","authors":["Momchil Hardalov","Todor Mihaylov","Dimitrina Zlatkova","Yoan Dinkov","Ivan Koychev","Preslav Nakov"],"demo_url":"","keywords":["cross-lingual answering","school answering","exams","fine-grained framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.438","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.871","main.2630","main.2278","main.1803","main.1379"],"title":"EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering","tldr":"We propose EXAMS \u2013 a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 ...","track":"Question Answering"},"forum":"main.1782","id":"main.1782","presentation_id":"38938985"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1784.png","content":{"abstract":"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.","authors":["Yitao Cai","Xiaojun Wan"],"demo_url":"","keywords":["context-dependent task","context-dependent drawn","decoding phase","encoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.560","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.1706","TACL.2121","main.3682","main.1488"],"title":"IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation","tldr":"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic inf...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1784","id":"main.1784","presentation_id":"38938986"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1787.png","content":{"abstract":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at \\url{https://github.com/thunlp/explore-and-evaluate}.","authors":["Zhiyuan Liu","Yixin Cao","Liangming Pan","Juanzi Li","Zhiyuan Liu","Tat-Seng Chua"],"demo_url":"","keywords":["entity alignment","ea","gnn-based methods","attributed encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.515","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1706","main.2877","main.3216","main.2406","main.2974"],"title":"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","tldr":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. Ho...","track":"Information Extraction"},"forum":"main.1787","id":"main.1787","presentation_id":"38938987"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1788.png","content":{"abstract":"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.","authors":["Miyoung Ko","Jinhyuk Lee","Hyunjae Kim","Gangwoo Kim","Jaewoo Kang"],"demo_url":"","keywords":["extractive models","qa models","bidaf","de-biasing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.84","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1022","main.1837","main.2586","main.959","main.3183"],"title":"Look at the First Sentence: Position Bias in Question Answering","tldr":"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribu...","track":"Question Answering"},"forum":"main.1788","id":"main.1788","presentation_id":"38938988"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.179.png","content":{"abstract":"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols\u2019 semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.","authors":["Jinghui Qin","Lihui Lin","Xiaodan Liang","Rumin Zhang","Liang Lin"],"demo_url":"","keywords":["one-unknown mwps","universal","uet","semantically-aligned solver"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.309","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1010","main.1061","main.2342","main.1739","main.1503"],"title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems","tldr":"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expressio...","track":"NLP Applications"},"forum":"main.179","id":"main.179","presentation_id":"38938662"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1797.png","content":{"abstract":"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.","authors":["Bodhisattwa Prasad Majumder","Harsh Jhamtani","Taylor Berg-Kirkpatrick","Julian McAuley"],"demo_url":"","keywords":["persona-consistent generation","persona-grounded models","dialog models","fine-grained personas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.739","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.419","main.2839","main.1972","main.787","main.128"],"title":"Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions","tldr":"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply l...","track":"Dialog and Interactive Systems"},"forum":"main.1797","id":"main.1797","presentation_id":"38938989"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1798.png","content":{"abstract":"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1\\%). In addition, our methods also surpass the Transformer-Big model, with only 54\\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.","authors":["Jianhao Yan","Fandong Meng","Jie Zhou"],"demo_url":"","keywords":["neural translation","transformer","machine tasks","inference process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.77","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.618","main.130","main.3337","main.701","main.1618"],"title":"Multi-Unit Transformers for Neural Machine Translation","tldr":"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investiga...","track":"Machine Translation and Multilinguality"},"forum":"main.1798","id":"main.1798","presentation_id":"38938990"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1803.png","content":{"abstract":"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.","authors":["Jonas Pfeiffer","Ivan Vuli\u0107","Iryna Gurevych","Sebastian Ruder"],"demo_url":"","keywords":["transfer","pre-training","cross transfer","named recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.617","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.74","main.1379","main.3688","main.871","main.2500"],"title":"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer","tldr":"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to l...","track":"Machine Translation and Multilinguality"},"forum":"main.1803","id":"main.1803","presentation_id":"38938991"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1817.png","content":{"abstract":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.","authors":["Marcin Kardas","Piotr Czapla","Pontus Stenetorp","Sebastian Ruder","Sebastian Riedel","Ross Taylor","Robert Stojnic"],"demo_url":"","keywords":["machine learning","table subtask","extraction","results extraction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.692","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3646","main.1669","main.1977","demo.72","main.3470"],"title":"AxCell: Automatic Extraction of Results from Machine Learning Papers","tldr":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses severa...","track":"Information Extraction"},"forum":"main.1817","id":"main.1817","presentation_id":"38938992"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1832.png","content":{"abstract":"Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the tree model. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.","authors":["Yugo Murawaki"],"demo_url":"","keywords":["analyzing dialects","tree model","statistical approaches","admixture analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.69","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2894","main.3551","main.2886","main.2535","main.2076"],"title":"Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact","tldr":"Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.1832","id":"main.1832","presentation_id":"38938993"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1834.png","content":{"abstract":"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference ({NLI}). We propose {G}en{NLI}, a generative classifier for {NLI} tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like {BERT}. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the ``infinilog loss''. Our experiments show that {GenNLI} outperforms both discriminative and pretrained baselines across several challenging {NLI} experimental settings, including small training sets, imbalanced label distributions, and label noise.","authors":["Xiaoan Ding","Tianyu Liu","Baobao Chang","Zhifang Sui","Kevin Gimpel"],"demo_url":"","keywords":["natural inference","nli tasks","discriminative fine-tuning","discriminative classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.657","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.3348","main.989","main.3023","main.3227"],"title":"Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference","tldr":"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference ({NLI}). We propose {...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1834","id":"main.1834","presentation_id":"38938994"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1835.png","content":{"abstract":"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work.  Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.","authors":["Jonathan Pilault","Raymond Li","Sandeep Subramanian","Chris Pal"],"demo_url":"","keywords":["neural summarization","summarization tasks","extractive step","transformer model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.748","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2650","main.2125","main.2506","main.471","main.714"],"title":"On Extractive and Abstractive Neural Document Summarization with Transformer Language Models","tldr":"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the trans...","track":"Summarization"},"forum":"main.1835","id":"main.1835","presentation_id":"38938995"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1837.png","content":{"abstract":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","authors":["Yuxiang Wu","Sebastian Riedel","Pasquale Minervini","Pontus Stenetorp"],"demo_url":"","keywords":["open-domain answering","adaptive computation","light-weight retriever","per-layer probability"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.244","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1022","main.3183","demo.54","main.3140","main.319"],"title":"Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering","tldr":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have show...","track":"Question Answering"},"forum":"main.1837","id":"main.1837","presentation_id":"38938996"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1846.png","content":{"abstract":"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to \"carryover'' the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\\% training data, and 3) Lev greatly improves the inference efficiency.","authors":["Zhaojiang Lin","Andrea Madotto","Genta Indra Winata","Pascale Fung"],"demo_url":"","keywords":["dialogue tracking","dialogue generation","end-to-end generation","minimalist learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.273","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2143","main.1201","main.478","main.1012","main.215"],"title":"MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems","tldr":"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, w...","track":"Dialog and Interactive Systems"},"forum":"main.1846","id":"main.1846","presentation_id":"38938997"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1857.png","content":{"abstract":"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains. Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem. In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings. We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.","authors":["Xin Wu","Yi Cai","Yang Kai","Tao Wang","Qing Li"],"demo_url":"","keywords":["natural tasks","downstream tasks","meta-embedding learning","meta-embedding methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.282","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1952","main.151","main.2078","TACL.2255","main.3298"],"title":"Task-oriented Domain-specific Meta-Embedding for Text Classification","tldr":"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-...","track":"Semantics: Lexical Semantics"},"forum":"main.1857","id":"main.1857","presentation_id":"38938998"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1862.png","content":{"abstract":"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation. We then propose a small empirical study to quantify the most common mistake's impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05. We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER. This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics. We finally call for unifying the evaluation setting in end-to-end RE.","authors":["Bruno Taill\u00e9","Vincent Guigue","Geoffrey Scoutheeten","Patrick Gallinari"],"demo_url":"","keywords":["end-to-end articles","language pretraining","meta-analysis","bert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.301","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3517","main.2268","main.1159","main.3648","main.2739"],"title":"Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!","tldr":"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patte...","track":"Information Extraction"},"forum":"main.1862","id":"main.1862","presentation_id":"38938999"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1863.png","content":{"abstract":"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.","authors":["Tao Yu","Shafiq Joty"],"demo_url":"","keywords":["conversation disentanglement","generalization","time-consuming engineering","disentanglement"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.512","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.916","main.645","main.2050","main.527","main.2839"],"title":"Online Conversation Disentanglement with Pointer Networks","tldr":"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information ...","track":"Discourse and Pragmatics"},"forum":"main.1863","id":"main.1863","presentation_id":"38939000"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1866.png","content":{"abstract":"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users' natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.","authors":["Yuntao Li","Bei Chen","Qian Liu","Yan Gao","Jian-Guang Lou","Yan Zhang","Dongmei Zhang"],"demo_url":"","keywords":["databases systems","text-to-sql technique","parsers","parser-independent approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.561","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.2590","TACL.1983","demo.54","main.3682"],"title":"\u201cWhat Do You Mean by That?\u201d A Parser-Independent Interactive Approach for Enhancing Text-to-SQL","tldr":"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1866","id":"main.1866","presentation_id":"38939001"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1877.png","content":{"abstract":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.","authors":["Siamak Shakeri","Cicero Nogueira dos Santos","Henghui Zhu","Patrick Ng","Feng Nan","Zhiguo Wang","Ramesh Nallapati","Bing Xiang"],"demo_url":"","keywords":["synthetic generation","end-to-end approach","transformer-based network","encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.439","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2635","main.3140","main.3054","main.2078","main.2586"],"title":"End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems","tldr":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the enco...","track":"Question Answering"},"forum":"main.1877","id":"main.1877","presentation_id":"38939002"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1892.png","content":{"abstract":"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.","authors":["Haejun Lee","Drew A. Hudson","Kangwook Lee","Christopher D. Manning"],"demo_url":"","keywords":["nlp","sentence-level modeling","discourse representation","pre-training methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.120","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2851","main.407","main.2635","main.1130","main.852"],"title":"SLM: Learning a Discourse Language Representation with Sentence Unshuffling","tldr":"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language r...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1892","id":"main.1892","presentation_id":"38939003"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1898.png","content":{"abstract":"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source--target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods' accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.","authors":["Eric Malmi","Aliaksei Severyn","Sascha Rothe"],"demo_url":"","keywords":["style transfer","sentence fusion","sentiment transfer","masker"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.699","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1581","main.2389","main.247","main.648","main.2382"],"title":"Unsupervised Text Style Transfer with Padded Masked Language Models","tldr":"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source--target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text sp...","track":"Language Generation"},"forum":"main.1898","id":"main.1898","presentation_id":"38939004"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1901.png","content":{"abstract":"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. \"under-training\").","authors":["Ivan Vuli\u0107","Sebastian Ruder","Anders S\u00f8gaard"],"demo_url":"","keywords":["aligning spaces","monolingual training","vector spaces","non-isomorphic spaces"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.257","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2131","main.865","main.2891","main.410","main.447"],"title":"Are All Good Word Vector Spaces Isomorphic?","tldr":"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to resu...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1901","id":"main.1901","presentation_id":"38939005"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1904.png","content":{"abstract":"We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q\\&A site, we collect posts containing military-relevant content written by active-duty military personnel. We then annotate the posts with two groups of experts: military experts and mental health experts. Our dataset includes 2,791 posts with 13,955 corresponding expert annotations of suicidal risk levels, and this dataset is available to researchers who consent to research ethics agreement. Using various fine-tuned state-of-the-art language models, we predict the level of suicide risk, reaching .88 F1 score for classifying the risks.","authors":["Sungjoon Park","Kiwoong Park","Jaimeen Ahn","Alice Oh"],"demo_url":"","keywords":["classifying risks","fine-tuned models","language models","suicidal personnel"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.198","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2510","main.789","main.851","main.838","main.3424"],"title":"Suicidal Risk Detection for Military Personnel","tldr":"We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q\\&A site, we collect posts cont...","track":"Computational Social Science and Social Media"},"forum":"main.1904","id":"main.1904","presentation_id":"38939006"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1906.png","content":{"abstract":"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.","authors":["Irshad Bhat","Talita Anthonio","Michael Roth"],"demo_url":"","keywords":["revision experiments","wikihow","edits","textual edits"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.675","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3507","main.607","main.928","main.2922","main.2112"],"title":"Towards Modeling Revision Requirements in wikiHow Instructions","tldr":"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this wo...","track":"NLP Applications"},"forum":"main.1906","id":"main.1906","presentation_id":"38939007"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1908.png","content":{"abstract":"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.","authors":["Qiao Jin","Chuanqi Tan","Mosha Chen","Xiaozhong Liu","Songfang Huang"],"demo_url":"","keywords":["evidence-based medicine","clinical task","manual collection","ctrp framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.114","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1942","main.1631","main.1923","main.2238","main.3486"],"title":"Predicting Clinical Trial Results by Implicit Evidence Integration","tldr":"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) tas...","track":"NLP Applications"},"forum":"main.1908","id":"main.1908","presentation_id":"38939008"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1923.png","content":{"abstract":"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit *pre-filled* text boxes, reduce previously observed issues with annotation artifacts.","authors":["Samuel R. Bowman","Jennimaria Palomaki","Livio Baldini Soares","Emily Pitler"],"demo_url":"","keywords":["benchmarking","language understanding","transfer applications","crowdsourcing protocol"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.658","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.74","main.2739","main.2721","main.2491","main.1540"],"title":"New Protocols and Negative Results for Textual Entailment Data Collection","tldr":"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was n...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1923","id":"main.1923","presentation_id":"38939009"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1928.png","content":{"abstract":"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.","authors":["Nader Akoury","Shufan Wang","Josh Whiting","Stephen Hood","Nanyun Peng","Mohit Iyyer"],"demo_url":"","keywords":["story generation","assessing text","story models","storium"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.525","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.252","main.1647","main.2758","main.3054","main.1130"],"title":"STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation","tldr":"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to...","track":"Language Generation"},"forum":"main.1928","id":"main.1928","presentation_id":"38939010"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1935.png","content":{"abstract":"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.","authors":["Daniel Loureiro","Jose Camacho-Collados"],"demo_url":"","keywords":["word disambiguation","word","wsd","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.283","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3224","main.2251","main.143","main.644","main.2349"],"title":"Don't Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation","tldr":"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotate...","track":"Semantics: Lexical Semantics"},"forum":"main.1935","id":"main.1935","presentation_id":"38939011"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1938.png","content":{"abstract":"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.","authors":["Rik van Noord","Antonio Toral","Johan Bos"],"demo_url":"","keywords":["discourse parsing","analysis","character-level representations","character representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.371","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1892","main.1957","main.2890","main.891","main.315"],"title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT","tldr":"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1938","id":"main.1938","presentation_id":"38939012"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1942.png","content":{"abstract":"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention  as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.","authors":["Jinyue Feng","Chantal Shaib","Frank Rudzicz"],"demo_url":"","keywords":["sepsis prediction","clinical models","hierarchical model","multi-task model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.115","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1908","main.110","main.2989","main.3648","main.2307"],"title":"Explainable Clinical Decision Support from Text","tldr":"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a ...","track":"NLP Applications"},"forum":"main.1942","id":"main.1942","presentation_id":"38939013"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1943.png","content":{"abstract":"The connection between dependency trees and spanning trees  is exploited  by the  NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has  missed  an  important  difference  between the  two  structures: only one edge  may emanate from the root in a dependency tree.  We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able  to  learn  that  trees  which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%.  Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan  (1984)  to  dependency  parsing,  which satisfies the constraint without compromising the original runtime.","authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"demo_url":"","keywords":["decoding runtime","dependency","dependency parsing","parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.390","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.447","TACL.2141","main.1258","main.1957","main.1494"],"title":"Please Mind the Root: Decoding Arborescences for Dependency Parsing","tldr":"The connection between dependency trees and spanning trees  is exploited  by the  NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has  missed  an  important  difference  between the  two  structures: o...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.1943","id":"main.1943","presentation_id":"38939014"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1949.png","content":{"abstract":"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.","authors":["Cicero Nogueira dos Santos","Xiaofei Ma","Ramesh Nallapati","Zhiheng Huang","Bing Xiang"],"demo_url":"","keywords":["information retrieval","ranking documents","ir tasks","discriminative functions"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.134","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3327","main.3240","main.1023","main.2931","main.693"],"title":"Beyond [CLS] through Ranking by Generation","tldr":"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural...","track":"Information Retrieval and Text Mining"},"forum":"main.1949","id":"main.1949","presentation_id":"38939015"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1952.png","content":{"abstract":"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.","authors":["Liqiang Xiao","Lu Wang","Hao He","Yaohui Jin"],"demo_url":"","keywords":["modeling importance","summarization","statistical methods","information theory"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.293","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2650","main.471","main.143","main.3437","main.714"],"title":"Modeling Content Importance for Summarization with Pre-trained Language Models","tldr":"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importan...","track":"Summarization"},"forum":"main.1952","id":"main.1952","presentation_id":"38939016"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1957.png","content":{"abstract":"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our \ufb01ndings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.","authors":["Tianze Shi","Igor Malioutov","Ozan Irsoy"],"demo_url":"","keywords":["span-based labeling","syntactic parsing","semantic labeling","conversion scheme"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.610","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1754","main.1061","main.1179","main.2419","TACL.2141"],"title":"Semantic Role Labeling as Syntactic Dependency Parsing","tldr":"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.1957","id":"main.1957","presentation_id":"38939017"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1960.png","content":{"abstract":"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.","authors":["Yong Wang","Longyue Wang","Victor Li","Zhaopeng Tu"],"demo_url":"","keywords":["nmt architectures","over-parameterization","underutilization resources","redundant parameters"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.78","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.1943","main.522","main.835","main.1351","main.2661"],"title":"On the Sparsity of Neural Machine Translation Models","tldr":"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investi...","track":"Machine Translation and Multilinguality"},"forum":"main.1960","id":"main.1960","presentation_id":"38939018"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1970.png","content":{"abstract":"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.","authors":["Ieva Stali\u016bnait\u0117","Ignacio Iacobacci"],"demo_url":"","keywords":["nlp tasks","conversational task","semantic labeling","contextualized embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.573","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2363","main.143","TACL.2411","TACL.2013","main.2179"],"title":"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","tldr":"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1970","id":"main.1970","presentation_id":"38939019"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1972.png","content":{"abstract":"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret 'I\u2019m starving.\u2019 in response to \u2018Hungry?\u2019, even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today\u2019s systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus 'Circa\u2019 with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes.","authors":["Annie Louis","Dan Roth","Filip Radlinski"],"demo_url":"","keywords":["pragmatic problem","dialog","dialog systems","language model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.601","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.41","main.1622","main.32","main.527","main.1797"],"title":"\u201cI'd rather just go to bed\u201d: Understanding Indirect Answers","tldr":"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret 'I\u2019m starving.\u2019 in response to \u2018Hungry?\u2019, even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natura...","track":"Discourse and Pragmatics"},"forum":"main.1972","id":"main.1972","presentation_id":"38939020"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1974.png","content":{"abstract":"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist. And therefore it raises a critical question of whether previous creditable approaches can still work well when facing these challenges. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. To further verify our conclusions, we also construct a new open NER dataset that focuses on entity types with weaker name regularity and lower mention coverage to verify our conclusion. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is critical for the models to generalize to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained encoders.","authors":["Hongyu Lin","Yaojie Lu","Jialong Tang","Xianpei Han","Le Sun","Zhicheng Wei","Nicholas Jing Yuan"],"demo_url":"","keywords":["randomization test","fine-tuning model","ner","creditable approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.592","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1159","main.387","main.1923","main.2943","main.1032"],"title":"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?","tldr":"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER t...","track":"Information Extraction"},"forum":"main.1974","id":"main.1974","presentation_id":"38939021"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1975.png","content":{"abstract":"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.","authors":["Kunlong Chen","Weidi Xu","Xingyi Cheng","Zou Xiaochuan","Yuyu Zhang","Le Song","Taifeng Wang","Yuan Qi","Wei Chu"],"demo_url":"","keywords":["numerical reasoning","machine task","natural understanding","arithmetic computation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.549","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.574","main.2253","main.782","main.2761","main.151"],"title":"Question Directed Graph Attention Network for Numerical Reasoning over Text","tldr":"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we ...","track":"Question Answering"},"forum":"main.1975","id":"main.1975","presentation_id":"38939022"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1977.png","content":{"abstract":"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.","authors":["Jian Liu","Yubo Chen","Kang Liu","Wei Bi","Xiaojiang Liu"],"demo_url":"","keywords":["event extraction","ee","information task","classification task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.128","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.96","main.1159","main.2608","main.2048","main.2739"],"title":"Event Extraction as Machine Reading Comprehension","tldr":"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, ...","track":"Information Extraction"},"forum":"main.1977","id":"main.1977","presentation_id":"38939023"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1986.png","content":{"abstract":"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.","authors":["Sungwon Lyu","Bokyung Son","Kichang Yang","Jaekyoung Bae"],"demo_url":"","keywords":["multilingual translation","multilingual industries","-","multilingual model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.476","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1680","main.3688","main.522","main.852","main.1263"],"title":"Revisiting Modularized Multilingual NMT to Meet Industrial Demands","tldr":"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industr...","track":"Machine Translation and Multilinguality"},"forum":"main.1986","id":"main.1986","presentation_id":"38939024"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1996.png","content":{"abstract":"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.","authors":["Chengyue Jiang","Yinggong Zhao","Shanbo Chu","Libin Shen","Kewei Tu"],"demo_url":"","keywords":["natural applications","training","text classification","neural networks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.258","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3348","main.2040","main.989","main.2851","main.345"],"title":"Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks","tldr":"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressio...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.1996","id":"main.1996","presentation_id":"38939025"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.1997.png","content":{"abstract":"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources. We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish. Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.","authors":["Tharindu Ranasinghe","Marcos Zampieri"],"demo_url":"","keywords":["bengali","cross-lingual embeddings","transfer learning","cyberaggression"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.470","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3046","main.2131","main.143","main.3453","main.1626"],"title":"Multilingual Offensive Language Identification with Cross-lingual Embeddings","tldr":"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyber...","track":"Computational Social Science and Social Media"},"forum":"main.1997","id":"main.1997","presentation_id":"38939026"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2005.png","content":{"abstract":"We study \\emph{semantic collisions}: texts that are semantically unrelated but judged as similar by NLP models.  We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts\\textemdash including paraphrase identification, document retrieval, response suggestion, and extractive summarization\\textemdash are vulnerable to semantic collisions.  For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3.  We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at \\url{https://github.com/csong27/collision-bert}.","authors":["Congzheng Song","Alexander Rush","Vitaly Shmatikov"],"demo_url":"","keywords":["paraphrase identification","document retrieval","response suggestion","extractive summarizationtextemdash"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.344","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3457","main.744","main.3327","main.210","main.371"],"title":"Adversarial Semantic Collisions","tldr":"We study \\emph{semantic collisions}: texts that are semantically unrelated but judged as similar by NLP models.  We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2005","id":"main.2005","presentation_id":"38939027"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2012.png","content":{"abstract":"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff's claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court's views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.","authors":["Yiquan Wu","Kun Kuang","Yating Zhang","Xiaozhong Liu","Changlong Sun","Jun Xiao","Yueting Zhuang","Luo Si","Fei Wu"],"demo_url":"","keywords":["court generation","legal ai","automatic generation","fact description"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.56","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2072","main.2962","main.387","main.638","main.362"],"title":"De-Biased Court's View Generation with Causality","tldr":"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) a...","track":"Language Generation"},"forum":"main.2012","id":"main.2012","presentation_id":"38939028"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.204.png","content":{"abstract":"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.","authors":["Hao Fei","Yafeng Ren","Donghong Ji"],"demo_url":"","keywords":["end tasks","structure integration","main task","semantic- tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.168","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2890","main.1618","main.1210","main.1339","main.2635"],"title":"Retrofitting Structure-aware Transformer Language Model for End Tasks","tldr":"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer struct...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.204","id":"main.204","presentation_id":"38938663"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2040.png","content":{"abstract":"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.","authors":["Prithviraj Sen","Marina Danilevsky","Yunyao Li","Siddhartha Brahma","Matthias Boehm","Laura Chiticariu","Rajasekar Krishnamurthy"],"demo_url":"","keywords":["interpretability models","sentence classification","le","human-machine models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.345","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1613","main.2851","main.3181","main.76","main.1996"],"title":"Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification","tldr":"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form...","track":"Machine Learning for NLP"},"forum":"main.2040","id":"main.2040","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2042.png","content":{"abstract":"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants. The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation. In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on  Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems. The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model. In those cases, a significant number of information leaking utterances can be detected by our models with high precision.","authors":["Qiongkai Xu","Lizhen Qu","Zeyu Gao","Gholamreza Haffari"],"demo_url":"","keywords":["leakage information","detection task","alignment problem","dataset persona-leakage"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.532","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.916","main.1863","main.2281","main.485","main.834"],"title":"Personal Information Leakage Detection in Conversations","tldr":"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy co...","track":"Dialog and Interactive Systems"},"forum":"main.2042","id":"main.2042","presentation_id":"38939030"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2048.png","content":{"abstract":"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.","authors":["Xiaozhi Wang","Ziqi Wang","Xu Han","Wangyi Jiang","Rong Han","Zhiyuan Liu","Juanzi Li","Peng Li","Yankai Lin","Jie Zhou"],"demo_url":"","keywords":["event detection","event","ed","identifying words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.129","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1749","main.1977","main.96","main.2427","main.2972"],"title":"MAVEN: A Massive General Domain Event Detection Dataset","tldr":"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit furth...","track":"Information Extraction"},"forum":"main.2048","id":"main.2048","presentation_id":"38939031"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2050.png","content":{"abstract":"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.","authors":["Weishi Wang","Steven C.H. Hoi","Shafiq Joty"],"demo_url":"","keywords":["response selection","dynamic task","encoding","dynamic disentanglement"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.533","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1863","main.645","main.215","main.1201","main.916"],"title":"Response Selection for Multi-Party Conversations with Dynamic Topic Tracking","tldr":"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation ...","track":"Dialog and Interactive Systems"},"forum":"main.2050","id":"main.2050","presentation_id":"38939032"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2054.png","content":{"abstract":"Recent work by Clark et al. (2020) shows that transformers can act as \"soft theorem provers'' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers  (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for \"depth 5\", indicating significant scope for future work.","authors":["Swarnadeep Saha","Sayan Ghosh","Shashank Srivastava","Mohit Bansal"],"demo_url":"","keywords":["inference","qa generation","generalization","qa task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.9","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2415","TACL.2049","demo.86","main.607","main.3035"],"title":"PRover: Proof Generation for Interpretable Reasoning over Rules","tldr":"Recent work by Clark et al. (2020) shows that transformers can act as \"soft theorem provers'' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by pr...","track":"Question Answering"},"forum":"main.2054","id":"main.2054","presentation_id":"38939033"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2055.png","content":{"abstract":"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.","authors":["Mikel Artetxe","Gorka Labaka","Eneko Agirre"],"demo_url":"","keywords":["human translation","cross-lingual learning","natural inference","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.618","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.522","main.3688","main.1572","main.701","main.888"],"title":"Translation Artifacts in Cross-lingual Transfer Learning","tldr":"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the t...","track":"Machine Translation and Multilinguality"},"forum":"main.2055","id":"main.2055","presentation_id":"38939034"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2057.png","content":{"abstract":"Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.","authors":["Nedjma Ousidhoum","Yangqiu Song","Dit-Yan Yeung"],"demo_url":"","keywords":["classification","data process","topic models","selection bias"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.199","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.457","main.504","main.3116","main.143","main.2452"],"title":"Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets","tldr":"Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to ...","track":"Computational Social Science and Social Media"},"forum":"main.2057","id":"main.2057","presentation_id":"38939035"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2058.png","content":{"abstract":"Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.","authors":["Ece Takmaz","Mario Giulianelli","Sandro Pezzelle","Arabella Sinclair","Raquel Fern\u00e1ndez"],"demo_url":"","keywords":["generation model","reference system","visual context","dialogue context"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.353","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2141","main.689","main.1522","main.3157","main.916"],"title":"Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts","tldr":"Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting p...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2058","id":"main.2058","presentation_id":"38939036"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2061.png","content":{"abstract":"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.","authors":["Jingyi Zhang","Josef van Genabith"],"demo_url":"","keywords":["qe task","qe","translation","supervised learning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.205","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.888","TACL.1997","main.1572","main.835","main.856"],"title":"Translation Quality Estimation by Jointly Learning to Score and Rank","tldr":"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of th...","track":"Machine Translation and Multilinguality"},"forum":"main.2061","id":"main.2061","presentation_id":"38939037"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2064.png","content":{"abstract":"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world\u2019s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/","authors":["Aditi Chaudhary","Antonios Anastasopoulos","Adithya Pratapa","David R. Mortensen","Zaid Sheikh","Yulia Tsvetkov","Graham Neubig"],"demo_url":"","keywords":["language preservation","cross-lingual transfer","descriptive language","first-pass specification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.422","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["CL.1","TACL.2013","main.1494","main.2847","main.1258"],"title":"Automatic Extraction of Rules Governing Morphological Agreement","tldr":"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devisin...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2064","id":"main.2064","presentation_id":"38939038"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2066.png","content":{"abstract":"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes. The classification is carried out by mapping text embeddings to the word graph embeddings of the classes. Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved. In particular, using the recently-released \\emph{Larson dataset}, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.","authors":["Paulo Cavalin","Victor Henrique Alves Ribeiro","Ana Appel","Claudio Pinhanez"],"demo_url":"","keywords":["intent classification","inverse task","classification","detection examples"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.324","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1611","main.1159","main.3292","main.1935"],"title":"Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes","tldr":"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The app...","track":"Dialog and Interactive Systems"},"forum":"main.2066","id":"main.2066","presentation_id":"38939039"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2068.png","content":{"abstract":"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.","authors":["Husam Quteineh","Spyridon Samothrakis","Richard Sutcliffe"],"demo_url":"","keywords":["data task","optimization problem","non-guided process","trec-"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.600","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2733","main.1923","main.493","main.471","main.1351"],"title":"Textual Data Augmentation for Efficient Active Learning on Tiny Datasets","tldr":"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data gener...","track":"NLP Applications"},"forum":"main.2068","id":"main.2068","presentation_id":"38939040"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.207.png","content":{"abstract":"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(n^6) down to O(n^3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra,  Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.","authors":["Caio Corro"],"demo_url":"","keywords":["span-based parsing","fully setting","chart-based algorithm","parser"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.219","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1258","main.447","main.1957","TACL.2141"],"title":"Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n6) down to O(n3)","tldr":"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.207","id":"main.207","presentation_id":"38938664"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2070.png","content":{"abstract":"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers\u2019 robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs \u2013 dialog response generation and user simulation, where our model outperforms previous strong baselines.","authors":["Kang Min Yoo","Hanbit Lee","Franck Dernoncourt","Trung Bui","Walter Chang","Sang-goo Lee"],"demo_url":"","keywords":["generative augmentation","nlp tasks","dialog tracking","dialog generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.274","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.128","main.3393","main.355","main.3318","main.3179"],"title":"Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation","tldr":"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking fo...","track":"Machine Learning for NLP"},"forum":"main.2070","id":"main.2070","presentation_id":"38939041"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2072.png","content":{"abstract":"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (\"_She daydreams about being a doctor_\") while a man is portrayed as more proactive and powerful (\"_He pursues his dream of being a doctor_\").  We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models.  Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","authors":["Xinyao Ma","Maarten Sap","Hannah Rashkin","Yejin Choi"],"demo_url":"","keywords":["bias correction","controllable debiasing","revision task","powertransformer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.602","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.32","main.84","demo.118","TACL.2411","main.1581"],"title":"PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction","tldr":"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (\"_She daydreams about b...","track":"Discourse and Pragmatics"},"forum":"main.2072","id":"main.2072","presentation_id":"38939042"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2075.png","content":{"abstract":"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4-way classification is \\textless  60\\%. This is a dramatic drop of 35\\% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines will be made freely available.","authors":["Wanqiu Long","Bonnie Webber","Deyi Xiong"],"demo_url":"","keywords":["-way classification","same-language transfer","same-domain transfer","ted-cdb"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.223","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3566","main.1702","main.2205","main.1379","main.2641"],"title":"TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks","tldr":"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in ...","track":"Discourse and Pragmatics"},"forum":"main.2075","id":"main.2075","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2076.png","content":{"abstract":"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the  help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved. However,  they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark  datasets, our framework demonstrates improvements that are both competitive and explainable.","authors":["Changlong Yu","Jialong Han","Peifeng Wang","Yangqiu Song","Hongming Zhang","Wilfred Ng","Shuming Shi"],"demo_url":"","keywords":["hypernymy detection","pattern-based ones","distributional methods","pattern-based model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.502","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2122","main.210","main.143","main.84","main.1997"],"title":"When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models","tldr":"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the  help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Rec...","track":"Semantics: Lexical Semantics"},"forum":"main.2076","id":"main.2076","presentation_id":"38939044"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2078.png","content":{"abstract":"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.","authors":["Rong Zhang","Revanth Gangi Reddy","Md Arafat Sultan","Vittorio Castelli","Anthony Ferritto","Radu Florian","Efsun Sarioglu Kayi","Salim Roukos","Avi Sil","Todd Ward"],"demo_url":"","keywords":["nlp tasks","fine-tuning","auxiliary tasks","lm transfer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.440","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2255","main.2476","main.2733","main.2087","main.1482"],"title":"Multi-Stage Pre-training for Low-Resource Domain Adaptation","tldr":"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning t...","track":"Question Answering"},"forum":"main.2078","id":"main.2078","presentation_id":"38939045"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2083.png","content":{"abstract":"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.","authors":["Qinxin Wang","Hao Tan","Sheng Shen","Michael Mahoney","Zhewei Yao"],"demo_url":"","keywords":["phrase localization","visually-aware representations","weakly-supervised scenarios","ablation studies"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.159","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2273","main.3609","main.148","main.3360","main.2702"],"title":"MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding","tldr":"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-availab...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2083","id":"main.2083","presentation_id":"38939046"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2087.png","content":{"abstract":"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.","authors":["Tu Vu","Tong Wang","Tsendsuren Munkhdalai","Alessandro Sordoni","Adam Trischler","Andrew Mattarella-Micke","Subhransu Maji","Mohit Iyyer"],"demo_url":"","keywords":["language modeling","nlp tasks","text classification","question answering"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.635","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.1923","main.2491","main.2500","main.74"],"title":"Exploring and Predicting Transferability across NLP Tasks","tldr":"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we ...","track":"Machine Learning for NLP"},"forum":"main.2087","id":"main.2087","presentation_id":"38939047"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2094.png","content":{"abstract":"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.","authors":["Aina Gar\u00ed Soler","Marianna Apidianaki"],"demo_url":"","keywords":["natural reasoning","intensity detection","indirect task","bert-based approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.598","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3457","main.210","main.2076","main.2122","main.1970"],"title":"BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations","tldr":"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approac...","track":"Semantics: Lexical Semantics"},"forum":"main.2094","id":"main.2094","presentation_id":"38939048"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2098.png","content":{"abstract":"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.","authors":["Dongqin Xu","Junhui Li","Muhua Zhu","Min Zhang","Guodong Zhou"],"demo_url":"","keywords":["abstract parsing","amr parsing","sequence-to-sequence parsing","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.196","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1649","main.2635","main.3227","main.2491","main.130"],"title":"Improving AMR Parsing with Sequence-to-Sequence Pre-training","tldr":"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trai...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2098","id":"main.2098","presentation_id":"38939049"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.210.png","content":{"abstract":"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable syntactic cues caused by its automatic generation process. Taking advantage of such cues, we show that even a simple rule-based model can perform on par with the state-of-the-art model. To remedy such limitations, we collect and release two crowdsourced benchmark datasets. We not only make sure that they contain significantly more diverse syntax, but also carefully control for their quality according to a well-defined set of criteria. While no satisfactory automatic metric exists, we apply fine-grained manual evaluation based on these criteria using crowdsourcing, showing that our datasets better represent the task and are significantly more challenging for the models.","authors":["Li Zhang","Huaiyu Zhu","Siddhartha Brahma","Yunyao Li"],"demo_url":"","keywords":["text task","fine-grained evaluation","automatic process","rule-based model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.91","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2076","main.2943","main.84","main.876"],"title":"Small but Mighty: New Benchmarks for Split and Rephrase","tldr":"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark datas...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.210","id":"main.210","presentation_id":"38938665"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2100.png","content":{"abstract":"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad  et  al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.","authors":["Xiang Kong","Zhisong Zhang","Eduard Hovy"],"demo_url":"","keywords":["translation tasks","local mechanism","non-autoregressive models","merging algorithm"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.79","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.453","TACL.2221","main.1402","main.1694","main.1432"],"title":"Incorporating a Local Translation Mechanism into Non-autoregressive Translation","tldr":"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of...","track":"Machine Translation and Multilinguality"},"forum":"main.2100","id":"main.2100","presentation_id":"38939050"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2112.png","content":{"abstract":"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century') within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.","authors":["Marius Pasca"],"demo_url":"","keywords":["automatically constituents","open-domain method","modifier constituents","constituent modifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.503","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["demo.89","main.2512","main.3507","main.3453","main.3216"],"title":"Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs","tldr":"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century') within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored unders...","track":"Semantics: Lexical Semantics"},"forum":"main.2112","id":"main.2112","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2114.png","content":{"abstract":"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.","authors":["Denis Emelin","Ivan Titov","Rico Sennrich"],"demo_url":"","keywords":["word disambiguation","nmt","prediction errors","adversarial strategy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.616","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.457","main.1935","main.2891","main.3224","main.2363"],"title":"Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks","tldr":"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-o...","track":"Machine Translation and Multilinguality"},"forum":"main.2114","id":"main.2114","presentation_id":"38939052"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2117.png","content":{"abstract":"Fact checking at scale is difficult---while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs --- in particular QABriefs --- increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.","authors":["Angela Fan","Aleksandra Piktus","Fabio Petroni","Guillaume Wenzek","Marzieh Saeidi","Andreas Vlachos","Antoine Bordes","Sebastian Riedel"],"demo_url":"","keywords":["fact checking","claim detection","media ecosystem","qabriefer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.580","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.872","main.3151","main.2570","main.1004","main.2506"],"title":"Generating Fact Checking Briefs","tldr":"Fact checking at scale is difficult---while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often erro...","track":"NLP Applications"},"forum":"main.2117","id":"main.2117","presentation_id":"38939053"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2120.png","content":{"abstract":"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.","authors":["Jose Manuel Gomez-Perez","Ra\u00fal Ortega"],"demo_url":"","keywords":["textbook answering","machine comprehension","visual answering","transformer models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.441","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1030","main.319","main.449","TACL.2041","main.1022"],"title":"ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention","tldr":"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential ...","track":"Question Answering"},"forum":"main.2120","id":"main.2120","presentation_id":"38939054"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2122.png","content":{"abstract":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"demo_url":"","keywords":["intrinsic probing","nlp systems","pre-trained representations","extrinsic probing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.15","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.498","main.947","main.2363","main.1052","TACL.2411"],"title":"Intrinsic Probing through Dimension Selection","tldr":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these repres...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2122","id":"main.2122","presentation_id":"38939055"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2125.png","content":{"abstract":"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.","authors":["Yue Dong","Shuohang Wang","Zhe Gan","Yu Cheng","Jackie Chi Kit Cheung","Jingjing Liu"],"demo_url":"","keywords":["news summarization","factual inconsistency","pre-trained systems","extractive strategies"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.749","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2437","main.2506","main.1835","main.2650","main.965"],"title":"Multi-Fact Correction in Abstractive Text Summarization","tldr":"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: ...","track":"Summarization"},"forum":"main.2125","id":"main.2125","presentation_id":"38939056"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2131.png","content":{"abstract":"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI,  parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.","authors":["Haim Dubossarsky","Ivan Vuli\u0107","Roi Reichart","Anna Korhonen"],"demo_url":"","keywords":["cross-lingual tasks","large-scale study","bli","parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.186","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.865","main.2278","main.2363","CL.2","main.1901"],"title":"The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures","tldr":"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approxima...","track":"Machine Translation and Multilinguality"},"forum":"main.2131","id":"main.2131","presentation_id":"38939057"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2133.png","content":{"abstract":"Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multitask learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.","authors":["Liying Cheng","Lidong Bing","Qian Yu","Wei Lu","Luo Si"],"demo_url":"","keywords":["peer review","argument task","sequence task","text task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.569","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.471","main.1159","main.2688","main.2476","main.574"],"title":"APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning","tldr":"Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extracti...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.2133","id":"main.2133","presentation_id":"38939058"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.214.png","content":{"abstract":"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.","authors":["Qian Liu","Bei Chen","Jian-Guang Lou","Bin Zhou","Dongmei Zhang"],"demo_url":"","keywords":["incomplete rewriting","machine task","semantic task","inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.227","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.891","main.1339","main.3257","main.2635","main.689"],"title":"Incomplete Utterance Rewriting as Semantic Segmentation","tldr":"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a no...","track":"Dialog and Interactive Systems"},"forum":"main.214","id":"main.214","presentation_id":"38938666"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2141.png","content":{"abstract":"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.","authors":["Song Feng","Hui Wan","Chulaka Gunasekara","Siva Patel","Sachindra Joshi","Luis Lastras"],"demo_url":"","keywords":["information-seeking conversations","dialogue tasks","dialogue flows","content elements"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.652","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1654","main.128","TACL.2143","main.1201","main.1702"],"title":"doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset","tldr":"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that c...","track":"Dialog and Interactive Systems"},"forum":"main.2141","id":"main.2141","presentation_id":"38939059"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.215.png","content":{"abstract":"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.","authors":["Qi Jia","Yizhu Liu","Siyu Ren","Kenny Zhu","Haifeng Tang"],"demo_url":"","keywords":["multi-turn selection","dialogue understanding","dialogue agents","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.150","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2209","main.1201","main.689","main.1522","main.128"],"title":"Multi-turn Response Selection using Dialogue Dependency Relations","tldr":"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the...","track":"Dialog and Interactive Systems"},"forum":"main.215","id":"main.215","presentation_id":"38938667"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2151.png","content":{"abstract":"We propose the novel \\emph{Within-Between} Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.","authors":["Oren Barkan","Avi Caciularu","Ido Dagan"],"demo_url":"","keywords":["recognizing relations","sub-space representation","lexical-semantic relations","relational signals"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.284","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.3593","main.2112","main.447","main.2419"],"title":"Within-Between Lexical Relation Classification","tldr":"We propose the novel \\emph{Within-Between} Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show t...","track":"Semantics: Lexical Semantics"},"forum":"main.2151","id":"main.2151","presentation_id":"38939060"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2163.png","content":{"abstract":"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account.  This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk.  An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.","authors":["Javier Iranzo-S\u00e1nchez","Adri\u00e0 Gim\u00e9nez Pastor","Joan Albert Silvestre-Cerd\u00e0","Pau Baquero-Arnal","Jorge Civera Saiz","Alfons Juan"],"demo_url":"","keywords":["st","streaming st","pipeline","automatic system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.206","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2915","TACL.2221","TACL.2107","main.106","main.888"],"title":"Direct Segmentation Models for Streaming Speech Translation","tldr":"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the AS...","track":"Machine Translation and Multilinguality"},"forum":"main.2163","id":"main.2163","presentation_id":"38939061"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2164.png","content":{"abstract":"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.","authors":["Reina Akama","Sho Yokoi","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["response generation","neural agents","scoring method","quality pairs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.68","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.128","main.478","main.1654","main.215","main.2141"],"title":"Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness","tldr":"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for sc...","track":"Dialog and Interactive Systems"},"forum":"main.2164","id":"main.2164","presentation_id":"38939062"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2167.png","content":{"abstract":"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of \\lmtc datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.","authors":["Ilias Chalkidis","Manos Fergadiotis","Sotiris Kotitsas","Prodromos Malakasiotis","Nikolaos Aletras","Ion Androutsopoulos"],"demo_url":"","keywords":["flat classification","hierarchical approaches","zero-shot learning","few learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.607","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1682","main.1611","main.1032","main.2289","main.148"],"title":"An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels","tldr":"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set...","track":"NLP Applications"},"forum":"main.2167","id":"main.2167","presentation_id":"38939063"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2179.png","content":{"abstract":"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99%), but generalization accuracy was substantially lower (16--35%) and showed high sensitivity to random seed (+-6--8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.","authors":["Najoung Kim","Tal Linzen"],"demo_url":"","keywords":["compositional generalization","language architectures","cogs","lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.731","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.1970","main.143","main.3457","TACL.2141"],"title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation","tldr":"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2179","id":"main.2179","presentation_id":"38939064"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2198.png","content":{"abstract":"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms \u2013 greedy search, beam search, top-k sampling, and nucleus sampling \u2013 are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.","authors":["Sean Welleck","Ilia Kulikov","Jaedeok Kim","Richard Yuanzhe Pang","Kyunghyun Cho"],"demo_url":"","keywords":["receiving sequences","neural models","recurrent model","common algorithms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.448","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3550","main.1377","main.648","main.2430","main.1445"],"title":"Consistency of a Recurrent Language Model With Respect to Incomplete Decoding","tldr":"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequence...","track":"Language Generation"},"forum":"main.2198","id":"main.2198","presentation_id":"38939066"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2205.png","content":{"abstract":"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":"","keywords":["rst-style parsing","data-driven approaches","deep-learning","scalable methodology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.603","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1892","TACL.2411","main.2075","main.3257","main.128"],"title":"MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision","tldr":"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse tree...","track":"Discourse and Pragmatics"},"forum":"main.2205","id":"main.2205","presentation_id":"38939067"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2208.png","content":{"abstract":"We demonstrate a program that learns to pronounce Chinese text in Mandarin,  without a pronunciation dictionary. From non-parallel streams of Chinese  characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable  accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.","authors":["Christopher Chu","Scot Fang","Kevin Knight"],"demo_url":"","keywords":["unsupervised methods","pronunciations","token-level accuracy","accuracy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.458","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2216","main.2766","main.3391","main.2818","main.1320"],"title":"Learning to Pronounce Chinese Without a Pronunciation Dictionary","tldr":"We demonstrate a program that learns to pronounce Chinese text in Mandarin,  without a pronunciation dictionary. From non-parallel streams of Chinese  characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters a...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2208","id":"main.2208","presentation_id":"38939068"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2209.png","content":{"abstract":"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.","authors":["Junfan Chen","Richong Zhang","Yongyi Mao","Jie Xu"],"demo_url":"","keywords":["multidomain models","mdst","parallel networks","pin"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.151","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.215","main.1522","main.689","main.1012","main.128"],"title":"Parallel Interactive Networks for Multi-Domain Dialogue State Generation","tldr":"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependenci...","track":"Dialog and Interactive Systems"},"forum":"main.2209","id":"main.2209","presentation_id":"38939069"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2212.png","content":{"abstract":"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and  their combination shows further improvement.","authors":["Xiuyi Chen","Fandong Meng","Peng Li","Feilong Chen","Shuang Xu","Bo Xu","Jie Zhou"],"demo_url":"","keywords":["knowledge selection","knowledge-grounded dialogue","dialogue generation","inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.275","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.787","main.1006","main.1846","main.3179","main.1522"],"title":"Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation","tldr":"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the dive...","track":"Dialog and Interactive Systems"},"forum":"main.2212","id":"main.2212","presentation_id":"38939070"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2215.png","content":{"abstract":"{L}arge {T}ransformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned {BERT}, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained {BERT} weights are potentially useful. We also study the ``good\" subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.","authors":["Sai Prasanna","Anna Rogers","Anna Rumshisky"],"demo_url":"","keywords":["fine-tuned","structured pruning","self-attention heads","self-attention layers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.259","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.763","main.2999","main.618","TACL.2041","main.317"],"title":"When BERT Plays the Lottery, All Tickets Are Winning","tldr":"{L}arge {T}ransformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2215","id":"main.2215","presentation_id":"38939071"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2216.png","content":{"abstract":"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.  We are able to decipher 75.1% of the cipher-word tokens correctly.","authors":["Christopher Chu","Raphael Valenti","Kevin Knight"],"demo_url":"","keywords":["neural model","word-based codes","decoding lattice","enciphered letters"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.471","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2208","main.3391","main.2818","main.1739","main.1901"],"title":"Solving Historical Dictionary Codes with a Neural Language Model","tldr":"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and age...","track":"Computational Social Science and Social Media"},"forum":"main.2216","id":"main.2216","presentation_id":"38939072"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2218.png","content":{"abstract":"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.  While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech.  We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.  Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another.  To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.","authors":["David Gaddy","Dan Klein"],"demo_url":"","keywords":["digitally speech","speech models","emg","silently words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.445","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2915","main.106","TACL.2221","main.2343","main.2208"],"title":"Digital Voicing of Silent Speech","tldr":"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.  While prior work has focused on tr...","track":"Speech and Multimodality"},"forum":"main.2218","id":"main.2218","presentation_id":"38939073"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2221.png","content":{"abstract":"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.","authors":["Ramakanth Pasunuru","Han Guo","Mohit Bansal"],"demo_url":"","keywords":["language tasks","optimization rewards","nlg tasks","question generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.625","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2410","main.2834","main.286","main.1734","main.3672"],"title":"DORB: Dynamically Optimizing Multiple Rewards with Bandits","tldr":"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in...","track":"Machine Learning for NLP"},"forum":"main.2221","id":"main.2221","presentation_id":"38939074"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2225.png","content":{"abstract":"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S\\&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.","authors":["Ramit Sawhney","Shivam Agarwal","Arnav Wadhwa","Rajiv Ratn Shah"],"demo_url":"","keywords":["risk modeling","profit generation","stock task","stock forecasting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.676","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2520","main.1180","main.3486","main.2784","main.1287"],"title":"Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations","tldr":"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock ...","track":"NLP Applications"},"forum":"main.2225","id":"main.2225","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2228.png","content":{"abstract":"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC contains over 98K  explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: \"X is a Y\" AND \"Y has Z\" IMPLIES \"X has Z\"). We find that generalized chains maintain performance while also being more robust to certain perturbations.\\footnote{Code and datasets can be found at https://allenai.org/data/eqasc.","authors":["Harsh Jhamtani","Peter Clark"],"demo_url":"","keywords":["multihop qa","multihop","eqasc","qasc"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.10","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2258","main.959","main.3035","TACL.2049","main.2380"],"title":"Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering","tldr":"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in...","track":"Question Answering"},"forum":"main.2228","id":"main.2228","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2238.png","content":{"abstract":"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings.  By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be  underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.","authors":["Dallas Card","Peter Henderson","Urvashi Khandelwal","Robin Jia","Kyle Mahowald","Dan Jurafsky"],"demo_url":"","keywords":["human studies","machine translation","power analysis","power analyses"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.745","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.84","main.2268","main.1540","TACL.2055","main.2122"],"title":"With Little Power Comes Great Responsibility","tldr":"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2238","id":"main.2238","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2251.png","content":{"abstract":"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.","authors":["Bianca Scarlini","Tommaso Pasini","Roberto Navigli"],"demo_url":"","keywords":["natural processing","english task","word-in-context task","contextualized embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.285","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.298","main.143","main.3224","main.1935","main.3093"],"title":"With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation","tldr":"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In...","track":"Semantics: Lexical Semantics"},"forum":"main.2251","id":"main.2251","presentation_id":"38939078"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2253.png","content":{"abstract":"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.","authors":["Jiangming Liu","Matt Gardner","Shay B. Cohen","Mirella Lapata"],"demo_url":"","keywords":["chained reasoning","black-box transformers","compositional model","neural networks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.245","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1975","demo.86","main.1191","main.531","main.607"],"title":"Multi-Step Inference for Reasoning Over Paragraphs","tldr":"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between thes...","track":"Question Answering"},"forum":"main.2253","id":"main.2253","presentation_id":"38939079"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2258.png","content":{"abstract":"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release  code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter","authors":["Priyanka Sen","Amir Saffari"],"demo_url":"","keywords":["question answering","reading comprehension","bert-based models","question variations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.190","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6C","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2586","main.2228","TACL.2049","main.319","main.2864"],"title":"What do Models Learn from Question Answering Datasets?","tldr":"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comp...","track":"Question Answering"},"forum":"main.2258","id":"main.2258","presentation_id":"38939080"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2261.png","content":{"abstract":"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.","authors":["Weicheng Ma","Ruibo Liu","Lili Wang","Soroush Vosoughi"],"demo_url":"","keywords":["natural tasks","emojis","linguistic components","multi-class setting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.542","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3329","main.3437","main.1675","main.1766","main.3013"],"title":"Multi-resolution Annotations for Emoji Prediction","tldr":"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary ta...","track":"NLP Applications"},"forum":"main.2261","id":"main.2261","presentation_id":"38939081"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2268.png","content":{"abstract":"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.","authors":["Xiang Zhou","Yixin Nie","Hao Tan","Mohit Bansal"],"demo_url":"","keywords":["nli","reading sets","model-selection routine","instability"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.659","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.84","main.2238","main.1159","demo.102","main.2739"],"title":"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions","tldr":"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability o...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2268","id":"main.2268","presentation_id":"38939082"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2271.png","content":{"abstract":"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.","authors":["Yordan Yordanov","Oana-Maria Camburu","Vid Kocijan","Thomas Lukasiewicz"],"demo_url":"","keywords":["hard resolution","commonsense reasoning","pronoun resolution","sequence ranking"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.402","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.767","main.2491","main.143","main.888"],"title":"Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution","tldr":"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categ...","track":"Machine Learning for NLP"},"forum":"main.2271","id":"main.2271","presentation_id":"38939083"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2273.png","content":{"abstract":"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as ``kitchen'' and ``bedroom'', and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating ``granite'' with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.","authors":["Gregory Yauney","Jack Hessel","David Mimno"],"demo_url":"","keywords":["image-text approaches","granular annotation","unsupervised method","object detection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.160","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2083","main.2702","main.143","main.1052","main.2342"],"title":"Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents","tldr":"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, u...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2273","id":"main.2273","presentation_id":"38939084"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2278.png","content":{"abstract":"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ``strong'' cross-lingual alignment, requiring semantically related \\textit{cross}-language pairs to be closer in representation space than unrelated \\textit{same}-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target ``weak\" alignment is not predictive of performance on LAReQA\\@. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at \\url{https://github.com/google-research-datasets/lareqa}.","authors":["Uma Roy","Noah Constant","Rami Al-Rfou","Aditya Barua","Aaron Phillips","Yinfei Yang"],"demo_url":"","keywords":["language-agnostic retrieval","cross-lingual tasks","cross-lingual retrieval","alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.477","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2630","main.1803","main.143","main.3216","main.1061"],"title":"LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool","tldr":"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ``strong'' cross-lingual alignment, requiring semantically related \\textit...","track":"Machine Translation and Multilinguality"},"forum":"main.2278","id":"main.2278","presentation_id":"38939085"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2281.png","content":{"abstract":"The lack  of  time  efficient  and  reliable  evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations  that  require  humans  to  converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and  tend  to  yield  low  quality  results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces  human-bot  conversations  with  conversations between bots.  Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot\u2019s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work  allows  for  frequent  evaluations  of  chatbots during their evaluation cycle.  We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work.  The framework is released asa ready-to-use tool.","authors":["Jan Deriu","Don Tuggener","Pius von D\u00e4niken","Jon Ander Campos","Alvaro Rodrigo","Thiziri Belkacem","Aitor Soroa","Eneko Agirre","Mark Cieliebak"],"demo_url":"","keywords":["evalu-ation methods","conversational systems","chat bots","spot bot"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.326","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1201","main.645","main.485","main.419"],"title":"Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems","tldr":"The lack  of  time  efficient  and  reliable  evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations  that  require  humans  to  converse with chat bots are time and cost intensive, put high cogni...","track":"Dialog and Interactive Systems"},"forum":"main.2281","id":"main.2281","presentation_id":"38939086"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2289.png","content":{"abstract":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as \u201cseed motifs\u201d, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","authors":["Dheeraj Mekala","Xinyang Zhang","Jingbo Shang"],"demo_url":"","keywords":["weakly learning","systematic modeling","meta","bootstrapping manner"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.670","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2167","main.1023","main.1159","main.693","main.3298"],"title":"META: Metadata-Empowered Weak Supervision for Text Classification","tldr":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata informa...","track":"Information Retrieval and Text Mining"},"forum":"main.2289","id":"main.2289","presentation_id":"38939087"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2298.png","content":{"abstract":"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.","authors":["Tahmid Hasan","Abhik Bhattacharjee","Kazi Samin","Masum Hasan","Madhusudan Basak","M. Sohel Rahman","Rifat Shahriyar"],"demo_url":"","keywords":["machine literature","erroneous segmentation","parallel creation","aligner ensembling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.207","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.522","main.1379","TACL.2107","main.888","main.852"],"title":"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation","tldr":"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; a...","track":"Machine Translation and Multilinguality"},"forum":"main.2298","id":"main.2298","presentation_id":"38939088"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2307.png","content":{"abstract":"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text --- a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.","authors":["Bhargavi Paranjape","Mandar Joshi","John Thickstun","Hannaneh Hajishirzi","Luke Zettlemoyer"],"demo_url":"","keywords":["language understanding","semi-supervised setting","complex models","explainer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.153","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2650","TACL.2411","main.2470","main.1023"],"title":"An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction","tldr":"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text --- a rationale. Models that condition predictions on a concise rationale, while being mor...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2307","id":"main.2307","presentation_id":"38939089"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2313.png","content":{"abstract":"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen)  model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels.  For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews.  Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.","authors":["Tianlu Wang","Xuezhi Wang","Yao Qin","Ben Packer","Kang Li","Jilin Chen","Alex Beutel","Ed Chi"],"demo_url":"","keywords":["sentiment classification","model re-training","nlp models","cat-gen model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.417","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2914","main.371","demo.104","main.2895","main.426"],"title":"CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation","tldr":"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen)  model that, given an input te...","track":"Language Generation"},"forum":"main.2313","id":"main.2313","presentation_id":"38939090"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2322.png","content":{"abstract":"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs---a natural expression of information need---from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.","authors":["Adam Fisch","Kenton Lee","Ming-Wei Chang","Jonathan Clark","Regina Barzilay"],"demo_url":"","keywords":["image task","visual images","captioning","capwap"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.705","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.284","main.1923","main.1085","main.3183","main.317"],"title":"CapWAP: Image Captioning with a Purpose","tldr":"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioni...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2322","id":"main.2322","presentation_id":"38939091"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.233.png","content":{"abstract":"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build  large-scale medical dialogue datasets -- MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System","authors":["Guangtao Zeng","Wenmian Yang","Zeqian Ju","Yue Yang","Sicheng Wang","Ruisi Zhang","Meng Zhou","Jiaqi Zeng","Xiangyu Dong","Ruoyu Zhang","Hongchao Fang","Penghui Zhu","Shu Chen","Pengtao Xie"],"demo_url":"","keywords":["telemedicine","transformer","low-resource tasks","medical tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.743","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.478","main.1702","main.1846","main.1201","main.1522"],"title":"MedDialog: Large-scale Medical Dialogue Datasets","tldr":"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we b...","track":"Dialog and Interactive Systems"},"forum":"main.233","id":"main.233","presentation_id":"38938668"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2331.png","content":{"abstract":"Leveraging large amounts of unlabeled data using  Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.","authors":["Kasturi Bhattacharjee","Miguel Ballesteros","Rishita Anubhai","Smaranda Muresan","Jie Ma","Faisal Ladhak","Yaser Al-Onaizan"],"demo_url":"","keywords":["learning representations","downstream tasks","cross-view cvt","sequence tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.636","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.493","main.345","main.2078","main.2793","main.956"],"title":"To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging","tldr":"Leveraging large amounts of unlabeled data using  Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tas...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2331","id":"main.2331","presentation_id":"38939092"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2337.png","content":{"abstract":"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.","authors":["Bernhard Kratzwald","Stefan Feuerriegel","Huan Sun"],"demo_url":"","keywords":["labeling","annotating questions","question qa","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.246","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2640","main.2739","main.1923","demo.54","main.1022"],"title":"Learning a Cost-Effective Annotation Policy for Question Answering","tldr":"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotat...","track":"Question Answering"},"forum":"main.2337","id":"main.2337","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2342.png","content":{"abstract":"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and  (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations.  Universal NLP can be probably achieved through different routines. In this work, we introduce  Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on  new entailment domains in a few-shot setting, and show its effectiveness  as a unified solver for several downstream NLP tasks such as question answering and  coreference resolution  when the end-task annotations are limited.","authors":["Wenpeng Yin","Nazneen Fatema Rajani","Dragomir Radev","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["nlp problems","textual entailment","nlp task","downstream tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.660","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3470","demo.54","main.3010","main.1923","demo.48"],"title":"Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start","tldr":"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new pro...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2342","id":"main.2342","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2343.png","content":{"abstract":"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.","authors":["Kevin Clark","Minh-Thang Luong","Quoc Le","Christopher D. Manning"],"demo_url":"","keywords":["representation text","downstream tasks","pre-training","electric"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.20","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2615","main.3483","main.2515","main.247","TACL.2107"],"title":"Pre-Training Transformers as Energy-Based Cloze Models","tldr":"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens...","track":"Machine Learning for NLP"},"forum":"main.2343","id":"main.2343","presentation_id":"38939095"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2349.png","content":{"abstract":"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.","authors":["Michele Bevilacqua","Marco Maru","Roberto Navigli"],"demo_url":"","keywords":["generative modeling","definition modeling","discriminative tasks","word disambiguation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.585","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.143","main.3093","main.1935","main.2363","main.2251"],"title":"Generationary or \u201cHow We Went beyond Word Sense Inventories and Learned to Gloss\u201d","tldr":"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce ...","track":"Semantics: Lexical Semantics"},"forum":"main.2349","id":"main.2349","presentation_id":"38939096"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2357.png","content":{"abstract":"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.","authors":["Lihao Wang","Xiaoqing Zheng"],"demo_url":"","keywords":["grammatical correction","sequence-to-sequence learning","neural networks","gec"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.228","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2047","main.2313","main.3227","demo.118","main.2389"],"title":"Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples","tldr":"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance ...","track":"Machine Learning for NLP"},"forum":"main.2357","id":"main.2357","presentation_id":"38939097"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2363.png","content":{"abstract":"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.","authors":["Ivan Vuli\u0107","Edoardo Maria Ponti","Robert Litschko","Goran Glava\u0161","Anna Korhonen"],"demo_url":"","keywords":["lexical tasks","pretrained models","lms","lexical strategies"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.586","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2630","main.1970","main.1130","main.143","TACL.2411"],"title":"Probing Pretrained Language Models for Lexical Semantics","tldr":"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic,...","track":"Semantics: Lexical Semantics"},"forum":"main.2363","id":"main.2363","presentation_id":"38939098"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2367.png","content":{"abstract":"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.","authors":["Michal Lukasik","Boris Dadachev","Kishore Papineni","Gon\u00e7alo Sim\u00f5es"],"demo_url":"","keywords":["document segmentation","nlp tasks","downstream tasks","information retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.380","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1129","main.151","main.2476","main.825","main.693"],"title":"Text Segmentation by Cross Segment Attention","tldr":"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three t...","track":"NLP Applications"},"forum":"main.2367","id":"main.2367","presentation_id":"38939099"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.237.png","content":{"abstract":"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.","authors":["Manling Li","Qi Zeng","Ying Lin","Kyunghyun Cho","Heng Ji","Jonathan May","Nathanael Chambers","Clare Voss"],"demo_url":"","keywords":["graph induction","downstream extraction","event schemas","event schema"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.50","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1116","main.574","main.2761","main.158","main.666"],"title":"Connecting the Dots: Event Graph Schema Induction with Path Language Modeling","tldr":"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important r...","track":"Information Extraction"},"forum":"main.237","id":"main.237","presentation_id":"38938669"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2370.png","content":{"abstract":"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover's distance (optimal transport), which we refer to as word rotator's distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines. The source code is avaliable at https://github.com/eumesy/wrd","authors":["Sho Yokoi","Ryo Takahashi","Reina Akama","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["assessing similarity","vector converter","word alignment","alignment-based approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.236","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1503","main.1935","main.1901","main.644","main.973"],"title":"Word Rotator's Distance","tldr":"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferi...","track":"Machine Learning for NLP"},"forum":"main.2370","id":"main.2370","presentation_id":"38939100"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2377.png","content":{"abstract":"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims.","authors":["Sungho Jeon","Michael Strube"],"demo_url":"","keywords":["automated scoring","neural models","coherence model","linguistic coherence"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.604","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1159","main.3010","main.2650","main.1892","main.128"],"title":"Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments","tldr":"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse struct...","track":"Discourse and Pragmatics"},"forum":"main.2377","id":"main.2377","presentation_id":"38939101"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2380.png","content":{"abstract":"Given questions regarding some prototypical situation --- such as Name something that people usually do before they leave the house for work? --- a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others.  This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show -- Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.","authors":["Michael Boratko","Xiang Li","Tim O'Gorman","Rajarshi Das","Dan Le","Andrew McCallum"],"demo_url":"","keywords":["generative task","artificial systems","common capabilities","model scores"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.85","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2228","TACL.2049","main.319","main.3186","main.2943"],"title":"ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning","tldr":"Given questions regarding some prototypical situation --- such as Name something that people usually do before they leave the house for work? --- a human can easily answer them via acquired experiences. There can be multiple right answers for such qu...","track":"Question Answering"},"forum":"main.2380","id":"main.2380","presentation_id":"38939102"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2382.png","content":{"abstract":"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model's rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.","authors":["Allison Hegel","Sudha Rao","Asli Celikyilmaz","Bill Dolan"],"demo_url":"","keywords":["sentence-level rewriting","document-level transfer","language generation","generative model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.526","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1130","main.1892","main.648","main.1707","main.3010"],"title":"Substance over Style: Document-Level Targeted Content Transfer","tldr":"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the ch...","track":"Language Generation"},"forum":"main.2382","id":"main.2382","presentation_id":"38939103"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2383.png","content":{"abstract":"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,860 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.","authors":["Yang Li","Gang Li","Luheng He","Jingjie Zheng","Hong Li","Zhiwei Guan"],"demo_url":"","keywords":["mobile uis","automatically descriptions","widget captioning","multimodal task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.443","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1928","demo.54","main.876","demo.72"],"title":"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements","tldr":"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning...","track":"Speech and Multimodality"},"forum":"main.2383","id":"main.2383","presentation_id":"38939104"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2389.png","content":{"abstract":"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed  exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.","authors":["Jianqiao Li","Chunyuan Li","Guoyin Wang","Hao Fu","Yuhchen Lin","Liqun Chen","Yizhe Zhang","Chenyang Tao","Ruiyi Zhang","Wenlin Wang","Dinghan Shen","Qian Yang","Lawrence Carin"],"demo_url":"","keywords":["testing","ot learning","machine translation","text summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.735","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.648","main.26","main.247","main.3353","main.891"],"title":"Improving Text Generation with Student-Forcing Optimal Transport","tldr":"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens,...","track":"Language Generation"},"forum":"main.2389","id":"main.2389","presentation_id":"38939105"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2391.png","content":{"abstract":"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -- Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation,  open-source framework for evaluating models,  and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.","authors":["Tatiana Shavrina","Alena Fenogenova","Emelyanov Anton","Denis Shevelev","Ekaterina Artemova","Valentin Malykh","Vladislav Mikhailov","Maria Tikhonova","Andrey Chertok","Andrey Evlampiev"],"demo_url":"","keywords":["natural inference","logical operations","human evaluation","evaluating models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.381","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.143","main.835","main.1892","main.623"],"title":"RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark","tldr":"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -- Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their br...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2391","id":"main.2391","presentation_id":"38939106"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2396.png","content":{"abstract":"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.  However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning.  We study GigaBERT's effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.","authors":["Wuwei Lan","Yang Chen","Wei Xu","Alan Ritter"],"demo_url":"","keywords":["cross-lingual transfer","arabic tasks","arabic nlp","zero-short transfer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.382","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.871","TACL.2107","main.1803","main.143","main.1263"],"title":"An Empirical Study of Pre-trained Transformers for Arabic Information Extraction","tldr":"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.  However, their performance on Arabic information extraction (IE...","track":"NLP Applications"},"forum":"main.2396","id":"main.2396","presentation_id":"38939107"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2406.png","content":{"abstract":"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.","authors":["Mikhail Galkin","Priyansh Trivedi","Gaurav Maheshwari","Ricardo Usbeck","Jens Lehmann"],"demo_url":"","keywords":["link prediction","kgs","message encoder","message -"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.596","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1787","main.1706","main.1648","main.1493","main.3517"],"title":"Message Passing for Hyper-Relational Knowledge Graphs","tldr":"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - St...","track":"Machine Learning for NLP"},"forum":"main.2406","id":"main.2406","presentation_id":"38939108"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2410.png","content":{"abstract":"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.  A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.  We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.  We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.","authors":["Natasha Jaques","Judy Hanwen Shen","Asma Ghandeharioun","Craig Ferguson","Agata Lapedriza","Noah Jones","Shixiang Gu","Rosalind Picard"],"demo_url":"","keywords":["dialog model","rl policy","rl","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.327","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1700","main.954","main.2839","main.2583","main.1201"],"title":"Human-centric dialog training via offline reinforcement learning","tldr":"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended co...","track":"Dialog and Interactive Systems"},"forum":"main.2410","id":"main.2410","presentation_id":"38939109"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2412.png","content":{"abstract":"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.","authors":["Jerin Philip","Alexandre Berard","Matthias Gall\u00e9","Laurent Besacier"],"demo_url":"","keywords":["adapter formalism","multilingual models","bilingual adapters","-language baseline"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.361","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1339","main.1803","main.835","main.1263","main.3688"],"title":"Monolingual Adapters for Zero-Shot Neural Machine Translation","tldr":"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingua...","track":"Machine Translation and Multilinguality"},"forum":"main.2412","id":"main.2412","presentation_id":"38939110"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2414.png","content":{"abstract":"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.","authors":["Nadir Durrani","Hassan Sajjad","Fahim Dalvi","Yonatan Belinkov"],"demo_url":"","keywords":["neuron-level analysis","linguistic tasks","deep models","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.395","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2851","main.2363","main.2696","main.2893"],"title":"Analyzing Individual Neurons in Pre-trained Language Models","tldr":"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2414","id":"main.2414","presentation_id":"38939111"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2415.png","content":{"abstract":"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions (\"and\", \"or\", \"but\", \"nor\") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions.","authors":["Swarnadeep Saha","Yixin Nie","Mohit Bansal"],"demo_url":"","keywords":["natural inference","conjnli","large-scale models","roberta"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.661","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2054","demo.86","TACL.2013","main.2470","main.1622"],"title":"ConjNLI: Natural Language Inference Over Conjunctive Sentences","tldr":"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not cons...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2415","id":"main.2415","presentation_id":"38939112"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2416.png","content":{"abstract":"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.","authors":["Md Mosharaf Hossain","Venelin Kovatchev","Pranoy Dutta","Tiffany Kao","Elizabeth Wei","Eduardo Blanco"],"demo_url":"","keywords":["natural inference","inference judgments","transformers","negation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.732","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2415","main.2253","TACL.2013","main.1970","main.2040"],"title":"An Analysis of Natural Language Inference Benchmarks through the Lens of Negation","tldr":"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for na...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2416","id":"main.2416","presentation_id":"38939113"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2419.png","content":{"abstract":"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.","authors":["Matthias Lindemann","Jonas Groschwitz","Alexander Koller"],"demo_url":"","keywords":["am parsing","neural parsing","linguistically method","parsers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.323","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.1957","main.447","main.2890","TACL.2141"],"title":"Fast semantic parsing with well-typedness guarantees","tldr":"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser a...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2419","id":"main.2419","presentation_id":"38939114"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2422.png","content":{"abstract":"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.","authors":["Xuefeng Bai","Linfeng Song","Yue Zhang"],"demo_url":"","keywords":["amr-to-text generation","text generation","graph encoders","decoders"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.92","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.870","main.1339","main.2795","main.2098","main.648"],"title":"Online Back-Parsing for AMR-to-Text Generation","tldr":"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being us...","track":"Language Generation"},"forum":"main.2422","id":"main.2422","presentation_id":"38939115"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2424.png","content":{"abstract":"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT\u2019s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT\u2019s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.","authors":["Valentin Hofmann","Janet Pierrehumbert","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["full finetuning","derivation generation","pretrained models","plms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.316","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1280","TACL.2141","main.2675","TACL.2041","main.1892"],"title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model","tldr":"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT\u2019s derivational capabilities in different settings, ranging from using...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2424","id":"main.2424","presentation_id":"38939116"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2426.png","content":{"abstract":"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.","authors":["Ye Liu","Sheng Zhang","Rui Song","Suo Feng","Yanghua Xiao"],"demo_url":"","keywords":["open extraction","question-answering task","information system","kg"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.693","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3462","main.300","main.3646","main.1669","main.1159"],"title":"Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning","tldr":"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information...","track":"Information Extraction"},"forum":"main.2426","id":"main.2426","presentation_id":"38939117"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2427.png","content":{"abstract":"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.","authors":["Lifu Huang","Heng Ji"],"demo_url":"","keywords":["event studies","semi-supervised induction","supervised detection","semi-supervised framework"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.53","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.96","main.1749","main.1116","main.2048","main.2508"],"title":"Semi-supervised New Event Type Induction and Event Detection","tldr":"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatica...","track":"Information Extraction"},"forum":"main.2427","id":"main.2427","presentation_id":"38939118"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2430.png","content":{"abstract":"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.","authors":["Tom Bosc","Pascal Vincent"],"demo_url":"","keywords":["generation","memorization","autoregressive models","variational autoencoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.350","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1446","main.3013","main.2851","main.3483","main.1892"],"title":"Do sequence-to-sequence VAEs learn global features of sentences?","tldr":"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation...","track":"Language Generation"},"forum":"main.2430","id":"main.2430","presentation_id":"38939119"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2437.png","content":{"abstract":"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.","authors":["Meng Cao","Yue Dong","Jiapeng Wu","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["factual evaluation","artificial correction","neural systems","self-supervised methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.506","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2125","main.2506","main.714","main.2650","main.1835"],"title":"Factual Error Correction for Abstractive Summarization Models","tldr":"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries fo...","track":"Summarization"},"forum":"main.2437","id":"main.2437","presentation_id":"38939120"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2438.png","content":{"abstract":"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why.  We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks.  Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.","authors":["Sean Papay","Roman Klinger","Sebastian Pad\u00f3"],"demo_url":"","keywords":["chunking","ner","code-switching detection","span tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.396","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2087","main.449","main.3470","TACL.2041","main.2363"],"title":"Dissecting Span Identification Tasks with Performance Prediction","tldr":"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2438","id":"main.2438","presentation_id":"38939121"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2444.png","content":{"abstract":"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and information elicitation in such settings, but has been limited to manual review of small corpora. We introduce **Interview**---a large-scale (105K conversations) media dialog dataset collected from news interview transcripts---which allows us to investigate such patterns at scale. We present a dialog model that leverages external knowledge as well as dialog acts via auxiliary losses and demonstrate that our model quantitatively and qualitatively outperforms strong discourse-agnostic baselines for dialog modeling---generating more specific and topical responses in interview-style conversations.","authors":["Bodhisattwa Prasad Majumder","Shuyang Li","Jianmo Ni","Julian McAuley"],"demo_url":"","keywords":["large-scale discourse","generative turns","interrogative patterns","modes persuasion"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.653","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.916","main.1622","main.527","main.128","main.2141"],"title":"Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding","tldr":"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understan...","track":"Dialog and Interactive Systems"},"forum":"main.2444","id":"main.2444","presentation_id":"38939122"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2448.png","content":{"abstract":"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","authors":["Felix Stahlberg","Shankar Kumar"],"demo_url":"","keywords":["sequence editing","natural tasks","nlp tasks","text normalization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.418","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3299","main.3457","main.1892","main.1503","main.2098"],"title":"Seq2Edits: Sequence Transduction Using Span-level Edit Operations","tldr":"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as...","track":"Machine Learning for NLP"},"forum":"main.2448","id":"main.2448","presentation_id":"38939123"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2452.png","content":{"abstract":"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.","authors":["Piotr Szyma\u0144ski","Kyle Gorman"],"demo_url":"","keywords":["natural models","bayesian technique","k-fold cross-validation","english taggers"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.172","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3115","main.2638","main.457","main.1455","main.3181"],"title":"Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing","tldr":"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the like...","track":"Machine Learning for NLP"},"forum":"main.2452","id":"main.2452","presentation_id":"38939124"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.246.png","content":{"abstract":"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus. Additionally,  we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].  To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.","authors":["Zhen Yang","Bojie Hu","Ambyera Han","Shen Huang","Qi Ju"],"demo_url":"","keywords":["neural nmt","lexicon induction","unsupervised nmt","pre-training method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.208","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.852","main.3688","main.888","main.522"],"title":"CSP:Code-Switching Pre-training for Neural Machine Translation","tldr":"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CS...","track":"Machine Translation and Multilinguality"},"forum":"main.246","id":"main.246","presentation_id":"38938670"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.247.png","content":{"abstract":"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.","authors":["Minki Kang","Moonsu Han","Sung Ju Hwang"],"demo_url":"","keywords":["self-supervised pre-training","question answering","task","reinforcement learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.493","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3483","main.2389","main.1299","main.1898","main.3023"],"title":"Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation","tldr":"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specif...","track":"Machine Learning for NLP"},"forum":"main.247","id":"main.247","presentation_id":"38938671"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2470.png","content":{"abstract":"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.","authors":["Shrey Desai","Jiacheng Xu","Greg Durrett"],"demo_url":"","keywords":["compressive systems","compressions","rouge","pre-trained model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.507","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2650","TACL.2411","main.2307","main.2506","main.471"],"title":"Compressive Summarization with Plausibility and Salience Modeling","tldr":"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose ...","track":"Summarization"},"forum":"main.2470","id":"main.2470","presentation_id":"38939125"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2476.png","content":{"abstract":"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.","authors":["Pierre Dognin","Igor Melnyk","Inkit Padhi","Cicero Nogueira dos Santos","Payel Das"],"demo_url":"","keywords":["kb conversion","dual approach","generative models","weak supervision"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.694","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2078","main.835","main.471","main.2635","main.1263"],"title":"DualTKB: A Dual Learning Bridge between Text and Knowledge Base","tldr":"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even ...","track":"Information Extraction"},"forum":"main.2476","id":"main.2476","presentation_id":"38939126"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2490.png","content":{"abstract":"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.","authors":["Shijie Wu","Mark Dredze"],"demo_url":"","keywords":["multilingual","unsupervised encoders","cross-lingual representation","contrastive objective"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.362","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1379","main.2278","main.143","main.1680","main.3688"],"title":"Do Explicit Alignments Robustly Improve Multilingual Encoders?","tldr":"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve ...","track":"Machine Translation and Multilinguality"},"forum":"main.2490","id":"main.2490","presentation_id":"38939127"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2491.png","content":{"abstract":"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT\\textsubscript{Base} on the GLUE benchmark using fewer than a quarter of the training tokens.","authors":["St\u00e9phane Aroca-Ouellette","Frank Rudzicz"],"demo_url":"","keywords":["pre-training","masked modelling","next prediction","nsp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.403","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.1351","main.2635","main.522","main.1631"],"title":"On Losses for Modern Language Models","tldr":"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's e...","track":"Machine Learning for NLP"},"forum":"main.2491","id":"main.2491","presentation_id":"38939128"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2493.png","content":{"abstract":"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.","authors":["Shruti Rijhwani","Antonios Anastasopoulos","Graham Neubig"],"demo_url":"","keywords":["natural models","general-purpose tools","ocr method","recognition rate"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.478","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2777","main.2847","main.517","main.870","main.1997"],"title":"OCR Post Correction for Endangered Language Texts","tldr":"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In...","track":"Machine Translation and Multilinguality"},"forum":"main.2493","id":"main.2493","presentation_id":"38939129"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2500.png","content":{"abstract":"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.","authors":["Anne Lauscher","Vinit Ravishankar","Ivan Vuli\u0107","Goran Glava\u0161"],"demo_url":"","keywords":["zero-shot transfer","downstream transfer","resource-lean scenarios","pos tagging"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.363","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.858","main.74","main.1803","main.1263","main.852"],"title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers","tldr":"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify thei...","track":"Machine Translation and Multilinguality"},"forum":"main.2500","id":"main.2500","presentation_id":"38939130"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2506.png","content":{"abstract":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.  Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.","authors":["Wojciech Kryscinski","Bryan McCann","Caiming Xiong","Richard Socher"],"demo_url":"","keywords":["assessing algorithms","natural inference","fact checking","auxiliary tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.750","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2125","main.1835","main.1023","main.2437","main.2470"],"title":"Evaluating the Factual Consistency of Abstractive Text Summarization","tldr":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying...","track":"Summarization"},"forum":"main.2506","id":"main.2506","presentation_id":"38939131"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2508.png","content":{"abstract":"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.","authors":["Wenlin Yao","Zeyu Dai","Maitreyi Ramaswamy","Bonan Min","Ruihong Huang"],"demo_url":"","keywords":["discourse analysis","event-centric applications","identification subevents","weakly approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.430","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1421","main.96","main.1977","main.1749"],"title":"Weakly Supervised Subevent Knowledge Acquisition","tldr":"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extr...","track":"Information Extraction"},"forum":"main.2508","id":"main.2508","presentation_id":"38939132"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2510.png","content":{"abstract":"Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user's historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.","authors":["Ramit Sawhney","Harshit Joshi","Saumya Gandhi","Rajiv Ratn Shah"],"demo_url":"","keywords":["identification users","suicide prevention","suicide ideation","screening risk"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.619","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1904","main.3101","main.2707","main.851","main.789"],"title":"A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media","tldr":"Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide i...","track":"Computational Social Science and Social Media"},"forum":"main.2510","id":"main.2510","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2511.png","content":{"abstract":"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often \u201crambling\u201d without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.","authors":["Xinyu Hua","Lu Wang"],"demo_url":"","keywords":["generation","pre-trained transformers","content-controlled framework","pair"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.57","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1707","main.3398","main.1634","main.1522","main.648"],"title":"PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation","tldr":"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often \u201crambling\u201d without coherently arranged content. In this work, we present a novel content-controlled text generation framewo...","track":"Language Generation"},"forum":"main.2511","id":"main.2511","presentation_id":"38939134"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2512.png","content":{"abstract":"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding.  However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs,  which in turn allows us to crowd-source wide-coverage data annotated with discourse relations,  via an intuitively appealing interface for composing such questions and answers. Based on our proposed  representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.","authors":["Valentina Pyatkin","Ayal Klein","Reut Tsarfaty","Ido Dagan"],"demo_url":"","keywords":["natural understanding","predicting relations","discourse relations","question-and-answer pairs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.224","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2922","main.2141","main.2476","main.574","main.2640"],"title":"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","tldr":"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding.  However, annotating discourse relations typically requires expert annotators. Recently...","track":"Discourse and Pragmatics"},"forum":"main.2512","id":"main.2512","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2515.png","content":{"abstract":"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.","authors":["Fei Tan","Yifan Hu","Changwei Hu","Keqian Li","Kevin Yen"],"demo_url":"","keywords":["content moderation","text manipulation","masked recovery","hate task"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.383","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2396","main.1898","main.3337","main.2389","main.247"],"title":"TNT: Text Normalization based Pre-training of Transformers for Content Moderation","tldr":"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation ...","track":"NLP Applications"},"forum":"main.2515","id":"main.2515","presentation_id":"38939136"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.252.png","content":{"abstract":"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, con\ufb02icting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.","authors":["Jian Guan","Minlie Huang"],"demo_url":"","keywords":["open-ended generation","story generation","evaluating generation","constructing samples"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.736","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1928","main.1647","main.2758","main.2650","main.2864"],"title":"UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation","tldr":"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many ...","track":"Language Generation"},"forum":"main.252","id":"main.252","presentation_id":"38938672"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2520.png","content":{"abstract":"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.","authors":["Ramit Sawhney","Piyush Khanna","Arshiya Aggarwal","Taru Jain","Puneet Mathur","Rajiv Ratn Shah"],"demo_url":"","keywords":["natural processing","stock forecasting","financial forecasting","risk modeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.643","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2225","main.1180","main.3486","main.1488","main.1289"],"title":"VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls","tldr":"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique inve...","track":"Speech and Multimodality"},"forum":"main.2520","id":"main.2520","presentation_id":"38939137"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2529.png","content":{"abstract":"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.","authors":["Yuanhe Tian","Yan Song","Fei Xia"],"demo_url":"","keywords":["supertagging","combinatory parsing","neural supertagging","parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.487","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3437","main.2650","main.2684","TACL.2411","main.1952"],"title":"Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks","tldr":"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2529","id":"main.2529","presentation_id":"38939138"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2533.png","content":{"abstract":"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.","authors":["Yun He","Zhuoer Wang","Yin Zhang","Ruihong Huang","James Caverlee"],"demo_url":"","keywords":["paraphrase identification","neural models","bert fine-tuning","fine-tuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.611","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3506","main.210","main.1970","main.3470","main.1159"],"title":"PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge","tldr":"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based o...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2533","id":"main.2533","presentation_id":"38939139"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2535.png","content":{"abstract":"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT -- a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.","authors":["Tsvetomila Mihaylova","Vlad Niculae","Andr\u00e9 F. T. Martins"],"demo_url":"","keywords":["pipeline systems","ste","latent models","end-to-end training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.171","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2307","main.1446","main.345","main.3292","main.87"],"title":"Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning","tldr":"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-...","track":"Machine Learning for NLP"},"forum":"main.2535","id":"main.2535","presentation_id":"38939140"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2549.png","content":{"abstract":"Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective polarity to event phrases. First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base. Second, we present a  discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data. The key idea is to exploit event phrases that occur with a coreferent sentiment expression.  The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier's predictions and the polarities of the event's coreferent sentiment expressions.  Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.","authors":["Yuan Zhuang","Tianyu Jiang","Ellen Riloff"],"demo_url":"","keywords":["affective classification","classification models","bert-based model","classifier"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.452","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9C","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1675","main.1550","main.1611","main.1654","main.3329"],"title":"Affective Event Classification with Discourse-enhanced Self-training","tldr":"Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective pol...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.2549","id":"main.2549","presentation_id":"38939141"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2553.png","content":{"abstract":"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.","authors":["Eleftheria Briakou","Marine Carpuat"],"demo_url":"","keywords":["detecting content","cross-lingual nlp","machine problem","annotation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.121","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.1379","main.1970","main.143","main.2890"],"title":"Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank","tldr":"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2553","id":"main.2553","presentation_id":"38939142"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2561.png","content":{"abstract":"Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal\u2014trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory.  Finally, we demonstrate that the features of condolence  individuals  find most helpful online differ substantially in their features from those seen in interpersonal settings.","authors":["Naitian Zhou","David Jurgens"],"demo_url":"","keywords":["offering condolence","appraisal theory","condolence","trite responses"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.45","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3D","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3352","main.3072","main.2707","main.476","main.851"],"title":"Condolence and Empathy in Online Communities","tldr":"Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal\u2014trite responses offer little actual sup...","track":"Computational Social Science and Social Media"},"forum":"main.2561","id":"main.2561","presentation_id":"38939143"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2570.png","content":{"abstract":"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification. Our source code is available at https://github.com/ShyamSubramanian/HESM.","authors":["Shyam Subramanian","Kyumin Lee"],"demo_url":"","keywords":["automated verification","claim verification","fact extraction","fact verification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.627","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2506","main.2962","main.2117","main.1159","main.3151"],"title":"Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification","tldr":"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leadin...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2570","id":"main.2570","presentation_id":"38939144"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2574.png","content":{"abstract":"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.","authors":["Xiaoxiao Guo","Mo Yu","Yupeng Gao","Chuang Gan","Murray Campbell","Shiyu Chang"],"demo_url":"","keywords":["language techniques","language challenges","action generation","if solving"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.624","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1578","main.2982","main.390","main.763","main.645"],"title":"Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning","tldr":"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding...","track":"Machine Learning for NLP"},"forum":"main.2574","id":"main.2574","presentation_id":"38939145"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2579.png","content":{"abstract":"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.","authors":["Patrick Xia","Shijie Wu","Benjamin Van Durme"],"demo_url":"","keywords":["language learning","pretrained encoders","model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.608","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2476","main.2931","main.883","main.2122","main.2078"],"title":"Which *BERT? A Survey Organizing Contextualized Encoders","tldr":"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While signific...","track":"NLP Applications"},"forum":"main.2579","id":"main.2579","presentation_id":"38939146"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2581.png","content":{"abstract":"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT'14 English $\\to$ German and WMT'18 English $\\to$ Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English $\\to$ German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.","authors":["William Chan","Mitchell Stern","Jamie Kiros","Jakob Uszkoreit"],"demo_url":"","keywords":["machine translation","insertion-based modeling","soft framework","transformer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.464","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.648","main.1707","main.2590","main.2382","main.870"],"title":"An Empirical Study of Generation Order for Machine Translation","tldr":"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary ora...","track":"Machine Translation and Multilinguality"},"forum":"main.2581","id":"main.2581","presentation_id":"38939147"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2583.png","content":{"abstract":"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of \\algo in the language-drift translation game.","authors":["Yuchen Lu","Soumye Singhal","Florian Strub","Olivier Pietquin","Aaron Courville"],"demo_url":"","keywords":["language drift","language-drift game","language models","word-based agents"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.325","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7A","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2389","main.74","main.128","main.1892","main.2851"],"title":"Supervised Seeded Iterated Learning for Interactive Language Learning","tldr":"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. ...","track":"Dialog and Interactive Systems"},"forum":"main.2583","id":"main.2583","presentation_id":"38939148"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2585.png","content":{"abstract":"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.","authors":["Hanjie Chen","Yangfeng Ji"],"demo_url":"","keywords":["finding explanations","model interpretability","classification","interpretable classifier"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.347","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.958","main.1612","main.2958","main.1159","main.2040"],"title":"Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers","tldr":"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many exis...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2585","id":"main.2585","presentation_id":"38939149"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2586.png","content":{"abstract":"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.","authors":["Raul Puri","Ryan Spring","Mohammad Shoeybi","Mostofa Patwary","Bryan Catanzaro"],"demo_url":"","keywords":["question generation","squad task","em","data method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.468","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3140","main.2258","main.319","main.3186","main.2721"],"title":"Training Question Answering Models From Synthetic Data","tldr":"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer...","track":"Question Answering"},"forum":"main.2586","id":"main.2586","presentation_id":"38939150"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2587.png","content":{"abstract":"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.","authors":["Vladimir Karpukhin","Barlas Oguz","Sewon Min","Patrick Lewis","Ledell Wu","Sergey Edunov","Danqi Chen","Wen-tau Yih"],"demo_url":"","keywords":["open-domain answering","passage retrieval","retrieval","sparse models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.550","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.693","main.449","main.1022","demo.93"],"title":"Dense Passage Retrieval for Open-Domain Question Answering","tldr":"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically ...","track":"Question Answering"},"forum":"main.2587","id":"main.2587","presentation_id":"38939151"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2590.png","content":{"abstract":"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.","authors":["Mihir Kale","Abhinav Rastogi"],"demo_url":"","keywords":["natural generation","generation","nlg","domain-independent model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.527","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3353","main.876","TACL.2135","main.2763","main.2641"],"title":"Template Guided Text Generation for Task-Oriented Dialogue","tldr":"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (N...","track":"Language Generation"},"forum":"main.2590","id":"main.2590","presentation_id":"38939152"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2596.png","content":{"abstract":"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.","authors":["Charles Welch","Jonathan K. Kummerfeld","Ver\u00f3nica P\u00e9rez-Rosas","Rada Mihalcea"],"demo_url":"","keywords":["language tasks","language modeling","word associations","word embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.334","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2792","main.1305","TACL.2093","main.585"],"title":"Compositional Demographic Word Embeddings","tldr":"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve lang...","track":"Semantics: Lexical Semantics"},"forum":"main.2596","id":"main.2596","presentation_id":"38939153"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.26.png","content":{"abstract":"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.","authors":["Yu Wan","Baosong Yang","Derek F. Wong","Yikai Zhou","Lidia S. Chao","Haibo Zhang","Boxing Chen"],"demo_url":"","keywords":["neural","curriculum learning","translation tasks","nmt"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.80","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1146","main.701","main.2389","main.1960","main.835"],"title":"Self-Paced Learning for Neural Machine Translation","tldr":"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule...","track":"Machine Translation and Multilinguality"},"forum":"main.26","id":"main.26","presentation_id":"38938638"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2608.png","content":{"abstract":"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL's speed and accuracy makes it a viable approach for large-scale real-world scenarios.","authors":["Alan Ramponi","Rob van der Goot","Rosario Lombardo","Barbara Plank"],"demo_url":"","keywords":["biomedical extraction","sequence labeling","large-scale scenarios","beesl"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.431","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1977","main.2972","main.96","main.3329","main.3646"],"title":"Biomedical Event Extraction as Sequence Labeling","tldr":"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling...","track":"Information Extraction"},"forum":"main.2608","id":"main.2608","presentation_id":"38939154"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2612.png","content":{"abstract":"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories.","authors":["Shirley Anugrah Hayati","Dongyeop Kang","Qingxiaoyang Zhu","Weiyan Shi","Zhou Yu"],"demo_url":"","keywords":["recommendation dialogs","movie recommendation","annotation scheme","automatic evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.654","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.1797","main.916","main.787","main.2444"],"title":"INSPIRED: Toward Sociable Recommendation Dialog Systems","tldr":"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with...","track":"Dialog and Interactive Systems"},"forum":"main.2612","id":"main.2612","presentation_id":"38939155"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2614.png","content":{"abstract":"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.","authors":["Xiaochuang Han","Yulia Tsvetkov"],"demo_url":"","keywords":["detecting toxicity","toxic detectors","toxic detector","disguised language"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.622","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.371","main.2114","main.2357","main.2914","main.635"],"title":"Fortifying Toxic Speech Detectors Against Veiled Toxicity","tldr":"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veile...","track":"Computational Social Science and Social Media"},"forum":"main.2614","id":"main.2614","presentation_id":"38939156"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2615.png","content":{"abstract":"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.","authors":["Shrey Desai","Greg Durrett"],"demo_url":"","keywords":["natural processing","natural inference","paraphrase detection","commonsense reasoning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.21","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2A","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2491","main.2893","main.2834","main.1552"],"title":"Calibration of Pre-trained Transformers","tldr":"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an ...","track":"Machine Learning for NLP"},"forum":"main.2615","id":"main.2615","presentation_id":"38939157"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2630.png","content":{"abstract":"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for \\langnum typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.","authors":["Zhengbao Jiang","Antonios Anastasopoulos","Jun Araki","Haibo Ding","Graham Neubig"],"demo_url":"","keywords":["factual retrieval","language models","lms","probing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.479","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2363","main.143","main.2278","main.3453","main.623"],"title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models","tldr":"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many language...","track":"Machine Translation and Multilinguality"},"forum":"main.2630","id":"main.2630","presentation_id":"38939158"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2632.png","content":{"abstract":"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.  We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.  The algorithm is designed to address the exposure bias problem.  On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.  Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.","authors":["Alexander Lin","Jeremy Wohlwend","Howard Chen","Tao Lei"],"demo_url":"","keywords":["natural tasks","knowledge distillation","exposure problem","prototypical tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.494","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1734","main.2430","main.1208","TACL.2041","main.2838"],"title":"Autoregressive Knowledge Distillation through Imitation Learning","tldr":"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-...","track":"Machine Learning for NLP"},"forum":"main.2632","id":"main.2632","presentation_id":"38939159"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2635.png","content":{"abstract":"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.","authors":["Shuohang Wang","Yuwei Fang","Siqi Sun","Zhe Gan","Yu Cheng","Jingjing Liu","Jing Jiang"],"demo_url":"","keywords":["pre-training encoder","large-scale tasks","question answering","predicting words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.30","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1339","main.891","main.1892","main.2491","main.148"],"title":"Cross-Thought for Sentence Encoder Pre-training","tldr":"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2635","id":"main.2635","presentation_id":"38939160"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2636.png","content":{"abstract":"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.","authors":["Lifu Tu","Tianyu Liu","Kevin Gimpel"],"demo_url":"","keywords":["natural processing","sequence labeling","semantic labeling","parsing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.449","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.148","main.2430","main.850","main.3348","main.1615"],"title":"An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks","tldr":"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, ...","track":"Machine Learning for NLP"},"forum":"main.2636","id":"main.2636","presentation_id":"38939161"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2638.png","content":{"abstract":"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability.  We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next.  The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.","authors":["Shiva Upadhye","Leon Bergen","Andrew Kehler"],"demo_url":"","keywords":["neural models","grammatical knowledge","referential biases","discourse ability"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.70","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3115","main.1455","main.3181","main.2452","main.1613"],"title":"Predicting Reference: What do Language Models Learn about Discourse Models?","tldr":"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability.  We add...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.2638","id":"main.2638","presentation_id":"38939162"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2640.png","content":{"abstract":"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge.  We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.","authors":["Pratyay Banerjee","Chitta Baral"],"demo_url":"","keywords":["data annotation","knowledge learning","knowledge","self-supervised task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.11","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["demo.93","main.2635","main.3140","main.2337","main.2476"],"title":"Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering","tldr":"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on...","track":"Question Answering"},"forum":"main.2640","id":"main.2640","presentation_id":"38939163"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2641.png","content":{"abstract":"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes  intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.","authors":["Weijia Xu","Batool Haider","Saab Mansour"],"demo_url":"","keywords":["natural understanding","natural","nlu","goal-oriented systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.410","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3046","main.870","main.1503","main.852"],"title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU","tldr":"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes  intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label p...","track":"Dialog and Interactive Systems"},"forum":"main.2641","id":"main.2641","presentation_id":"38939164"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2644.png","content":{"abstract":"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","authors":["Nikita Nangia","Clara Vania","Rasika Bhalerao","Samuel R. Bowman"],"demo_url":"","keywords":["nlp tasks","pretrained models","masked models","mlms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.154","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2886","main.834","main.353","TACL.2011","main.2893"],"title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models","tldr":"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicit...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2644","id":"main.2644","presentation_id":"38939165"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2650.png","content":{"abstract":"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS and BART on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.","authors":["Jiacheng Xu","Shrey Desai","Greg Durrett"],"demo_url":"","keywords":["analyzing models","seqseq models","summarization decoders","pegasus"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.508","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1835","main.2470","main.2125","main.3437","main.471"],"title":"Understanding Neural Abstractive Summarization Models via Uncertainty","tldr":"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and white...","track":"Summarization"},"forum":"main.2650","id":"main.2650","presentation_id":"38939166"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2651.png","content":{"abstract":"Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators\u2019 political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators\u2019 attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.","authors":["Gregory Spell","Brian Guay","Sunshine Hillygus","Lawrence Carin"],"demo_url":"","keywords":["tweeting","embedding-based model","vector embeddings","neural network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.46","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3D","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.789","main.2784","main.2996","main.2596","main.1675"],"title":"An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets","tldr":"Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators\u2019 political attitudes. In this paper we introduce a method of measurin...","track":"Computational Social Science and Social Media"},"forum":"main.2651","id":"main.2651","presentation_id":"38939167"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2661.png","content":{"abstract":"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.","authors":["Anna Currey","Prashant Mathur","Georgiana Dinu"],"demo_url":"","keywords":["translation","neural translation","multi-domain model","high-resource conditions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.364","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.856","main.1960","main.891","main.3337","main.1100"],"title":"Distilling Multiple Domains for Neural Machine Translation","tldr":"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale wel...","track":"Machine Translation and Multilinguality"},"forum":"main.2661","id":"main.2661","presentation_id":"38939168"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.267.png","content":{"abstract":"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.","authors":["Nils Reimers","Iryna Gurevych"],"demo_url":"","keywords":["training","sentence models","monolingual models","monolingual model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.365","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.410","main.3688","main.852","main.870","main.3116"],"title":"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation","tldr":"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should...","track":"Machine Translation and Multilinguality"},"forum":"main.267","id":"main.267","presentation_id":"38938673"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2674.png","content":{"abstract":"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.","authors":["Wenxiang Jiao","Xing Wang","Shilin He","Irwin King","Michael Lyu","Zhaopeng Tu"],"demo_url":"","keywords":["data rejuvenation","neural models","nmt models","identification model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.176","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3227","TACL.2047","main.894","main.522","main.1960"],"title":"Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation","tldr":"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to...","track":"Machine Translation and Multilinguality"},"forum":"main.2674","id":"main.2674","presentation_id":"38939169"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2675.png","content":{"abstract":"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are  interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely  related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard  monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches out-perform existing ones on all languages by up to 11.4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly low-resource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages.","authors":["Manuel Mager","\u00d6zlem \u00c7etino\u011flu","Katharina Kann"],"demo_url":"","keywords":["morphological generation","canonical segmentation","lstm pointer-generator","sequence-to-sequence model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.423","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3656","main.852","main.143","main.2298","main.522"],"title":"Tackling the Low-resource Challenge for Canonical Segmentation","tldr":"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are  interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2675","id":"main.2675","presentation_id":"38939170"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2684.png","content":{"abstract":"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Few-shot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.","authors":["Haoyue Shi","Karen Livescu","Kevin Gimpel"],"demo_url":"","keywords":["few-shot parsing","unsupervised parsing","hyperparameter tuning","model selection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.614","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1494","main.143","TACL.2411","main.3540","main.2851"],"title":"On the Role of Supervision in Unsupervised Constituency Parsing","tldr":"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existi...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2684","id":"main.2684","presentation_id":"38939171"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2688.png","content":{"abstract":"When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data. Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.","authors":["Roy Bar-Haim","Yoav Kantor","Lilach Eden","Roni Friedman","Dan Lahav","Noam Slonim"],"demo_url":"","keywords":["multi-document summarization","key analysis","automatic points","automatic analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.3","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1A","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1023","main.3389","main.2506","main.965","main.1159"],"title":"Quantitative argument summarization and beyond: Cross-domain key point analysis","tldr":"When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on c...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.2688","id":"main.2688","presentation_id":"38939172"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2696.png","content":{"abstract":"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.","authors":["Satwik Bhattamishra","Kabir Ahuja","Navin Goyal"],"demo_url":"","keywords":["nlp tasks","construction","transformers","lstms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.576","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2414","main.1130","main.2179","TACL.2411"],"title":"On the Ability and Limitations of Transformers to Recognize Formal Languages","tldr":"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular la...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2696","id":"main.2696","presentation_id":"38939173"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2702.png","content":{"abstract":"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy.  We show that using an extension of probabilistic context-free grammar  model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more `abstract' categories (e.g., +55.1% recall on VPs).","authors":["Yanpeng Zhao","Ivan Titov"],"demo_url":"","keywords":["exploiting groundings","language understanding","gradient estimates","fully-differentiable learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.354","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1388","main.3360","main.2122","main.2083","TACL.2411"],"title":"Visually Grounded Compound PCFGs","tldr":"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2702","id":"main.2702","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2705.png","content":{"abstract":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","authors":["Swabha Swayamdipta","Roy Schwartz","Nicholas Lourie","Yizhong Wang","Hannaneh Hajishirzi","Noah A. Smith","Yejin Choi"],"demo_url":"","keywords":["nlp research","out-of-distribution generalization","model optimization","data maps"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.746","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2886","main.3648","main.387","main.2535"],"title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","tldr":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leve...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2705","id":"main.2705","presentation_id":"38939175"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2707.png","content":{"abstract":"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.","authors":["Ashish Sharma","Adam Miner","David Atkins","Tim Althoff"],"demo_url":"","keywords":["mental support","empathy measurement","empathy","communication empathy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.425","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.165","main.3072","main.2561","main.476","main.3352"],"title":"A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support","tldr":"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platf...","track":"Computational Social Science and Social Media"},"forum":"main.2707","id":"main.2707","presentation_id":"38939176"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2712.png","content":{"abstract":"Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.","authors":["Emily Allaway","Kathleen McKeown"],"demo_url":"","keywords":["stance detection","zero-shot detection","classifying stance","linguistic phenomena"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.717","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1952","main.2792","main.2996","main.30","main.1287"],"title":"Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations","tldr":"Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifyi...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.2712","id":"main.2712","presentation_id":"38939177"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2718.png","content":{"abstract":"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.","authors":["Arturo Oncevay","Barry Haddow","Alexandra Birch"],"demo_url":"","keywords":["multilingual translation","language clustering","multilingual transfer","singular analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.187","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["CL.2","main.3116","main.750","main.870","main.2891"],"title":"Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations","tldr":"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisati...","track":"Machine Translation and Multilinguality"},"forum":"main.2718","id":"main.2718","presentation_id":"38939178"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2721.png","content":{"abstract":"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.","authors":["Daniel Khashabi","Tushar Khot","Ashish Sabharwal"],"demo_url":"","keywords":["linguistic tasks","deep models","boolq","boolq models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.12","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2586","main.3054","main.2068","main.3140"],"title":"More Bang for Your Buck: Natural Perturbation for Robust Question Answering","tldr":"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so b...","track":"Question Answering"},"forum":"main.2721","id":"main.2721","presentation_id":"38939179"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2724.png","content":{"abstract":"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.","authors":["Xiaoyu Yang","Feng Nie","Yufei Feng","Quan Liu","Zhigang Chen","Xiaodan Zhu"],"demo_url":"","keywords":["fact verification","real-life applications","symbolic operations","programs"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.628","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.782","main.1086","main.1010","main.2506","main.574"],"title":"Program Enhanced Fact Verification with Verbalization and Graph Attention Network","tldr":"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. I...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2724","id":"main.2724","presentation_id":"38939180"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2733.png","content":{"abstract":"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.","authors":["Bosheng Ding","Linlin Liu","Lidong Bing","Canasai Kruengkrai","Thien Hai Nguyen","Shafiq Joty","Luo Si","Chunyan Miao"],"demo_url":"","keywords":["machine learning","generalization","low-resource tasks","named recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.488","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2068","main.2078","main.345","main.148","main.3540"],"title":"DAGA: Data Augmentation with a Generation Approach forLow-resource Tagging Tasks","tldr":"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks ...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2733","id":"main.2733","presentation_id":"38939181"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2739.png","content":{"abstract":"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.","authors":["Shachar Rosenman","Alon Jacovi","Yoav Goldberg"],"demo_url":"","keywords":["data process","re collection","sota models","tacred"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.302","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1923","main.2763","main.3470","main.3506"],"title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data","tldr":"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on T...","track":"Information Extraction"},"forum":"main.2739","id":"main.2739","presentation_id":"38939182"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2746.png","content":{"abstract":"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.","authors":["Ahmed El-Kishky","Vishrav Chaudhary","Francisco Guzm\u00e1n","Philipp Koehn"],"demo_url":"","keywords":["cross-lingual alignment","mining sentences","cross-lingual nlp","cross-lingual representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.480","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1298","main.1061","main.3453","main.2131","main.3046"],"title":"CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs","tldr":"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with a...","track":"Machine Translation and Multilinguality"},"forum":"main.2746","id":"main.2746","presentation_id":"38939183"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2750.png","content":{"abstract":"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim's factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","authors":["Adithya Pratapa","Sai Muralidhar Jayanthi","Kavya Nerella"],"demo_url":"","keywords":["fact-verification systems","shared tasks","reasoning process","fact-verification"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.629","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1086","main.2054","main.2570","main.3035","main.2416"],"title":"Constrained Fact Verification for FEVER","tldr":"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim's factuality, there is little work on understanding ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2750","id":"main.2750","presentation_id":"38939184"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2756.png","content":{"abstract":"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.","authors":["Xuemeng Hu","Rui Wang","Deyu Zhou","Yuxuan Xiong"],"demo_url":"","keywords":["neural modeling","deep models","adversarial-neural model","adversarially network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.725","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.30","TACL.2093","TACL.2083","main.2430","main.2931"],"title":"Neural Topic Modeling with Cycle-Consistent Adversarial Training","tldr":"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior t...","track":"Information Retrieval and Text Mining"},"forum":"main.2756","id":"main.2756","presentation_id":"38939185"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2758.png","content":{"abstract":"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset  will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.","authors":["Khyathi Raghavi Chandu","Ruo-Ping Dong","Alan W Black"],"demo_url":"","keywords":["generating narratives","artificial intelligence","prediction steps","generating steps"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.93","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.390","main.2702","main.252","main.1928","main.373"],"title":"Reading Between the Lines: Exploring Infilling in Visual Narratives","tldr":"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The gene...","track":"Language Generation"},"forum":"main.2758","id":"main.2758","presentation_id":"38939186"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2761.png","content":{"abstract":"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.","authors":["Yuwei Fang","Siqi Sun","Zhe Gan","Rohit Pillai","Shuohang Wang","Jingjing Liu"],"demo_url":"","keywords":["multi-hop answering","paragraph selection","supporting extraction","answer prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.710","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.782","main.574","main.1648","TACL.2121","main.158"],"title":"Hierarchical Graph Network for Multi-hop Question Answering","tldr":"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity ...","track":"Question Answering"},"forum":"main.2761","id":"main.2761","presentation_id":"38939187"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2763.png","content":{"abstract":"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","authors":["Taylor Shin","Yasaman Razeghi","Robert L. Logan IV","Eric Wallace","Sameer Singh"],"demo_url":"","keywords":["pretraining","fill-in-the-blanks problems","cloze tests","sentiment analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.346","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8B","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.1983","main.2630","main.2590","main.3506","main.41"],"title":"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts","tldr":"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging su...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2763","id":"main.2763","presentation_id":"38939188"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2764.png","content":{"abstract":"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.","authors":["Deyu Zhou","Xuemeng Hu","Rui Wang"],"demo_url":"","keywords":["natural community","graph networks","gnns","message passing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.310","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.782","main.1488","TACL.2121","main.2761","main.151"],"title":"Neural Topic Modeling by Incorporating Document Relationship Graph","tldr":"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural...","track":"NLP Applications"},"forum":"main.2764","id":"main.2764","presentation_id":"38939189"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2766.png","content":{"abstract":"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis  and discuss the challenges for the computational pun models.","authors":["Zhiwei Yu","Hongyu Zang","Xiaojun Wan"],"demo_url":"","keywords":["punning","error analysis","computational models","homophones"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.229","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.920","main.2208","main.1658","TACL.2013","main.1625"],"title":"Homophonic Pun Generation with Lexically Constrained Rewriting","tldr":"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity...","track":"Language Generation"},"forum":"main.2766","id":"main.2766","presentation_id":"38939190"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2767.png","content":{"abstract":"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically \"refills\" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.","authors":["Kevin Yang","Violet Yao","John DeNero","Dan Klein"],"demo_url":"","keywords":["variable-length decoding","decoding","variable-width search","semantic parsing"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.366","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2169","main.3236","main.1960","main.3074","TACL.1943"],"title":"A Streaming Approach For Efficient Batched Beam Search","tldr":"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically \"refills\" the batch before proceeding w...","track":"Machine Translation and Multilinguality"},"forum":"main.2767","id":"main.2767","presentation_id":"38939191"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2777.png","content":{"abstract":"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.","authors":["Emrah Budur","R\u0131za \u00d6z\u00e7elik","Tunga Gungor","Christopher Potts"],"demo_url":"","keywords":["nli","morphological parsing","commercial systems","in-language embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.662","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1379","main.870","main.3597","main.1061","main.522"],"title":"Data and Representation for Turkish Natural Language Inference","tldr":"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, com...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2777","id":"main.2777","presentation_id":"38939192"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2779.png","content":{"abstract":"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.","authors":["Zhiwei Yu","Hongyu Zang","Xiaojun Wan"],"demo_url":"","keywords":["recipe generation","content selection","recipe task","routing method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.311","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3327","main.2943","demo.119","main.3541","main.2382"],"title":"Routing Enforced Generative Model for Recipe Generation","tldr":"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much inform...","track":"NLP Applications"},"forum":"main.2779","id":"main.2779","presentation_id":"38939193"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2783.png","content":{"abstract":"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.","authors":["Wei Zhang","Lu Hou","Yichun Yin","Lifeng Shang","Xiao Chen","Xin Jiang","Qun Liu"],"demo_url":"","keywords":["natural tasks","training process","transformer-based models","bert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.37","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.956","main.3543","main.1485","main.1552","main.3394"],"title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT","tldr":"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained device...","track":"Machine Learning for NLP"},"forum":"main.2783","id":"main.2783","presentation_id":"38939194"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2784.png","content":{"abstract":"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology --left, center, or right--, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.","authors":["Ramy Baly","Giovanni Da San Martino","James Glass","Preslav Nakov"],"demo_url":"","keywords":["article-level prediction","adversarial adaptation","pre-trained transformers","leading ideology"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.404","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3644","main.635","main.2651","main.789","main.2430"],"title":"We Can Detect Your Bias: Predicting the Political Ideology of News Articles","tldr":"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology --left, center, or right--, which is well-...","track":"Machine Learning for NLP"},"forum":"main.2784","id":"main.2784","presentation_id":"38939195"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.279.png","content":{"abstract":"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.","authors":["Zhenjie Zhao","Evangelos Papalexakis","Xiaojuan Ma"],"demo_url":"","keywords":["human-robot interaction","physical learning","natural processing","model generalization"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.266","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.666","main.2873","main.684","main.923","main.1010"],"title":"Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization","tldr":"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer fro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.279","id":"main.279","presentation_id":"38938674"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2790.png","content":{"abstract":"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over \\emph{well formed} relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also \\emph{semantically similar}. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.","authors":["Michal Lukasik","Himanshu Jain","Aditya Menon","Seungyeon Kim","Srinadh Bhojanapalli","Felix Yu","Sanjiv Kumar"],"demo_url":"","keywords":["classification","label de-noising","seqseq settings","machine translation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.405","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.148","main.1720","main.891","main.1356","main.3609"],"title":"Semantic Label Smoothing for Sequence to Sequence Problems","tldr":"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challe...","track":"Machine Learning for NLP"},"forum":"main.2790","id":"main.2790","presentation_id":"38939196"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2792.png","content":{"abstract":"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.","authors":["Suzanna Sia","Ayush Dalmia","Sabrina J. Mielke"],"demo_url":"","keywords":["weighted clustering","reranking words","dimensionality reduction","topic models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.135","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2931","main.2596","main.30","main.3093"],"title":"Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!","tldr":"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain ...","track":"Information Retrieval and Text Mining"},"forum":"main.2792","id":"main.2792","presentation_id":"38939197"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2793.png","content":{"abstract":"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient --- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.","authors":["Trapit Bansal","Rishikesh Jha","Tsendsuren Munkhdalai","Andrew McCallum"],"demo_url":"","keywords":["nlp applications","fine-tuning","meta-learning problem","supervised tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.38","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.16","main.74","main.345","main.1482","main.2893"],"title":"Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks","tldr":"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fi...","track":"Machine Learning for NLP"},"forum":"main.2793","id":"main.2793","presentation_id":"38939198"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2795.png","content":{"abstract":"AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.","authors":["Yan Zhang","Zhijiang Guo","Zhiyang Teng","Wei Lu","Shay B. Cohen","Zuozhu Liu","Lidong Bing"],"demo_url":"","keywords":["amr-to-text generation","graph representations","graph gcns","gcns"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.169","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2422","main.1485","TACL.2121","main.1707","main.471"],"title":"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation","tldr":"AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to e...","track":"Language Generation"},"forum":"main.2795","id":"main.2795","presentation_id":"38939199"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2799.png","content":{"abstract":"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by $6\\%$ to $16\\%$ absolute points over prior meta-learning based systems.","authors":["Yi Yang","Arzoo Katiyar"],"demo_url":"","keywords":["few-shot tasks","nearest learning","structured inference","supervised model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.516","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3217","main.989","TACL.2103","main.1738","main.2974"],"title":"Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning","tldr":"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, ...","track":"Information Extraction"},"forum":"main.2799","id":"main.2799","presentation_id":"38939200"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2809.png","content":{"abstract":"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).  For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard benchmark datasets. We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques. Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models. To our knowledge, this is one of the first works that use GANs for keyphrase generation. We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.","authors":["Avinash Swaminathan","Haimin Zhang","Debanjan Mahata","Rakesh Gosangi","Rajiv Ratn Shah","Amanda Stent"],"demo_url":"","keywords":["generation keyphrases","keyphrase generation","keyphrase approach","generative gans"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.645","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2313","demo.118","main.1490","main.3126"],"title":"A Preliminary Exploration of GANs for Keyphrase Generation","tldr":"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).  For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated key...","track":"Summarization"},"forum":"main.2809","id":"main.2809","presentation_id":"38939201"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2814.png","content":{"abstract":"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using ``fidelity curves'' to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.","authors":["Samuel Carton","Anirudh Rathore","Chenhao Tan"],"demo_url":"","keywords":["evaluating rationales","model retraining","human rationales","rationales"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.747","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2307","main.959","TACL.2049","main.3648","main.2258"],"title":"Evaluating and Characterizing Human Rationales","tldr":"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rational...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2814","id":"main.2814","presentation_id":"38939202"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2818.png","content":{"abstract":"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.","authors":["Jiaji Huang","Xingyu Cai","Kenneth Church"],"demo_url":"","keywords":["monolingual task","bilingual induction","low regime","hubness"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.100","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1901","main.3181","main.2891","main.865","main.3115"],"title":"Improving Bilingual Lexicon Induction for Low Frequency Words","tldr":"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondl...","track":"Machine Learning for NLP"},"forum":"main.2818","id":"main.2818","presentation_id":"38939203"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2825.png","content":{"abstract":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.","authors":["Sean MacAvaney","Arman Cohan","Nazli Goharian"],"demo_url":"","keywords":["zero-shot algorithm","neural model","zero-shot methods","covid- search"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.341","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["demo.124","demo.109","main.748","main.1631","main.2943"],"title":"SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search","tldr":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these ar...","track":"Information Retrieval and Text Mining"},"forum":"main.2825","id":"main.2825","presentation_id":"38939204"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2834.png","content":{"abstract":"Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs. However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available. In this paper, we study \\emph{ensemble distillation} as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles. We validate this framework on two tasks: named-entity recognition and machine translation. We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.","authors":["Steven Reich","David Mueller","Nicholas Andrews"],"demo_url":"","keywords":["structured prediction","named-entity recognition","machine translation","ensemble distillation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.450","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2615","main.2491","TACL.2411","main.345","main.1923"],"title":"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast\u2014Choose Three","tldr":"Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be use...","track":"Machine Learning for NLP"},"forum":"main.2834","id":"main.2834","presentation_id":"38939205"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2838.png","content":{"abstract":"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained  throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.","authors":["Zirui Wang","Sanket Vaibhav Mehta","Barnabas Poczos","Jaime Carbonell"],"demo_url":"","keywords":["lifelong learning","local adaptation","text benchmarks","multi-task learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.39","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1734","main.1208","TACL.2041","main.74","main.3470"],"title":"Efficient Meta Lifelong-Learning with Limited Memory","tldr":"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained  throughout their lifetime, a challenge known as lifelong learning. Sta...","track":"Machine Learning for NLP"},"forum":"main.2838","id":"main.2838","presentation_id":"38939206"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2839.png","content":{"abstract":"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work.","authors":["Jie Lei","Licheng Yu","Tamara Berg","Mohit Bansal"],"demo_url":"","keywords":["video-and-language prediction","ai models","vlep","adversarial procedure"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.706","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1085","main.645","main.1797","main.3179","TACL.2143"],"title":"What is More Likely to Happen Next? Video-and-Language Future Event Prediction","tldr":"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of co...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2839","id":"main.2839","presentation_id":"38939207"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.284.png","content":{"abstract":"Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family \u2013 LXMERT \u2013 finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT\u2019s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.","authors":["Jaemin Cho","Jiasen Lu","Dustin Schwenk","Hannaneh Hajishirzi","Aniruddha Kembhavi"],"demo_url":"","keywords":["multimodal tasks","visual answering","visual grounding","generative captioning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.707","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2322","TACL.2041","main.3360","main.2758","main.3183"],"title":"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","tldr":"Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual gro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.284","id":"main.284","presentation_id":"38938675"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2847.png","content":{"abstract":"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language's inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.","authors":["Sarah Moeller","Ling Liu","Changbing Yang","Katharina Kann","Mans Hulden"],"demo_url":"","keywords":["linguistic analysis","natural systems","igt-to-paradigms","igtp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.424","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.870","main.143","main.2363","main.1970"],"title":"IGT2P: From Interlinear Glossed Texts to Paradigms","tldr":"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses ab...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.2847","id":"main.2847","presentation_id":"38939208"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2849.png","content":{"abstract":"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities'', \"the capital cities'' and \"two European cities''. Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.","authors":["Cl\u00e9ment Jumel","Annie Louis","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["generation","paraphrasing","semantic entities","tesa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.646","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.911","main.327","main.3010","main.1528","main.3216"],"title":"TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization","tldr":"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities'', \"the capital cities'' an...","track":"Summarization"},"forum":"main.2849","id":"main.2849","presentation_id":"38939209"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2851.png","content":{"abstract":"Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models' syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.","authors":["Ethan Wilcox","Peng Qian","Richard Futrell","Ryosuke Kohita","Roger Levy","Miguel Ballesteros"],"demo_url":"","keywords":["learning outcomes","syntactic representations","neural models","n-gram baseline"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.375","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2411","main.1892","main.1613","main.2430","main.76"],"title":"Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models","tldr":"Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this beha...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.2851","id":"main.2851","presentation_id":"38939210"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2853.png","content":{"abstract":"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.","authors":["Linjie Li","Yen-Chun Chen","Yu Cheng","Zhe Gan","Licheng Yu","Jingjing Liu"],"demo_url":"","keywords":["large-scale learning","pre-training tasks","video-subtitle matching","text-based retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.161","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.355","TACL.2107","main.1113","main.3483","main.628"],"title":"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training","tldr":"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal f...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2853","id":"main.2853","presentation_id":"38939211"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.286.png","content":{"abstract":"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.","authors":["Yuning Mao","Yanru Qu","Yiqing Xie","Xiang Ren","Jiawei Han"],"demo_url":"","keywords":["single-document summarization","single-document sds","multi-document summarization","multi-document mds"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.136","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.965","main.471","main.714","main.1023","main.1504"],"title":"Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning","tldr":"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS...","track":"Information Retrieval and Text Mining"},"forum":"main.286","id":"main.286","presentation_id":"38938676"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2864.png","content":{"abstract":"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.","authors":["Anthony Chen","Gabriel Stanovsky","Sameer Singh","Matt Gardner"],"demo_url":"","keywords":["reading comprehension","generation problem","mocha","lerc"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.528","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3186","main.2973","main.2258","TACL.2049","main.449"],"title":"MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics","tldr":"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token o...","track":"Language Generation"},"forum":"main.2864","id":"main.2864","presentation_id":"38939212"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2865.png","content":{"abstract":"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA  models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.","authors":["Runzhi Tian","Yongyi Mao","Richong Zhang"],"demo_url":"","keywords":["learning models","learning","vae","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.101","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2430","main.1446","main.30","main.1498"],"title":"Learning VAE-LDA Models with Rounded Reparameterization Trick","tldr":"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameteri...","track":"Machine Learning for NLP"},"forum":"main.2865","id":"main.2865","presentation_id":"38939213"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.287.png","content":{"abstract":"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.","authors":["Kun Qian","Poornima Chozhiyath Raman","Yunyao Li","Lucian Popa"],"demo_url":"","keywords":["entity-related tasks","entity normalization","variant generation","implicit names"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.517","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.1528","main.666","main.327","main.911"],"title":"Learning Structured Representations of Entity Names using ActiveLearning and Weak Supervision","tldr":"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is partic...","track":"Information Extraction"},"forum":"main.287","id":"main.287","presentation_id":"38938677"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2873.png","content":{"abstract":"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human\u2019s ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models. The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.","authors":["Xiaoyu Kou","Yankai Lin","Shaobo Liu","Peng Li","Jie Zhou","Yan Zhang"],"demo_url":"","keywords":["real-world applications","catastrophic problem","graph methods","ge models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.237","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1508","main.658","main.279","TACL.2121"],"title":"Disentangle-based Continual Graph Representation Learning","tldr":"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since...","track":"Information Extraction"},"forum":"main.2873","id":"main.2873","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2877.png","content":{"abstract":"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations. Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.","authors":["Zhichun Wang","Jinjian Yang","Xiaoju Ye"],"demo_url":"","keywords":["knowledge fusion","knowledge integration","kg alignment","knowledge alignment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.130","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1787","main.2974","main.1706","main.300","main.666"],"title":"Knowledge Graph Alignment with Entity-Pair Embedding","tldr":"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These a...","track":"Information Extraction"},"forum":"main.2877","id":"main.2877","presentation_id":"38939215"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2886.png","content":{"abstract":"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.  Various metrics have been proposed to quantify biases in model predictions.  In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering.  Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.","authors":["Jieyu Zhao","Kai-Wei Chang"],"demo_url":"","keywords":["evaluating bias","toxicity classification","object tasks","machine techniques"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.155","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.802","main.2076","main.2644","main.2122","main.345"],"title":"LOGAN: Local Group Bias Detection by Clustering","tldr":"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.  Various metrics have been proposed to...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2886","id":"main.2886","presentation_id":"38939216"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2890.png","content":{"abstract":"We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in  labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.","authors":["Maryam Aminian","Mohammad Sadegh Rasooli","Mona Diab"],"demo_url":"","keywords":["syntactic parsing","auxiliary task","multitask setup","annotation projection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.663","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1061","main.891","main.1957","main.750","main.1379"],"title":"Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies","tldr":"We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic pa...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2890","id":"main.2890","presentation_id":"38939217"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2891.png","content":{"abstract":"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation. Since our approach is language independent, we perform WSD experiments on several languages. The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD. To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.","authors":["Yixing Luan","Bradley Hauer","Lili Mou","Grzegorz Kondrak"],"demo_url":"","keywords":["monolingual disambiguation","monolingual wsd","english wsd","wsd systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.332","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3224","main.1935","main.1503","main.2718","main.639"],"title":"Improving Word Sense Disambiguation with Translations","tldr":"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generat...","track":"Semantics: Lexical Semantics"},"forum":"main.2891","id":"main.2891","presentation_id":"38939218"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2893.png","content":{"abstract":"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.","authors":["Alex Warstadt","Yian Zhang","Xiaocheng Li","Haokun Liu","Samuel R. Bowman"],"demo_url":"","keywords":["self-supervised tasks","language understanding","ambiguous tasks","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.16","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2851","main.3023","main.2793","main.1146"],"title":"Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)","tldr":"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2893","id":"main.2893","presentation_id":"38939219"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2894.png","content":{"abstract":"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.","authors":["Matthew Sims","David Bamman"],"demo_url":"","keywords":["modeling propagation","information propagation","speaker attribution","structural holes"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.47","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3D","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3357","main.868","main.151","main.2996","main.2764"],"title":"Measuring Information Propagation in Literary Social Networks","tldr":"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pi...","track":"Computational Social Science and Social Media"},"forum":"main.2894","id":"main.2894","presentation_id":"38939220"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2895.png","content":{"abstract":"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.","authors":["Wenjuan Han","Liwen Zhang","Yong Jiang","Kewei Tu"],"demo_url":"","keywords":["adversarial attacks","classification problems","structured tasks","nlp tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.182","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2914","demo.104","main.2313","main.47","main.1614"],"title":"Adversarial Attack and Defense of Structured Prediction Models","tldr":"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classifica...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.2895","id":"main.2895","presentation_id":"38939221"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2900.png","content":{"abstract":"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.","authors":["Kexiang Wang","Baobao Chang","Zhifang Sui"],"demo_url":"","keywords":["multi-document summarization","mds task","combinatorial impact","multi-document mds"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.32","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.540","main.286","main.3012","main.1123","main.471"],"title":"A Spectral Method for Unsupervised Multi-Document Summarization","tldr":"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called s...","track":"Summarization"},"forum":"main.2900","id":"main.2900","presentation_id":"38939222"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2914.png","content":{"abstract":"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models.  Our code is publicly available at https://github.com/AI-secure/T3/.","authors":["Boxin Wang","Hengzhi Pei","Boyuan Pan","Qian Chen","Shuohang Wang","Bo Li"],"demo_url":"","keywords":["adversarial generation","nlp tasks","sentiment analysis","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.495","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2313","main.2895","main.47","demo.104","main.426"],"title":"T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack","tldr":"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to...","track":"Machine Learning for NLP"},"forum":"main.2914","id":"main.2914","presentation_id":"38939223"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2915.png","content":{"abstract":"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.","authors":["Ashkan Alinejad","Anoop Sarkar"],"demo_url":"","keywords":["automatic task","neural task","speech translation","end-to-end approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.644","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2107","main.1339","TACL.2221","main.856","main.2661"],"title":"Effectively pretraining a speech translation decoder with Machine Translation data","tldr":"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the re...","track":"Speech and Multimodality"},"forum":"main.2915","id":"main.2915","presentation_id":"38939224"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2916.png","content":{"abstract":"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.  Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness.  Additionally, we evaluate how a phrase-based data augmentation method can improve performance.  We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.  Data augmentation further improves control on difficult, randomly generated utterance plans.","authors":["Chris Kedzie","Kathleen McKeown"],"demo_url":"","keywords":["natural generation","training","data augmentation","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.419","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3353","main.2389","main.699","main.1006","main.2511"],"title":"Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies","tldr":"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.  Using two task-oriented dialogue generation benchmarks, we systematically...","track":"Language Generation"},"forum":"main.2916","id":"main.2916","presentation_id":"38939225"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2920.png","content":{"abstract":"In politics, neologisms are frequently invented for partisan objectives. For example, ``undocumented workers\u201d and ``illegal aliens\u201d refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g.,  \u201cimmigrants\" vs. \u201caliens\", \u201cestate tax\" vs. \u201cdeath tax\") move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.","authors":["Albert Webson","Zhizhong Chen","Carsten Eickhoff","Ellie Pavlick"],"demo_url":"","keywords":["reference-based theories","nlp","intrinsic interpretability","extrinsic application"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.335","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7C","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2072","main.3450","main.32","TACL.2011","main.353"],"title":"Do \u201cUndocumented Workers\u201d == \u201cIllegal Aliens\u201d? Differentiating Denotation and Connotation in Vector Spaces","tldr":"In politics, neologisms are frequently invented for partisan objectives. For example, ``undocumented workers\u201d and ``illegal aliens\u201d refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations...","track":"Semantics: Lexical Semantics"},"forum":"main.2920","id":"main.2920","presentation_id":"38939226"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2922.png","content":{"abstract":"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. The data set is publicly available.","authors":["Jiarui Yao","Haoling Qiu","Bonan Min","Nianwen Xue"],"demo_url":"","keywords":["temporal anaphora","temporal annotation","tdgs","representation scheme"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.432","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2512","main.574","main.2739","main.1569","main.3057"],"title":"Annotating Temporal Dependency Graphs via Crowdsourcing","tldr":"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous ...","track":"Information Extraction"},"forum":"main.2922","id":"main.2922","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2927.png","content":{"abstract":"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents' verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.","authors":["Hisashi Kamezawa","Noriki Nishida","Nobuyuki Shimizu","Takashi Miyazaki","Hideki Nakayama"],"demo_url":"","keywords":["real-world dialogue","social interactions","production responses","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.267","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1009","main.373","main.1201","main.647","main.128"],"title":"A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses","tldr":"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In th...","track":"Dialog and Interactive Systems"},"forum":"main.2927","id":"main.2927","presentation_id":"38939228"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2931.png","content":{"abstract":"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.","authors":["Alexander Miserlis Hoyle","Pranav Goel","Philip Resnik"],"demo_url":"","keywords":["topic models","knowledge distillation","probabilistic models","pretrained transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.137","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2792","TACL.2093","main.1952","main.2476","main.3581"],"title":"Improving Neural Topic Models using Knowledge Distillation","tldr":"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular m...","track":"Information Retrieval and Text Mining"},"forum":"main.2931","id":"main.2931","presentation_id":"38939229"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2938.png","content":{"abstract":"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.","authors":["Dianbo Sui","Yubo Chen","Jun Zhao","Yantao Jia","Yuantao Xie","Weijian Sun"],"demo_url":"","keywords":["privacy protection","knowledge distillation","medical model","privacy-preserving model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.165","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.426","main.748","main.1923","demo.119","demo.71"],"title":"FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction","tldr":"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts a...","track":"Information Extraction"},"forum":"main.2938","id":"main.2938","presentation_id":"38939230"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2943.png","content":{"abstract":"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named \\model, performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences \\emph{independently} of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.","authors":["Dirk Groeneveld","Tushar Khot","Mausam","Ashish Sabharwal"],"demo_url":"","keywords":["multi-hop answering","named recognition","graph-based reasoning","question decomposition"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.711","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.449","main.3517","demo.54","main.1032","main.210"],"title":"A Simple Yet Strong Pipeline for HotpotQA","tldr":"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. How...","track":"Question Answering"},"forum":"main.2943","id":"main.2943","presentation_id":"38939231"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2947.png","content":{"abstract":"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.","authors":["Chen Jia","Yuefeng Shi","Qinrong Yang","Yue Zhang"],"demo_url":"","keywords":["chinese ner","pre-training","ner fine-tuning","ner"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.518","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.989","demo.49","main.1738","main.3216","main.2635"],"title":"Entity Enhanced BERT Pre-training for Chinese NER","tldr":"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhance...","track":"Information Extraction"},"forum":"main.2947","id":"main.2947","presentation_id":"38939232"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2958.png","content":{"abstract":"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND -- a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).","authors":["Piyawat Lertvittayakumjorn","Lucia Specia","Francesca Toni"],"demo_url":"","keywords":["real-world classifiers","classifiers","find","deep classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.24","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2585","main.3540","main.1834","main.1575","main.76"],"title":"FIND: Human-in-the-Loop Debugging Deep Text Classifiers","tldr":"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. Th...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2958","id":"main.2958","presentation_id":"38939233"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2959.png","content":{"abstract":"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.","authors":["Lingkai Kong","Haoming Jiang","Yuchen Zhuang","Jie Lyu","Tuo Zhao","Chao Zhang"],"demo_url":"","keywords":["augmented training","in-distribution calibration","text classification","expectation error"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.102","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1458","main.3023","main.1834","main.2793","main.1046"],"title":"Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data","tldr":"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method int...","track":"Machine Learning for NLP"},"forum":"main.2959","id":"main.2959","presentation_id":"38939234"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2962.png","content":{"abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.","authors":["David Wadden","Shanchuan Lin","Kyle Lo","Lucy Lu Wang","Madeleine van Zuylen","Arman Cohan","Hannaneh Hajishirzi"],"demo_url":"","keywords":["scientific verification","scifact","domain techniques","covid-"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.609","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3151","main.2570","main.2117","TACL.2049","main.2506"],"title":"Fact or Fiction: Verifying Scientific Claims","tldr":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we...","track":"NLP Applications"},"forum":"main.2962","id":"main.2962","presentation_id":"38939235"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2972.png","content":{"abstract":"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.","authors":["Rujun Han","Yichao Zhou","Nanyun Peng"],"demo_url":"","keywords":["extracting relations","information extraction","natural understanding","maximum inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.461","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1116","main.1159","main.3462","main.1569","main.850"],"title":"Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction","tldr":"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the ta...","track":"Information Extraction"},"forum":"main.2972","id":"main.2972","presentation_id":"38939236"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2973.png","content":{"abstract":"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system's performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.","authors":["James Ferguson","Matt Gardner","Hannaneh Hajishirzi","Tushar Khot","Pradeep Dasigi"],"demo_url":"","keywords":["reading tasks","reading datasets","iirc","discrete reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.86","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.449","main.3186","main.928","main.2864","main.3529"],"title":"IIRC: A Dataset of Incomplete Information Reading Comprehension Questions","tldr":"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not eval...","track":"Question Answering"},"forum":"main.2973","id":"main.2973","presentation_id":"38939237"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2974.png","content":{"abstract":"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.","authors":["Ledell Wu","Fabio Petroni","Martin Josifoski","Sebastian Riedel","Luke Zettlemoyer"],"demo_url":"","keywords":["retrieval","non-zero-shot evaluations","bi-encoder linking","bert-based model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.519","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.300","main.2877","main.1787","main.1755"],"title":"Scalable Zero-shot Entity Linking with Dense Entity Retrieval","tldr":"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is ...","track":"Information Extraction"},"forum":"main.2974","id":"main.2974","presentation_id":"38939238"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2975.png","content":{"abstract":"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.","authors":["Qingfu Zhu","Wei-Nan Zhang","Ting Liu","William Yang Wang"],"demo_url":"","keywords":["open-domain generation","data problem","training","counterfactual reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.276","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.318","main.1196","main.664","main.3179","main.3495"],"title":"Counterfactual Off-Policy Training for Neural Dialogue Generation","tldr":"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfact...","track":"Dialog and Interactive Systems"},"forum":"main.2975","id":"main.2975","presentation_id":"38939239"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.298.png","content":{"abstract":"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.","authors":["Ming Wang","Yinglin Wang"],"demo_url":"","keywords":["word disambiguation","word","sense enhancement","contextual embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.504","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2251","main.3224","main.1935","main.2891","main.1395"],"title":"A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation","tldr":"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this ...","track":"Semantics: Lexical Semantics"},"forum":"main.298","id":"main.298","presentation_id":"38938678"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2982.png","content":{"abstract":"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.","authors":["Seraphina Goldfarb-Tarrant","Tuhin Chakrabarty","Ralph Weischedel","Nanyun Peng"],"demo_url":"","keywords":["story generation","high-quality planning","large models","plot-generation model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.351","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.390","main.2650","main.2758","main.2511","main.3010"],"title":"Content Planning for Neural Story Generation with Aristotelian Rescoring","tldr":"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be...","track":"Language Generation"},"forum":"main.2982","id":"main.2982","presentation_id":"38939240"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2989.png","content":{"abstract":"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.","authors":["Yun He","Ziwei Zhu","Yin Zhang","Qin Chen","James Caverlee"],"demo_url":"","keywords":["health-related tasks","consumer answering","medical inference","disease recognition"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.372","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.748","main.110","main.1528","TACL.2049","main.3151"],"title":"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","tldr":"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question an...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2989","id":"main.2989","presentation_id":"38939241"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2990.png","content":{"abstract":"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5~ outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.","authors":["Colin Clement","Dawn Drain","Jonathan Timcheck","Alexey Svyatkovskiy","Neel Sundaresan"],"demo_url":"","keywords":["automated understanding","docstring generation","method generation","docstring summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.728","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2590","main.2382","main.246","main.648","main.852"],"title":"PyMT5: multi-mode translation of natural language and Python code with transformers","tldr":"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transforme...","track":"NLP Applications"},"forum":"main.2990","id":"main.2990","presentation_id":"38939242"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2991.png","content":{"abstract":"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50\\% of the training data, SSCR achieves a comparable result to using complete data.","authors":["Tsu-Jui Fu","Xin Wang","Scott Grafton","Miguel Eckstein","William Yang Wang"],"demo_url":"","keywords":["iterative tasks","data scarcity","editing tasks","self-supervised scenario"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.357","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2758","main.284","main.373","main.2072","main.2382"],"title":"SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning","tldr":"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.2991","id":"main.2991","presentation_id":"38939243"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2994.png","content":{"abstract":"Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40,000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.","authors":["AmirAli Bagher Zadeh","Yansheng Cao","Simon Hessner","Paul Pu Liang","Soujanya Poria","Louis-Philippe Morency"],"demo_url":"","keywords":["modeling language","natural processing","multilingual studies","cmu-moseas"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.141","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2777","main.3566","main.1379","main.870","main.852"],"title":"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","tldr":"Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datase...","track":"Speech and Multimodality"},"forum":"main.2994","id":"main.2994","presentation_id":"38939244"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2995.png","content":{"abstract":"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.","authors":["Seungtaek Choi","Haeju Park","Jinyoung Yeo","Seung-won Hwang"],"demo_url":"","keywords":["attention supervision","machine-augmented supervision","text tasks","sentiment analysis"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.543","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.151","main.2733","main.1023","main.2476","main.3540"],"title":"Less is More: Attention Supervision with Counterfactuals for Text Classification","tldr":"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this g...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.2995","id":"main.2995","presentation_id":"38939245"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2996.png","content":{"abstract":"Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.","authors":["Jonathan Kobbe","Ioana Hulpus","Heiner Stuckenschmidt"],"demo_url":"","keywords":["online deliberation","stance claims","unsupervised method","topic-specific models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.4","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1A","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1750","main.2688","main.1287","main.789","main.2712"],"title":"Unsupervised stance detection for arguments from consequences","tldr":"Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a to...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.2996","id":"main.2996","presentation_id":"38939246"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.2999.png","content":{"abstract":"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalization\u2014the combination of input specification, loss function, and reuse of pretrained parameters\u2014by users of the dataset, rather than improvements in the pretrained model\u2019s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model\u2019s extreme sensitivity to hyperparameters.  We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.","authors":["Haokun Liu","William Huang","Dhara Mungra","Samuel R. Bowman"],"demo_url":"","keywords":["task formalization","input specification","ablation","formalization decisions"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.664","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.210","main.607","TACL.2041","main.3470","TACL.2411"],"title":"Precise Task Formalization Matters in Winograd Schema Evaluations","tldr":"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly la...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.2999","id":"main.2999","presentation_id":"38939247"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.30.png","content":{"abstract":"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts. Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics. We observe that our model can highly improve short text topic modeling performance. Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.","authors":["Xiaobao Wu","Chunping Li","Yan Zhu","Yishu Miao"],"demo_url":"","keywords":["decoding","short modeling","topic models","neural model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.138","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.2756","main.2792","TACL.2083","main.2430"],"title":"Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder","tldr":"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield rep...","track":"Information Retrieval and Text Mining"},"forum":"main.30","id":"main.30","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.300.png","content":{"abstract":"Few-shot Knowledge Graph\u00a0(KG) completion is a focus of current research, where each task aims at querying\u00a0unseen facts\u00a0of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at https://github.com/JiaweiSheng/FAAN.","authors":["Jiawei Sheng","Shu Guo","Zhenyu Chen","Juwei Yue","Lihong Wang","Tingwen Liu","Hongbo Xu"],"demo_url":"","keywords":["few-shot completion","knowledge acquisition","link prediction","adaptive network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.131","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2974","main.2761","main.1528","main.666","main.2426"],"title":"Adaptive Attentional Network for Few-Shot Knowledge Graph Completion","tldr":"Few-shot Knowledge Graph\u00a0(KG) completion is a focus of current research, where each task aims at querying\u00a0unseen facts\u00a0of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of e...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.300","id":"main.300","presentation_id":"38938679"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3010.png","content":{"abstract":"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.","authors":["Dongyeop Kang","Eduard Hovy"],"demo_url":"","keywords":["nlp tasks","guiding realization","paragraph task","content prediction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.529","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1892","main.2377","main.1023","main.2382","main.128"],"title":"Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task","tldr":"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how t...","track":"Language Generation"},"forum":"main.3010","id":"main.3010","presentation_id":"38939248"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3012.png","content":{"abstract":"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.","authors":["Hanlu Wu","Tengfei Ma","Lingfei Wu","Tariro Manyumwa","Shouling Ji"],"demo_url":"","keywords":["summarization task","document system","rouge","unsupervised learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.294","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.965","main.1023","main.3552","main.2125","main.471"],"title":"Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning","tldr":"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated refe...","track":"Summarization"},"forum":"main.3012","id":"main.3012","presentation_id":"38939249"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3013.png","content":{"abstract":"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.","authors":["Qianli Ma","Zhenxi Lin","Jiangyue Yan","Zipeng Chen","Liuhong Yu"],"demo_url":"","keywords":["sentence classification","extracting features","generalization","cnn models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.544","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2430","main.989","main.471","main.930","main.3656"],"title":"MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification","tldr":"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redun...","track":"NLP Applications"},"forum":"main.3013","id":"main.3013","presentation_id":"38939250"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3022.png","content":{"abstract":"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).","authors":["Yijun Wang","Changzhi Sun","Yuanbin Wu","Junchi Yan","Peng Gao","Guotong Xie"],"demo_url":"","keywords":["entity task","pre-trained encoder","general-purpose encoder","universal models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.132","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2635","main.3287","main.1803","main.1503","main.2877"],"title":"Pre-training Entity Relation Encoder with Intra-span and Inter-span Information","tldr":"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span...","track":"Information Extraction"},"forum":"main.3022","id":"main.3022","presentation_id":"38939251"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3023.png","content":{"abstract":"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.","authors":["Hai Ye","Qingyu Tan","Ruidan He","Juntao Li","Hwee Tou Ng","Lidong Bing"],"demo_url":"","keywords":["unsupervised adaptation","self-training","pre-trained models","bert"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.599","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.16","main.2893","main.852","main.2491","main.1631"],"title":"Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training","tldr":"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tun...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3023","id":"main.3023","presentation_id":"38939252"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3028.png","content":{"abstract":"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.","authors":["Faeze Brahman","Snigdha Chaturvedi"],"demo_url":"","keywords":["story process","emotion-reinforced models","emorl models","reinforcement learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.426","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.476","main.3532","main.2982","main.668","main.916"],"title":"Modeling Protagonist Emotions for Emotion-Aware Storytelling","tldr":"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that a...","track":"Computational Social Science and Social Media"},"forum":"main.3028","id":"main.3028","presentation_id":"38939253"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3032.png","content":{"abstract":"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI, a high-quality (91.5% coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel model architectures.","authors":["Niket Tandon","Keisuke Sakaguchi","Bhavana Dalvi","Dheeraj Rajagopal","Peter Clark","Michal Guerquin","Kyle Richardson","Eduard Hovy"],"demo_url":"","keywords":["tracking text","fog removal","task formulation","crowdsourcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.520","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.605","main.2849","demo.72","main.2974","main.652"],"title":"A Dataset for Tracking Entities in Open Domain Procedural Text","tldr":"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being fogg...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3032","id":"main.3032","presentation_id":"38939254"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3035.png","content":{"abstract":"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of \\emph{contrastive support sufficiency}, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments suggest that there hasn't been much progress in multi-hop QA in the reading comprehension setting. For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning (19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction.","authors":["Harsh Trivedi","Niranjan Balasubramanian","Tushar Khot","Ashish Sabharwal"],"demo_url":"","keywords":["multi-hop question-answering","automatic datasets","disconnected reasoning","multi-hop qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.712","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2049","main.2228","main.1648","main.2380","main.2943"],"title":"Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning","tldr":"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and def...","track":"Question Answering"},"forum":"main.3035","id":"main.3035","presentation_id":"38939255"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3046.png","content":{"abstract":"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.","authors":["Ramy Eskander","Smaranda Muresan","Michael Collins"],"demo_url":"","keywords":["part-of-speech tagging","unsupervised approach","cross-lingual projection","bi-lstm architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.391","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2641","main.143","main.852","main.407","main.1061"],"title":"Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios","tldr":"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which PO...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3046","id":"main.3046","presentation_id":"38939256"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3049.png","content":{"abstract":"In recent years, there has been an increasing interest in the application of Artificial Intelligence \u2013 and especially Machine Learning \u2013 to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","authors":["Costanza Conforti","Stephanie Hirmer","Dai Morgan","Marco Basaldella","Yau Ben Or"],"demo_url":"","keywords":["sustainable development","sd","project sustainability","community profiling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.677","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.748","main.1669","main.1923","main.2068","main.387"],"title":"Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling","tldr":"In recent years, there has been an increasing interest in the application of Artificial Intelligence \u2013 and especially Machine Learning \u2013 to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this...","track":"NLP Applications"},"forum":"main.3049","id":"main.3049","presentation_id":"38939257"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3051.png","content":{"abstract":"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard ``mask-predict'' algorithm, and provide analyses of its behavior on machine translation tasks.","authors":["Julia Kreutzer","George Foster","Colin Cherry"],"demo_url":"","keywords":["non-autoregressive tasks","machine translation","masked inference","machine tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.465","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2389","main.2198","main.247","main.3483","main.1356"],"title":"Inference Strategies for Machine Translation with Conditional Masking","tldr":"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference stra...","track":"Machine Translation and Multilinguality"},"forum":"main.3051","id":"main.3051","presentation_id":"38939258"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3054.png","content":{"abstract":"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.\\footnote{Our code is available at \\url{https://github.com/WangsyGit/PathQG}.}","authors":["Siyuan Wang","Zhongyu Wei","Zhihao Fan","Zengfeng Huang","Weijian Sun","Qi Zhang","Xuanjing Huang"],"demo_url":"","keywords":["question generation","query learning","query-based generation","sequence problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.729","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.3186","main.2650","main.2586","main.3327","main.1022"],"title":"PathQG: Neural Question Generation from Facts","tldr":"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate fact...","track":"NLP Applications"},"forum":"main.3054","id":"main.3054","presentation_id":"38939259"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3057.png","content":{"abstract":"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a high-performance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.","authors":["Kazumasa Omura","Daisuke Kawahara","Sadao Kurohashi"],"demo_url":"","keywords":["automatic extraction","language research","crowdsourcing","transfer model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.192","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6D","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1923","main.2922","main.210","main.76","main.3470"],"title":"A Method for Building a Commonsense Inference Dataset based on Basic Events","tldr":"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic event...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3057","id":"main.3057","presentation_id":"38939260"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3064.png","content":{"abstract":"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.","authors":["Yunjie Ji","Hao Liu","Bolei He","Xinyan Xiao","Hua Wu","Yanhua Yu"],"demo_url":"","keywords":["neural","aspect-level classification","dmsc","diversified"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.570","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1675","main.3185","main.3286","main.1289","main.1023"],"title":"Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification","tldr":"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online s...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3064","id":"main.3064","presentation_id":"38939261"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3065.png","content":{"abstract":"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.","authors":["Ruibo Liu","Guangxuan Xu","Chenyan Jia","Weicheng Ma","Lili Wang","Soroush Vosoughi"],"demo_url":"","keywords":["data augmentation","nlu tasks","data boost","text tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.726","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2068","main.2733","main.1923","main.1356","main.493"],"title":"Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation","tldr":"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforceme...","track":"NLP Applications"},"forum":"main.3065","id":"main.3065","presentation_id":"38939262"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3068.png","content":{"abstract":"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice  forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems.  We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present  preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research.","authors":["Venkata Subrahmanyan Govindarajan","Benjamin Chen","Rebecca Warholic","Katrin Erk","Junyi Jessy Li"],"demo_url":"","keywords":["advice-seeking","advice-seeking online","advice-giving","advice identification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.427","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.41","main.485","main.2763","main.2590","main.1201"],"title":"Help! Need Advice on Identifying Advice","tldr":"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice  forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly...","track":"Computational Social Science and Social Media"},"forum":"main.3068","id":"main.3068","presentation_id":"38939263"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3072.png","content":{"abstract":"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.","authors":["Ritam Dutt","Rishabh Joshi","Carolyn Rose"],"demo_url":"","keywords":["face utilization","politeness theory","computational models","notion face"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.605","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13A","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2561","main.3352","main.2707","main.851","main.165"],"title":"Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions","tldr":"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critic...","track":"Discourse and Pragmatics"},"forum":"main.3072","id":"main.3072","presentation_id":"38939264"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3074.png","content":{"abstract":"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.","authors":["Ziheng Wang","Jeremy Wohlwend","Tao Lei"],"demo_url":"","keywords":["natural tasks","model compression","language tasks","pruning embeddings"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.496","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1130","main.1960","main.1351","main.1446"],"title":"Structured Pruning of Large Language Models","tldr":"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises ...","track":"Machine Learning for NLP"},"forum":"main.3074","id":"main.3074","presentation_id":"38939265"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3084.png","content":{"abstract":"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.","authors":["Jiapeng Wu","Meng Cao","Jackie Chi Kit Cheung","William L. Hamilton"],"demo_url":"","keywords":["data imputation","tkg tasks","tkgs","time-dependent representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.462","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1465","main.3617","main.2972","main.684","main.300"],"title":"TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion","tldr":"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, thes...","track":"Information Extraction"},"forum":"main.3084","id":"main.3084","presentation_id":"38939266"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3088.png","content":{"abstract":"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.","authors":["Yue Wang","Jing Li","Michael Lyu","Irwin King"],"demo_url":"","keywords":["keyphrase prediction","text modeling","classification","generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.268","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1091","main.2261","main.702","main.1287","main.3298"],"title":"Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings","tldr":"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich feature...","track":"Computational Social Science and Social Media"},"forum":"main.3088","id":"main.3088","presentation_id":"38939267"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3093.png","content":{"abstract":"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.","authors":["Dhanasekar Sundararaman","Shijing Si","Vivek Subramanian","Guoyin Wang","Devamanyu Hazarika","Lawrence Carin"],"demo_url":"","keywords":["numerical reasoning","question answering","list maximum","decoding"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.384","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3292","main.3635","main.1305","main.2251","main.2596"],"title":"Methods for Numeracy-Preserving Word Embeddings","tldr":"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving t...","track":"NLP Applications"},"forum":"main.3093","id":"main.3093","presentation_id":"38939268"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3101.png","content":{"abstract":"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.","authors":["Hsin-Yu Chen","Cheng-Te Li"],"demo_url":"","keywords":["computational cyberbullying","text sessions","model explainability","explainable detection"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.200","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3424","main.384","main.2996","main.1626","main.1287"],"title":"HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media","tldr":"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is...","track":"Computational Social Science and Social Media"},"forum":"main.3101","id":"main.3101","presentation_id":"38939269"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3111.png","content":{"abstract":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.","authors":["Zhengjue Wang","Zhibin Duan","Hao Zhang","Chaojie Wang","Long Tian","Bo Chen","Mingyuan Zhou"],"demo_url":"","keywords":["abstractive summarization","document understanding","summary generation","ta"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.35","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3398","main.471","main.1835","main.965","main.714"],"title":"Friendly Topic Assistant for Transformer Based Abstractive Summarization","tldr":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are be...","track":"Summarization"},"forum":"main.3111","id":"main.3111","presentation_id":"38939270"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3115.png","content":{"abstract":"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.","authors":["Charles Yu","Ryan Sie","Nicolas Tedeschi","Leon Bergen"],"demo_url":"","keywords":["reflexive anaphora","grammatical tasks","neural models","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.331","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7B","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3181","main.1282","main.1613","main.2638","TACL.2013"],"title":"Word Frequency Does Not Predict Grammatical Knowledge in Language Models","tldr":"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb a...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.3115","id":"main.3115","presentation_id":"38939271"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3116.png","content":{"abstract":"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.","authors":["Hyung Won Chung","Dan Garrette","Kiat Chuan Tan","Jason Riesa"],"demo_url":"","keywords":["massively applications","multilingual generation","cross-lingual sharing","multilingual models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.367","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.870","main.852","main.143","main.1445","main.410"],"title":"Improving Multilingual Models with Language-Clustered Vocabularies","tldr":"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applicatio...","track":"Machine Translation and Multilinguality"},"forum":"main.3116","id":"main.3116","presentation_id":"38939272"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3126.png","content":{"abstract":"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT's architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.","authors":["Thanh Tran","Yifan Hu","Changwei Hu","Kevin Yen","Fei Tan","Kyumin Lee","Se Rim Park"],"demo_url":"","keywords":["downstream task","hatespeech classification","habertor model","bert model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.606","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.426","main.3434","main.47","main.2914","TACL.2389"],"title":"HABERTOR: An Efficient and Effective Deep Hatespeech Detector","tldr":"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classific...","track":"NLP Applications"},"forum":"main.3126","id":"main.3126","presentation_id":"38939273"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3136.png","content":{"abstract":"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product  descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product  information is necessary for the task. Our code and dataset are available at https://github.com/jd-aig/JAVE.","authors":["Tiangang Zhu","Yue Wang","Haoran Li","Youzheng Wu","Xiaodong He","Bowen Zhou"],"demo_url":"","keywords":["e-commerce scenarios","product retrieval","attribute tasks","multimodal method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.166","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2426","main.1787","main.1205","main.3287","main.2273"],"title":"Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product","tldr":"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time...","track":"Information Extraction"},"forum":"main.3136","id":"main.3136","presentation_id":"38939274"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3140.png","content":{"abstract":"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown  to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.","authors":["Steven Rennie","Etienne Marcheret","Neil Mallinar","David Nahamoo","Vaibhava Goel"],"demo_url":"","keywords":["question-answering tasks","self-supervised tasks","word masking","sentence entailment"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.87","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2586","main.319","main.3183","main.2635","main.1837"],"title":"Unsupervised Adaptation of Question Answering Systems via Generative Self-training","tldr":"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and senten...","track":"Question Answering"},"forum":"main.3140","id":"main.3140","presentation_id":"38939275"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3143.png","content":{"abstract":"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Hence, we carry out an empirical study on position embedding of mainstream pre-trained Transformers mainly focusing on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings by feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future works to choose the suitable positional encoding function for specific tasks given the application property.","authors":["Yu-An Wang","Yun-Nung Chen"],"demo_url":"","keywords":["nlp tasks","transformers","position transformers","iconic tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.555","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.1485","TACL.2411","main.2635","main.858"],"title":"What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding","tldr":"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attentio...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3143","id":"main.3143","presentation_id":"38939276"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.315.png","content":{"abstract":"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models,  CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.","authors":["Deming Ye","Yankai Lin","Jiaju Du","Zhenghao Liu","Peng Li","Maosong Sun","Zhiyuan Liu"],"demo_url":"","keywords":["downstream tasks","coreferential reasoning","common tasks","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.582","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1970","main.2476","main.1892","main.1130","main.3647"],"title":"Coreferential Reasoning Learning for Language Representation","tldr":"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most exist...","track":"Question Answering"},"forum":"main.315","id":"main.315","presentation_id":"38938680"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3151.png","content":{"abstract":"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.","authors":["Neema Kotonya","Francesca Toni"],"demo_url":"","keywords":["fact-checking","fact-checking studies","explainable fact-checking","public health"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.623","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2962","main.2117","main.2570","TACL.2049","main.2506"],"title":"Explainable Automated Fact-Checking for Public Health Claims","tldr":"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for o...","track":"NLP Applications"},"forum":"main.3151","id":"main.3151","presentation_id":"38939277"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3157.png","content":{"abstract":"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.","authors":["Shaoxiong Feng","Xuancheng Ren","Hongshen Chen","Bin Sun","Kan Li","Xu Sun"],"demo_url":"","keywords":["inference","generative systems","imitation framework","dialogue model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.534","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1522","main.2209","main.2058","main.3179","main.1006"],"title":"Regularizing Dialogue Generation by Imitating Implicit Scenarios","tldr":"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialo...","track":"Dialog and Interactive Systems"},"forum":"main.3157","id":"main.3157","presentation_id":"38939278"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.317.png","content":{"abstract":"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.","authors":["Zibo Lin","Deng Cai","Yan Wang","Xiaojiang Liu","Haitao Zheng","Shuming Shi"],"demo_url":"","keywords":["response selection","retrieval-based systems","learning-to-rank problem","learning-to-rank"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.741","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.699","main.1700","main.471","main.3183","main.3179"],"title":"The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection","tldr":"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each ...","track":"Dialog and Interactive Systems"},"forum":"main.317","id":"main.317","presentation_id":"38938681"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3174.png","content":{"abstract":"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.","authors":["Wenshuo Yang","Jiyi Li","Fumiyo Fukumoto","Yanming Ye"],"demo_url":"","keywords":["data problem","multi-label classification","hybrid solution","general networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.545","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3013","main.1575","main.493","main.426","main.1217"],"title":"HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification","tldr":"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases...","track":"NLP Applications"},"forum":"main.3174","id":"main.3174","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3179.png","content":{"abstract":"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.","authors":["Wei-Jen Ko","Avik Ray","Yilin Shen","Hongxia Jin"],"demo_url":"","keywords":["generation responses","regression task","open-domain models","end-to-end classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.352","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.128","main.699","main.215","main.891","main.1700"],"title":"Generating Dialogue Responses from a Semantic Latent Space","tldr":"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multi...","track":"Language Generation"},"forum":"main.3179","id":"main.3179","presentation_id":"38939280"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.318.png","content":{"abstract":"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","authors":["Haochen Liu","Wentao Wang","Yiqi Wang","Hui Liu","Zitao Liu","Jiliang Tang"],"demo_url":"","keywords":["nlp tasks","word embedding","dialogue systems","debiasing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.64","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.834","main.1522","main.3179","main.3157","main.128"],"title":"Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning","tldr":"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect ...","track":"Dialog and Interactive Systems"},"forum":"main.318","id":"main.318","presentation_id":"38938682"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3181.png","content":{"abstract":"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments.  We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.","authors":["Robert Hawkins","Takateru Yamakoshi","Thomas Griffiths","Adele Goldberg"],"demo_url":"","keywords":["grammatical construction","dais","neural models","transformer architectures"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.376","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3115","TACL.2013","main.1613","main.1282","main.2363"],"title":"Investigating representations of verb bias in neural language models","tldr":"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as verb bias. ...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.3181","id":"main.3181","presentation_id":"38939281"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3183.png","content":{"abstract":"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present \\textit{MUTANT}, a training paradigm that exposes the model to perceptually similar, yet semantically distinct \\textit{mutations} of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, \\textit{MUTANT} does not rely on the knowledge about the nature of train and test answer distributions. \\textit{MUTANT} establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.","authors":["Tejas Gokhale","Pratyay Banerjee","Chitta Baral","Yezhou Yang"],"demo_url":"","keywords":["generalization","ood generalization","question answering","training paradigm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.63","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1196","main.1837","main.3140","main.1022","TACL.2041"],"title":"MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering","tldr":"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a pro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3183","id":"main.3183","presentation_id":"38939282"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3184.png","content":{"abstract":"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data's supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.","authors":["Zihao Fu","Bei Shi","Wai Lam","Lidong Bing","Zhiyuan Liu"],"demo_url":"","keywords":["data-to-text task","generation task","dataset problem","over-generation problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.738","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1482","main.1923","main.714","main.3010","main.2342"],"title":"Partially-Aligned Data-to-Text Generation with Distant Supervision","tldr":"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data whic...","track":"Language Generation"},"forum":"main.3184","id":"main.3184","presentation_id":"38939283"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3185.png","content":{"abstract":"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.","authors":["Yuncong Li","Cunxiang Yin","Sheng-hua Zhong","Xu Pan"],"demo_url":"","keywords":["aspect-category analysis","aspect-category","acsa","aspect representation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.287","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1675","main.3064","main.3286","main.1217","main.1289"],"title":"Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis","tldr":"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an as...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3185","id":"main.3185","presentation_id":"38939284"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3186.png","content":{"abstract":"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets.  We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.","authors":["Wei-Jen Ko","Te-yuan Chen","Yiyan Huang","Greg Durrett","Junyi Jessy Li"],"demo_url":"","keywords":["inquisitive questions","automatic systems","text comprehension","data-driven approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.530","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2586","main.2973","main.3054","main.41","main.1022"],"title":"Inquisitive Question Generation for High Level Text Comprehension","tldr":"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news arti...","track":"Language Generation"},"forum":"main.3186","id":"main.3186","presentation_id":"38939285"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.319.png","content":{"abstract":"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.","authors":["Ethan Perez","Patrick Lewis","Wen-tau Yih","Kyunghyun Cho","Douwe Kiela"],"demo_url":"","keywords":["question qa","labeling questions","one-to-n transduction","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.713","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2586","main.449","main.3140","main.1580","main.2258"],"title":"Unsupervised Question Decomposition for Question Answering","tldr":"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to prod...","track":"Question Answering"},"forum":"main.319","id":"main.319","presentation_id":"38938683"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.32.png","content":{"abstract":"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for `Donald is a' substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.","authors":["Vered Shwartz","Rachel Rudinger","Oyvind Tafjord"],"demo_url":"","keywords":["grounding","downstream tasks","reading probes","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.556","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.2363","main.3181","main.1052","main.2382"],"title":"\u201cYou are grounded!\u201d: Latent Name Artifacts in Pre-trained Language Models","tldr":"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associat...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.32","id":"main.32","presentation_id":"38938640"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3205.png","content":{"abstract":"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models. Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport. Experimental results on MUSE and VecMap datasets show significant improvement of our models. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed method.","authors":["Xu Zhao","Zihao Wang","Hao Wu","Yong Zhang"],"demo_url":"","keywords":["bilingual induction","prior transport","semi-supervision","bli"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.238","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.865","main.1379","main.471","main.3609","main.410"],"title":"Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction","tldr":"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further impr...","track":"Machine Learning for NLP"},"forum":"main.3205","id":"main.3205","presentation_id":"38939286"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3216.png","content":{"abstract":"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20~million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.","authors":["Jan A. Botha","Zifei Shan","Daniel Gillick"],"demo_url":"","keywords":["multilingual linking","auxiliary task","cross-lingual task","zero- evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.630","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3453","main.2278","main.2974","main.1803","main.1061"],"title":"Entity Linking in 100 Languages","tldr":"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, ne...","track":"Information Extraction"},"forum":"main.3216","id":"main.3216","presentation_id":"38939287"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3217.png","content":{"abstract":"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.","authors":["Jianguo Zhang","Kazuma Hashimoto","Wenhao Liu","Chien-Sheng Wu","Yao Wan","Philip Yu","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["intent detection","detecting intents","oos detection","large-scale task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.411","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2799","main.148","main.1834","main.2793","main.1032"],"title":"Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference","tldr":"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detectio...","track":"Dialog and Interactive Systems"},"forum":"main.3217","id":"main.3217","presentation_id":"38939288"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3224.png","content":{"abstract":"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations. We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets. We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks. Our results indicate a significant improvement over the application of the dense word representations.","authors":["G\u00e1bor Berend"],"demo_url":"","keywords":["fine-grained disambiguation","part-of-speech tagging","sparse representations","task-specific models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.683","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1935","main.2251","main.298","main.2891","main.644"],"title":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","tldr":"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relie...","track":"Semantics: Lexical Semantics"},"forum":"main.3224","id":"main.3224","presentation_id":"38939289"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3227.png","content":{"abstract":"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.","authors":["Prathyusha Jwalapuram","Shafiq Joty","Youlin Shen"],"demo_url":"","keywords":["pronoun translations","pronoun translation","neural training","backtranslation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.177","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.888","main.1572","main.852","main.3688","TACL.2107"],"title":"Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses","tldr":"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that w...","track":"Machine Translation and Multilinguality"},"forum":"main.3227","id":"main.3227","presentation_id":"38939290"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3231.png","content":{"abstract":"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.","authors":["Hieu Man Duc Trong","Duc Trong Le","Amir Pouran Ben Veyseh","Thuat Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":["detecting events","event detection","ed","manual annotation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.433","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2048","main.1749","main.3101","main.2895","main.3390"],"title":"Introducing a New Dataset for Event Detection in Cybersecurity Texts","tldr":"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In...","track":"Information Extraction"},"forum":"main.3231","id":"main.3231","presentation_id":"38939291"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3236.png","content":{"abstract":"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.","authors":["Ruiqing Zhang","Chuanqiang Zhang","Zhongjun He","Hua Wu","Haifeng Wang"],"demo_url":"","keywords":["simultaneous translation","translation","segmentation","chinese-english translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.178","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2661","main.1402","demo.111","main.1572","main.2100"],"title":"Learning Adaptive Segmentation Policy for Simultaneous Translation","tldr":"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency w...","track":"Machine Translation and Multilinguality"},"forum":"main.3236","id":"main.3236","presentation_id":"38939292"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3239.png","content":{"abstract":"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.","authors":["Wanyun Cui","Guangyu Zheng","Wei Wang"],"demo_url":"","keywords":["natural problem","plain inference","task-agnostic pretraining","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.444","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.376","main.3360","main.76","main.891","main.1834"],"title":"Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning","tldr":"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and vi...","track":"Speech and Multimodality"},"forum":"main.3239","id":"main.3239","presentation_id":"38939293"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3240.png","content":{"abstract":"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.","authors":["Yosi Mass","Haggai Roitman"],"demo_url":"","keywords":["ad-hoc retrieval","manually data","weakly-supervised method","deep models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.343","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1949","main.2733","main.2083","main.693","main.471"],"title":"Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2","tldr":"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the cor...","track":"Information Retrieval and Text Mining"},"forum":"main.3240","id":"main.3240","presentation_id":"38939294"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3257.png","content":{"abstract":"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package  containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3)  A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.","authors":["Emanuele Bastianelli","Andrea Vanzo","Pawel Swietojanski","Verena Rieser"],"demo_url":"","keywords":["end-user applications","entity labelling","spoken understanding","slurp"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.588","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.891","main.1892","main.143","main.3353","main.214"],"title":"SLURP: A Spoken Language Understanding Resource Package","tldr":"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we rel...","track":"Dialog and Interactive Systems"},"forum":"main.3257","id":"main.3257","presentation_id":"38939295"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3259.png","content":{"abstract":"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.","authors":["Andrew Drozdov","Subendhu Rongali","Yi-Pei Chen","Tim O'Gorman","Mohit Iyyer","Andrew McCallum"],"demo_url":"","keywords":["fine-tuning","unsupervised parsing","deep autoencoder","diora"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.392","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1494","main.1943","main.3348","main.2684","main.2098"],"title":"Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders","tldr":"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover tha...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3259","id":"main.3259","presentation_id":"38939296"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.327.png","content":{"abstract":"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.","authors":["Difeng Wang","Wei Hu","Ermei Cao","Weijian Sun"],"demo_url":"","keywords":["relation extraction","document-level re","re","entity representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.303","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.158","main.1159","main.911","main.2849","main.3216"],"title":"Global-to-Local Neural Networks for Document-Level Relation Extraction","tldr":"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. I...","track":"Information Extraction"},"forum":"main.327","id":"main.327","presentation_id":"38938684"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3270.png","content":{"abstract":"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling.  In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text  generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.","authors":["Lianhui Qin","Vered Shwartz","Peter West","Chandra Bhagavatula","Jena D. Hwang","Ronan Le Bras","Antoine Bosselut","Yejin Choi"],"demo_url":"","keywords":["nonmonotonic tasks","abductive generation","generation","counterfactual revision"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.58","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.2415","TACL.2411","TACL.2041","main.623","main.2054"],"title":"Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning","tldr":"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorpora...","track":"Language Generation"},"forum":"main.3270","id":"main.3270","presentation_id":"38939297"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3272.png","content":{"abstract":"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.","authors":["Justin Chiu","Alexander Rush"],"demo_url":"","keywords":["sequence modeling","hidden model","hidden hmm","hidden"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.103","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3348","main.1446","main.2430","main.2198","main.1615"],"title":"Scaling Hidden Markov Language Models","tldr":"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fall...","track":"Machine Learning for NLP"},"forum":"main.3272","id":"main.3272","presentation_id":"38939298"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3278.png","content":{"abstract":"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.","authors":["Lang Yu","Allyson Ettinger"],"demo_url":"","keywords":["nlp tasks","systematic representations","deep models","phrasal representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.397","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2650","TACL.2411","main.1618","main.1970","main.1130"],"title":"Assessing Phrasal Representation and Composition in Transformers","tldr":"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases,...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3278","id":"main.3278","presentation_id":"38939299"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.328.png","content":{"abstract":"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.","authors":["Jiaming Shen","Heng Ji","Jiawei Han"],"demo_url":"","keywords":["linguistic steganography","syntactical modification","neural models","neural lms"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.22","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.371","main.2914","main.3391","main.60","main.125"],"title":"Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding","tldr":"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neur...","track":"NLP Applications"},"forum":"main.328","id":"main.328","presentation_id":"38938685"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3282.png","content":{"abstract":"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.  Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.","authors":["Lu Xu","Hao Li","Wei Lu","Lidong Bing"],"demo_url":"","keywords":["aspect extraction","triplet process","aste","pipeline approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.183","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1675","main.3286","main.983","main.3375","main.2261"],"title":"Position-Aware Tagging for Aspect Sentiment Triplet Extraction","tldr":"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pip...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3282","id":"main.3282","presentation_id":"38939300"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3286.png","content":{"abstract":"Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans. In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs. Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.","authors":["Lu Xu","Lidong Bing","Wei Lu","Fei Huang"],"demo_url":"","keywords":["predicting polarity","classification","aspect analysis","attention-based models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.288","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1675","main.1289","main.3185","main.3375","main.3282"],"title":"Aspect Sentiment Classification with Aspect-Specific Opinion Spans","tldr":"Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3286","id":"main.3286","presentation_id":"38939301"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3287.png","content":{"abstract":"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders -- a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.","authors":["Jue Wang","Wei Lu"],"demo_url":"","keywords":["named recognition","relation extraction","table-filling problem","representation process"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.133","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1D","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3022","TACL.2103","main.1787","main.2974","main.3216"],"title":"Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders","tldr":"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they t...","track":"Information Extraction"},"forum":"main.3287","id":"main.3287","presentation_id":"38939302"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3291.png","content":{"abstract":"When does a sequence of events define an everyday scenario  and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.","authors":["Noah Weber","Rachel Rudinger","Benjamin Van Durme"],"demo_url":"","keywords":["script induction","correlation-based approach","causal events","interventions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.612","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13C","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1103","main.607","main.928","main.1191","main.1455"],"title":"Causal Inference of Script Knowledge","tldr":"When does a sequence of events define an everyday scenario  and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus....","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3291","id":"main.3291","presentation_id":"38939303"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3292.png","content":{"abstract":"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.","authors":["Machel Reid","Edison Marrese-Taylor","Yutaka Matsuo"],"demo_url":"","keywords":["definition modeling","estimation","generative model","variational inference"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.513","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4H","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3093","main.1130","main.2349","main.143","main.3635"],"title":"VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling","tldr":"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rat...","track":"Discourse and Pragmatics"},"forum":"main.3292","id":"main.3292","presentation_id":"38939304"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3298.png","content":{"abstract":"Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.","authors":["Yuyang Nie","Yuanhe Tian","Xiang Wan","Yan Song","Bo Dai"],"demo_url":"","keywords":["named recognition","data problems","semantic augmentation","pre-trained embeddings"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.107","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.989","main.911","main.1952","main.2733","main.3635"],"title":"Named Entity Recognition for Social Media Texts with Semantic Augmentation","tldr":"Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given ...","track":"Computational Social Science and Social Media"},"forum":"main.3298","id":"main.3298","presentation_id":"38939305"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3299.png","content":{"abstract":"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.","authors":["Mengyun Chen","Tao Ge","Xingxing Zhang","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["erroneous detection","erroneous correction","inference","language-independent approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.581","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2448","main.767","main.2357","TACL.2047","main.639"],"title":"Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction","tldr":"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically...","track":"NLP Applications"},"forum":"main.3299","id":"main.3299","presentation_id":"38939306"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3304.png","content":{"abstract":"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).","authors":["Bill Yuchen Lin","Seyeon Lee","Rahul Khanna","Xiang Ren"],"demo_url":"","keywords":["probing task","fine-tuning","testing","pre-trained models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.557","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2893","main.2349","main.3183","main.2122"],"title":"Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models","tldr":"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we fi...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3304","id":"main.3304","presentation_id":"38939307"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3318.png","content":{"abstract":"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.","authors":["Yichi Zhang","Zhijian Ou","Min Hu","Junlan Feng"],"demo_url":"","keywords":["user tracking","database query","task-oriented systems","end-to-end systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.740","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16A","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2070","main.3393","main.1846","main.1797","main.148"],"title":"A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning","tldr":"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at allevia...","track":"Dialog and Interactive Systems"},"forum":"main.3318","id":"main.3318","presentation_id":"38939308"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3321.png","content":{"abstract":"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.","authors":["Weijie Yu","Chen Xu","Jun Xu","Liang Pang","Xiaopeng Gao","Xiaozhao Wang","Ji-Rong Wen"],"demo_url":"","keywords":["real-world practices","text matching","matching models","match method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.239","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3609","demo.79","main.2790","main.2076","main.973"],"title":"Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains","tldr":"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is of...","track":"NLP Applications"},"forum":"main.3321","id":"main.3321","presentation_id":"38939309"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3327.png","content":{"abstract":"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.","authors":["Ruey-Cheng Chen","Chia-Jung Lee"],"demo_url":"","keywords":["query suggestion","conditional problem","search session","querying"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.251","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1949","main.3054","demo.54","main.2931","main.693"],"title":"Incorporating Behavioral Hypotheses for Query Generation","tldr":"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a ...","track":"Information Retrieval and Text Mining"},"forum":"main.3327","id":"main.3327","presentation_id":"38939310"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3329.png","content":{"abstract":"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26% (p<0.001) in F1 measure.","authors":["Chaofa Yuan","Chuang Fan","Jianzhu Bao","Ruifeng Xu"],"demo_url":"","keywords":["emotion-cause extraction","sequence problem","tagging scheme","linked components"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.289","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3594","main.2261","main.548","main.983","main.3282"],"title":"Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme","tldr":"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3329","id":"main.3329","presentation_id":"38939311"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3336.png","content":{"abstract":"Personal knowledge about users\u2019 professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.","authors":["Anna Tigunova","Andrew Yates","Paramita Mirza","Gerhard Weikum"],"demo_url":"","keywords":["recommenders","keyword extraction","document retrieval","supervised methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.434","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.1287","main.1390","main.1052","main.2996"],"title":"CHARM: Inferring Personal Attributes from Conversations","tldr":"Personal knowledge about users\u2019 professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source o...","track":"Information Extraction"},"forum":"main.3336","id":"main.3336","presentation_id":"38939312"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3337.png","content":{"abstract":"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.","authors":["Pei Zhang","Boxing Chen","Niyu Ge","Kai Fan"],"demo_url":"","keywords":["document-level translation","document-level systems","context-aware architecture","transformer"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.81","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1960","main.2661","main.1618","main.835","main.891"],"title":"Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation","tldr":"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline mo...","track":"Machine Translation and Multilinguality"},"forum":"main.3337","id":"main.3337","presentation_id":"38939313"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.334.png","content":{"abstract":"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F^2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F^2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.","authors":["Byung-Ju Choi","Jimin Hong","David Park","Sang Wan Lee"],"demo_url":"","keywords":["neural generation","sub-optimal generation","learning model","mefmax"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.737","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1770","main.648","main.2650","main.471","main.2430"],"title":"F2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax","tldr":"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects...","track":"Language Generation"},"forum":"main.334","id":"main.334","presentation_id":"38938686"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3344.png","content":{"abstract":"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small  vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners\u2019 performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent\u2019s performance. To enable the agent to behave like a learner, we leverage entailment modeling\u2019s capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning.","authors":["Yun-Hsuan Jen","Chieh-Yang Huang","MeiHua Chen","Ting-Hao Huang","Lun-Wei Ku"],"demo_url":"","keywords":["sentence tasks","classroom study","english-as-a-second learners","inference-based agent"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.312","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2630","main.76","main.471","main.41","TACL.2041"],"title":"Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent","tldr":"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small  vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafte...","track":"NLP Applications"},"forum":"main.3344","id":"main.3344","presentation_id":"38939314"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3348.png","content":{"abstract":"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.","authors":["Sida Gao","Matthew R. Gormley"],"demo_url":"","keywords":["nlp","sequence tagging","named recognition","neural architectures"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.406","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.1615","main.345","main.2430","main.2068"],"title":"Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors","tldr":"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequ...","track":"Machine Learning for NLP"},"forum":"main.3348","id":"main.3348","presentation_id":"38939315"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3352.png","content":{"abstract":"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.","authors":["Jiaxin Pei","David Jurgens"],"demo_url":"","keywords":["intimacy","deep model","language","social information"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.428","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2561","main.3072","main.851","main.3450","main.838"],"title":"Quantifying Intimacy in Language","tldr":"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new com...","track":"Computational Social Science and Social Media"},"forum":"main.3352","id":"main.3352","presentation_id":"38939316"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3353.png","content":{"abstract":"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task. The system trained using this approach scored 100\\% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.","authors":["Henry Elder","Alexander O'Connor","Jennifer Foster"],"demo_url":"","keywords":["generation","neural systems","data approach","neural approaches"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.230","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2590","main.891","main.3495","main.2389","main.1634"],"title":"How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue","tldr":"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restric...","track":"Language Generation"},"forum":"main.3353","id":"main.3353","presentation_id":"38939317"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3357.png","content":{"abstract":"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.","authors":["Loitongbam Gyanendro Singh","Anasua Mitra","Sanasam Ranbir Singh"],"demo_url":"","keywords":["sentiment classification","multiple tweet","node embedding","classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.718","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1675","main.1766","main.1289","main.789","main.1287"],"title":"Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding","tldr":"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representation...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3357","id":"main.3357","presentation_id":"38939318"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3358.png","content":{"abstract":"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks -- contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.","authors":["John Wieting","Graham Neubig","Taylor Berg-Kirkpatrick"],"demo_url":"","keywords":["source separation","semantic encoding","data distributions","unsupervised evaluations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.122","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.267","main.865","main.2851","main.410"],"title":"A Bilingual Generative Transformer for Semantic Sentence Embedding","tldr":"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3358","id":"main.3358","presentation_id":"38939319"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3360.png","content":{"abstract":"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named \u201cvokenization\u201d that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call \u201cvokens\u201d). The \u201cvokenizer\u201d is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.","authors":["Hao Tan","Mohit Bansal"],"demo_url":"","keywords":["speaking","writing","text-only self-supervision","pure-language tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.162","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.407","main.1892","main.1130","main.1388","main.2083"],"title":"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision","tldr":"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a vi...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3360","id":"main.3360","presentation_id":"38939320"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3370.png","content":{"abstract":"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.","authors":["Shiyue Zhang","Benjamin Frey","Mohit Bansal"],"demo_url":"","keywords":["machine research","in-domain evaluation","semi-supervised learning","multilingual training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.43","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2493","main.2777","main.852","main.2298","main.1379"],"title":"ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization","tldr":"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the worl...","track":"Machine Translation and Multilinguality"},"forum":"main.3370","id":"main.3370","presentation_id":"38939321"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3375.png","content":{"abstract":"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.","authors":["Amir Pouran Ben Veyseh","Nasim Nouri","Franck Dernoncourt","Dejing Dou","Thien Huu Nguyen"],"demo_url":"","keywords":["towe","aspect analysis","absa","deep models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.719","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1289","main.1550","main.1675","main.1766","main.983"],"title":"Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning","tldr":"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to explo...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3375","id":"main.3375","presentation_id":"38939322"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3389.png","content":{"abstract":"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.","authors":["Sangwoo Cho","Kaiqiang Song","Chen Li","Dong Yu","Hassan Foroosh","Fei Liu"],"demo_url":"","keywords":["highlighting","summarization","abstractive summarizers","determinantal processes"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.509","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.965","main.2650","main.2125","main.1023","main.2367"],"title":"Better Highlighting: Creating Sub-Sentence Summary Highlights","tldr":"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be...","track":"Summarization"},"forum":"main.3389","id":"main.3389","presentation_id":"38939323"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3390.png","content":{"abstract":"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.","authors":["Viet Dac Lai","Tuan Ngo Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":["event detection","ed","event prediction","graph networks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.435","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.237","main.1488","main.2724","main.1766","TACL.2121"],"title":"Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks","tldr":"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-bas...","track":"Information Extraction"},"forum":"main.3390","id":"main.3390","presentation_id":"38939324"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3391.png","content":{"abstract":"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As ``alternatives'' to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding. Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs.  Experiments demonstrate that our approach  outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging. The source code is available at https://github.com/abdulrafae/coding_nmt.","authors":["Abdul Rafae Khan","Jia Xu","Weiwei Sun"],"demo_url":"","keywords":["natural tasks","nlp","neural-network-based systems","machine translation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.104","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["CL.4","main.648","main.246","main.1613","main.2448"],"title":"Coding Textual Inputs Boosts the Accuracy of Neural Networks","tldr":"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As ``alternatives'' to a text representation, we...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3391","id":"main.3391","presentation_id":"38939325"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3393.png","content":{"abstract":"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of ``request\" carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.","authors":["Semih Yavuz","Kazuma Hashimoto","Wenhao Liu","Nitish Shirish Keskar","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":["da tagging","da","da taggers","maskaugment"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.412","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1201","main.478","main.128","main.1846","main.1702"],"title":"Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging","tldr":"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of ``request\" carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one do...","track":"Dialog and Interactive Systems"},"forum":"main.3393","id":"main.3393","presentation_id":"38939326"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3394.png","content":{"abstract":"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained  to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.","authors":["Siqi Sun","Zhe Gan","Yuwei Fang","Yu Cheng","Shuohang Wang","Jingjing Liu"],"demo_url":"","keywords":["contrastive distillation","compress models","pre-training stages","existing methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.36","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1485","TACL.2411","main.1892","TACL.2041","main.1208"],"title":"Contrastive Distillation on Intermediate Representations for Language Model Compression","tldr":"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions o...","track":"Machine Learning for NLP"},"forum":"main.3394","id":"main.3394","presentation_id":"38939327"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3398.png","content":{"abstract":"We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.","authors":["Shashi Narayan","Joshua Maynez","Jakub Adamek","Daniele Pighin","Blaz Bratanic","Ryan McDonald"],"demo_url":"","keywords":["extractive summarization","stepwise summarization","sentence filtering","rotowire generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.339","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3111","main.1835","main.2511","main.3437","main.1618"],"title":"Stepwise Extractive Summarization and Planning with Structured Transformers","tldr":"We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer ...","track":"Summarization"},"forum":"main.3398","id":"main.3398","presentation_id":"38939328"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3403.png","content":{"abstract":"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.","authors":["Tianxiao Shen","Victor Quach","Regina Barzilay","Tommi Jaakkola"],"demo_url":"","keywords":["text tasks","filling snippets","style transfer","damaged restoration"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.420","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.648","main.2382","main.2389","main.1898","main.1892"],"title":"Blank Language Models","tldr":"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The mo...","track":"Language Generation"},"forum":"main.3403","id":"main.3403","presentation_id":"38939329"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3408.png","content":{"abstract":"The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.","authors":["Jianfei Yu","Jing Jiang","Ling Min Serena Khoo","Hai Leong Chieu","Rui Xia"],"demo_url":"","keywords":["automatic verification","automatic rv","rv task","rv"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.108","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1863","TACL.2411","main.2529","main.2430","main.1734"],"title":"Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations","tldr":"The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with mult...","track":"Computational Social Science and Social Media"},"forum":"main.3408","id":"main.3408","presentation_id":"38939330"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3419.png","content":{"abstract":"We present a query-based biomedical information retrieval task across two vastly different genres -- newswire and research literature -- where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.","authors":["Chaoyuan Zuo","Narayan Acharya","Ritwik Banerjee"],"demo_url":"","keywords":["query-based task","cross-genre ir","incorporating knowledge","ir approach"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.139","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.748","main.693","main.574","demo.109","main.2962"],"title":"Querying Across Genres for Medical Claims in News","tldr":"We present a query-based biomedical information retrieval task across two vastly different genres -- newswire and research literature -- where the goal is to find the research publication that supports the primary claim made in a health-related news ...","track":"Information Retrieval and Text Mining"},"forum":"main.3419","id":"main.3419","presentation_id":"38939331"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3424.png","content":{"abstract":"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of \\emph{attribution tie detection} of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34\\% on attribution detection and 81.37\\% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.","authors":["Rupak Sarkar","Sayantan Mahinder","Hirak Sarkar","Ashiqur KhudaBukhsh"],"demo_url":"","keywords":["political problem","automatically factors","prediction task","poor planning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.109","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.789","main.2996","main.3101","main.2784","main.3644"],"title":"Social Media Attributions in the Context of Water Crisis","tldr":"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viab...","track":"Computational Social Science and Social Media"},"forum":"main.3424","id":"main.3424","presentation_id":"38939332"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3431.png","content":{"abstract":"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic. A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products. Hence, multi-product AD post generation is meaningful and important. We propose a novel end-to-end model named S-MG Net to generate the AD post. Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network). (2) generate a post including selected products via the MGenNet (Multi-Generator Network). Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products. Then, MGenNet generates the description copywriting of each product. Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.","authors":["Zhangming Chan","Yuchi Zhang","Xiuying Chen","Shen Gao","Zhiqiang Zhang","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["multi-product generation","real-world problem","ad task","human evaluations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.313","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.702","main.3088","main.3136","main.317","main.471"],"title":"Selection and Generation: Learning towards Multi-Product Advertisement Post Generation","tldr":"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the cust...","track":"NLP Applications"},"forum":"main.3431","id":"main.3431","presentation_id":"38939333"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3434.png","content":{"abstract":"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of \\emph{randomly} masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over \\emph{subsets} of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.","authors":["Thuy-Trang Vu","Dinh Phung","Gholamreza Haffari"],"demo_url":"","keywords":["combinatorial problem","unsupervised tasks","named recognition","broad-coverage models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.497","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.426","main.745","main.2313","TACL.2389"],"title":"Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models","tldr":"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small pe...","track":"Machine Learning for NLP"},"forum":"main.3434","id":"main.3434","presentation_id":"38939334"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3437.png","content":{"abstract":"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.","authors":["Ruipeng Jia","Yanan Cao","Hengzhu Tang","Fang Fang","Cong Cao","Shi Wang"],"demo_url":"","keywords":["sentence-level summarization","node task","network mining","text summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.295","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1835","main.2761","main.714","main.1023"],"title":"Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network","tldr":"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is...","track":"Summarization"},"forum":"main.3437","id":"main.3437","presentation_id":"38939335"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3438.png","content":{"abstract":"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event's life cycle. Such coarse-grained methods failed to capture the event's unique features in different states. To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation. Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events. For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction. This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection. Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems. We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.","authors":["Rui Xia","Kaizhou Xuan","Jianfei Yu"],"demo_url":"","keywords":["automatic detection","rumor detection","rumor prediction","early detection"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.727","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15B","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1749","main.1465","main.2508","main.1135","main.1116"],"title":"A State-independent and Time-evolving Network for Early Rumor Detection in Social Media","tldr":"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However,...","track":"NLP Applications"},"forum":"main.3438","id":"main.3438","presentation_id":"38939336"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3441.png","content":{"abstract":"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.","authors":["Xuanfu Wu","Yang Feng","Chenze Shao"],"demo_url":"","keywords":["neural","inference","chinese-english tasks","nmt"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.82","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.891","main.2661","main.1960","main.3227","main.2389"],"title":"Generating Diverse Translation from Model Distribution with Dropout","tldr":"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with...","track":"Machine Translation and Multilinguality"},"forum":"main.3441","id":"main.3441","presentation_id":"38939337"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.345.png","content":{"abstract":"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model.  For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.","authors":["Michelle Yuan","Hsuan-Tien Lin","Jordan Boyd-Graber"],"demo_url":"","keywords":["uncertainty sampling","cold-start setting","pre-trained models","fine-tuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.637","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2793","main.2733","main.3348","main.76","main.2068"],"title":"Cold-start Active Learning through Self-supervised Language Modeling","tldr":"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model.  For instance, uncertainty sampling depends on poorly calibrated mo...","track":"Machine Learning for NLP"},"forum":"main.345","id":"main.345","presentation_id":"38938687"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3450.png","content":{"abstract":"Social norms\u2014the unspoken commonsense rules about acceptable social behavior\u2014are crucial in understanding the underlying causes and intents of people\u2019s actions in narratives. For example, underlying an action such as \"wanting to call cops on my neighbor\" are social norms that inform our conduct, such as \"It is expected that you report crimes.\"  We present SOCIAL CHEMISTRY, a new conceptual formalism to study people\u2019s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \u201cIt is rude to run a blender at 5am\u201d as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people\u2019s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.  Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","authors":["Maxwell Forbes","Jena D. Hwang","Vered Shwartz","Maarten Sap","Yejin Choi"],"demo_url":"","keywords":["computational norms","conceptual formalism","neural models","neural transformer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.48","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3D","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1103","main.3352","main.3072","main.1972","main.2179"],"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms","tldr":"Social norms\u2014the unspoken commonsense rules about acceptable social behavior\u2014are crucial in understanding the underlying causes and intents of people\u2019s actions in narratives. For example, underlying an action such as \"wanting to call cops on my neigh...","track":"Computational Social Science and Social Media"},"forum":"main.3450","id":"main.3450","presentation_id":"38939338"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3453.png","content":{"abstract":"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia\u2019s interlanguage links and thus suffer when the foreign language\u2019s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.","authors":["Xingyu Fu","Weijia Shi","Xiaodong Yu","Zian Zhao","Dan Roth"],"demo_url":"","keywords":["cross-lingual linking","cross-lingual","xel","grounding entities"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.521","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3216","main.2630","main.1061","main.2278","main.2363"],"title":"Design Challenges in Low-resource Cross-lingual Entity Linking","tldr":"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, ...","track":"Information Extraction"},"forum":"main.3453","id":"main.3453","presentation_id":"38939339"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3454.png","content":{"abstract":"While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection,  (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.","authors":["Li Kong","Chuanyi Li","Jidong Ge","Bin Luo","Vincent Ng"],"demo_url":"","keywords":["figurative community","sentence-level detection","automatic task","hyperbole"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.571","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2766","main.2452","main.868","main.1455","main.3593"],"title":"Identifying Exaggerated Language","tldr":"While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-le...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3454","id":"main.3454","presentation_id":"38939340"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3457.png","content":{"abstract":"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply \\method{} to causal generation, the task of predicting a proposition's plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.","authors":["Nathaniel Weir","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":"","keywords":["causal generation","cods","neural models","seqseqs"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.421","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2179","main.2448","main.2650","TACL.2013","main.2005"],"title":"COD3S: Diverse Generation with Discrete Semantic Signatures","tldr":"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and th...","track":"Language Generation"},"forum":"main.3457","id":"main.3457","presentation_id":"38939341"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3462.png","content":{"abstract":"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.","authors":["Meixi Wu","Wenya Wang","Sinno Jialin Pan"],"demo_url":"","keywords":["nlp tasks","training process","logic programs","satisfiability problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.453","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9C","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3375","main.2972","main.2426","main.471","main.16"],"title":"Deep Weighted MaxSAT for Aspect-based Opinion Extraction","tldr":"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other han...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3462","id":"main.3462","presentation_id":"38939342"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3464.png","content":{"abstract":"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning. In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer's performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.","authors":["Logan Lebanoff","Franck Dernoncourt","Doo Soon Kim","Lidan Wang","Walter Chang","Fei Liu"],"demo_url":"","keywords":["sentence fusion","transformer","modeling correspondence","summarization systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.338","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1835","main.2125","main.965","main.3437"],"title":"Learning to Fuse Sentences with Transformers for Summarization","tldr":"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusi...","track":"Summarization"},"forum":"main.3464","id":"main.3464","presentation_id":"38939343"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3470.png","content":{"abstract":"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model\u2019s ability to solve each task. Moreover, the dataset\u2019s structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.","authors":["Orion Weller","Nicholas Lourie","Matt Gardner","Matthew Peters"],"demo_url":"","keywords":["task-oriented evaluation","systematic generalization","machine systems","nlp systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.105","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1B","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["TACL.2041","main.2342","main.1923","main.2838","main.2491"],"title":"Learning from Task Descriptions","tldr":"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a fra...","track":"Machine Learning for NLP"},"forum":"main.3470","id":"main.3470","presentation_id":"38939344"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3483.png","content":{"abstract":"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context.  This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.","authors":["Bin Bi","Chenliang Li","Chen Wu","Ming Yan","Wei Wang","Songfang Huang","Fei Huang","Luo Si"],"demo_url":"","keywords":["natural generation","language tasks","generative answering","conversational generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.700","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1707","TACL.2107","main.2430","main.247","main.1446"],"title":"PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation","tldr":"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transform...","track":"Language Generation"},"forum":"main.3483","id":"main.3483","presentation_id":"38939345"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3486.png","content":{"abstract":"We conduct a large scale empirical investigation  of  contextualized  number  prediction  in running  text.    Specifically,  we  consider  two tasks: (1)masked number prediction\u2013 predict-ing  a  missing  numerical  value  within  a  sentence, and (2)numerical anomaly detection\u2013detecting  an  errorful  numeric  value  within  a sentence.   We  experiment  with  novel  combinations of contextual encoders and output distributions over the real number line.   Specifically,  we introduce a suite of output distribution  parameterizations  that  incorporate  latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent  and  transformer-based  encoder  architectures.   We  evaluate  these  models  on  two  numeric  datasets  in  the  financial  and  scientific domain.   Our  findings  show  that  output  distributions that incorporate discrete latent variables  and  allow  for  multiple  modes  outperform  simple  flow-based  counterparts  on  all datasets, yielding more accurate numerical pre-diction and anomaly detection.  We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.","authors":["Taylor Berg-Kirkpatrick","Daniel Spokoyny"],"demo_url":"","keywords":["contextualized prediction","prediction","detecting","numerical pre-diction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.385","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2650","main.1159","TACL.2411","main.2307"],"title":"An Empirical Investigation of Contextualized Number Prediction","tldr":"We conduct a large scale empirical investigation  of  contextualized  number  prediction  in running  text.    Specifically,  we  consider  two tasks: (1)masked number prediction\u2013 predict-ing  a  missing  numerical  value  within  a  sentence, and (2...","track":"NLP Applications"},"forum":"main.3486","id":"main.3486","presentation_id":"38939346"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.349.png","content":{"abstract":"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.","authors":["Lijie Wang","Ao Zhang","Kun Wu","Ke Sun","Zhenghua Li","Hua Wu","Min Zhang","Haifeng Wang"],"demo_url":"","keywords":["text-to-sql parsing","cross-domain task","manually questions","sql queries"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.562","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1866","main.3682","main.3544","main.3507","main.1784"],"title":"DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset","tldr":"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the c...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.349","id":"main.349","presentation_id":"38938688"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3495.png","content":{"abstract":"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based program, built to exploit these patterns, had comparative performance to that of the neural models. In this paper we share our findings about the four types of patterns in the ShARC corpus and how the neural models exploit them. Motivated by the above findings, we create and share a modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.","authors":["Nikhil Verma","Abhishek Sharma","Dhiraj Madan","Danish Contractor","Harshit Kumar","Sachindra Joshi"],"demo_url":"","keywords":["neural tasks","sharc task","sharc","heuristic-based program"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.589","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3353","main.1960","main.2763","main.2389","main.3470"],"title":"Neural Conversational QA: Learning to Reason vs Exploiting Patterns","tldr":"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/pa...","track":"Dialog and Interactive Systems"},"forum":"main.3495","id":"main.3495","presentation_id":"38939347"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3496.png","content":{"abstract":"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures. We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms. To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task. We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation. Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines. Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.","authors":["Milan Aggarwal","Hiresh Gupta","Mausoom Sarkar","Balaji Krishnamurthy"],"demo_url":"","keywords":["document extraction","semantic task","image resolution","structure extraction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.314","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2367","main.652","main.1755","main.1129","main.1159"],"title":"Form2Seq : A Framework for Higher-Order Form Structure Extraction","tldr":"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to whi...","track":"NLP Applications"},"forum":"main.3496","id":"main.3496","presentation_id":"38939348"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3497.png","content":{"abstract":"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence  without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution -- Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.","authors":["Mingming Sun","Wenyue Hua","Zoey Liu","Xin Wang","Kangjie Zheng","Ping Li"],"demo_url":"","keywords":["inference operations","oie algorithms","featured strategies","pipeline"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.167","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["demo.48","main.1179","demo.72","main.3453","main.2342"],"title":"A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression","tldr":"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE...","track":"Information Extraction"},"forum":"main.3497","id":"main.3497","presentation_id":"38939349"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3504.png","content":{"abstract":"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable.  In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance?  Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others.  Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.","authors":["Wanrong Zhu","Xin Wang","Pradyumna Narayana","Kazoo Sone","Sugato Basu","William Yang Wang"],"demo_url":"","keywords":["visually generation","vision-and-language tasks","cider","utilities"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.708","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1388","main.3360","main.1928","main.2864","main.1923"],"title":"Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations","tldr":"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmark...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.3504","id":"main.3504","presentation_id":"38939350"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3506.png","content":{"abstract":"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.  We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers.  To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.","authors":["Silei Xu","Sina Semnani","Giovanni Campagna","Monica Lam"],"demo_url":"","keywords":["database operations","automatic paraphrasing","autoqa","semantic parsers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.31","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.3597","main.2763","main.888","main.2586","main.2590"],"title":"AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data","tldr":"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for traini...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3506","id":"main.3506","presentation_id":"38939351"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3507.png","content":{"abstract":"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.","authors":["Jianqiang Ma","Zeyu Yan","Shuai Pang","Yang Zhang","Jianping Shen"],"demo_url":"","keywords":["text-to-sql systems","slot- approach","dedicated models","modularized systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.563","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3682","main.3517","main.349","main.3216","main.1706"],"title":"Mention Extraction and Linking for SQL Query Generation","tldr":"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturi...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3507","id":"main.3507","presentation_id":"38939352"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3513.png","content":{"abstract":"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.","authors":["Aakriti Budhraja","Madhura Pande","Preksha Nema","Pratyush Kumar","Mitesh M. Khapra"],"demo_url":"","keywords":["nlp tasks","pruning heads","fine-tuning","pruning"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.260","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.618","main.1552","main.2696","main.1485","main.557"],"title":"On the weak link between importance and prunability of attention heads","tldr":"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3513","id":"main.3513","presentation_id":"38939353"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3517.png","content":{"abstract":"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.","authors":["Belinda Z. Li","Sewon Min","Srinivasan Iyer","Yashar Mehdad","Wen-tau Yih"],"demo_url":"","keywords":["mention detection","mention linking","downstream systems","elq"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.522","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2943","main.3507","main.449","main.3216","main.1862"],"title":"Efficient One-Pass End-to-End Entity Linking for Questions","tldr":"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities p...","track":"Information Extraction"},"forum":"main.3517","id":"main.3517","presentation_id":"38939354"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3519.png","content":{"abstract":"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.","authors":["Kai Sun","Richong Zhang","Samuel Mensah","Yongyi Mao","Xudong Liu"],"demo_url":"","keywords":["entity task","relation task","prediction","learning interactions"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.304","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1116","main.327","main.861","main.3287","main.605"],"title":"Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations","tldr":"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learni...","track":"Information Extraction"},"forum":"main.3519","id":"main.3519","presentation_id":"38939355"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3529.png","content":{"abstract":"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.","authors":["Sahana Ramnath","Preksha Nema","Deep Sahni","Mitesh M. Khapra"],"demo_url":"","keywords":["nlp tasks","reading answering","contextual understanding","answer prediction"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.261","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1970","main.449","main.2973","main.928","main.3186"],"title":"Towards Interpreting BERT for Reading Comprehension Based QA","tldr":"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight int...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3529","id":"main.3529","presentation_id":"38939356"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.353.png","content":{"abstract":"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are \"hallucinatory\", e.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of 'the doctor removed his mask' is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","authors":["Ana Valeria Gonz\u00e1lez","Maria Barrett","Rasmus Hvingelby","Kellie Webster","Anders S\u00f8gaard"],"demo_url":"","keywords":["nlp tasks","russian","gender bias","coreferential reading"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.209","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.143","main.2278","main.1379","main.623","main.2363"],"title":"Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias","tldr":"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are \"hallucinatory\", e.g., disambiguating ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.353","id":"main.353","presentation_id":"38938689"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3532.png","content":{"abstract":"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available.","authors":["Abu Awal Md Shoeb","Gerard de Melo"],"demo_url":"","keywords":["emojis","human-solicited ratings","emotions","dataset"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.720","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2261","main.1287","main.476","main.384","main.3028"],"title":"EmoTag1200: Understanding the Association between Emojis and Emotions","tldr":"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-triv...","track":"Computational Social Science and Social Media"},"forum":"main.3532","id":"main.3532","presentation_id":"38939357"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3540.png","content":{"abstract":"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance,  self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training,  a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).","authors":["Shaolei Wang","Zhongyuan Wang","Wanxiang Che","Ting Liu"],"demo_url":"","keywords":["disfluency detection","self-supervised techniques","unsupervised paradigm","noisy training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.142","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2733","main.2684","main.1146","main.345","main.471"],"title":"Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection","tldr":"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance,  self-supervised learning techniques, bu...","track":"Speech and Multimodality"},"forum":"main.3540","id":"main.3540","presentation_id":"38939358"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3541.png","content":{"abstract":"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.","authors":["Luyu Gao","Zhuyun Dai","Jamie Callan"],"demo_url":"","keywords":["information retrieval","ranking process","text representation","interaction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.342","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8A","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1949","main.2943","main.2931","main.204","main.3517"],"title":"Modularized Transfomer-based Ranking Framework","tldr":"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking proce...","track":"Information Retrieval and Text Mining"},"forum":"main.3541","id":"main.3541","presentation_id":"38939359"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3543.png","content":{"abstract":"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes:  General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as  i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure,  which maintains 97% performance while using at-most 10% of the original neurons.","authors":["Fahim Dalvi","Hassan Sajjad","Nadir Durrani","Yonatan Belinkov"],"demo_url":"","keywords":["transformer-based models","pretrained models","bert","xlnet"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.398","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1552","main.2414","main.1446","TACL.2041","main.2615"],"title":"Analyzing Redundancy in Pretrained Transformer Models","tldr":"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundanc...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3543","id":"main.3543","presentation_id":"38939360"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3544.png","content":{"abstract":"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.","authors":["Ruiqi Zhong","Tao Yu","Dan Klein"],"demo_url":"","keywords":["text-to-sql models","distilled suite","spider board","test accuracy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.29","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.349","main.3507","main.1866","main.3682","main.84"],"title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suites","tldr":"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At ev...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3544","id":"main.3544","presentation_id":"38939361"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.355.png","content":{"abstract":"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt  BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.","authors":["Yue Wang","Shafiq Joty","Michael Lyu","Irwin King","Caiming Xiong","Steven C.H. Hoi"],"demo_url":"","keywords":["visual dialog","vision-language task","visual tasks","answer ranking"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.269","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2070","main.1113","main.647","main.2853","TACL.2041"],"title":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","tldr":"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such int...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.355","id":"main.355","presentation_id":"38938690"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3550.png","content":{"abstract":"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-$(k,m)$, the language of well-nested brackets (of $k$ types) and $m$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use $O(k^{\\frac{m}{2}})$ memory (hidden units) to generate these languages. We prove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with $o(m \\log k)$ hidden units.","authors":["John Hewitt","Michael Hahn","Surya Ganguli","Percy Liang","Christopher D. Manning"],"demo_url":"","keywords":["recurrent networks","rnns","rnn","finite-precision setting"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.156","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2198","main.2696","TACL.2411","main.2851","main.1996"],"title":"RNNs can generate bounded hierarchical languages with optimal memory","tldr":"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RN...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.3550","id":"main.3550","presentation_id":"38939362"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3551.png","content":{"abstract":"Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message. For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models. To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT predicts micro-dialects with 9.9% F1, \u0018 76\u0002 better than a majority class baseline. Our new language model also establishes new state-ofthe- art on several external tasks.","authors":["Muhammad Abdul-Mageed","Chiyu Zhang","AbdelRahim Elmadany","Lyle Ungar"],"demo_url":"","keywords":["prediction dialects","language task","micro-dialect identification","mdi"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.472","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.517","main.2675","main.2076","main.143","main.2363"],"title":"Toward Micro-Dialect Identification in Diaglossic and Code-Switched Environments","tldr":"Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Id...","track":"Computational Social Science and Social Media"},"forum":"main.3551","id":"main.3551","presentation_id":"38939363"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3552.png","content":{"abstract":"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).","authors":["Manik Bhandari","Pranav Narayan Gour","Atabak Ashfaq","Pengfei Liu","Graham Neubig"],"demo_url":"","keywords":["manual evaluation","text-generation tasks","text summarization","top-scoring systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.751","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3012","main.965","main.1023","main.2125","main.714"],"title":"Re-evaluating Evaluation in Text Summarization","tldr":"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 yea...","track":"Summarization"},"forum":"main.3552","id":"main.3552","presentation_id":"38939364"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3563.png","content":{"abstract":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages' gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","authors":["Arya D. McCarthy","Adina Williams","Shijia Liu","David Yarowsky","Ryan Cotterell"],"demo_url":"","keywords":["cluster evaluation","community detection","grammatical system","gender systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.456","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["CL.2","main.3181","TACL.2013","main.2131","main.2718"],"title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","tldr":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing ...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3563","id":"main.3563","presentation_id":"38939365"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3566.png","content":{"abstract":"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to text corpora availability. We investigate the performance of GPT-2 on AAVE text by creating a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating syntactic structure and AAVE- or SAE-specific language for each pair. We evaluate each sample and its GPT-2 generated text with pretrained sentiment classifiers and find that while AAVE text results in more classifications of negative sentiment than SAE, the use of GPT-2 generally increases occurrences of positive sentiment for both. Additionally, we conduct human evaluation of AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall quality.","authors":["Sophie Groenwold","Lily Ou","Aesha Parekh","Samhita Honnavalli","Sharon Levy","Diba Mirza","William Yang Wang"],"demo_url":"","keywords":["human evaluation","nlp models","gpt-","pretrained classifiers"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.473","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.870","main.1320","main.2994","main.3181","main.2075"],"title":"Investigating African-American Vernacular English in Transformer-Based Text Generation","tldr":"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, s...","track":"Computational Social Science and Social Media"},"forum":"main.3566","id":"main.3566","presentation_id":"38939366"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3567.png","content":{"abstract":"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..","authors":["Wei Song","Kai Zhang","Ruiji Fu","Lizhen Liu","Ting Liu","Miaomiao Cheng"],"demo_url":"","keywords":["supervised fine-tuning","pre-training method","weakly pre-training","essay scorer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.546","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4G","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1299","main.2635","main.2947","main.2793","main.1032"],"title":"Multi-Stage Pre-training for Automated Chinese Essay Scoring","tldr":"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is ...","track":"NLP Applications"},"forum":"main.3567","id":"main.3567","presentation_id":"38939367"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.357.png","content":{"abstract":"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.","authors":["Rafael Anchi\u00eata","Thiago Pardo"],"demo_url":"","keywords":["abstract representation","abstract","amr","graph-based formalism"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.123","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1649","main.2891","main.1957","CL.2","main.1061"],"title":"Semantically Inspired AMR Alignment for the Portuguese Language","tldr":"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. How...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.357","id":"main.357","presentation_id":"38938691"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3573.png","content":{"abstract":"State of the art research for date-time\\footnote{We use date-time entities, date entities, time entities and temporal entities interchangeably to denote entities associated with dates and/ or times.} entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don\u2019t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.","authors":["Barun Patra","Chala Fufa","Pamela Bhattacharya","Charles Lee"],"demo_url":"","keywords":["entity extraction","generic extraction","date-time extraction","identifying constraints"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.678","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3617","main.1135","main.1116","main.3497","main.2849"],"title":"To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints","tldr":"State of the art research for date-time\\footnote{We use date-time entities, date entities, time entities and temporal entities interchangeably to denote entities associated with dates and/ or times.} entity extraction from text is task agnostic. Cons...","track":"NLP Applications"},"forum":"main.3573","id":"main.3573","presentation_id":"38939368"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3579.png","content":{"abstract":"The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation. To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper. Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling. Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.","authors":["Chenggong Gong","Jianfei Yu","Rui Xia"],"demo_url":"","keywords":["aspect-based analysis","absa task","feature-based adaptation","auxiliary tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.572","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4E","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1217","main.1675","TACL.2255","main.3013","main.471"],"title":"Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis","tldr":"The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based a...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3579","id":"main.3579","presentation_id":"38939369"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3580.png","content":{"abstract":"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work","authors":["Hui Su","Xiaoyu Shen","Zhou Xiao","Zheng Zhang","Ernie Chang","Cheng Zhang","Cheng Niu","Jie Zhou"],"demo_url":"","keywords":["in-depth chat","intent prediction","knowledge retrieval","neural approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.535","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2839","main.3179","main.317","main.1797","main.699"],"title":"MovieChats: Chat like Humans in a Closed Domain","tldr":"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-gra...","track":"Dialog and Interactive Systems"},"forum":"main.3580","id":"main.3580","presentation_id":"38939370"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3581.png","content":{"abstract":"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on \\emph{arbitrary} aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.","authors":["Bowen Tan","Lianhui Qin","Eric Xing","Zhiting Hu"],"demo_url":"","keywords":["aspect-based summarization","weak method","aspect scheme","supervision data"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.510","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1023","main.693","main.2476","main.3012","main.983"],"title":"Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach","tldr":"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of s...","track":"Summarization"},"forum":"main.3581","id":"main.3581","presentation_id":"38939371"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.359.png","content":{"abstract":"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases.  Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.","authors":["Joe Stacey","Pasquale Minervini","Haim Dubossarsky","Sebastian Riedel","Tim Rockt\u00e4schel"],"demo_url":"","keywords":["neural networks","adversarial training","sentence representations","nli models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.665","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5B","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2313","TACL.2129","main.2430","main.3434","main.1834"],"title":"Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training","tldr":"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.359","id":"main.359","presentation_id":"38938692"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3593.png","content":{"abstract":"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.","authors":["Jing Lu","Vincent Ng"],"demo_url":"","keywords":["entity resolution"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.536","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1621","main.3647","main.883","main.2416","main.315"],"title":"Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art","tldr":"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing ...","track":"Dialog and Interactive Systems"},"forum":"main.3593","id":"main.3593","presentation_id":"38939372"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3594.png","content":{"abstract":"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.","authors":["Zixiang Ding","Rui Xia","Jianfei Yu"],"demo_url":"","keywords":["emotion-cause extraction","emotion extraction","emotion-cause filtering","ecpe"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.290","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3329","main.2261","main.3579","main.392","main.548"],"title":"End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning","tldr":"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then p...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.3594","id":"main.3594","presentation_id":"38939373"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3597.png","content":{"abstract":"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.  We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.","authors":["Mehrad Moradshahi","Giovanni Campagna","Sina Semnani","Silei Xu","Monica Lam"],"demo_url":"","keywords":["english answering","english qa","machine translation","semantic localizer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.481","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3506","main.143","main.852","main.2777"],"title":"Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation","tldr":"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by a...","track":"Machine Translation and Multilinguality"},"forum":"main.3597","id":"main.3597","presentation_id":"38939374"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.360.png","content":{"abstract":"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.","authors":["Boaz Shmueli","Lun-Wei Ku","Soumya Ray"],"demo_url":"","keywords":["sarcasm detection","affective computing","affective domains","reactive supervision"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.201","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1287","main.2712","main.3532","main.3101","main.2261"],"title":"Reactive Supervision: A New Method for Collecting Sarcasm Data","tldr":"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations o...","track":"Computational Social Science and Social Media"},"forum":"main.360","id":"main.360","presentation_id":"38938693"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3609.png","content":{"abstract":"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.","authors":["Yan Zhang","Ruidan He","Zuozhu Liu","Kwan Hui Lim","Lidong Bing"],"demo_url":"","keywords":["sentence-pair tasks","clustering","semantic search","downstream tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.124","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3635","main.2083","main.471","main.3205","main.2790"],"title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization","tldr":"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantical...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3609","id":"main.3609","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3617.png","content":{"abstract":"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks.  We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.","authors":["Prachi Jain","Sushant Rathi","Mausam","Soumen Chakrabarti"],"demo_url":"","keywords":["predicting entities","link prediction","time prediction","prediction tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.305","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3084","main.1465","main.3573","main.607","main.1116"],"title":"Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols","tldr":"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time pre...","track":"Information Extraction"},"forum":"main.3617","id":"main.3617","presentation_id":"38939376"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.362.png","content":{"abstract":"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks. Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks. In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models. We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors. In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL. We conduct experiments on two English datasets and one Chinese dataset. Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions' consistency.","authors":["Wenqing Chen","Jidong Tian","Liqiang Xiao","Hao He","Yaohui Jin"],"demo_url":"","keywords":["logically tasks","logically mtl","causal inference","label transfer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.173","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.387","TACL.2411","main.1103","main.1191","main.2307"],"title":"Exploring Logically Dependent Multi-task Learning with Causal Inference","tldr":"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignore...","track":"Machine Learning for NLP"},"forum":"main.362","id":"main.362","presentation_id":"38938694"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3621.png","content":{"abstract":"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77). In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.","authors":["Di Wu","Liang Ding","Fan Lu","Jian Xie"],"demo_url":"","keywords":["slot filling","intent detection","spoken system","joint detection"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.152","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1E","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3257","main.3051","main.1225","main.1432","main.557"],"title":"SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling","tldr":"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel tw...","track":"Dialog and Interactive Systems"},"forum":"main.3621","id":"main.3621","presentation_id":"38939377"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3635.png","content":{"abstract":"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \\url{https://github.com/bohanli/BERT-flow}.","authors":["Bohan Li","Hao Zhou","Junxian He","Mingxuan Wang","Yiming Yang","Lei Li"],"demo_url":"","keywords":["natural processing","semantic task","semantic tasks","pre-trained representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.733","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.3358","main.3609","TACL.2411","main.3093","main.2851"],"title":"On the Sentence Embeddings from Pre-trained Language Models","tldr":"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3635","id":"main.3635","presentation_id":"38939378"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3644.png","content":{"abstract":"In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.","authors":["Shamik Roy","Dan Goldwasser"],"demo_url":"","keywords":["immigration","minimally approach","nuanced frames","policy frames"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.620","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2784","main.2996","main.2995","main.2057","main.789"],"title":"Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media","tldr":"In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subfram...","track":"Computational Social Science and Social Media"},"forum":"main.3644","id":"main.3644","presentation_id":"38939379"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3646.png","content":{"abstract":"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost.   On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that  establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time.  Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination  analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous  analyzers. Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.","authors":["Keshav Kolluru","Vaibhav Adlakha","Samarth Aggarwal","Mausam","Soumen Chakrabarti"],"demo_url":"","keywords":["extractions","-d task","coordination analysis","neural system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.306","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2C","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1669","demo.48","main.1159","main.2426","demo.72"],"title":"OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction","tldr":"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost.   On the other hand,sequence labeling appr...","track":"Information Extraction"},"forum":"main.3646","id":"main.3646","presentation_id":"38939380"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3647.png","content":{"abstract":"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.","authors":["Yuval Varkel","Amir Globerson"],"demo_url":"","keywords":["collecting data","coreference resolution","self-supervised tasks","mention representation"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.687","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.883","main.1621","main.315","main.1518","main.2890"],"title":"Pre-training Mention Representations in Coreference Models","tldr":"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerf...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3647","id":"main.3647","presentation_id":"38939381"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3648.png","content":{"abstract":"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval","authors":["Jinlan Fu","Pengfei Liu","Graham Neubig"],"demo_url":"","keywords":["natural tasks","interpretable evaluation","named task","analysis tool"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.489","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1159","main.1669","main.3216","main.387","main.143"],"title":"Interpretable Multi-dataset Evaluation for Named Entity Recognition","tldr":"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 doe...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.3648","id":"main.3648","presentation_id":"38939382"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3651.png","content":{"abstract":"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.","authors":["Jinghui Yan","Yining Wang","Lu Xiang","Yu Zhou","Chengqing Zong"],"demo_url":"","keywords":["medical normalization","medical processing","chinese normalization","multi-implication procedures"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.116","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.911","main.3216","main.3298","main.2947","main.1952"],"title":"A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization","tldr":"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. H...","track":"NLP Applications"},"forum":"main.3651","id":"main.3651","presentation_id":"38939383"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3656.png","content":{"abstract":"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: https://github.com/neulab/InterpretEval","authors":["Jinlan Fu","Pengfei Liu","Qi Zhang","Xuanjing Huang"],"demo_url":"","keywords":["cws task","fine-grained evaluation","negative problem","chinese systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.457","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10A","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.930","main.557","main.1733","main.2349","main.3013"],"title":"RethinkCWS: Is Chinese Word Segmentation a Solved Task?","tldr":"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.3656","id":"main.3656","presentation_id":"38939384"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3672.png","content":{"abstract":"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.","authors":["Yuncheng Hua","Yuan-Fang Li","Gholamreza Haffari","Guilin Qi","Tongtong Wu"],"demo_url":"","keywords":["program induction","meta-training","cqa","neural approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.469","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10D","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.3054","main.74","main.319","main.2838","main.2586"],"title":"Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning","tldr":"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, ha...","track":"Question Answering"},"forum":"main.3672","id":"main.3672","presentation_id":"38939385"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3676.png","content":{"abstract":"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.","authors":["Xiao Li","Guanyi Chen","Chenghua Lin","Ruizhe Li"],"demo_url":"","keywords":["text transfer","dgst","dual-generator architecture","discriminators"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.578","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11B","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2476","main.2367","main.2412","main.2422","main.1581"],"title":"DGST: a Dual-Generator Network for Text Style Transfer","tldr":"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experim...","track":"NLP Applications"},"forum":"main.3676","id":"main.3676","presentation_id":"38939386"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3682.png","content":{"abstract":"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking.  We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider  despite its structural simplicity.  Many remaining errors are attributable to corpus noise.  This suggests schema linking is the crux for the current text-to-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks.","authors":["Wenqiang Lei","Weixin Wang","Zhixin Ma","Tian Gan","Wei Lu","Min-Yen Kan","Tat-Seng Chua"],"demo_url":"","keywords":["schema linking","data-driven study","text-to-sql task","text-to-sql tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.564","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4F","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.349","main.3507","main.1866","main.1784","main.3544"],"title":"Re-examining the Role of Schema Linking in Text-to-SQL","tldr":"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of s...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.3682","id":"main.3682","presentation_id":"38939387"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.3688.png","content":{"abstract":"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then \ufb01ne-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves signi\ufb01cant performance improvement compared to directly training on those target pairs. It is the \ufb01rst time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.","authors":["Zehui Lin","Xiao Pan","Mingxuan Wang","Xipeng Qiu","Jiangtao Feng","Hao Zhou","Lei Li"],"demo_url":"","keywords":["machine mt","mt","rich mt","universal model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.210","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1680","main.522","main.852","main.267","TACL.2107"],"title":"Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information","tldr":"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-tra...","track":"Machine Translation and Multilinguality"},"forum":"main.3688","id":"main.3688","presentation_id":"38939388"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.371.png","content":{"abstract":"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.","authors":["Siddhant Garg","Goutham Ramakrishnan"],"demo_url":"","keywords":["nlp","generating examples","automatic evaluations","modern models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.498","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2313","main.2914","main.47","main.2357","demo.104"],"title":"BAE: BERT-based Adversarial Examples for Text Classification","tldr":"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to gene...","track":"Machine Learning for NLP"},"forum":"main.371","id":"main.371","presentation_id":"38938695"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.373.png","content":{"abstract":"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural---particularly when gaze is encoded with a dedicated recurrent component.","authors":["Ece Takmaz","Sandro Pezzelle","Lisa Beinborn","Raquel Fern\u00e1ndez"],"demo_url":"","keywords":["image process","language production","image generation","visual processing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.377","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2758","main.1402","main.1113","main.647","main.2702"],"title":"Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze","tldr":"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting p...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.373","id":"main.373","presentation_id":"38938696"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.376.png","content":{"abstract":"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.","authors":["Yao-Hung Hubert Tsai","Martin Ma","Muqiao Yang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"demo_url":"","keywords":["human-centric tasks","sentiment analysis","emotion recognition","multimodal learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.143","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3239","main.2994","main.1670","main.2430","main.1402"],"title":"Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis","tldr":"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentim...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.376","id":"main.376","presentation_id":"38938697"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.384.png","content":{"abstract":"Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account. In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT). However, graph-based neural networks do not take sequential information into account. In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure. Accordingly, our RGAT model can capture both the speaker dependency and the sequential information. Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations. In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.","authors":["Taichi Ishiwatari","Yuki Yasuda","Taro Miyazaki","Jun Goto"],"demo_url":"","keywords":["emotion recognition","recognizing emotions","erc","erc methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.597","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12D","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1287","main.916","main.2261","main.3532","main.782"],"title":"Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations","tldr":"Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships bet...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.384","id":"main.384","presentation_id":"38938698"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.387.png","content":{"abstract":"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.","authors":["Xiangji Zeng","Yunliang Li","Yuchen Zhai","Yin Zhang"],"demo_url":"","keywords":["named recognition","neural models","counterfactual generator","structural model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.590","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2506","main.1923","main.2739","main.911"],"title":"Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition","tldr":"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we d...","track":"Information Extraction"},"forum":"main.387","id":"main.387","presentation_id":"38938699"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.390.png","content":{"abstract":"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.","authors":["Zhixing Tian","Yuanzhe Zhang","Kang Liu","Jun Zhao","Yantao Jia","Zhicheng Sheng"],"demo_url":"","keywords":["machine comprehension","graph network","graph gdin","gdin"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.247","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2758","main.2982","main.928","main.1129","main.605"],"title":"Scene Restoring for Narrative Machine Reading Comprehension","tldr":"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, whi...","track":"Question Answering"},"forum":"main.390","id":"main.390","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.392.png","content":{"abstract":"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach.","authors":["Dong Zhang","Xincheng Ju","Junhui Li","Shoushan Li","Qiaoming Zhu","Guodong Zhou"],"demo_url":"","keywords":["natural community","multi-label detection","multi-modal detection","multi-modal approach"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.291","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2261","main.3329","main.376","main.3594","main.1603"],"title":"Multi-modal Multi-label Emotion Detection with Modality and Label Dependence","tldr":"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modal...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.392","id":"main.392","presentation_id":"38938701"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.400.png","content":{"abstract":"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks. To address the issues, we propose a meta graph learning (MGL) method. Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages. Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer. Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.","authors":["Zheng Li","Mukul Kumar","William Headden","Bing Yin","Ying Wei","Yu Zhang","Qiang Yang"],"demo_url":"","keywords":["cross-lingual transfer","clt task","multilingual","mplm"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.179","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5D","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.74","main.1803","main.871","main.3688","main.2630"],"title":"Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages","tldr":"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt gen...","track":"Machine Translation and Multilinguality"},"forum":"main.400","id":"main.400","presentation_id":"38938702"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.407.png","content":{"abstract":"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\\% of the target language training data, achieves comparable performance to the supervised training with all the training data.","authors":["Zihan Liu","Genta Indra Winata","Peng Xu","Zhaojiang Lin","Pascale Fung"],"demo_url":"","keywords":["spoken systems","cross-lingual task","few-shot setting","cross-lingual models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.587","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.74","main.3046","main.1892","main.522","main.852"],"title":"Cross-lingual Spoken Language Understanding with Regularized Representation Alignment","tldr":"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub...","track":"Dialog and Interactive Systems"},"forum":"main.407","id":"main.407","presentation_id":"38938703"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.41.png","content":{"abstract":"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as \"what is the definition of...\" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.","authors":["Vered Shwartz","Peter West","Ronan Le Bras","Chandra Bhagavatula","Yejin Choi"],"demo_url":"","keywords":["natural understanding","multiple-choice tasks","commonsense reasoning","pre-trained models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.373","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2763","main.3186","TACL.2049","main.2630","main.1052"],"title":"Unsupervised Commonsense Question Answering with Self-Talk","tldr":"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KB...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.41","id":"main.41","presentation_id":"38938641"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.410.png","content":{"abstract":"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space. In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques. We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.","authors":["Pratik Jawanpuria","Mayank Meghwanshi","Bamdev Mishra"],"demo_url":"","keywords":["learning alignment","unsupervised alignment","bilingual induction","cross-lingual similarity"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.240","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.267","main.3116","main.865","CL.2","main.852"],"title":"A Simple Approach to Learning Unsupervised Multilingual Embeddings","tldr":"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-prob...","track":"Machine Learning for NLP"},"forum":"main.410","id":"main.410","presentation_id":"38938704"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.419.png","content":{"abstract":"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.","authors":["Hyunwoo Kim","Byeongchang Kim","Gunhee Kim"],"demo_url":"","keywords":["training","dialogue agents","generative agent","persona-based agents"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.65","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4D","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1797","main.1846","main.128","main.1522","main.699"],"title":"Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness","tldr":"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining co...","track":"Dialog and Interactive Systems"},"forum":"main.419","id":"main.419","presentation_id":"38938705"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.426.png","content":{"abstract":"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.","authors":["Xinyin Ma","Yongliang Shen","Gongfan Fang","Chen Chen","Chenghao Jia","Weiming Lu"],"demo_url":"","keywords":["nlp tasks","nlp","compressing models","text generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.499","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.3434","main.2313","main.1485","main.1046"],"title":"Adversarial Self-Supervised Data-Free Distillation for Text Classification","tldr":"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a reso...","track":"Machine Learning for NLP"},"forum":"main.426","id":"main.426","presentation_id":"38938706"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.438.png","content":{"abstract":"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence's attackability is associated with many of these characteristics regarding the sentence's content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.","authors":["Yohan Jo","Seojin Bang","Emaad Manzoor","Eduard Hovy","Chris Reed"],"demo_url":"","keywords":["refutation argumentation","argumentation","large-scale analysis","sentence attackability"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.1","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1A","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.440","main.96","main.2416","main.3495","main.2895"],"title":"Detecting Attackable Sentences in Arguments","tldr":"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentatio...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.438","id":"main.438","presentation_id":"38938707"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.440.png","content":{"abstract":"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.","authors":["Yohan Jo","Jacky Visser","Chris Reed","Eduard Hovy"],"demo_url":"","keywords":["argumentation","imperatives argumentation","argument mining","rhetorical tools"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.2","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1A","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1750","main.438","main.2996","main.955","main.868"],"title":"Extracting Implicitly Asserted Propositions in Argumentation","tldr":"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to u...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.440","id":"main.440","presentation_id":"38938708"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.445.png","content":{"abstract":"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively \"easy:\" speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are \"grounded\" and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.","authors":["Jack Hessel","Zhenhai Zhu","Bo Pang","Radu Soricut"],"demo_url":"","keywords":["pretraining","video tasks","pretraining model","features"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.709","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5F","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.527","main.2839","main.2758","main.3580","main.1085"],"title":"Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube","tldr":"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech re...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.445","id":"main.445","presentation_id":"38938709"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.447.png","content":{"abstract":"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.","authors":["Anders S\u00f8gaard"],"demo_url":"","keywords":["treebank size","average length","morphological complexity","domain differences"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.220","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["TACL.2141","main.1943","main.1957","main.2890","main.1901"],"title":"Some Languages Seem Easier to Parse Because Their Treebanks Leak","tldr":"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.447","id":"main.447","presentation_id":"38938710"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.449.png","content":{"abstract":"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.","authors":["Elad Segal","Avia Efrat","Mor Shoham","Amir Globerson","Jonathan Berant"],"demo_url":"","keywords":["reading comprehension","rc","learning problem","sequence problem"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.248","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.319","main.2973","main.1022","main.2943","main.2586"],"title":"A Simple and Effective Model for Answering Multi-span Questions","tldr":"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, fo...","track":"Question Answering"},"forum":"main.449","id":"main.449","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.450.png","content":{"abstract":"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as ``mix-up\", ``self-ensembling\", ``distinctiveness score\",  is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.","authors":["Jianfeng He","Xuchao Zhang","Shuo Lei","Zhiqian Chen","Fanglan Chen","Abdulaziz Alhamadani","Bei Xiao","ChangTien Lu"],"demo_url":"","keywords":["uncertainty classified","rectification","text classification","mix-up"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.671","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.1997","main.958","main.493","main.345","main.1159"],"title":"Towards More Accurate Uncertainty Estimation In Text Classification","tldr":"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether addit...","track":"Information Retrieval and Text Mining"},"forum":"main.450","id":"main.450","presentation_id":"38938712"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.453.png","content":{"abstract":"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates. In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline. Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model. On the competitive WMT'14 En$\\rightarrow$De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.","authors":["Chitwan Saharia","William Chan","Saurabh Saxena","Mohammad Norouzi"],"demo_url":"","keywords":["non-autoregressive translation","machine translation","single-step translation","re-scoring"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.83","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1A","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2100","main.1432","TACL.2107","main.1694","main.522"],"title":"Non-Autoregressive Machine Translation with Latent Alignments","tldr":"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve stat...","track":"Machine Translation and Multilinguality"},"forum":"main.453","id":"main.453","presentation_id":"38938713"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.457.png","content":{"abstract":"We report that state-of-the-art parsers consistently failed to identify \u201chers\u201d and \u201ctheirs\u201d as pronouns but identified the masculine equivalent \u201chis\u201d. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.","authors":["Robert Munro","Alex (Carmen) Morrison"],"demo_url":"","keywords":["measuring models","parsers","language models","machine models"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.157","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3181","main.3115","main.2114","main.32","main.1282"],"title":"Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation","tldr":"We report that state-of-the-art parsers consistently failed to identify \u201chers\u201d and \u201ctheirs\u201d as pronouns but identified the masculine equivalent \u201chis\u201d. We find that the same biases exist in recent language models like BERT. While some of the bias come...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.457","id":"main.457","presentation_id":"38938714"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.47.png","content":{"abstract":"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \\textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at \\url{https://github.com/LinyangLee/BERT-Attack}.","authors":["Linyang Li","Ruotian Ma","Qipeng Guo","Xiangyang Xue","Xipeng Qiu"],"demo_url":"","keywords":["adversarial attacks","downstream tasks","calculation","gradient-based methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.500","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2895","main.60","main.371","demo.104"],"title":"BERT-ATTACK: Adversarial Attack Against BERT Using BERT","tldr":"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack m...","track":"Machine Learning for NLP"},"forum":"main.47","id":"main.47","presentation_id":"38938642"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.470.png","content":{"abstract":"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC). Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines. As a result, a high Kappa score of 61% is achieved for inter-annotator agreement. Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2). Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.","authors":["Changmao Li","Elaine Fisher","Rebecca Thomas","Steve Pittard","Vicki Hertzberg","Jinho D. Choi"],"demo_url":"","keywords":["resume classification","job applications","real platforms","triple annotation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.679","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.471","main.2724","main.977","main.3617","main.143"],"title":"Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models","tldr":"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are ...","track":"NLP Applications"},"forum":"main.470","id":"main.470","presentation_id":"38938715"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.471.png","content":{"abstract":"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.  In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.","authors":["Ryosuke Kohita","Akifumi Wachi","Yang Zhao","Ryuki Tachibana"],"demo_url":"","keywords":["abstractive textsummarization","unsupervised summarization","unsupervised summarizers","unsupervised methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.34","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1023","main.714","main.1835","main.2650","main.965"],"title":"Q-learning with Language Model for Edit-based Unsupervised Summarization","tldr":"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.  In this paper, we...","track":"Summarization"},"forum":"main.471","id":"main.471","presentation_id":"38938716"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.476.png","content":{"abstract":"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.","authors":["Navonil Majumder","Pengfei Hong","Shanshan Peng","Jiankun Lu","Deepanway Ghosal","Alexander Gelbukh","Rada Mihalcea","Soujanya Poria"],"demo_url":"","keywords":["empathetic generation","emotion mixture","automatic- evaluations","mime"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.721","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.165","main.3028","main.3532","main.2561","main.2707"],"title":"MIME: MIMicking Emotions for Empathetic Response Generation","tldr":"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a v...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.476","id":"main.476","presentation_id":"38938717"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.478.png","content":{"abstract":"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.","authors":["Rongsheng Zhang","Yinhe Zheng","Jianzhi Shao","Xiaoxi Mao","Yadong Xi","Minlie Huang"],"demo_url":"","keywords":["collecting data","automatic evaluation","open-domain systems","neural models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.277","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1846","main.3393","main.2164","TACL.2143","main.1012"],"title":"Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data","tldr":"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we p...","track":"Dialog and Interactive Systems"},"forum":"main.478","id":"main.478","presentation_id":"38938718"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.485.png","content":{"abstract":"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles. Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue. In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task. We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve). Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.","authors":["Sopan Khosla","Shikhar Vashishth","Jill Fain Lehman","Carolyn Rose"],"demo_url":"","keywords":["information extraction","communication information","machines","information task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.626","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14D","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1622","main.1179","main.1654","main.128","main.689"],"title":"MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge","tldr":"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may diff...","track":"Information Extraction"},"forum":"main.485","id":"main.485","presentation_id":"38938719"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.486.png","content":{"abstract":"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.","authors":["David Vilares","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":"","keywords":["discontinuous parsing","sequence labeling","constituent parsing","labeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.221","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1625","main.1938","main.1957","main.214","main.2419"],"title":"Discontinuous Constituent Parsing as Sequence Labeling","tldr":"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.486","id":"main.486","presentation_id":"38938720"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.493.png","content":{"abstract":"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.","authors":["Liat Ein-Dor","Alon Halfon","Ariel Gera","Eyal Shnarch","Lena Dankin","Leshem Choshen","Marina Danilevsky","Ranit Aharonov","Yoav Katz","Noam Slonim"],"demo_url":"","keywords":["text classification","nlp tasks","bert-based classification","binary classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.638","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2068","main.345","main.1575","main.426","main.1923"],"title":"Active Learning for BERT: An Empirical Study","tldr":"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-train...","track":"Machine Learning for NLP"},"forum":"main.493","id":"main.493","presentation_id":"38938721"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.498.png","content":{"abstract":"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.","authors":["Ilia Kuznetsov","Iryna Gurevych"],"demo_url":"","keywords":["downstream tasks","pre-training","probing","linguistic tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.13","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2122","main.2363","TACL.2411","main.1970","main.1551"],"title":"A matter of framing: The impact of linguistic formalism on probing results","tldr":"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. W...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.498","id":"main.498","presentation_id":"38938722"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.504.png","content":{"abstract":"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on  state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.","authors":["Thomas Scialom","Paul-Alexis Dray","Sylvain Lamprier","Benjamin Piwowarski","Jacopo Staiano"],"demo_url":"","keywords":["mlsum","cross-lingual analyses","large-scale dataset","online newspapers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.647","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.875","main.3116","main.3257","main.1049","main.871"],"title":"MLSUM: The Multilingual Summarization Corpus","tldr":"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with Engli...","track":"Summarization"},"forum":"main.504","id":"main.504","presentation_id":"38938723"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.517.png","content":{"abstract":"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim's labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.","authors":["Nitish Shirish Keskar","Bryan McCann","Caiming Xiong","Richard Socher"],"demo_url":"","keywords":["pre-training","natural processing","victim model","extraction process"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.501","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4B","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3116","main.143","main.1997","main.2076","main.2630"],"title":"The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual APIs","tldr":"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim's labels for that data. We di...","track":"Machine Learning for NLP"},"forum":"main.517","id":"main.517","presentation_id":"38938724"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.522.png","content":{"abstract":"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM \"disagrees\" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.","authors":["Christos Baziotis","Barry Haddow","Alexandra Birch"],"demo_url":"","keywords":["neural translation","neural tm","knowledge distillation","training time"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.615","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.852","main.3688","main.1680","main.74","main.1960"],"title":"Language Model Prior for Low-Resource Neural Machine Translation","tldr":"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to i...","track":"Machine Translation and Multilinguality"},"forum":"main.522","id":"main.522","presentation_id":"38938725"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.527.png","content":{"abstract":"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as ``liking'' messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user's prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a \\textsc{bert} content model by 13 mean reciprocal rank points.","authors":["Pedro Rodriguez","Paul Crook","Seungwhan Moon","Zhiguang Wang"],"demo_url":"","keywords":["open-ended learning","wizard-of-oz task","digital assistants","crowd-sourcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.655","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2444","main.485","main.916","main.1622","main.41"],"title":"Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity","tldr":"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as ``liking'' m...","track":"Dialog and Interactive Systems"},"forum":"main.527","id":"main.527","presentation_id":"38938726"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.531.png","content":{"abstract":"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.","authors":["Jin Dong","Marc-Antoine Rondeau","William L. Hamilton"],"demo_url":"","keywords":["relational reasoning","reasoning task","cross-modal transfer","text-based systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.551","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.666","main.1648","main.923","main.1159","main.1231"],"title":"Distilling Structured Knowledge for Text-Based Relational Reasoning","tldr":"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap be...","track":"Question Answering"},"forum":"main.531","id":"main.531","presentation_id":"38938727"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.540.png","content":{"abstract":"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models.","authors":["Yao Lu","Yue Dong","Laurent Charlin"],"demo_url":"","keywords":["multi-document summarization","multi-document task","multi-xscience","extreme summarization"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.648","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2900","main.693","main.1123","main.3581","main.714"],"title":"Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles","tldr":"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challen...","track":"Summarization"},"forum":"main.540","id":"main.540","presentation_id":"38938728"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.548.png","content":{"abstract":"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.","authors":["Xinhong Chen","Qing Li","Jianping Wang"],"demo_url":"","keywords":["extraction clauses","prediction","manual annotation","negative sampling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.252","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3329","main.1159","main.387","main.3532","main.1289"],"title":"Conditional Causal Relationships between Emotions and Causes in Texts","tldr":"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the...","track":"Information Retrieval and Text Mining"},"forum":"main.548","id":"main.548","presentation_id":"38938729"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.55.png","content":{"abstract":"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.","authors":["Akshay Smit","Saahil Jain","Pranav Rajpurkar","Anuj Pareek","Andrew Ng","Matthew Lungren"],"demo_url":"","keywords":["extraction labels","large-scale models","labeling","medical labeling"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.117","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1F","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1271","main.1942","demo.72","main.1611","main.2337"],"title":"Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT","tldr":"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual...","track":"NLP Applications"},"forum":"main.55","id":"main.55","presentation_id":"38938643"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.557.png","content":{"abstract":"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.","authors":["Sufeng Duan","Hai Zhao"],"demo_url":"","keywords":["chinese segmentation","chinese","greedy segmentation","smoother training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.317","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","main.930","main.1733","main.618","main.1952"],"title":"Attention Is All You Need for Chinese Word Segmentation","tldr":"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.557","id":"main.557","presentation_id":"38938730"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.574.png","content":{"abstract":"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.","authors":["Chen Zheng","Parisa Kordjamshidi"],"demo_url":"","keywords":["qa","graph network","semantic sub-graphs","graph encoder"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.714","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5E","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1648","main.2761","main.782","main.158","main.237"],"title":"SRLGRN: Semantic Role Labeling Graph Reasoning Network","tldr":"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supportin...","track":"Question Answering"},"forum":"main.574","id":"main.574","presentation_id":"38938731"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.585.png","content":{"abstract":"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.","authors":["Michelle Yuan","Mozhi Zhang","Benjamin Van Durme","Leah Findlater","Jordan Boyd-Graber"],"demo_url":"","keywords":["classification problem","cross-lingual embeddings","clime","interactive system"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.482","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1061","main.3093","main.1305","main.1130","main.2718"],"title":"Interactive Refinement of Cross-Lingual Word Embeddings","tldr":"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given...","track":"Machine Translation and Multilinguality"},"forum":"main.585","id":"main.585","presentation_id":"38938732"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.593.png","content":{"abstract":"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.","authors":["Oskar van der Wal","Silvan de Boer","Elia Bruni","Dieuwke Hupkes"],"demo_url":"","keywords":["unsupervised techniques","ugi techniques","referential games","emergent languages"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.270","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["CL.1","TACL.2141","main.3115","TACL.1936","TACL.2013"],"title":"The Grammar of Emergent Languages","tldr":"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appro...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.593","id":"main.593","presentation_id":"38938733"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.595.png","content":{"abstract":"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.","authors":["Johannes Bjerva","Nikita Bhutani","Behzad Golshan","Wang-Chiew Tan","Isabelle Augenstein"],"demo_url":"","keywords":["sentiment analysis","word-sense disambiguation","question qa","qa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.442","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3G","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1023","main.3186","main.1675","main.2973","main.1540"],"title":"SubjQA: A Dataset for Subjectivity and Review Comprehension","tldr":"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect...","track":"Question Answering"},"forum":"main.595","id":"main.595","presentation_id":"38938734"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.598.png","content":{"abstract":"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.","authors":["Jie Huang","Zilong Wang","Kevin Chang","Wen-mei Hwu","JinJun Xiong"],"demo_url":"","keywords":["natural processing","artificial intelligence","linear regression","semantic capacity"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.684","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2D","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2179","main.315","main.1938","main.1970","demo.89"],"title":"Exploring Semantic Capacity of Terms","tldr":"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity...","track":"Semantics: Lexical Semantics"},"forum":"main.598","id":"main.598","presentation_id":"38938735"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.60.png","content":{"abstract":"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.","authors":["Eric Wallace","Mitchell Stern","Dawn Song"],"demo_url":"","keywords":["machine mt","machine","mt","production systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.446","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9B","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2914","main.47","main.2895","main.1614","main.2313"],"title":"Imitation Attacks and Defenses for Black-box Machine Translation Systems","tldr":"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We ...","track":"Machine Learning for NLP"},"forum":"main.60","id":"main.60","presentation_id":"38938644"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.605.png","content":{"abstract":"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned. In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking. In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions. Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes. We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.","authors":["Jizhi Tang","Yansong Feng","Dongyan Zhao"],"demo_url":"","keywords":["procedural comprehension","state tracking","interactive network","interactive"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.591","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1528","main.1159","main.2377","main.1116","main.911"],"title":"Understanding Procedural Text using Interactive Entity Networks","tldr":"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent effort...","track":"Information Extraction"},"forum":"main.605","id":"main.605","presentation_id":"38938736"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.607.png","content":{"abstract":"We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (\u201clearn poses\u201d is a step in the larger goal of \u201cdoing yoga\u201d) and step-step temporal relations (\u201cbuy a yoga mat\u201d typically precedes \u201clearn poses\u201d). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.","authors":["Li Zhang","Qing Lyu","Chris Callison-Burch"],"demo_url":"","keywords":["reasoning tasks","common-sense inference","out-of-domain tasks","swag"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.374","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3H","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.928","main.3470","main.1191","demo.86","main.2054"],"title":"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow","tldr":"We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (\u201clearn poses\u201d is a step in the larger goal of \u201cdoing yoga\u201d) and step-step temporal relations (\u201cbuy a yoga mat\u201d typically precedes \u201clearn p...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.607","id":"main.607","presentation_id":"38938737"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.616.png","content":{"abstract":"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and speci\ufb01c linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The \ufb01ndings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These \ufb01ndings provide insights into the inner workings of Transformers.","authors":["Goro Kobayashi","Tatsuki Kuribayashi","Sho Yokoi","Kentaro Inui"],"demo_url":"","keywords":["natural processing","norm-based analyses","word alignment","transformers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.574","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1618","main.2696","main.1798","demo.60","main.618"],"title":"Attention is Not Only a Weight: Analyzing Transformers with Vector Norms","tldr":"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.616","id":"main.616","presentation_id":"38938738"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.618.png","content":{"abstract":"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish\u2192English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English\u2192German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.","authors":["Maximiliana Behnke","Kenneth Heafield"],"demo_url":"","keywords":["machine translation","inference","attention mechanism","transformer architecture"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.211","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1485","main.1798","main.130","main.3337","main.1960"],"title":"Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation","tldr":"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower ...","track":"Machine Translation and Multilinguality"},"forum":"main.618","id":"main.618","presentation_id":"38938739"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.619.png","content":{"abstract":"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.","authors":["Yvette Graham","Barry Haddow","Philipp Koehn"],"demo_url":"","keywords":["machine evaluation","human-parity mt","human translation","significance tests"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.6","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.84","main.894","main.888","main.2055","TACL.2221"],"title":"Statistical Power and Translationese in Machine Translation Evaluation","tldr":"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclus...","track":"Machine Translation and Multilinguality"},"forum":"main.619","id":"main.619","presentation_id":"38938740"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.623.png","content":{"abstract":"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur\u00edmac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.","authors":["Edoardo Maria Ponti","Goran Glava\u0161","Olga Majewska","Qianchu Liu","Ivan Vuli\u0107","Anna Korhonen"],"demo_url":"","keywords":["machine reasoning","cross-lingual transfer","causal reasoning","multilingual pretraining"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.185","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.143","main.2630","main.1803","main.1130","main.1379"],"title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning","tldr":"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired ...","track":"Machine Translation and Multilinguality"},"forum":"main.623","id":"main.623","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.628.png","content":{"abstract":"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which  reduces manual annotation cost.","authors":["Nayu Liu","Xian Sun","Hongfeng Yu","Wenkai Zhang","Guangluan Xu"],"demo_url":"","keywords":["multimodal summarization","multimodal tasks","multiencoder-decoder frameworks","multistage network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.144","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.702","main.3013","main.2853","main.3174","main.471"],"title":"Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos","tldr":"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods la...","track":"Summarization"},"forum":"main.628","id":"main.628","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.635.png","content":{"abstract":"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.","authors":["Reuben Tan","Bryan Plummer","Kate Saenko"],"demo_url":"","keywords":["large-scale disinformation","detecting inconsistencies","defense mechanism","adversaries"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.163","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2784","main.47","main.1614","main.2914","TACL.2129"],"title":"Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News","tldr":"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and in...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.635","id":"main.635","presentation_id":"38938743"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.638.png","content":{"abstract":"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or ''probe'' -- to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either ''the representation being rich in knowledge'', or ''the probe learning the task'', which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the ''good probe'' criteria proposed by the two papers, *selectivity* (Hewitt and Liang, 2019) and *information gain* (Pimentel et al., 2020), are equivalent -- the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.","authors":["Zining Zhu","Frank Rudzicz"],"demo_url":"","keywords":["supervised classification","diagnostic classification","neural representations","diagnostic classifier"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.744","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16B","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.947","main.2122","main.498","main.3181","main.1052"],"title":"An information theoretic view on selecting linguistic probes","tldr":"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or ''probe'' -- to perform supervised classification from internal representations. Howev...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.638","id":"main.638","presentation_id":"38938744"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.639.png","content":{"abstract":"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala\u2013English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.","authors":["Brian Thompson","Philipp Koehn"],"demo_url":"","keywords":["candidate generation","candidate re-scoring","wmt task","mt"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.483","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1503","main.754","main.825","main.2891","main.1061"],"title":"Exploiting Sentence Order in Document Alignment","tldr":"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result o...","track":"Machine Translation and Multilinguality"},"forum":"main.639","id":"main.639","presentation_id":"38938745"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.644.png","content":{"abstract":"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear---resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this---one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. $\\rho = 0.40$ in English). We then test our main hypothesis---that a word's lexical ambiguity should negatively correlate with its contextual uncertainty---and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.","authors":["Tiago Pimentel","Rowan Hall Maudslay","Damian Blasi","Ryan Cotterell"],"demo_url":"","keywords":["bert-based ambiguity","human annotation","lexical ambiguity","ambiguous words"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.328","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7B","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1935","main.3224","main.2891","main.2363","main.2251"],"title":"Speakers Fill Lexical Semantic Gaps with Context","tldr":"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language l...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.644","id":"main.644","presentation_id":"38938746"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.645.png","content":{"abstract":"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers---remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.","authors":["Jiaao Chen","Diyi Yang"],"demo_url":"","keywords":["text summarization","nlp","summarizing text","human-humanmachine interaction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.336","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1522","main.215","main.1702","main.699","main.128"],"title":"Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization","tldr":"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human...","track":"Summarization"},"forum":"main.645","id":"main.645","presentation_id":"38938747"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.647.png","content":{"abstract":"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans -- an Observer and a Locator -- complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task -- providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer's location within 3m in unseen buildings, vs. 70.4% for human Locators.","authors":["Meera Hahn","Jacob Krantz","Dhruv Batra","Devi Parikh","James Rehg","Stefan Lee","Peter Anderson"],"demo_url":"","keywords":["cooperative task","embodied dialog","cooperative localization","locator"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.59","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4C","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1113","main.355","main.373","main.1797","main.995"],"title":"Where Are You? Localization from Embodied Dialog","tldr":"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans -- an Observer and a Locator -- complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views whi...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.647","id":"main.647","presentation_id":"38938748"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.648.png","content":{"abstract":"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new  metrics for comparing sparse or truncated distributions: $\\epsilon$-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.","authors":["Pedro Henrique Martins","Zita Marinho","Andr\u00e9 F. T. Martins"],"demo_url":"","keywords":["story completion","dialogue generation","text generators","language models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.348","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8C","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.870","main.1707","main.2389","main.2491","main.3348"],"title":"Sparse Text Generation","tldr":"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncat...","track":"Language Generation"},"forum":"main.648","id":"main.648","presentation_id":"38938749"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.652.png","content":{"abstract":"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding.  Effective designs encode within the layout and formatting signals that point to where the important information can be found.  In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection.  Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models.  A qualitative post-hoc analysis illustrates how these features function within the model.","authors":["Yansen Wang","Zhen Fan","Carolyn Rose"],"demo_url":"","keywords":["kpe","nlp task","information retrieval","navigation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.140","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3646","demo.48","main.3496","main.3453","main.2688"],"title":"Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction","tldr":"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy...","track":"Information Retrieval and Text Mining"},"forum":"main.652","id":"main.652","presentation_id":"38938750"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.658.png","content":{"abstract":"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.  Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.","authors":["Zhen Han","Peng Chen","Yunpu Ma","Volker Tresp"],"demo_url":"","keywords":["representation kgs","temporal tasks","kgs","embedding approaches"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.593","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2873","main.2877","main.3084","main.973"],"title":"DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion","tldr":"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.  Temporal KGs often exhibit multiple simultaneous non-Euclidean structures,...","track":"Machine Learning for NLP"},"forum":"main.658","id":"main.658","presentation_id":"38938751"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.664.png","content":{"abstract":"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.","authors":["Sihan Wang","Kaijie Zhou","Kunfeng Lai","Jianping Shen"],"demo_url":"","keywords":["task-completion learning","decision-time planning","simulated task","monte search"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.278","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1522","main.1846","TACL.2143","main.215","main.699"],"title":"Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network","tldr":"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and...","track":"Dialog and Interactive Systems"},"forum":"main.664","id":"main.664","presentation_id":"38938752"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.666.png","content":{"abstract":"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.  Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.","authors":["Tao Shen","Yi Mao","Pengcheng He","Guodong Long","Adam Trischler","Weizhu Chen"],"demo_url":"","keywords":["self-supervised tasks","pre-training","entity linking","finetuning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.722","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.531","main.1231","main.1159","TACL.2121","main.923"],"title":"Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning","tldr":"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.  Building upon entity-level masked language models, our firs...","track":"Information Retrieval and Text Mining"},"forum":"main.666","id":"main.666","presentation_id":"38938753"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.668.png","content":{"abstract":"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events). Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses. Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision. We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.","authors":["Sudipta Kar","Gustavo Aguilar","Mirella Lapata","Thamar Solorio"],"demo_url":"","keywords":["characterizing stories","multi-view model","theme","style"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.454","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9C","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.645","main.1129","main.2650","main.1928","main.1952"],"title":"Multi-view Story Characterization from Movie Plot Synopses and Reviews","tldr":"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attr...","track":"Information Retrieval and Text Mining"},"forum":"main.668","id":"main.668","presentation_id":"38938754"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.675.png","content":{"abstract":"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not  come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.","authors":["Rishi Bommasani","Claire Cardie"],"demo_url":"","keywords":["dataset construction","large-scale evaluation","large-scale datasets","summarization research"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.649","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3012","main.3648","main.1389","demo.107","main.3552"],"title":"Intrinsic Evaluation of Summarization Datasets","tldr":"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sou...","track":"Summarization"},"forum":"main.675","id":"main.675","presentation_id":"38938755"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.684.png","content":{"abstract":"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.","authors":["Xin Lv","Xu Han","Lei Hou","Juanzi Li","Zhiyuan Liu","Wei Zhang","Yichi Zhang","Hao Kong","Suhui Wu"],"demo_url":"","keywords":["knowledge completion","reasoning","path search","multi-hop reasoning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.459","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10B","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1648","main.3084","main.923","main.300","main.1466"],"title":"Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph","tldr":"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot w...","track":"Information Extraction"},"forum":"main.684","id":"main.684","presentation_id":"38938756"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.689.png","content":{"abstract":"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.","authors":["Kun Xu","Haochen Tan","Linfeng Song","Han Wu","Haisong Zhang","Linqi Song","Dong Yu"],"demo_url":"","keywords":["multi-turn rewriting","attentive models","semantic labeling","semantic"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.537","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.215","main.2209","main.1561","main.1006","main.1201"],"title":"Semantic Role Labeling Guided Multi-turn Dialogue ReWriter","tldr":"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior foc...","track":"Dialog and Interactive Systems"},"forum":"main.689","id":"main.689","presentation_id":"38938757"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.693.png","content":{"abstract":"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.","authors":["Yumo Xu","Mirella Lapata"],"demo_url":"","keywords":["modeling interactions","query summarization","assembling summaries","question answering"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.296","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3581","main.1123","main.2587","TACL.2095","main.1023"],"title":"Coarse-to-Fine Query Focused Multi-Document Summarization","tldr":"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant su...","track":"Summarization"},"forum":"main.693","id":"main.693","presentation_id":"38938758"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.699.png","content":{"abstract":"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems.  In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.","authors":["Yufan Zhao","Can Xu","Wei Wu"],"demo_url":"","keywords":["multi-turn generation","response generation","word recovery","utterance recovery"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.279","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3179","main.1846","main.1522","main.1201","main.215"],"title":"Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks","tldr":"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the ...","track":"Dialog and Interactive Systems"},"forum":"main.699","id":"main.699","presentation_id":"38938759"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.701.png","content":{"abstract":"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.  Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named \\textsc{FEnmt}). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our \\textsc{FEnmt} could improve translation quality by effectively reducing unfaithful translations.","authors":["Rongxiang Weng","Heng Yu","Xiangpeng Wei","Weihua Luo"],"demo_url":"","keywords":["neural nmt","neural","nmt","training strategy"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.212","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.26","main.894","main.888","main.891","main.2055"],"title":"Towards Enhancing Faithfulness for Neural Machine Translation","tldr":"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.  Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g....","track":"Machine Translation and Multilinguality"},"forum":"main.701","id":"main.701","presentation_id":"38938760"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.702.png","content":{"abstract":"A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.","authors":["Mingzhe Li","Xiuying Chen","Shen Gao","Zhangming Chan","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["video-based summarization","human evaluations","vmsmo","dual-interaction-based summarizer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.752","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.628","main.3088","main.471","main.1085","main.3111"],"title":"VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles","tldr":"A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatic...","track":"Summarization"},"forum":"main.702","id":"main.702","presentation_id":"38938761"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.714.png","content":{"abstract":"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.","authors":["Yanyan Zou","Xingxing Zhang","Wei Lu","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["abstractive summarization","sequence-to-sequence problem","sentence reordering","next generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.297","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.471","main.2125","main.1835","main.2650","main.1023"],"title":"Pre-training for Abstractive Document Summarization by Reinstating Source Text","tldr":"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents ...","track":"Summarization"},"forum":"main.714","id":"main.714","presentation_id":"38938762"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.730.png","content":{"abstract":"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.","authors":["Lei Sha"],"demo_url":"","keywords":["lexically generation","real-world applications","lexically-constrained generation","unsupervised problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.701","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1707","main.2590","main.648","main.471","main.3010"],"title":"Gradient-guided Unsupervised Lexically Constrained Text Generation","tldr":"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generat...","track":"Language Generation"},"forum":"main.730","id":"main.730","presentation_id":"38938763"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.733.png","content":{"abstract":"Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect's sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models' performance on ARTS by up to 32.85%. Our code and new test set are available at https://github.com/zhijing-jin/ARTS_TestSet","authors":["Xiaoyu Xing","Zhijing Jin","Di Jin","Bingning Wang","Qi Zhang","Xuanjing Huang"],"demo_url":"","keywords":["aspect-based analysis","absa","absa models","arts"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.292","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2H","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1675","main.2784","main.3286","main.789","main.3579"],"title":"Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis","tldr":"Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-targe...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining"},"forum":"main.733","id":"main.733","presentation_id":"38938764"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.74.png","content":{"abstract":"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset).  A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.","authors":["Farhad Nooralahzadeh","Giannis Bekoulis","Johannes Bjerva","Isabelle Augenstein"],"demo_url":"","keywords":["strategic knowledge","downstream task","multilingual applications","natural tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.368","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.858","main.1803","main.407","main.400","main.2500"],"title":"Zero-Shot Cross-Lingual Transfer with Meta Learning","tldr":"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in t...","track":"Machine Translation and Multilinguality"},"forum":"main.74","id":"main.74","presentation_id":"38938645"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.744.png","content":{"abstract":"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.","authors":["Yuanmeng Yan","Keqing He","Hong Xu","Sihong Liu","Fanyu Meng","Min Hu","Weiran Xu"],"demo_url":"","keywords":["neural-based models","robust method","open-vocabulary slots","file name"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.490","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4I","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2005","main.891","main.2587","main.143","main.2790"],"title":"Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots","tldr":"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction no...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.744","id":"main.744","presentation_id":"38938765"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.745.png","content":{"abstract":"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained  on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple  source domains and must make predictions on a domain for which no labelled data has been seen.   Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where  the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training,  to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their  performance, suggesting that large transformer-based models are already relatively robust across  domains. Additionally, we show that mixture of experts leads to significant performance improvements  by comparing several variants of mixing functions, including one novel metric based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective metrics for mixing their predictions.","authors":["Dustin Wright","Isabelle Augenstein"],"demo_url":"","keywords":["unsupervised adaptation","cnns","rnns","domain classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.639","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3434","main.1458","TACL.2389","main.3023","main.493"],"title":"Transformer Based Multi-Source Domain Adaptation","tldr":"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained  on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where ...","track":"Machine Learning for NLP"},"forum":"main.745","id":"main.745","presentation_id":"38938766"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.748.png","content":{"abstract":"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.","authors":["Marco Basaldella","Fangyu Liu","Ehsan Shareghi","Nigel Collier"],"demo_url":"","keywords":["entity linking","el","string- models","to models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.253","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["demo.124","main.3049","main.1540","main.387","main.1631"],"title":"COMETA: A Corpus for Medical Entity Linking in the Social Media","tldr":"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understa...","track":"Information Retrieval and Text Mining"},"forum":"main.748","id":"main.748","presentation_id":"38938767"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.750.png","content":{"abstract":"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.","authors":["Ahmet \u00dcst\u00fcn","Arianna Bisazza","Gosse Bouma","Gertjan van Noord"],"demo_url":"","keywords":["multilingual parsing","soft sharing","multilingual approach","contextual modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.180","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1803","main.2718","main.3116","main.2278","main.267"],"title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing","tldr":"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel mul...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"main.750","id":"main.750","presentation_id":"38938768"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.754.png","content":{"abstract":"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.","authors":["Yuki Arase","Jun'ichi Tsujii"],"demo_url":"","keywords":["phrase alignment","modelling interactions","textual recognition","phrase problem"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.125","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.639","main.1957","main.1938","main.825","main.1503"],"title":"Compositional Phrase Alignment and Beyond","tldr":"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alig...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.754","id":"main.754","presentation_id":"38938769"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.76.png","content":{"abstract":"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model\u2019s structure. We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.","authors":["Weixin Liang","James Zou","Zhou Yu"],"demo_url":"","keywords":["active learning","data learning","learning","visual tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.355","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z8D","start_time":"Tue, 17 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2851","main.345","main.1611","main.1923","main.2040"],"title":"ALICE: Active Learning with Contrastive Natural Language Explanations","tldr":"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.76","id":"main.76","presentation_id":"38938646"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.763.png","content":{"abstract":"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.","authors":["Subhajit Chaudhury","Daiki Kimura","Kartik Talamadupula","Michiaki Tatsubori","Asim Munawar","Ryuki Tachibana"],"demo_url":"","keywords":["irrelevant removal","generalization","observation pruning","reinforcement methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.241","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1578","main.664","main.2389","main.3672","main.2583"],"title":"Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games","tldr":"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for ...","track":"Machine Learning for NLP"},"forum":"main.763","id":"main.763","presentation_id":"38938770"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.767.png","content":{"abstract":"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.","authors":["Simon Flachs","Oph\u00e9lie Lacroix","Helen Yannakoudakis","Marek Rei","Anders S\u00f8gaard"],"demo_url":"","keywords":["gec applications","gec","gec systems","internal model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.680","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5C","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["TACL.2047","TACL.2013","main.3181","main.2271","main.3115"],"title":"Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses","tldr":"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and re...","track":"NLP Applications"},"forum":"main.767","id":"main.767","presentation_id":"38938771"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.782.png","content":{"abstract":"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for textual multi-hop reasoning. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.","authors":["Nan Shao","Yiming Cui","Ting Liu","Shijin Wang","Guoping Hu"],"demo_url":"","keywords":["nlp areas","textual reasoning","graph networks","hotpotqa"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.583","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2761","main.574","main.1010","TACL.2121","main.1648"],"title":"Is Graph Structure Necessary for Multi-hop Question Answering?","tldr":"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop r...","track":"Question Answering"},"forum":"main.782","id":"main.782","presentation_id":"38938772"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.787.png","content":{"abstract":"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.","authors":["Jaehun Jung","Bokyung Son","Sungwon Lyu"],"demo_url":"","keywords":["dialogue systems","knowledge problem","path traversal","real-world systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.280","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1140","main.2212","main.128","main.485","main.689"],"title":"AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue","tldr":"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path tra...","track":"Dialog and Interactive Systems"},"forum":"main.787","id":"main.787","presentation_id":"38938773"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.789.png","content":{"abstract":"Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors' approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users' political opinions based on their content on social media. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors' approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians' approval from tweets.","authors":["Indira Sen","Fabian Fl\u00f6ck","Claudia Wagner"],"demo_url":"","keywords":["social-media-based estimates","labeling approval","sentiment methods","stance methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.110","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2651","main.2784","main.2996","main.3424","main.1287"],"title":"On the Reliability and Validity of Detecting Approval of Political Actors in Tweets","tldr":"Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors' approval. However, new challenges related to the reliability and validity of social-media-based esti...","track":"Computational Social Science and Social Media"},"forum":"main.789","id":"main.789","presentation_id":"38938774"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.802.png","content":{"abstract":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark.  Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","authors":["Emily Dinan","Angela Fan","Ledell Wu","Jason Weston","Douwe Kiela","Adina Williams"],"demo_url":"","keywords":["detecting bias","machine models","nlp models","fine-grained framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.23","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2886","main.838","main.834","main.1611","TACL.2011"],"title":"Multi-Dimensional Gender Bias Classification","tldr":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in tex...","track":"NLP Applications"},"forum":"main.802","id":"main.802","presentation_id":"38938775"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.809.png","content":{"abstract":"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.","authors":["Elena V. Epure","Guillaume Salha","Manuel Moussallam","Romain Hennequin"],"demo_url":"","keywords":["music perception","unsupervised annotation","musicology","music retrieval"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.386","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["CL.2","main.1613","main.2718","main.3116","main.2596"],"title":"Modeling the Music Genre Perception across Language-Bound Cultures","tldr":"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in...","track":"NLP Applications"},"forum":"main.809","id":"main.809","presentation_id":"38938776"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.820.png","content":{"abstract":"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of  class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.","authors":["Bai Li","Guillaume Thomas","Yang Xu","Frank Rudzicz"],"demo_url":"","keywords":["linguistic typology","contextualized embeddings","deep models","word flexibility"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.71","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1C","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3181","main.2363","main.2349","main.644","main.3224"],"title":"Word class flexibility: A deep contextualized approach","tldr":"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"main.820","id":"main.820","presentation_id":"38938777"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.821.png","content":{"abstract":"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.","authors":["Matthew Khoury","Rumen Dangovski","Longwu Ou","Preslav Nakov","Yichen Shen","Li Jing"],"demo_url":"","keywords":["natural applications","neural translation","neural nmt","neural"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.640","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5A","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1485","main.2491","main.1960","main.3337","main.522"],"title":"Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications","tldr":"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size re...","track":"NLP Applications"},"forum":"main.821","id":"main.821","presentation_id":"38938778"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.825.png","content":{"abstract":"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \\emph{and} document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.","authors":["Xuhui Zhou","Nikolaos Pappas","Noah A. Smith"],"demo_url":"","keywords":["text alignment","citation recommendation","plagiarism detection","predicting relationships"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.407","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3C","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.639","main.2382","main.1694","main.2367","main.3010"],"title":"Multilevel Text Alignment with Cross-Document Attention","tldr":"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence \\emph{and} document levels....","track":"Machine Learning for NLP"},"forum":"main.825","id":"main.825","presentation_id":"38938779"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.834.png","content":{"abstract":"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods---including the quantity of gendered words, a dialogue safety classifier, and human assessments---all of which show that our models generate less gendered, but equally engaging chit-chat responses.","authors":["Emily Dinan","Angela Fan","Adina Williams","Jack Urbanek","Douwe Kiela","Jason Weston"],"demo_url":"","keywords":["counterfactual augmentation","targeted collection","bias training","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.656","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5H","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.318","main.2444","main.1700","main.916","main.2072"],"title":"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation","tldr":"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative ch...","track":"Dialog and Interactive Systems"},"forum":"main.834","id":"main.834","presentation_id":"38938780"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.835.png","content":{"abstract":"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.","authors":["Ricardo Rei","Craig Stewart","Ana C Farinha","Alon Lavie"],"demo_url":"","keywords":["cross-lingual modeling","wmt task","high-performing systems","comet"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.213","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3688","main.1960","main.1803","main.143","main.870"],"title":"COMET: A Neural Framework for MT Evaluation","tldr":"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrai...","track":"Machine Translation and Multilinguality"},"forum":"main.835","id":"main.835","presentation_id":"38938781"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.838.png","content":{"abstract":"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.","authors":["Anjalie Field","Yulia Tsvetkov"],"demo_url":"","keywords":["appearance sexualization","unsupervised approach","adversarial learning","female politicians"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.44","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3D","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["TACL.2011","main.802","main.851","main.3352","main.789"],"title":"Unsupervised Discovery of Implicit Gender Bias","tldr":"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and presen...","track":"Computational Social Science and Social Media"},"forum":"main.838","id":"main.838","presentation_id":"38938782"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.84.png","content":{"abstract":"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.","authors":["Markus Freitag","David Grangier","Isaac Caswell"],"demo_url":"","keywords":["machine translation","automated evaluation","paraphrasing task","human evaluation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.5","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.888","main.619","main.210","main.2076","main.143"],"title":"BLEU might be Guilty but References are not Innocent","tldr":"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical...","track":"Machine Translation and Multilinguality"},"forum":"main.84","id":"main.84","presentation_id":"38938647"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.850.png","content":{"abstract":"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user\u2019s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).  In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.","authors":["Xilun Chen","Asish Ghoshal","Yashar Mehdad","Luke Zettlemoyer","Sonal Gupta"],"demo_url":"","keywords":["task-oriented parsing","low-resource adaptation","generalization","virtual assistants"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.413","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1130","main.891","main.1179","main.1061","main.148"],"title":"Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing","tldr":"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user\u2019s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully...","track":"Dialog and Interactive Systems"},"forum":"main.850","id":"main.850","presentation_id":"38938783"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.851.png","content":{"abstract":"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character\u2019s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.","authors":["Victor Martinez","Krishna Somandepalli","Yalda Tehranian-Uhls","Shrikanth Narayanan"],"demo_url":"","keywords":["creative production","movie representations","multi-task approach","violent content"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.387","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3B","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3352","main.3072","main.2561","main.838","main.789"],"title":"Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts","tldr":"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited i...","track":"NLP Applications"},"forum":"main.851","id":"main.851","presentation_id":"38938784"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.852.png","content":{"abstract":"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.","authors":["Alexandra Chronopoulou","Dario Stojanovski","Alexander Fraser"],"demo_url":"","keywords":["language model","lm","unsupervised system","unmt model"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.214","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.522","main.3688","TACL.2107","main.1680","main.267"],"title":"Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT","tldr":"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, howe...","track":"Machine Translation and Multilinguality"},"forum":"main.852","id":"main.852","presentation_id":"38938785"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.856.png","content":{"abstract":"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser\u2019s distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation.","authors":["Huda Khayrallah","Brian Thompson","Matt Post","Philipp Koehn"],"demo_url":"","keywords":["machine mt","mt","simulated training","simulated"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.7","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.888","main.2661","main.891","main.522","main.1572"],"title":"Simulated multiple reference training improves low-resource machine translation","tldr":"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel M...","track":"Machine Translation and Multilinguality"},"forum":"main.856","id":"main.856","presentation_id":"38938786"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.858.png","content":{"abstract":"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.","authors":["Phillip Keung","Yichao Lu","Julian Salazar","Vikas Bhardwaj"],"demo_url":"","keywords":["zero-shot learning","mldoc task","model selection","mldoc tasks"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.40","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.74","main.2500","main.1803","main.1032","main.407"],"title":"Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings","tldr":"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, publis...","track":"Machine Translation and Multilinguality"},"forum":"main.858","id":"main.858","presentation_id":"38938787"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.861.png","content":{"abstract":"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task  learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.","authors":["Miguel Ballesteros","Rishita Anubhai","Shuai Wang","Nima Pourdamghani","Yogarshi Vyas","Jie Ma","Parminder Bhatia","Kathleen McKeown","Yaser Al-Onaizan"],"demo_url":"","keywords":["predicting relations","transfer","neural architecture","training methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.436","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3E","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2579","main.1669","main.883","main.1116","main.1159"],"title":"Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events","tldr":"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Befo...","track":"Information Extraction"},"forum":"main.861","id":"main.861","presentation_id":"38938788"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.865.png","content":{"abstract":"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages.  In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.","authors":["Tasnim Mohiuddin","M Saiful Bari","Shafiq Joty"],"demo_url":"","keywords":["bilingual induction","bilingual","bli","semi-supervised method"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.215","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.410","main.2131","main.3205","main.3358","main.852"],"title":"LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space","tldr":"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric str...","track":"Machine Translation and Multilinguality"},"forum":"main.865","id":"main.865","presentation_id":"38938789"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.868.png","content":{"abstract":"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies' use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.","authors":["Tal August","Lauren Kim","Katharina Reinecke","Noah A. Smith"],"demo_url":"","keywords":["science communication","writing strategies","annotation scheme","transformer-based classifiers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.429","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3D","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1455","main.498","main.84","main.2444","main.3495"],"title":"Writing Strategies for Science Communication: Data and Computational Analysis","tldr":"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that...","track":"Computational Social Science and Social Media"},"forum":"main.868","id":"main.868","presentation_id":"38938790"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.87.png","content":{"abstract":"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.","authors":["Nicola De Cao","Michael Sejr Schlichtkrull","Wilker Aziz","Ivan Titov"],"demo_url":"","keywords":["model prediction","approximate search","erasure","sentiment classification"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.262","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2L","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2307","main.2834","main.2535","main.3183","TACL.2041"],"title":"How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking","tldr":"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objec...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.87","id":"main.87","presentation_id":"38938648"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.870.png","content":{"abstract":"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English.  We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics.  We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.","authors":["Angela Fan","Claire Gardent"],"demo_url":"","keywords":["multilingual generation","cross-lingual embeddings","pretraining","multilingual models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.231","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2I","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3116","main.648","main.267","main.2641","main.1649"],"title":"Multilingual AMR-to-Text Generation","tldr":"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an ad...","track":"Language Generation"},"forum":"main.870","id":"main.870","presentation_id":"38938791"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.871.png","content":{"abstract":"In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.","authors":["Yaobo Liang","Nan Duan","Yeyun Gong","Ning Wu","Fenfei Guo","Weizhen Qi","Ming Gong","Linjun Shou","Daxin Jiang","Guihong Cao","Xiaodong Fan","Ruofei Zhang","Rahul Agrawal","Edward Cui","Sining Wei","Taroon Bharti","Ying Qiao","Jiun-Hung Chen","Winnie Wu","Shuguang Liu","Fan Yang","Daniel Campos","Rangan Majumder","Ming Zhou"],"demo_url":"","keywords":["large-scale models","cross-lingual tasks","natural tasks","cross-lingual pre-training"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.484","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4A","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1803","main.852","main.143","main.1379","main.1680"],"title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation","tldr":"In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (...","track":"Machine Translation and Multilinguality"},"forum":"main.871","id":"main.871","presentation_id":"38938792"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.872.png","content":{"abstract":"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users\u2019 consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters' followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.","authors":["Nguyen Vo","Kyumin Lee"],"demo_url":"","keywords":["fact-checking","fact-checking systems","fact-checked information","misinformation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.621","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14B","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2117","main.789","main.527","main.3151","main.1004"],"title":"Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News","tldr":"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of...","track":"Computational Social Science and Social Media"},"forum":"main.872","id":"main.872","presentation_id":"38938793"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.875.png","content":{"abstract":"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.","authors":["Phillip Keung","Yichao Lu","Gy\u00f6rgy Szarvas","Noah A. Smith"],"demo_url":"","keywords":["multilingual classification","supervised classification","zero-shot learning","marc"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.369","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3A","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.504","main.2746","main.1298","main.1023","main.3116"],"title":"The Multilingual Amazon Reviews Corpus","tldr":"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected be...","track":"Machine Translation and Multilinguality"},"forum":"main.875","id":"main.875","presentation_id":"38938794"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.876.png","content":{"abstract":"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user. For example, for queries like 'ask my wife if she can pick up the kids' or 'remind me to take my pills', we need to rephrase the content to 'can you pick up the kids' and  'take your pills'. In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset  of 3000 pairs of original query and rephrased query. We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it. We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model","authors":["Arash Einolghozati","Anchit Gupta","Keith Diedrick","Sonal Gupta"],"demo_url":"","keywords":["rephrasing","messaging","virtual assistants","intent-slot tagging"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.414","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3F","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1179","main.210","main.2382","demo.54"],"title":"Sound Natural: Content Rephrasing in Dialog Systems","tldr":"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in ...","track":"Dialog and Interactive Systems"},"forum":"main.876","id":"main.876","presentation_id":"38938795"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.877.png","content":{"abstract":"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each node of a syntax tree is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation. We design a tree-parallel mini-batch strategy for efficient training and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.","authors":["Haiyan Wu","Ying Liu","Shaoyun Shi"],"demo_url":"","keywords":["sentence task","training predicting","tree-based modeling","modularized network"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.222","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2J","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3375","main.3013","main.1952","main.1957","main.2040"],"title":"Modularized Syntactic Neural Networks for Sentence Classification","tldr":"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modulari...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.877","id":"main.877","presentation_id":"38938796"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.883.png","content":{"abstract":"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity's representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3% relative loss in F1 on OntoNotes 5.0.","authors":["Patrick Xia","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":"","keywords":["modeling resolution","incremental algorithm","contextualized encoders","neural components"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.695","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5D","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1518","main.3647","main.1621","main.2579","main.891"],"title":"Incremental Neural Coreference Resolution in Constant Memory","tldr":"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scor...","track":"Information Extraction"},"forum":"main.883","id":"main.883","presentation_id":"38938797"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.888.png","content":{"abstract":"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser\u2019s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric\u2014conditioning on the source instead of the reference\u2014and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.","authors":["Brian Thompson","Matt Post"],"demo_url":"","keywords":["machine evaluation","zero-shot task","wmt task","quality estimation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.8","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.856","main.3688","main.522","main.3227","TACL.2107"],"title":"Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing","tldr":"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating par...","track":"Machine Translation and Multilinguality"},"forum":"main.888","id":"main.888","presentation_id":"38938798"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.891.png","content":{"abstract":"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.","authors":["Xiangpeng Wei","Heng Yu","Yue Hu","Rongxiang Weng","Luxi Xing","Weihua Luo"],"demo_url":"","keywords":["sequence-to-sequence task","nmt","inference","translation tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.216","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2635","main.522","main.856","main.214","main.1960"],"title":"Uncertainty-Aware Semantic Augmentation for Neural Machine Translation","tldr":"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only obs...","track":"Machine Translation and Multilinguality"},"forum":"main.891","id":"main.891","presentation_id":"38938799"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.894.png","content":{"abstract":"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.","authors":["Shamil Chollampatt","Raymond Hendy Susanto","Liling Tan","Ewa Szymanska"],"demo_url":"","keywords":["automatic post-editing","automatic","machine translations","ape task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.217","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.701","main.888","main.522","TACL.2107","main.3227"],"title":"Can Automatic Post-Editing Improve NMT?","tldr":"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine...","track":"Machine Translation and Multilinguality"},"forum":"main.894","id":"main.894","presentation_id":"38938800"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.903.png","content":{"abstract":"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.","authors":["Bang An","Jie Lyu","Zhenyi Wang","Chunyuan Li","Changwei Hu","Fei Tan","Ruiyi Zhang","Yifan Hu","Changyou Chen"],"demo_url":"","keywords":["natural applications","attention collapse","neural mechanism","bayesian perspective"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.17","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3B","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3513","main.1734","main.1670","main.355","main.2650"],"title":"Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference","tldr":"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. ...","track":"Machine Learning for NLP"},"forum":"main.903","id":"main.903","presentation_id":"38938801"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.910.png","content":{"abstract":"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.","authors":["Sebastian Goodman","Nan Ding","Radu Soricut"],"demo_url":"","keywords":["machine benchmark","news benchmarks","sequence models","teacher-forcing"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.702","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5G","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1356","main.2389","TACL.2255","main.2430","main.2078"],"title":"TeaForN: Teacher-Forcing with N-grams","tldr":"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, t...","track":"Language Generation"},"forum":"main.910","id":"main.910","presentation_id":"38938802"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.911.png","content":{"abstract":"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.","authors":["Ikuya Yamada","Akari Asai","Hiroyuki Shindo","Hideaki Takeda","Yuji Matsumoto"],"demo_url":"","keywords":["natural tasks","pretraining task","transformer","entity-related tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.523","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4C","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1528","main.1159","main.2849","main.989","main.327"],"title":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","tldr":"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and ...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"main.911","id":"main.911","presentation_id":"38938803"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.916.png","content":{"abstract":"Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn's existing contents. Further, an encoder-decoder neural framework is employed to  continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.","authors":["Lingzhi Wang","Jing Li","Xingshan Zeng","Haisong Zhang","Kam-Fai Wong"],"demo_url":"","keywords":["persuasions","automatic generation","language generation","encoder-decoder framework"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.538","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1863","main.1522","main.2444","main.128","main.645"],"title":"Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations","tldr":"Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an o...","track":"Computational Social Science and Social Media"},"forum":"main.916","id":"main.916","presentation_id":""},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.920.png","content":{"abstract":"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.","authors":["Rajat Agarwal","Katharina Kann"],"demo_url":"","keywords":["computational creativity","generation task","acrostic generation","pretraining"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.94","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1I","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2349","main.106","main.1658","main.3093","main.2766"],"title":"Acrostic Poem Generation","tldr":"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task...","track":"Language Generation"},"forum":"main.920","id":"main.920","presentation_id":"38938805"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.923.png","content":{"abstract":"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.","authors":["Haozhe Ji","Pei Ke","Shaohan Huang","Furu Wei","Xiaoyan Zhu","Minlie Huang"],"demo_url":"","keywords":["text tasks","generation","commonsense-aware generation","generative models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.54","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4B","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1648","main.666","main.531","main.1647","main.1706"],"title":"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph","tldr":"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate com...","track":"Language Generation"},"forum":"main.923","id":"main.923","presentation_id":"38938806"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.928.png","content":{"abstract":"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as ``what happened before/after [some event]?'' We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.","authors":["Qiang Ning","Hao Wu","Rujun Han","Nanyun Peng","Matt Gardner","Dan Roth"],"demo_url":"","keywords":["machine benchmarks","torque","roberta-large","temporal relationships"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.88","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1H","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2973","main.3186","main.607","demo.59","main.449"],"title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions","tldr":"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have p...","track":"Question Answering"},"forum":"main.928","id":"main.928","presentation_id":"38938807"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.930.png","content":{"abstract":"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model. Besides, we utilize a transfer learning method to improve the performance of OOV words. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our method achieves the state-of-the-art performances on all datasets. Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.","authors":["Kaiyu Huang","Degen Huang","Zhuang Liu","Fengran Mo"],"demo_url":"","keywords":["natural","chinese segmentation","chinese","chinese tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.318","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2E","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3656","main.557","main.3013","main.1733","main.16"],"title":"A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation","tldr":"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing metho...","track":"Phonology, Morphology and Word Segmentation"},"forum":"main.930","id":"main.930","presentation_id":"38938808"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.947.png","content":{"abstract":"To measure how well  pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier  trained to predict the  property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect  differences in representations.  For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates \"the amount of effort\" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.","authors":["Elena Voita","Ivan Titov"],"demo_url":"","keywords":["random tasks","estimating mdl","representations","pretrained representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.14","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1D","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2122","main.1551","main.638","main.3093","main.2893"],"title":"Information-Theoretic Probing with Minimum Description Length","tldr":"To measure how well  pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier  trained to predict the  property from the representations. Despite widespread adoption of probes, differences...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.947","id":"main.947","presentation_id":"38938809"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.954.png","content":{"abstract":"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a \u201cTwo-Teacher One-Student\u201d learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.","authors":["Wanwei He","Min Yang","Rui Yan","Chengming Li","Ying Shen","Ruifeng Xu"],"demo_url":"","keywords":["task completion","generating responses","task-oriented dialogue","task-oriented systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.281","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2B","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1201","main.1522","main.699","main.128"],"title":"Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training","tldr":"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a \u201cTwo-Teacher One-Student\u201d l...","track":"Dialog and Interactive Systems"},"forum":"main.954","id":"main.954","presentation_id":"38938810"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.955.png","content":{"abstract":"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii)  a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.","authors":["Wei Song","Ziyao Song","Ruiji Fu","Lizhen Liu","Miaomiao Cheng","Ting Liu"],"demo_url":"","keywords":["self-attention","structural encodings","inter-sentence attentions","sentence representation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.225","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2F","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.128","main.2141","main.2377","main.2512","main.1750"],"title":"Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays","tldr":"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent s...","track":"Discourse and Pragmatics"},"forum":"main.955","id":"main.955","presentation_id":"38938811"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.956.png","content":{"abstract":"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover's Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model's performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression","authors":["Jianquan Li","Xiaokang Liu","Honghong Zhao","Ruifeng Xu","Min Yang","Yaohong Jin"],"demo_url":"","keywords":["natural tasks","nlp tasks","matching","many-to-many mapping"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.242","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1485","main.3394","main.2783","main.1707","TACL.2041"],"title":"BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance","tldr":"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-c...","track":"NLP Applications"},"forum":"main.956","id":"main.956","presentation_id":"38938812"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.958.png","content":{"abstract":"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.","authors":["Pepa Atanasova","Jakob Grue Simonsen","Christina Lioma","Isabelle Augenstein"],"demo_url":"","keywords":["downstream tasks","machine learning","explainability techniques","diverse techniques"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.263","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2307","main.2585","main.76","main.2650"],"title":"A Diagnostic Study of Explainability Techniques for Text Classification","tldr":"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.958","id":"main.958","presentation_id":"38938813"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.959.png","content":{"abstract":"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"demo_url":"","keywords":["reasoning process","user study","model selection","explainable systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.575","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11A","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2228","main.1022","TACL.2049","main.2258","main.2380"],"title":"F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering","tldr":"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.959","id":"main.959","presentation_id":"38938814"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.96.png","content":{"abstract":"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).","authors":["Xinya Du","Claire Cardie"],"demo_url":"","keywords":["event extraction","entity recognition","error propagation","question task"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.49","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z4A","start_time":"Tue, 17 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1977","main.1116","main.2427","main.1421","main.2972"],"title":"Event Extraction by Answering (Almost) Natural Questions","tldr":"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the...","track":"Information Extraction"},"forum":"main.96","id":"main.96","presentation_id":"38938649"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.965.png","content":{"abstract":"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.","authors":["Dandan Huang","Leyang Cui","Sen Yang","Guangsheng Bao","Kun Wang","Jun Xie","Yue Zhang"],"demo_url":"","keywords":["text summarization","deep learning","automatic summarizers","summarization systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.33","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1023","main.3012","main.2125","main.3389","main.471"],"title":"What Have We Achieved on Text Summarization?","tldr":"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human profes...","track":"Summarization"},"forum":"main.965","id":"main.965","presentation_id":"38938815"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.973.png","content":{"abstract":"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.","authors":["Max Ryabinin","Sergei Popov","Liudmila Prokhorenkova","Elena Voita"],"demo_url":"","keywords":["word tasks","word embeddings","graphglove","unsupervised representations"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.594","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3093","main.2877","TACL.2121","demo.58","main.666"],"title":"Embedding Words in Non-Vector Space with Unsupervised Graph Learning","tldr":"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to ...","track":"Machine Learning for NLP"},"forum":"main.973","id":"main.973","presentation_id":"38938816"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.977.png","content":{"abstract":"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.","authors":["Yexiang Wang","Yi Guo","Siqi Zhu"],"demo_url":"","keywords":["dst","sa","annotation span","multiwoz"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.243","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2A","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.689","main.1702","main.1179","main.2209","main.1561"],"title":"Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking","tldr":"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models l...","track":"Machine Learning for NLP"},"forum":"main.977","id":"main.977","presentation_id":"38938817"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.983.png","content":{"abstract":"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.","authors":["Zhuang Chen","Tieyun Qian"],"demo_url":"","keywords":["aspect extraction","ate","neural taggers","taggers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.164","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5A","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.3375","main.1159","main.3581","main.1952","main.989"],"title":"Enhancing Aspect Term Extraction with Soft Prototypes","tldr":"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, s...","track":"Information Extraction"},"forum":"main.983","id":"main.983","presentation_id":"38938818"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.989.png","content":{"abstract":"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.  However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags),  while it is another challenge to obtain such effective resources.  In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector  based  on  reinforcement  learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks.  Extensive experiments on two  CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using   any annotated lexicon or corpus.","authors":["Ying Luo","Hai Zhao","Junlang Zhan"],"demo_url":"","keywords":["named recognition","entity detection","type prediction","deep models"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.723","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15A","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.1159","main.2851","main.2799","main.911","main.3013"],"title":"Named Entity Recognition Only from Word Embeddings","tldr":"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.  However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human anno...","track":"Information Retrieval and Text Mining"},"forum":"main.989","id":"main.989","presentation_id":"38938819"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.995.png","content":{"abstract":"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.  We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.","authors":["Yicong Hong","Cristian Rodriguez","Qi Wu","Stephen Gould"],"demo_url":"","keywords":["vision-and-language navigation","navigation","agent","sub-instruction modules"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.271","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2G","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1113","main.355","main.647","main.1402","TACL.2041"],"title":"Sub-Instruction Aware Vision-and-Language Navigation","tldr":"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visua...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.995","id":"main.995","presentation_id":"38938820"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//main.999.png","content":{"abstract":"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.","authors":["Haoyu Song","Yan Wang","Wei-Nan Zhang","Zhengyu Zhao","Ting Liu","Xiaojiang Liu"],"demo_url":"","keywords":["attribute consistency","profile identification","dialogue agents","key-value model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.539","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4D","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.689","main.2141","main.1654","main.485","main.2209"],"title":"Profile Consistency Identification for Open-domain Dialogue Agents","tldr":"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few effort...","track":"Dialog and Interactive Systems"},"forum":"main.999","id":"main.999","presentation_id":"38938821"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.1.png","content":{"abstract":"The formalism for Lexical-Functional Grammar (LFG) was introduced in the 1980\u2019s as one of the first constraint-based grammatical formalisms for natural language. It has led to substantial contributions to the linguistic literature and to the construction of large-scale descriptions of particular languages. Investigations of its mathematical properties have shown that, without further restrictions, the recognition, emptiness, and generation problems are undecidable, and that they are intractable in the worst case even with commonly applied restrictions. However, grammars of real languages appear not to invoke the full expressive power of the formalism, as indicated by the fact that algorithms and implementations for recognition and generation have been developed that run\u2014even for broad-coverage grammars\u2014in typically polynomial time. This paper formalizes some restrictions on the notation and its interpretation that are compatible with conventions and principles that have been implicit or informally stated in linguistic theory. We show that LFG grammars that respect these restrictions, although still suitable for the description of natural languages, are equivalent to linear context-free rewriting systems and allow for tractable computation.","authors":["J\u00fcrgen Wedekind","Ronald M. Kaplan"],"demo_url":"","keywords":["recognition","emptiness","tractable computation","constraint-based formalisms"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.593","main.2064","TACL.2141","TACL.2013","main.2179"],"title":"Tractable Lexical-Functional Grammar","tldr":"The formalism for Lexical-Functional Grammar (LFG) was introduced in the 1980\u2019s as one of the first constraint-based grammatical formalisms for natural language. It has led to substantial contributions to the linguistic literature and to the construc...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"CL.1","id":"CL.1","presentation_id":"38939389"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.2.png","content":{"abstract":"Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a methodology for comparing languages based on their organization of semantic concepts. We propose to conduct an adapted version of representational similarity analysis of a selected set of concepts in computational multilingual representations. Using this analysis method, we can reconstruct a phylogenetic tree that closely resembles those assumed by linguistic experts. These results indicate that multilingual distributional representations that are only trained on monolingual text and bilingual dictionaries preserve relations between languages without the need for any etymological information. In addition, we propose a measure to identify semantic drift between language families.We perform experiments on word-based and sentence-based multilingual models and provide both quantitative results and qualitative examples. Analyses of semantic drift in multilingual representations can serve two purposes: They can indicate unwanted characteristics of the computational models and they provide a quantitative means to study linguistic phenomena across languages.","authors":["Lisa Beinborn","Rochelle Choenni"],"demo_url":"","keywords":["multilingual representations","computational representations","representational analysis","analysis method"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6B","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2718","main.143","main.3116","main.410","main.3181"],"title":"Semantic Drift in Multilingual Representations","tldr":"Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a metho...","track":"Machine Translation and Multilinguality"},"forum":"CL.2","id":"CL.2","presentation_id":"38939390"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.3.png","content":{"abstract":"Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, very few attempts to incorporate out-of-game signals have been made. Specifically, it was previously unclear whether linguistic signals gathered from players\u2019 interviews can add information that does not appear in performance metrics. To bridge that gap, we define text classification tasks of predicting deviations from mean in NBA players\u2019 in-game actions, which are associated with strategic choices, player behavior, and risk, using their choice of language prior to the game. We collected a data set of transcripts from key NBA players\u2019 pre-game interviews and their in-game performance metrics, totalling 5,226 interview-metric pairs. We design neural models for players\u2019 action prediction based on increasingly more complex aspects of the language signals in their openended interviews. Our models can make their predictions based on the textual signal alone, or on a combination of that signal with signals from past-performance metrics. Our text-based models outperform strong baselines trained on performance metrics only, demonstrating the importance of language usage for action prediction. Moreover, the models that utilize both textual input and past-performance metrics produced the best results. Finally, as neural networks are notoriously difficult to interpret, we propose a method for gaining further insight into what our models have learned. Particularly, we present a latent Dirichlet allocation\u2013based analysis, where we interpretmodel predictions in terms of correlated topics. We find that our best performing textual modelis most associated with topics that are intuitively related to each prediction task and that bettermodels yield higher correlation with more informative topics.","authors":["Nadav Oved","Amir Feder","Roi Reichart"],"demo_url":"","keywords":["computer science","player prediction","text tasks","players prediction"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13B","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1578","main.2574","main.527","main.2410","main.851"],"title":"Predicting In-game Actions from Interviews of NBA Players","tldr":"Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, v...","track":"NLP Applications"},"forum":"CL.3","id":"CL.3","presentation_id":"38939391"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.4.png","content":{"abstract":"The transcription bottleneck is often cited as a major obstacle for efforts to document the world\u2019s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine translation, and recruit linguists to provide narrow phonetic transcriptions and sentence-aligned translations. However, I believe that these approaches are not a good fit with the available data and skills, or with long-established practices that are essentially word based. In seeking a more effective approach, I consider a century of transcription practice and a wide range of computational approaches, before proposing a computational model based on spoken term detection which I call \u201csparse transcription.\u201d This represents a shift away from current assumptions that we transcribe phones, transcribe fully, and transcribe first. Instead, sparse transcription combines the older practice of word-level transcription with interpretive, iterative, and interactive processes which are amenable to wider participation and which open the way to new methods for processing oral languages.","authors":["Steven Bird"],"demo_url":"","keywords":["transcription bottleneck","automatic recognition","machine translation","word-level transcription"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z9A","start_time":"Wed, 18 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["TACL.2221","main.648","main.3046","main.3257","main.3391"],"title":"Sparse Transcription","tldr":"The transcription bottleneck is often cited as a major obstacle for efforts to document the world\u2019s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine trans...","track":"Speech and Multimodality"},"forum":"CL.4","id":"CL.4","presentation_id":"38939392"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//CL.5.png","content":{"abstract":"Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outside values cannot. We view outside values as functions from inside values to the total value of all derivations, and we analyze outside computation in terms of function composition. This viewpoint helps explain why efficient outside computation is possible in many settings, despite the lack of a general outside algorithm for semiring operations.","authors":["Daniel Gildea"],"demo_url":"","keywords":["outside computation","semiring operations","weighted systems","parsing algorithms"],"material":null,"paper_type":"CL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.1936","demo.86","main.2253","main.2419","main.1625"],"title":"Efficient Outside Computation","tldr":"Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outsid...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"CL.5","id":"CL.5","presentation_id":"38939393"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1936.png","content":{"abstract":"Learning probabilistic context-free grammars from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). ** We proceed via a reparameterisation of (top-down) PCFGs which we call a bottom-up weighted context-free grammar. We show that if the grammar is anchored and satisfies additional restrictions on its ambiguity, then the parameters can be directly related to distributional properties of the anchoring strings; we show the asymptotic correctness of a naive estimator and present some simulations using synthetic data that show that algorithms based on this approach have good finite sample behaviour.","authors":["Alexander Clark","Nathana\u00ebl Fijalkow"],"demo_url":"","keywords":["computational linguistics","learning grammars","distributional learning","pcfgs"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2141","main.1494","main.2179","main.2702","main.2122"],"title":"Consistent Unsupervised Estimators for Anchored PCFGs","tldr":"Learning probabilistic context-free grammars from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs ...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.1936","id":"TACL.1936","presentation_id":"38939394"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1943.png","content":{"abstract":"Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model's architecture or training recipe can mean the difference between a positive and negative research result or between a state-of-the-art and under-performing system. While recent literature has proposed methods for automatic hyperparameter optimization (HPO), there has been limited work on applying these methods to NMT, due in part to the high costs associated with experiments that train large numbers of model variants. To facilitate research in this space, we introduce a lookup-based approach that uses a library of pre-trained models for fast, low cost HPO experimentation. Our contributions include (1) the release of a large collection of trained NMT models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for evaluating HPO methods on NMT, and (3) a reproducible benchmark of several HPO methods against our model library, including novel graph-based and multi-objective methods.","authors":["Xuan Zhang","Kevin Duh"],"demo_url":"","keywords":["hyperparameter selection","neural systems","automatic optimization","nmt"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 02:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z10C","start_time":"Wed, 18 Nov 2020 01:00:00 GMT"}],"similar_paper_uids":["main.1960","main.522","main.3227","TACL.1997","main.2661"],"title":"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems","tldr":"Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model's architecture or training recipe can mean the difference between a positive and ne...","track":"Machine Translation and Multilinguality"},"forum":"TACL.1943","id":"TACL.1943","presentation_id":"38939395"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1983.png","content":{"abstract":"Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as \"Obama is a _ by profession\". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as \"Obama worked as a _\" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.","authors":["Zhengbao Jiang","Frank F. Xu","Jun Araki","Graham Neubig"],"demo_url":"","keywords":["querying process","extracting knowledge","language models","lm"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15D","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2763","main.2630","TACL.2049","main.41","main.1866"],"title":"How Can We Know What Language Models Know","tldr":"Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as \"Obama is a _ by profession\". These prompts are usually manually created, and quite possibly...","track":"Language Generation"},"forum":"TACL.1983","id":"TACL.1983","presentation_id":"38939396"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.1997.png","content":{"abstract":"Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By employing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivalling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE.","authors":["Marina Fomicheva","Shuo Sun","Lisa Yankovskaya","Fr\u00e9d\u00e9ric Blain","Francisco Guzm\u00e1n","Mark Fishel","Nikolaos Aletras","Vishrav Chaudhary","Lucia Specia"],"demo_url":"","keywords":["machine mt","real-world applications","qe","uncertainty quantification"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1B","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2061","TACL.1943","main.888","main.894","main.522"],"title":"Unsupervised Quality Estimation for Neural Machine Translation","tldr":"Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of exper...","track":"Machine Translation and Multilinguality"},"forum":"TACL.1997","id":"TACL.1997","presentation_id":"38939397"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2011.png","content":{"abstract":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology which not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighbouring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric - Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02%. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","authors":["Vaibhav Kumar","Tenzin Bhotia","Vaibhav Kumar","Tanmoy Chakraborty"],"demo_url":"","keywords":["word embeddings","semantic words","coreference resolution","post-processing methods"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11D","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1018","main.3093","main.838","main.1399","main.2596"],"title":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings","tldr":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-proces...","track":"Semantics: Lexical Semantics"},"forum":"TACL.2011","id":"TACL.2011","presentation_id":"38939398"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2013.png","content":{"abstract":"We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP), a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs, i.e. pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and TransformerXL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.","authors":["Alex Warstadt","Alicia Parrish","Haokun Liu","Anhad Monananey","Wei Peng","Sheng-Fu Wang","Samuel Bowman"],"demo_url":"","keywords":["linguistic","blimp","lms","linguist-crafted templates"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7B","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3181","main.143","TACL.2141","main.2847","main.2179"],"title":"BLiMP: The Benchmark of Linguistic Minimal Pairs for English","tldr":"We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP), a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,00...","track":"Linguistic Theories, Cognitive Modeling and Psycholinguistics"},"forum":"TACL.2013","id":"TACL.2013","presentation_id":"38939399"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2041.png","content":{"abstract":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.","authors":["Alon Talmor","Yanai Elazar","Yoav Goldberg","Jonathan Berant"],"demo_url":"","keywords":["symbolic tasks","reasoning tasks","zero-shot evaluation","pre-training"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2491","main.1130","main.74","TACL.2411","main.2838"],"title":"oLMpics - On what Language Model Pre-training Captures","tldr":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited an...","track":"Question Answering"},"forum":"TACL.2041","id":"TACL.2041","presentation_id":"38939400"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2047.png","content":{"abstract":"Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning data in the BEA-2019 shared task. Building upon recent work in Neural Machine Translation (NMT), we make use of both kinds of data by deriving example-level scores on our large pretraining data based on a smaller, higher-quality dataset. In this work, we perform an empirical study to discover how to best incorporate delta-log-perplexity, a type of example scoring, into a training schedule for GEC. In doing so, we perform experiments that shed light on the function and applicability of delta-log-perplexity. Models trained on scored data achieve state-of-the-art results on common GEC test sets.","authors":["Jared Lichtarge","Chris Alberti","Shankar Kumar"],"demo_url":"","keywords":["neural nmt","neural","example scoring","gec"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2B","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.767","main.1351","main.3227","main.1960","main.2491"],"title":"Data Weighted Training Strategies for Grammatical Error Correction","tldr":"Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning...","track":"NLP Applications"},"forum":"TACL.2047","id":"TACL.2047","presentation_id":"38939401"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2049.png","content":{"abstract":"Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning\u2014two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of \u201chops\u201d in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.","authors":["Kyle Richardson","Ashish Sabharwal"],"demo_url":"","keywords":["knowledge challenges","benchmark tasks","diagnostic tasks","taxonomic reasoning"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z1C","start_time":"Mon, 16 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3035","main.2258","main.41","main.2228","main.2380"],"title":"What Does My QA Model Know? Devising Controlled Probes using Expert","tldr":"Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing wheth...","track":"Question Answering"},"forum":"TACL.2049","id":"TACL.2049","presentation_id":"38939402"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2055.png","content":{"abstract":"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.","authors":["Lifu Tu","Garima Lalwani","Spandana Gella","He He"],"demo_url":"","keywords":["generalization","natural inference","paraphrase identification","pre-trained models"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 00:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z15C","start_time":"Wed, 18 Nov 2020 23:00:00 GMT"}],"similar_paper_uids":["main.2491","TACL.2041","TACL.2047","main.1631","main.1196"],"title":"An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models","tldr":"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"TACL.2055","id":"TACL.2055","presentation_id":"38939403"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2083.png","content":{"abstract":"We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents; a word is generated by a hidden semantic vector encoding its contextual semantic meaning; and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared to existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.","authors":["Lixing Zhu","Deyu Zhou","Yulan He"],"demo_url":"","keywords":["word evaluation","word disambiguation","sentiment classification","generative model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["TACL.2093","main.30","main.2792","main.3093","main.3358"],"title":"A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings","tldr":"We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents; a word is generated by a hidden...","track":"Machine Learning for NLP"},"forum":"TACL.2083","id":"TACL.2083","presentation_id":"38939404"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2093.png","content":{"abstract":"Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (ETM), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the ETM models each word with a categorical distribution whose natural parameter is the inner product between the word's embedding and an embedding of its assigned topic. To fit the ETM, we develop an efficient amortized variational inference algorithm. The ETM discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation (LDA), in terms of both topic quality and predictive performance.","authors":["Adji Bousso Dieng","Francisco Ruiz","David Blei"],"demo_url":"","keywords":["generative documents","topic modeling","topic models","embedded model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14C","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2792","main.30","TACL.2083","main.2931","main.1498"],"title":"Topic Modeling in Embedding Spaces","tldr":"Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (ETM), ...","track":"Machine Learning for NLP"},"forum":"TACL.2093","id":"TACL.2093","presentation_id":"38939405"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2095.png","content":{"abstract":"For many NLP applications, such as question answering and summarisation, the goal is to select the best solution from a large space of candidates to meet a particular user\u2019s needs. To address the lack of user or task-specific training data, we propose an interactive text ranking approach that actively selects pairs of candidates, from which the user selects the best. Unlike previous strategies, which attempt to learn a ranking across the whole candidate space, our method employs Bayesian optimisation to focus the user\u2019s labelling effort on high quality candidates and integrate prior knowledge to cope better with small data scenarios. We apply our method to community question answering (cQA) and extractive multi-document summarisation, finding that it significantly outperforms existing interactive approaches. We also show that the ranking function learned by our method is an effective reward function for reinforcement learning, which improves the state of the art for interactive summarisation.","authors":["Edwin Simpson","Yang Gao","Iryna Gurevych"],"demo_url":"","keywords":["nlp applications","question answering","question summarisation","community answering"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5C","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.693","main.1023","main.471","main.1949","demo.54"],"title":"Interactive Text Ranking with Bayesian Optimisation: A Case Study on Community QA and Summarisation","tldr":"For many NLP applications, such as question answering and summarisation, the goal is to select the best solution from a large space of candidates to meet a particular user\u2019s needs. To address the lack of user or task-specific training data, we propos...","track":"Machine Learning for NLP"},"forum":"TACL.2095","id":"TACL.2095","presentation_id":"38939406"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2103.png","content":{"abstract":"When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outside-to-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving the F1-scores of 85.82%, 84.34%, and 77.36% on ACE-2004, ACE-2005, and GENIA datasets, respectively","authors":["Takashi Shibuya","Eduard Hovy"],"demo_url":"","keywords":["inference","flat tasks","neural model","decoding method"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12B","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.1755","main.989","main.3216","main.2799","main.1528"],"title":"Nested Named Entity Recognition via Second-best Sequence Learning and Decoding","tldr":"When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an ob...","track":"Information Extraction"},"forum":"TACL.2103","id":"TACL.2103","presentation_id":"38939407"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2107.png","content":{"abstract":"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.","authors":["Jiatao Gu","Yinhan Liu","Naman Goyal","Xian Li","Sergey Edunov","Marjan Ghazvininejad","Mike Lewis","Luke Zettlemoyer"],"demo_url":"","keywords":["machine tasks","pre-training","multilingual pre-training","mbart"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3C","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1680","main.852","main.3688","main.522","main.888"],"title":"Multilingual Denoising Pre-training for Neural Machine Translation","tldr":"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-sc...","track":"Machine Translation and Multilinguality"},"forum":"TACL.2107","id":"TACL.2107","presentation_id":"38939408"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2121.png","content":{"abstract":"Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models which encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.","authors":["Leonardo F. R. Ribeiro","Yue Zhang","Claire Gardent","Iryna Gurevych"],"demo_url":"","keywords":["graph-to-text models","global aggregation","node representations","global encoding"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2761","main.782","main.666","main.1231","main.1010"],"title":"Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs","tldr":"Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as...","track":"Language Generation"},"forum":"TACL.2121","id":"TACL.2121","presentation_id":"38939409"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2129.png","content":{"abstract":"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model. We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself (41.0F1).","authors":["Max Bartolo","Alastair Roberts","Johannes Welbl","Sebastian Riedel","Pontus Stenetorp"],"demo_url":"","keywords":["annotation methodology","annotation process","training","rc models"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z11C","start_time":"Wed, 18 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.2313","main.1923","main.359","main.3183","TACL.2389"],"title":"Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension","tldr":"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, suc...","track":"Question Answering"},"forum":"TACL.2129","id":"TACL.2129","presentation_id":"38939410"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2135.png","content":{"abstract":"Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to describe with brief sentences, and sometimes require examples to fully convey the user\u2019s intent. We present a framework for regex synthesis in this setting where both natural language (NL) and examples are available. First, a semantic parser (either grammar-based or neural) maps the natural language description into an intermediate sketch, which is an incomplete regex containing holes to denote missing components. Then a program synthesizer searches over the regex space defined by the sketch and finds a regex that is consistent with the given string examples. Our semantic parser can be trained purely from weak supervision based on correctness of the synthesized regex, or it can leverage heuristically-derived sketches. We evaluate on two prior datasets (Kushman and Barzilay, 2013; Locascio et al., 2016) and a real-world dataset from Stack Overflow. Our system achieves state-of-the-art performance on the prior datasets and solves 57% of the real-world dataset, which existing neural systems completely fail on.","authors":["Xi Ye","Qiaochu Chen","Xinyu Wang","Isil Dillig","Greg Durrett"],"demo_url":"","keywords":["regex synthesis","semantic parser","program synthesizer","neural systems"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2D","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.2590","main.1130","main.1647","main.2342","main.648"],"title":"Sketch-Driven Regular Expression Generation from Natural Language and Examples","tldr":"Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to descr...","track":"Semantics: Sentence-level Semantics, Textual Inference and Other areas"},"forum":"TACL.2135","id":"TACL.2135","presentation_id":"38939411"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2141.png","content":{"abstract":"In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g. lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs which allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone. Code is available at https://github.com/neulab/neural-lpcfg.","authors":["Hao Zhu","Yonatan Bisk","Graham Neubig"],"demo_url":"","keywords":["grammar induction","unsupervised induction","context methods","syntactic formalisms"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z13D","start_time":"Wed, 18 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["TACL.2013","main.2363","TACL.1936","main.1957","main.2179"],"title":"The Return of Lexical Dependencies: Neural Lexicalized PCFGs","tldr":"In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either c...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.2141","id":"TACL.2141","presentation_id":"38939412"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2143.png","content":{"abstract":"We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset and code for replicating experiments are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines. ","authors":["Jacob Andreas","John Bufe","David Burkett","Charles Chen","Josh Clausman","Jean Crawford","Kate Crim","Jordan DeLoach","Leah Dorner","Jason Eisner","Hao Fang","Alan Guo","David Hall","Kristin Hayes","Kellie Hill","Diana Ho","Wendy Iwaszuk","Smriti Jha","Dan Klein","Jayant Krishnamurthy","Theo Lanman","Percy Liang","Christopher Lin","Ilya Lintsbakh","Andy McGovern","Alexander Nisnevich","Adam Pauls","Brent Read","Dan Roth","Subhro Roy","Beth Short","Div Slomin","Ben Snyder","Stephon Striplin","Yu Su","Zachary Tellman","Sam Thomson","Andrei Vorobev","Izabela Witoszko","Jason Wolfe","Abby Wray","Yuchen Zhang","Alexander Zotov","Jesse Rusak","Dmitrij Petters"],"demo_url":"","keywords":["task-oriented dialogue","revision","dialogue agent","metacomputation operators"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Mon, 16 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z2C","start_time":"Mon, 16 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["main.1846","main.1702","main.1140","main.1201","main.1012"],"title":"Task-Oriented Dialogue as Dataflow Synthesis","tldr":"We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and...","track":"Dialog and Interactive Systems"},"forum":"TACL.2143","id":"TACL.2143","presentation_id":"38939413"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2169.png","content":{"abstract":"Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search since the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is beam search--a pruned version of breadth-first search. Quite surprisingly, beam search often returns better results than exact inference due to beneficial search bias for NLP tasks. In this work, we show that the standard implementation of beam search can be made up to 10x faster in practice. Our method assumes that the scoring function is monotonic in the sequence length, which allows us to safely prune hypotheses that cannot be in the final set of hypotheses early on. We devise effective monotonic approximations to popular nonmonontic scoring functions, including length normalization and mutual information decoding. Lastly, we propose a memory-reduced variant of Best-First Beam Search, which has a similar beneficial search bias in terms of downstream performance, but runs in a fraction of the time.","authors":["Clara Meister","Ryan Cotterell","Tim Vieira"],"demo_url":"","keywords":["nlp tasks","exact search","decoding","heuristic algorithm"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 09:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z5B","start_time":"Tue, 17 Nov 2020 08:00:00 GMT"}],"similar_paper_uids":["main.1377","main.2767","main.3348","main.2198","main.730"],"title":"A* Beam Search","tldr":"Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search since the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is bea...","track":"Language Generation"},"forum":"TACL.2169","id":"TACL.2169","presentation_id":"38939414"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2221.png","content":{"abstract":"The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explores end-to-end trainable direct models that translate without transcribing. However, transcripts can be an indispensable output in practical applications, which often display transcripts alongside the translations to users. ** We make this common requirement explicit and explore the task of jointly transcribing and translating speech. While high accuracy of transcript and translation are crucial, even highly accurate systems can suffer from inconsistencies between both outputs that degrade the user experience. We introduce a methodology to evaluate consistency and compare several modeling approaches, including the traditional cascaded approach and end-to-end models. We find that direct models are poorly suited to the joint transcription/translation task, but that end-to-end models that feature a coupled inference procedure are able to achieve strong consistency. We further introduce simple techniques for directly optimizing for consistency, and analyze the resulting trade-offs between consistency, transcription accuracy, and translation accuracy.","authors":["Matthias Sperber","Hendra Setiawan","Christian Gollan","Udhay Nallasamy","Matthias Paulik"],"demo_url":"","keywords":["speech translation","jointly speech","joint task","speech step"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 18:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z14A","start_time":"Wed, 18 Nov 2020 17:00:00 GMT"}],"similar_paper_uids":["CL.4","main.2915","main.1402","main.1100","demo.111"],"title":"Consistent Transcription and Translation of Speech","tldr":"The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explo...","track":"Speech and Multimodality"},"forum":"TACL.2221","id":"TACL.2221","presentation_id":"38939415"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2255.png","content":{"abstract":"Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT (Devlin et al., 2019) with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves in-domain model performance, yields effective reduced-size models and increases model stability.","authors":["Roi Reichart","Eyal Ben David","Carmel Rabinovitz"],"demo_url":"","keywords":["domain adaptation","nlp","sentiment setups","in-domain model"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12C","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2078","main.1733","main.3292","main.910","main.1428"],"title":"PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models","tldr":"Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target d...","track":"Machine Learning for NLP"},"forum":"TACL.2255","id":"TACL.2255","presentation_id":"38939416"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2389.png","content":{"abstract":"There is an increasing focus on model-based dialog evaluation metrics such as ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign a high score to all relevant responses and a low score to all irrelevant responses. Ideally, such models should be trained using multiple relevant and irrelevant responses for any given context. However, no such data is publicly available, and hence existing models are usually trained using a single relevant response and multiple randomly selected responses from other contexts (random negatives). To allow for better training and robust evaluation of model-based metrics, we introduce the DailyDialog++ dataset, consisting of (i) five relevant responses for each context and (ii) five \\textit{adversarially crafted} irrelevant responses for each context. Using this dataset, we first show that even in the presence of multiple correct references, n-gram based metrics and embedding based metrics do not perform well at separating relevant responses from even random negatives. While model-based metrics perform better than n-gram and embedding based metrics on random negatives, their performance drops substantially when evaluated on adversarial examples. To check if large scale pretraining could help, we propose a new BERT-based evaluation metric called DEB, which is pretrained on 727M Reddit conversations and then finetuned on our dataset. DEB significantly outperforms existing models, showing better correlation with human judgements and better performance on random negatives (88.27% accuracy). However, its performance again drops substantially, when evaluated on adversarial responses, thereby highlighting that even large-scale pretrained evaluation models are not robust to the adversarial examples in our dataset. The dataset and code are publicly available. (Dataset: https://iitmnlp.github.io/DailyDialog-plusplus/ and Code: https://github.com/iitmnlp/Dialogue-Evaluation-with-BERT).","authors":["Ananya Sai","Akash Mohan Kumar","Siddhartha Arora","Mitesh Khapra"],"demo_url":"","keywords":["large pretraining","embedding metrics","n-gram metrics","deb"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z12A","start_time":"Wed, 18 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.3434","main.1923","main.2313","main.2914","TACL.2129"],"title":"Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining","tldr":"There is an increasing focus on model-based dialog evaluation metrics such as ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign a high score to all relevant responses and a low score to all irrelevant responses. Ideally,...","track":"Dialog and Interactive Systems"},"forum":"TACL.2389","id":"TACL.2389","presentation_id":"38939417"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//TACL.2411.png","content":{"abstract":"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases. To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model. Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM. Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark. Our findings demonstrate the benefits of syntactic biases, even for representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are helpful in benchmarks of natural language understanding.","authors":["Adhiguna Kuncoro","Lingpeng Kong","Daniel Fried","Dani Yogatama","Laura Rimell","Chris Dyer","Phil Blunsom"],"demo_url":"","keywords":["bert pretraining","structured tasks","natural understanding","textual learners"],"material":null,"paper_type":"TACL","pdf_url":"","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 10:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z6A","start_time":"Tue, 17 Nov 2020 09:00:00 GMT"}],"similar_paper_uids":["main.2851","main.3635","main.2363","main.1970","TACL.2041"],"title":"Syntactic Structure Distillation Pretraining for Bidirectional Encoders","tldr":"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether s...","track":"Syntax: Tagging, Chunking, and Parsing"},"forum":"TACL.2411","id":"TACL.2411","presentation_id":"38939418"},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.102.png","content":{"abstract":"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models---including classification, seq2seq, and structured prediction---and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.","authors":["Ian Tenney","James Wexler","Jasmijn Bastings","Tolga Bolukbasi","Andy Coenen","Sebastian Gehrmann","Ellen Jiang","Mahima Pushkarna","Carey Radebaugh","Emily Reif","Ann Yuan"],"demo_url":"","keywords":["visualization models","counterfactual generation","rapid exploration","rapid analysis"],"material":"[Language Interpretability Tool website](https://pair-code.github.io/lit)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.15","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["demo.48","main.2268","demo.127","main.3648","main.1866"],"title":"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models","tldr":"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What ...","track":"System Demonstrations"},"forum":"demo.102","id":"demo.102","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.104.png","content":{"abstract":"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack\u2019s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16  adversarial attacks  from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.","authors":["John Morris","Eli Lifland","Jin Yong Yoo","Jake Grigsby","Di Jin","Yanjun Qi"],"demo_url":"","keywords":["adversarial attacks","data augmentation","adversarial nlp","glue tasks"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.16","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4K","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2895","main.2313","main.47","main.1614"],"title":"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP","tldr":"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper ...","track":"System Demonstrations"},"forum":"demo.104","id":"demo.104","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.107.png","content":{"abstract":"High-quality and large-scale data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators efficiently; and (3) reproducibility. To address these problems, we introduce CROWDAQ, an open-source platform that standardizes the data collection pipeline with customizable user-interface components, automated annotator qualification, and saved pipelines in a re-usable format. We show that CROWDAQ simplifies data annotation significantly on a diverse set of data collection use cases and we hope it will be a convenient tool for the community.","authors":["Qiang Ning","Hao Wu","Pradeep Dasigi","Dheeru Dua","Matt Gardner","Robert L Logan IV","Ana Marasovi\u0107","Zhen Nie"],"demo_url":"","keywords":["ai systems","automated qualification","data annotation","crowdaq"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.17","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4K","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["demo.72","main.1923","main.1389","main.675","main.210"],"title":"Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ","tldr":"High-quality and large-scale data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators...","track":"System Demonstrations"},"forum":"demo.107","id":"demo.107","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.109.png","content":{"abstract":"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight has so far served over 15K users with over 42K page views and 13\\% returns.","authors":["Tom Hope","Jason Portenoy","Kishore Vasan","Jonathan Borchardt","Eric Horvitz","Daniel Weld","Marti Hearst","Jevin West"],"demo_url":"","keywords":["targeted queries","discovery connections","exploratory research","scisight"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.18","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3419","demo.124","main.2825","main.748","demo.102"],"title":"SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search","tldr":"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery...","track":"System Demonstrations"},"forum":"demo.109","id":"demo.109","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.111.png","content":{"abstract":"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.","authors":["Xutai Ma","Mohammad Javad Dousti","Changhan Wang","Jiatao Gu","Juan Pino"],"demo_url":"","keywords":["simultaneous translation","real-time scenario","evaluating models","simultaneous scenario"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.19","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1402","TACL.2221","main.1572","main.3236","main.2100"],"title":"SIMULEVAL: An Evaluation Toolkit for Simultaneous Translation","tldr":"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline mode...","track":"System Demonstrations"},"forum":"demo.111","id":"demo.111","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.116.png","content":{"abstract":"Customer support agents play a crucial role as an interface between an organization and its end-users. We propose CAIRAA: Conversational Approach to Information Retrieval for Agent Assistance, to reduce the cognitive workload of support agents who engage with users through conversation systems. CAIRAA monitors an evolving conversation and recommends both responses and URLs of documents the agent can use in replies to their client. We combine traditional information retrieval (IR) approaches with more recent Deep Learning (DL) models to ensure high accuracy and efficient run-time performance in the deployed system. Here, we describe the CAIRAA system and demonstrate its effectiveness in a pilot study via a short video.","authors":["Kshitij Fadnis","Nathaniel Mills","Jatin Ganhotra","Haggai Roitman","Gaurav Pandey","Doron Cohen","Yosi Mass","Shai Erera","Chulaka Gunasekara","Danish Contractor","Siva Patel","Q. Vera Liao","Sachindra Joshi","Luis Lastras","David Konopnicki"],"demo_url":"","keywords":["information retrieval","agent assistance","run-time","customer agents"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.20","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1390","main.2281","main.1522","main.2141","main.527"],"title":"Agent Assist through Conversation Analysis","tldr":"Customer support agents play a crucial role as an interface between an organization and its end-users. We propose CAIRAA: Conversational Approach to Information Retrieval for Agent Assistance, to reduce the cognitive workload of support agents who en...","track":"System Demonstrations"},"forum":"demo.116","id":"demo.116","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.118.png","content":{"abstract":"We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use richer representations of the context. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a simple unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at neuspell.github.io.","authors":["Sai Muralidhar Jayanthi","Danish Pruthi","Graham Neubig"],"demo_url":"","keywords":["spelling correction","spell-checkers","neuspell","open-source toolkit"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.21","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4K","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2357","main.648","main.2490","main.3227","main.2491"],"title":"NeuSpell: A Neural Spelling Correction Toolkit","tldr":"We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately l...","track":"System Demonstrations"},"forum":"demo.118","id":"demo.118","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.119.png","content":{"abstract":"LibKGE ( https://github.com/uma-pi1/kge )  is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LibKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LibKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates in-depth analysis. LibKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LibKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.","authors":["Samuel Broscheit","Daniel Ruffinelli","Adrian Kochsiek","Patrick Betz","Rainer Gemulla"],"demo_url":"","keywords":["training","hyperparameter optimization","link prediction","in-depth analysis"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.22","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1493","main.1787","demo.58","main.1706","demo.48"],"title":"LibKGE - A knowledge graph embedding library for reproducible research","tldr":"LibKGE ( https://github.com/uma-pi1/kge )  is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible r...","track":"System Demonstrations"},"forum":"demo.119","id":"demo.119","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.123.png","content":{"abstract":"A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary systems, but they support English reverse dictionary queries only and their performance is far from perfect. In this paper, we present a new open-source online reverse dictionary system named WantWords (https://wantwords.thunlp.org/). It not only significantly outperforms other reverse dictionary systems on English reverse dictionary performance, but also supports Chinese and English-Chinese as well as Chinese-English cross-lingual reverse dictionary queries for the first time. Moreover, it has user-friendly front-end design which can help users find the words they need quickly and easily. All the code and data are available at https://github.com/thunlp/WantWords.","authors":["Fanchao Qi","Lei Zhang","Yanhui Yang","Zhiyuan Liu","Maosong Sun"],"demo_url":"","keywords":["english dictionary","reverse dictionaries","language learners","online systems"],"material":"[Online System](https://wantwords.thunlp.org/)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.23","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["demo.48","main.3453","main.246","main.1061","main.3391"],"title":"WantWords: An Open-source Online Reverse Dictionary System","tldr":"A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners....","track":"System Demonstrations"},"forum":"demo.123","id":"demo.123","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.124.png","content":{"abstract":"We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BEN- NERD mainly covers biomedical domain, es- pecially new entity types (e.g., coronavirus, vi- ral proteins, immune responses) by address- ing CORD-NER dataset. It includes several NLP tools to process biomedical texts includ- ing tokenization, flat and nested entity recog- nition, and candidate generation and rank- ing for EL that have been pre-trained using the CORD-NER corpus. To the best of our knowledge, this is the first attempt that ad- dresses NER and EL on COVID-19-related entities, such as COVID-19 virus, potential vaccines, and spreading mechanism, that may benefit research on COVID-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and CORD-NERD dataset for leveraging EL task. The BENNERD system is available at https://aistairc.github.io/BENNERD/.","authors":["Mohammad Golam Sohrab","Khoa Duong","Makoto Miwa","Goran Topic","Ikeda Masami","Takamura Hiroya"],"demo_url":"","keywords":["corona research","flat recog-","flat nition","candidate generation"],"material":"[BENNERD web page](https://aistairc.github.io/BENNERD/)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.24","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.748","main.2825","main.3216","main.3651","main.1755"],"title":"BENNERD: A Neural Named Entity Linking System for COVID-19","tldr":"We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research....","track":"System Demonstrations"},"forum":"demo.124","id":"demo.124","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.126.png","content":{"abstract":"In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present Real or Fake Text (RoFT), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles.","authors":["Liam Dugan","Daphne Ippolito","Arun Kirubarajan","Chris Callison-Burch"],"demo_url":"","keywords":["natural generation","evaluating differences","detecting text","detection articles"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.25","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3353","main.125","main.2590","main.2763","main.648"],"title":"RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text","tldr":"In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans pe...","track":"System Demonstrations"},"forum":"demo.126","id":"demo.126","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.127.png","content":{"abstract":"Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation, and visualization. We establish a unified open-source framework to support fast development of such sophisticated NLP workflows in a composable manner. The framework introduces a uniform data representation to encode heterogeneous results by a wide range of NLP tasks. It offers a large repository of processors for NLP tasks, visualization, and annotation, which can be easily assembled with full interoperability under the unified representation. The highly extensible framework allows plugging in custom processors from external off-the-shelf NLP and deep learning libraries. The whole framework is delivered through two modularized yet integratable open-source projects, namely Forte (for workflow infrastructure and NLP function processors) and Stave (for user interaction, visualization, and annotation).","authors":["Zhengzhong Liu","Guanxiong Ding","Avinash Bukkittu","Mansi Gupta","Pengzhi Gao","Atif Ahmed","Shikun Zhang","Xin Gao","Swapnil Singhavi","Linwei Li","Wei Wei","Zecong Hu","Haoran Shi","Xiaodan Liang","Teruko Mitamura","Eric Xing","Zhiting Hu"],"demo_url":"","keywords":["data ingestion","human annotation","text retrieval","analysis"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.26","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["demo.48","main.1706","demo.72","demo.102","demo.54"],"title":"A Data-Centric Framework for Composable NLP Workflows","tldr":"Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation...","track":"System Demonstrations"},"forum":"demo.127","id":"demo.127","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.128.png","content":{"abstract":"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular crowdsourcing platforms.  CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo: https://github.com/aribornstein/corefi","authors":["Ari Bornstein","Arie Cattan","Ido Dagan"],"demo_url":"","keywords":["coreference annotation","annotation","crowdsourcing","reviewing phase"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.27","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3647","main.2512","main.1621","main.315","demo.72"],"title":"CoRefi: A Crowd Sourcing Suite for Coreference Annotation","tldr":"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference...","track":"System Demonstrations"},"forum":"demo.128","id":"demo.128","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.131.png","content":{"abstract":"Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face signi\ufb01cant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write English papers, especially in the natural language processing (NLP) \ufb01eld. Our system can suggest \ufb02uent, academic-style sentences to writers based on their rough, incomplete phrases or sentences. The system also encourages interaction between human writers and the computerized revision system. The experimental results demonstrated that Langsmith helps non-native English-speaker students write papers in English. The system is available at https://emnlp-demo.editor. langsmith.co.jp/.","authors":["Takumi Ito","Tatsuki Kuribayashi","Masatoshi Hidaka","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["natural eld","langsmith editor","human writers","computerized system"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.28","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.767","main.955","main.868","main.3403","main.3257"],"title":"Langsmith: An Interactive Academic Text Revision System","tldr":"Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face signi\ufb01cant obstacles when writing papers in English. This paper presents the Langsmith editor, which assist...","track":"System Demonstrations"},"forum":"demo.131","id":"demo.131","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.132.png","content":{"abstract":"Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ancestor of modern Chinese. As the Chinese writing system is the oldest continuously-used system in the world, the study of OBS plays an important role in both linguistic and historical research. In order to utilize advanced machine learning methods to automatically process OBS, we construct an information system for OBS (IsOBS) to symbolize, serialize, and store OBS data at the character-level, based on efficient databases and retrieval modules. Moreover, we also apply few-shot learning methods to build an effective OBS character recognition module, which can recognize a large number of OBS characters (especially those characters with a handful of examples) and make the system easy to use. The demo system of IsOBS can be found from \\url{http://isobs.thunlp.org/}. In the future, we will add more OBS data to the system, and hopefully our IsOBS can support further efforts in automatically processing OBS and advance the scientific progress in this field.","authors":["Xu Han","Yuzhuo Bai","Keyue Qiu","Zhiyuan Liu","Maosong Sun"],"demo_url":"","keywords":["linguistic research","obs","chinese system","machine methods"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.29","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["demo.48","main.349","main.471","main.3506","main.3656"],"title":"IsOBS: An Information System for Oracle Bone Script","tldr":"Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ancestor of modern Chinese. As the Chinese writing system is the oldest continuously-used system in the world, the study of OBS plays an important role in both ling...","track":"System Demonstrations"},"forum":"demo.132","id":"demo.132","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.48.png","content":{"abstract":"Natural language processing covers a wide variety of tasks with token-level or sentence-level understandings.  In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and extensible toolkit called OpenUE for various extraction tasks. OpenUE allows developers to train custom models to extract information from the text and supports quick model validation for researchers. Besides, OpenUE provides various functional modules to maintain sufficient modularity and extensibility. Except for the toolkit, we also deploy an online demo with restful APIs to support real-time extraction without training and deploying. Additionally, the online system can extract information in various tasks, including relational triple extraction, slot & intent detection, event extraction, and so on. We release the source code, datasets, and pre-trained models to promote future researches in http://github.com/zjunlp/openue.","authors":["Ningyu Zhang","Shumin Deng","Zhen Bi","Haiyang Yu","Jiacheng Yang","Mosha Chen","Fei Huang","Wei Zhang","Huajun Chen"],"demo_url":"","keywords":["natural processing","extraction tasks","real-time extraction","relational extraction"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.1","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2590","main.3506","main.1669","main.2342","main.1061"],"title":"OpenUE: An Open Toolkit of Universal Extraction from Text","tldr":"Natural language processing covers a wide variety of tasks with token-level or sentence-level understandings.  In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a proto...","track":"System Demonstrations"},"forum":"demo.48","id":"demo.48","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.49.png","content":{"abstract":"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet","authors":["Dat Quoc Nguyen","Thanh Vu","Anh Tuan Nguyen"],"demo_url":"","keywords":["tweet tasks","part-of-speech tagging","named-entity recognition","text classification"],"material":"[BERTweet](https://github.com/VinAIResearch/BERTweet)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.2","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2947","main.2491","main.852","main.1351","main.871"],"title":"BERTweet: A pre-trained language model for English Tweets","tldr":"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Ex...","track":"System Demonstrations"},"forum":"demo.49","id":"demo.49","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.54.png","content":{"abstract":"Existing tools for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline (query expansion, retrieval, reading, and explanation/sensemaking). To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure (e.g.,  ElasticSearch instances and reader models trained with the HuggingFace Transformers API) and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion (CQE) using a masked language model (MLM) as well as relevant snippets (\\(RelSnip\\)) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and large scale search deployment. Code and documentation for NeuralQA is available as open source on  \\href{https://github.com/victordibia/neuralqa}{Github}.","authors":["Victor Dibia"],"demo_url":"","keywords":["question qa","question","query expansion","retrieval"],"material":"[Project Code Repository](https://github.com/victordibia/neuralqa)|[Pre-recorded Screencast](https://vimeo.com/472721886)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.3","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4K","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2342","main.1837","main.2943","main.3506","main.449"],"title":"NeuralQA: A Usable Library for Question Answering (Contextual Query Expansion + BERT) on Large Datasets","tldr":"Existing tools for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of s...","track":"System Demonstrations"},"forum":"demo.54","id":"demo.54","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.58.png","content":{"abstract":"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a state-of-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io/.","authors":["Ikuya Yamada","Akari Asai","Jin Sakuma","Hiroyuki Shindo","Hideaki Takeda","Yoshiyasu Takefuji","Yuji Matsumoto"],"demo_url":"","keywords":["natural tasks","wikipediavec","python-based tool","embeddings entities"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.4","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3093","demo.48","main.1493","main.3453","main.3216"],"title":"Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia","tldr":"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for le...","track":"System Demonstrations"},"forum":"demo.58","id":"demo.58","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.59.png","content":{"abstract":"We introduce ARES (A Reading Comprehension  Ensembling  Service):   a  novel  Machine Reading  Comprehension  (MRC)  demonstration  system  which  utilizes  an  ensemble  of models  to  increase  F1  by  2.3  points.   While many  of  the  top  leaderboard  submissions  in popular  MRC  benchmarks  such  as  the  Stanford Question  Answering  Dataset  (SQuAD) and Natural Questions (NQ) use model ensembles, the accompanying papers do not publish their ensembling strategies.  In this work, we detail and evaluate various ensembling strategies using the NQ dataset.   ARES  leverages the CFO (Chakravarti et al., 2019) and ReactJS  distributed  frameworks  to  provide  a  scalable  interactive  Question  Answering  experience that capitalizes on the agreement (or lack thereof)  between  models  to  improve  the  answer visualization experience.","authors":["Anthony Ferritto","Lin Pan","Rishav Chakravarti","Salim Roukos","Radu Florian","J William Murdock","Avi Sil"],"demo_url":"","keywords":["reading service","service","machine","interactive"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.5","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.928","main.2864","main.2258","main.449","main.1322"],"title":"ARES: A Reading Comprehension Ensembling Service","tldr":"We introduce ARES (A Reading Comprehension  Ensembling  Service):   a  novel  Machine Reading  Comprehension  (MRC)  demonstration  system  which  utilizes  an  ensemble  of models  to  increase  F1  by  2.3  points.   While many  of  the  top  leade...","track":"System Demonstrations"},"forum":"demo.59","id":"demo.59","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.60.png","content":{"abstract":"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.  The library is available at https://github.com/huggingface/transformers.","authors":["Thomas Wolf","Julien Chaumond","Lysandre Debut","Victor Sanh","Clement Delangue","Anthony Moi","Pierric Cistac","Morgan Funtowicz","Joe Davison","Sam Shleifer","Remi Louf","Patrick von Platen","Tim Rault","Yacine Jernite","Teven Le Scao","Sylvain Gugger","Julien Plu","Clara Ma","Canwei Shen","Mariama Drame","Quentin Lhoest","Alexander Rush"],"demo_url":"","keywords":["natural processing","model pretraining","model architecture","transformer architectures"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.6","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1618","main.1960","main.1446","main.1351","main.1803"],"title":"Transformers: State-of-the-Art Natural Language Processing","tldr":"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectivel...","track":"System Demonstrations"},"forum":"demo.60","id":"demo.60","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.71.png","content":{"abstract":"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters---small learnt bottleneck layers inserted within each layer of a pre-trained model--- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml","authors":["Jonas Pfeiffer","Andreas R\u00fcckl\u00e9","Clifton Poth","Aishwarya Kamath","Ivan Vuli\u0107","Sebastian Ruder","Kyunghyun Cho","Iryna Gurevych"],"demo_url":"","keywords":["modus operandi","downloading models","downloading","nlp methods"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.7","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2M","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.1803","main.1485","main.1552","demo.60","main.1130"],"title":"AdapterHub: A Framework for Adapting Transformers","tldr":"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress t...","track":"System Demonstrations"},"forum":"demo.71","id":"demo.71","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.72.png","content":{"abstract":"A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the high specialisation of most annotation tools can result in having to switch to another tool if the task only slightly changes.  We introduce HUMAN, a novel web-based annotation tool that addresses the above problems by a) covering a variety of annotation tasks on both textual and image data, and b) the usage of an internal deterministic state machine, allowing the researcher to chain different annotation tasks in an interdependent manner. Further, the modular nature of the tool makes it easy to define new annotation tasks and integrate machine learning algorithms e.g., for active learning. HUMAN comes with an easy-to-use graphical user interface that simplifies the annotation task and management.","authors":["Moritz Wolf","Dana Ruiter","Ashwin Geet D'Sa","Liane Reiners","Jan Alexandersson","Dietrich Klakow"],"demo_url":"","keywords":["annotation tasks","active learning","annotation management","annotation tools"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.8","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2342","main.1231","demo.48","demo.127","main.3646"],"title":"HUMAN: Hierarchical Universal Modular ANnotator","tldr":"A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phe...","track":"System Demonstrations"},"forum":"demo.72","id":"demo.72","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.79.png","content":{"abstract":"We present DeezyMatch, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for transfer learning in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned DeezyMatch models can be used to generate rich vector representations from string inputs. The candidate ranker component in DeezyMatch uses these vector representations to find, for a given query, the best matching candidates in a knowledge base. It uses an adaptive searching algorithm applicable to large knowledge bases and query sets. We describe DeezyMatch's functionality, design and implementation, accompanied by a use case in toponym matching and candidate ranking in realistic noisy datasets.","authors":["Kasra Hosseini","Federico Nanni","Mariona Coll Ardanuy"],"demo_url":"","keywords":["fuzzy matching","candidate ranking","fine-tuning model","toponym matching"],"material":"[Project Code Repository](https://github.com/Living-with-machines/DeezyMatch)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.9","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3321","main.471","main.1669","main.3348","main.989"],"title":"DeezyMatch: A Flexible Deep Learning Approach to Fuzzy String Matching","tldr":"We present DeezyMatch, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a p...","track":"System Demonstrations"},"forum":"demo.79","id":"demo.79","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.86.png","content":{"abstract":"This work presents CoSaTa, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text.  The stand-alone CoSaTa solver allows easily expressing complex compositional \"inference patterns\" for how knowledge from different tables tends to connect to support inference and explanation construction in question answering and other downstream tasks, while including advanced declarative features and the ability to operate over multiple representations of text (words, lemmas, or part-of-speech tags).  CoSaTa also includes a hybrid imperative/declarative interpreted language for expressing simple models through minimally-specified simulations grounded in constraint patterns, helping bridge the gap between question answering, question explanation, and model simulation.  The solver and interpreter are released as open source. Screencast Demo: https://youtu.be/t93Acsz7LyE","authors":["Peter Jansen"],"demo_url":"","keywords":["inference construction","question answering","question explanation","model simulation"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.10","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1J","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2253","main.2054","main.607","main.2415","main.1179"],"title":"CoSaTa: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences","tldr":"This work presents CoSaTa, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text.  The stand-alone CoSaTa solver allows easily expressing complex compositional \"inference ...","track":"System Demonstrations"},"forum":"demo.86","id":"demo.86","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.89.png","content":{"abstract":"Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http://nlp.uniroma1.it/invero.","authors":["Simone Conia","Fabrizio Brignone","Davide Zanfardino","Roberto Navigli"],"demo_url":"","keywords":["srl","neural models","neural architecture","web interface"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.11","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1957","main.1625","main.2179","main.1970","main.1061"],"title":"InVeRo: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles","tldr":"Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and...","track":"System Demonstrations"},"forum":"demo.89","id":"demo.89","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.91.png","content":{"abstract":"Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates \\textit{Youling}, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, \\textit{Youling} supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, \\textit{Youling} allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the system is available at \\href{https://youtu.be/DFeNpHk0pm4}{https://youtu.be/DFeNpHk0pm4}.","authors":["Rongsheng Zhang","Xiaoxi Mao","Le Li","Lin Jiang","Lin Chen","Zhiwei Hu","Yadong Xi","Changjie Fan","Minlie Huang"],"demo_url":"","keywords":["lyrics generation","generation process","lyrics creation","lyrics process"],"material":"[Pre-recorded screencast](https://youtu.be/DFeNpHk0pm4)|[Link to our system](https://yl.fuxi.netease.com/) . Visitors can log in with the public account (youlingtest@163.com) and password (youling666)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.12","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4K","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.920","main.2590","main.2511","main.1928","main.1797"],"title":"Youling: an AI-assisted Lyrics Creation System","tldr":"Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with hu...","track":"System Demonstrations"},"forum":"demo.91","id":"demo.91","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.93.png","content":{"abstract":"In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.","authors":["Wenhao Yu","Lingfei Wu","Yu Deng","Ruchi Mahindru","Qingkai Zeng","Sinem Guven","Meng Jiang"],"demo_url":"","keywords":["transtqa","siamese network","deep strategy","automatic responses"],"material":"","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.13","program":"demo","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5J","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2640","main.2587","main.1022","main.2078","main.2635"],"title":"A Technical Question Answering System with Transfer Learning","tldr":"In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel sy...","track":"System Demonstrations"},"forum":"demo.93","id":"demo.93","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//demo.97.png","content":{"abstract":"Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-quality type systems for popular fictional domains, and provides recommendations towards reference type systems for given input texts. Users can exploit the richness and diversity of these reference type systems for fine-grained supervised typing, in addition, they can choose among and combine four other typing modules: pre-trained real-world models, unsupervised dependency-based typing, knowledge base lookups, and constraint-based candidate consolidation. The demonstrator is available at: https://d5demos.mpi-inf.mpg.de/entyfi.","authors":["Cuong Xuan Chu","Simon Razniewski","Gerhard Weikum"],"demo_url":"","keywords":["fine-grained mentions","fine-grained typing","demonstrator","nlp methodologies"],"material":"[Project page](https://www.mpi-inf.mpg.de/yago-naga/entyfi)|[Pre-recorded Screencast](https://www.youtube.com/watch?v=g_ESaONagFQ)","paper_type":"demo","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-demos.14","program":"demo","sessions":[{"end_time":"Tue, 17 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g3I","start_time":"Tue, 17 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.1267","main.3453","main.2533","main.3457","main.2076"],"title":"ENTYFI: A System for Fine-grained Entity Typing in Fictional Texts","tldr":"Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-q...","track":"System Demonstrations"},"forum":"demo.97","id":"demo.97","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.1.png","content":{"abstract":"State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.","authors":["Gabriele Prato","Ella Charlaix","Mehdi Rezagholizadeh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.1","program":"findings","sessions":[],"similar_paper_uids":["findings.1"],"title":"Fully Quantized Transformer for Machine Translation","tldr":"State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: ...","track":"Findings of EMNLP"},"forum":"findings.1","id":"findings.1","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.2.png","content":{"abstract":"Online search engines are a popular source of medical information for users, where users can enter questions and obtain relevant answers. It is desirable to generate answer summaries for online search engines, particularly summaries that can reveal direct answers to questions. Moreover, answer summaries are expected to reveal the most relevant information in response to questions; hence, the summaries should be generated with a focus on the question, which is a challenging topic-focused summarization task. In this paper, we propose an approach that utilizes graph convolution networks and question-focused dual attention for Chinese medical answer summarization. We first organize the original long answer text into a medical concept graph with graph convolution networks to better understand the internal structure of the text and the correlation between medical concepts. Then, we introduce a question-focused dual attention mechanism to generate summaries relevant to questions. Experimental results demonstrate that the proposed model can generate more coherent and informative summaries compared with baseline models.","authors":["Ningyu Zhang","Shumin Deng","Juan Li","Xi Chen","Wei Zhang","Huajun Chen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.2","program":"findings","sessions":[],"similar_paper_uids":["findings.2"],"title":"Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention","tldr":"Online search engines are a popular source of medical information for users, where users can enter questions and obtain relevant answers. It is desirable to generate answer summaries for online search engines, particularly summaries that can reveal d...","track":"Findings of EMNLP"},"forum":"findings.2","id":"findings.2","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.3.png","content":{"abstract":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where the questioner is not given the context from which answers are drawn, but must reason pragmatically about how to acquire new information, given the shared conversation history. We identify two core challenges: (1) formally defining the informativeness of potential questions, and (2) exploring the prohibitively large space of potential questions to find the good candidates. To generate pragmatic questions, we use reinforcement learning to optimize an informativeness metric we propose, combined with a reward function designed to promote more specific questions. We demonstrate that the resulting pragmatic questioner substantially improves the informativeness and specificity of questions generated over a baseline model, as evaluated by our metrics as well as humans.","authors":["Peng Qi","Yuhao Zhang","Christopher D. Manning"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.3","program":"findings","sessions":[],"similar_paper_uids":["findings.3"],"title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations","tldr":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where t...","track":"Findings of EMNLP"},"forum":"findings.3","id":"findings.3","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.4.png","content":{"abstract":"Domain adaptation or transfer learning using pre-trained language models such as BERT has proven to be an effective approach for many natural language processing tasks. In this work, we propose to formulate word sense disambiguation as a relevance ranking task, and fine-tune BERT on sequence-pair ranking task to select the most probable sense definition given a context sentence and a list of candidate sense definitions. We also introduce a data augmentation technique for WSD using existing example sentences from WordNet. Using the proposed training objective and data augmentation technique, our models are able to achieve state-of-the-art results on the English all-words benchmark datasets.","authors":["Boon Peng Yap","Andrew Koh","Eng Siong Chng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.4","program":"findings","sessions":[],"similar_paper_uids":["findings.4"],"title":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","tldr":"Domain adaptation or transfer learning using pre-trained language models such as BERT has proven to be an effective approach for many natural language processing tasks. In this work, we propose to formulate word sense disambiguation as a relevance ra...","track":"Findings of EMNLP"},"forum":"findings.4","id":"findings.4","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.5.png","content":{"abstract":"In this paper, we propose a sequence contrast loss driven text generation framework, which learns the difference between real texts and generated texts and uses that difference. Specifically, our discriminator contains a discriminative sequence generator instead of a binary classifier, and measures the \u2018relative realism\u2019 of generated texts against real texts by making use of them simultaneously. Moreover, our generator uses discriminative sequences to directly improve itself, which not only replaces the gradient propagation process from the discriminator to the generator, but also avoids the time-consuming sampling process of estimating rewards in some previous methods. We conduct extensive experiments with various metrics, substantiating that our framework brings improvements in terms of training stability and the quality of generated texts.","authors":["Ke Wang","Xiaojun Wan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.5","program":"findings","sessions":[],"similar_paper_uids":["findings.5"],"title":"Adversarial Text Generation via Sequence Contrast Discrimination","tldr":"In this paper, we propose a sequence contrast loss driven text generation framework, which learns the difference between real texts and generated texts and uses that difference. Specifically, our discriminator contains a discriminative sequence gener...","track":"Findings of EMNLP"},"forum":"findings.5","id":"findings.5","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.6.png","content":{"abstract":"In this paper, we focus on the imbalance issue, which is rarely studied in aspect term extraction and aspect sentiment classification when regarding them as sequence labeling tasks. Besides, previous works usually ignore the interaction between aspect terms when labeling polarities. We propose a GRadient hArmonized and CascadEd labeling model (GRACE) to solve these problems. Specifically, a cascaded labeling module is developed to enhance the interchange between aspect terms and improve the attention of sentiment tokens when labeling sentiment polarities. The polarities sequence is designed to depend on the generated aspect terms labels. To alleviate the imbalance issue, we extend the gradient harmonized mechanism used in object detection to the aspect-based sentiment analysis by adjusting the weight of each label dynamically. The proposed GRACE adopts a post-pretraining BERT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results.","authors":["Huaishao Luo","Lei Ji","Tianrui Li","Daxin Jiang","Nan Duan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.6","program":"findings","sessions":[],"similar_paper_uids":["findings.6"],"title":"GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis","tldr":"In this paper, we focus on the imbalance issue, which is rarely studied in aspect term extraction and aspect sentiment classification when regarding them as sequence labeling tasks. Besides, previous works usually ignore the interaction between aspec...","track":"Findings of EMNLP"},"forum":"findings.6","id":"findings.6","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.7.png","content":{"abstract":"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model\u2019s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.","authors":["Po-Sen Huang","Huan Zhang","Ray Jiang","Robert Stanforth","Johannes Welbl","Jack Rae","Vishal Maini","Dani Yogatama","Pushmeet Kohli"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.7","program":"findings","sessions":[],"similar_paper_uids":["findings.7"],"title":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation","tldr":"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social ...","track":"Findings of EMNLP"},"forum":"findings.7","id":"findings.7","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.8.png","content":{"abstract":"Recent studies show that integrating syntactic tree models with sequential semantic models can bring improved task performance, while these methods mostly employ shallow integration of syntax and semantics. In this paper, we propose a deep neural communication model between syntax and semantics to improve the performance of text understanding. Local communication is performed between syntactic tree encoder and sequential semantic encoder for mutual learning of information exchange. Global communication can further ensure comprehensive information propagation. Results on multiple syntax-dependent tasks show that our model outperforms strong baselines by a large margin. In-depth analysis indicates that our method is highly effective in composing sentence semantics.","authors":["Hao Fei","Yafeng Ren","Donghong Ji"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.8","program":"findings","sessions":[],"similar_paper_uids":["findings.8"],"title":"Improving Text Understanding via Deep Syntax-Semantics Communication","tldr":"Recent studies show that integrating syntactic tree models with sequential semantic models can bring improved task performance, while these methods mostly employ shallow integration of syntax and semantics. In this paper, we propose a deep neural com...","track":"Findings of EMNLP"},"forum":"findings.8","id":"findings.8","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.9.png","content":{"abstract":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.","authors":["Wanzheng Zhu","Suma Bhat"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.9","program":"findings","sessions":[],"similar_paper_uids":["findings.9"],"title":"GRUEN for Evaluating Linguistic Quality of Generated Text","tldr":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge th...","track":"Findings of EMNLP"},"forum":"findings.9","id":"findings.9","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.10.png","content":{"abstract":"This paper presents a simple and effective discrete optimization method for training binarized knowledge graph embedding model B-CP. Unlike the prior work using a SGD-based method and quantization of real-valued vectors, the proposed method directly optimizes binary embedding vectors by a series of bit flipping operations. On the standard knowledge graph completion tasks, the B-CP model trained with the proposed method achieved comparable performance with that trained with SGD as well as state-of-the-art real-valued models with similar embedding dimensions.","authors":["Katsuhiko Hayashi","Koki Kishimoto","Masashi Shimbo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.10","program":"findings","sessions":[],"similar_paper_uids":["findings.10"],"title":"A Greedy Bit-flip Training Algorithm for Binarized Knowledge Graph Embeddings","tldr":"This paper presents a simple and effective discrete optimization method for training binarized knowledge graph embedding model B-CP. Unlike the prior work using a SGD-based method and quantization of real-valued vectors, the proposed method directly ...","track":"Findings of EMNLP"},"forum":"findings.10","id":"findings.10","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.11.png","content":{"abstract":"In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a difference-aware knowledge selection method. It first computes the difference between the candidate knowledge sentences provided at the current turn and those chosen in the previous turns. Then, the differential information is fused with or disentangled from the contextual information to facilitate final knowledge selection. Automatic, human observational, and interactive evaluation shows that our method is able to select knowledge more accurately and generate more informative responses, significantly outperforming the state-of-the-art baselines.","authors":["Chujie Zheng","Yunbo Cao","Daxin Jiang","Minlie Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.11","program":"findings","sessions":[],"similar_paper_uids":["findings.11"],"title":"Difference-aware Knowledge Selection for Knowledge-grounded Conversation Generation","tldr":"In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a di...","track":"Findings of EMNLP"},"forum":"findings.11","id":"findings.11","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.12.png","content":{"abstract":"Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistical models, we introduce an attention-based neural model to incrementally predict final verbs on incomplete sentences in Japanese and German SOV sentences. To offer flexibility to the model, we further incorporate synonym awareness. Our approach both better predicts the final verbs in Japanese and German and provides more interpretable explanations of why those verbs are selected.","authors":["Wenyan Li","Alvin Grissom II","Jordan Boyd-Graber"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.12","program":"findings","sessions":[],"similar_paper_uids":["findings.12"],"title":"An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs","tldr":"Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistica...","track":"Findings of EMNLP"},"forum":"findings.12","id":"findings.12","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.13.png","content":{"abstract":"Pronouns are often dropped in Chinese conversations and recovering the dropped pronouns is important for NLP applications such as Machine Translation. Existing approaches usually formulate this as a sequence labeling task of predicting whether there is a dropped pronoun before each token and its type. Each utterance is considered to be a sequence and labeled independently. Although these approaches have shown promise, labeling each utterance independently ignores the dependencies between pronouns in neighboring utterances. Modeling these dependencies is critical to improving the performance of dropped pronoun recovery. In this paper, we present a novel framework that combines the strength of Transformer network with General Conditional Random Fields (GCRF) to model the dependencies between pronouns in neighboring utterances. Results on three Chinese conversation datasets show that the Transformer-GCRF model outperforms the state-of-the-art dropped pronoun recovery models. Exploratory analysis also demonstrates that the GCRF did help to capture the dependencies between pronouns in neighboring utterances, thus contributes to the performance improvements.","authors":["Jingxuan Yang","Kerui Xu","Jun Xu","Si Li","Sheng Gao","Jun Guo","Ji-Rong Wen","Nianwen Xue"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.13","program":"findings","sessions":[],"similar_paper_uids":["findings.13"],"title":"Transformer-GCRF: Recovering Chinese Dropped Pronouns with General Conditional Random Fields","tldr":"Pronouns are often dropped in Chinese conversations and recovering the dropped pronouns is important for NLP applications such as Machine Translation. Existing approaches usually formulate this as a sequence labeling task of predicting whether there ...","track":"Findings of EMNLP"},"forum":"findings.13","id":"findings.13","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.14.png","content":{"abstract":"Several approaches to neural speed reading have been presented at major NLP and machine learning conferences in 2017\u201320; i.e., \u201chuman-inspired\u201d recurrent network architectures that learn to \u201cread\u201d text faster by skipping irrelevant words, typically optimizing the joint objective of minimizing classification error rate and FLOPs used at inference time. This paper reflects on the meaningfulness of the speed reading task, showing that (a) better and faster approaches to, say, document classification, already exist, which also learn to ignore part of the input (I give an example with 7% error reduction and a 136x speed-up over the state of the art in neural speed reading); and that (b) any claims that neural speed reading is \u201chuman-inspired\u201d, are ill-founded.","authors":["Anders S\u00f8gaard"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.14","program":"findings","sessions":[],"similar_paper_uids":["findings.14"],"title":"Neural Speed Reading Audited","tldr":"Several approaches to neural speed reading have been presented at major NLP and machine learning conferences in 2017\u201320; i.e., \u201chuman-inspired\u201d recurrent network architectures that learn to \u201cread\u201d text faster by skipping irrelevant words, typically o...","track":"Findings of EMNLP"},"forum":"findings.14","id":"findings.14","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.15.png","content":{"abstract":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI love you.\u201d We designed a system to allow virtual assistants to take a voice message from one user, convert the point of view of the message, and then deliver the result to its target user. We developed a rule-based model, which integrates a linear text classification model, part-of-speech tagging, and constituency parsing with rule-based transformation methods. We also investigated Neural Machine Translation (NMT) approaches, including LSTMs, CopyNet, and T5. We explored 5 metrics to gauge both naturalness and faithfulness automatically, and we chose to use BLEU plus METEOR for faithfulness and relative perplexity using a separately trained language model (GPT) for naturalness. Transformer-Copynet and T5 performed similarly on faithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a METEOR score of 83.0. CopyNet was the most natural, with a relative perplexity of 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly released our dataset, which is composed of 46,565 crowd-sourced samples.","authors":["Gunhee Lee","Vera Zu","Sai Srujana Buddi","Dennis Liang","Purva Kulkarni","Jack FitzGerald"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.15","program":"findings","sessions":[],"similar_paper_uids":["findings.15"],"title":"Converting the Point of View of Messages Spoken to Virtual Assistants","tldr":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI lov...","track":"Findings of EMNLP"},"forum":"findings.15","id":"findings.15","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.16.png","content":{"abstract":"Revealing the robustness issues of natural language processing models and improving their robustness is important to their performance under difficult situations. In this paper, we study the robustness of paraphrase identification models from a new perspective \u2013 via modification with shared words, and we show that the models have significant robustness issues when facing such modifications. To modify an example consisting of a sentence pair, we either replace some words shared by both sentences or introduce new shared words. We aim to construct a valid new example such that a target model makes a wrong prediction. To find a modification solution, we use beam search constrained by heuristic rules, and we leverage a BERT masked language model for generating substitution words compatible with the context. Experiments show that the performance of the target models has a dramatic drop on the modified examples, thereby revealing the robustness issue. We also show that adversarial training can mitigate this issue.","authors":["Zhouxing Shi","Minlie Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.16","program":"findings","sessions":[],"similar_paper_uids":["findings.16"],"title":"Robustness to Modification with Shared Words in Paraphrase Identification","tldr":"Revealing the robustness issues of natural language processing models and improving their robustness is important to their performance under difficult situations. In this paper, we study the robustness of paraphrase identification models from a new p...","track":"Findings of EMNLP"},"forum":"findings.16","id":"findings.16","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.17.png","content":{"abstract":"As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present FewshotWOZ, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FewshotWOZ and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations.","authors":["Baolin Peng","Chenguang Zhu","Chunyuan Li","Xiujun Li","Jinchao Li","Michael Zeng","Jianfeng Gao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.17","program":"findings","sessions":[],"similar_paper_uids":["findings.17"],"title":"Few-shot Natural Language Generation for Task-Oriented Dialog","tldr":"As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical mo...","track":"Findings of EMNLP"},"forum":"findings.17","id":"findings.17","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.18.png","content":{"abstract":"Syntax has been shown useful for various NLP tasks, while existing work mostly encodes singleton syntactic tree using one hierarchical neural network. In this paper, we investigate a simple and effective method, Knowledge Distillation, to integrate heterogeneous structure knowledge into a unified sequential LSTM encoder. Experimental results on four typical syntax-dependent tasks show that our method outperforms tree encoders by effectively integrating rich heterogeneous structure syntax, meanwhile reducing error propagation, and also outperforms ensemble methods, in terms of both the efficiency and accuracy.","authors":["Hao Fei","Yafeng Ren","Donghong Ji"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.18","program":"findings","sessions":[],"similar_paper_uids":["findings.18"],"title":"Mimic and Conquer: Heterogeneous Tree Structure Distillation for Syntactic NLP","tldr":"Syntax has been shown useful for various NLP tasks, while existing work mostly encodes singleton syntactic tree using one hierarchical neural network. In this paper, we investigate a simple and effective method, Knowledge Distillation, to integrate h...","track":"Findings of EMNLP"},"forum":"findings.18","id":"findings.18","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.19.png","content":{"abstract":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.","authors":["Chenguang Zhu","Ruochen Xu","Michael Zeng","Xuedong Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.19","program":"findings","sessions":[],"similar_paper_uids":["findings.19"],"title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining","tldr":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization in...","track":"Findings of EMNLP"},"forum":"findings.19","id":"findings.19","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.20.png","content":{"abstract":"Distant supervision has been a widely used method for neural relation extraction for its convenience of automatically labeling datasets. However, existing works on distantly supervised relation extraction suffer from the low quality of test set, which leads to considerable biased performance evaluation. These biases not only result in unfair evaluations but also mislead the optimization of neural relation extraction. To mitigate this problem, we propose a novel evaluation method named active testing through utilizing both the noisy test set and a few manual annotations. Experiments on a widely used benchmark show that our proposed approach can yield approximately unbiased evaluations for distantly supervised relation extractors.","authors":["Pengshuai Li","Xinsong Zhang","Weijia Jia","Wei Zhao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.20","program":"findings","sessions":[],"similar_paper_uids":["findings.20"],"title":"Active Testing: An Unbiased Evaluation Method for Distantly Supervised Relation Extraction","tldr":"Distant supervision has been a widely used method for neural relation extraction for its convenience of automatically labeling datasets. However, existing works on distantly supervised relation extraction suffer from the low quality of test set, whic...","track":"Findings of EMNLP"},"forum":"findings.20","id":"findings.20","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.21.png","content":{"abstract":"In sequence-to-sequence models, classical optimal transport (OT) can be applied to semantically match generated sentences with target sentences. However, in non-parallel settings, target sentences are usually unavailable. To tackle this issue without losing the benefits of classical OT, we present a semantic matching scheme based on the Optimal Partial Transport (OPT). Specifically, our approach partially matches semantically meaningful words between source and partial target sequences. To overcome the difficulty of detecting active regions in OPT (corresponding to the words needed to be matched), we further exploit prior knowledge to perform partial matching. Extensive experiments are conducted to evaluate the proposed approach, showing consistent improvements over sequence-to-sequence tasks.","authors":["Ruiyi Zhang","Changyou Chen","Xinyuan Zhang","Ke Bai","Lawrence Carin"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.21","program":"findings","sessions":[],"similar_paper_uids":["findings.21"],"title":"Semantic Matching for Sequence-to-Sequence Learning","tldr":"In sequence-to-sequence models, classical optimal transport (OT) can be applied to semantically match generated sentences with target sentences. However, in non-parallel settings, target sentences are usually unavailable. To tackle this issue without...","track":"Findings of EMNLP"},"forum":"findings.21","id":"findings.21","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.22.png","content":{"abstract":"Recent progress in pre-trained language models led to systems that are able to generate text of an increasingly high quality. While several works have investigated the fluency and grammatical correctness of such models, it is still unclear to which extent the generated text is consistent with factual world knowledge. Here, we go beyond fluency and also investigate the verifiability of text generated by state-of-the-art pre-trained language models. A generated sentence is verifiable if it can be corroborated or disproved by Wikipedia, and we find that the verifiability of generated text strongly depends on the decoding strategy. In particular, we discover a tradeoff between factuality (i.e., the ability of generating Wikipedia corroborated text) and repetitiveness. While decoding strategies such as top-k and nucleus sampling lead to less repetitive generations, they also produce less verifiable text. Based on these finding, we introduce a simple and effective decoding strategy which, in comparison to previously used decoding strategies, produces less repetitive and more verifiable text.","authors":["Luca Massarelli","Fabio Petroni","Aleksandra Piktus","Myle Ott","Tim Rockt\u00e4schel","Vassilis Plachouras","Fabrizio Silvestri","Sebastian Riedel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.22","program":"findings","sessions":[],"similar_paper_uids":["findings.22"],"title":"How Decoding Strategies Affect the Verifiability of Generated Text","tldr":"Recent progress in pre-trained language models led to systems that are able to generate text of an increasingly high quality. While several works have investigated the fluency and grammatical correctness of such models, it is still unclear to which e...","track":"Findings of EMNLP"},"forum":"findings.22","id":"findings.22","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.23.png","content":{"abstract":"Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length associated with error accumulation. These methods introduce exposure bias, which may cause the models overfit to the frequent label combination, thus limiting the generalization ability. We propose a novel Sequence-to-Unordered-Multi-Tree (Seq2UMTree) model to minimize the effects of exposure bias by limiting the decoding length to three within a triplet and removing the order among triplets. We evaluate our model on two datasets, DuIE and NYT, and systematically study how exposure bias alters the performance of Seq2Seq models. Experiments show that the state-of-the-art Seq2Seq model overfits to both datasets while Seq2UMTree shows significantly better generalization. Our code is available at https://github.com/WindChimeRan/OpenJERE.","authors":["Ranran Haoran Zhang","Qianying Liu","Aysa Xuemo Fan","Heng Ji","Daojian Zeng","Fei Cheng","Daisuke Kawahara","Sadao Kurohashi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.23","program":"findings","sessions":[],"similar_paper_uids":["findings.23"],"title":"Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation Extraction","tldr":"Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unorder...","track":"Findings of EMNLP"},"forum":"findings.23","id":"findings.23","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.24.png","content":{"abstract":"Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade Model that overwhelms the gradients without affecting the predictions. This Facade Model can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (sentiment analysis, NLI, and QA), we show that the merged model effectively fools different analysis tools: saliency maps differ significantly from the original model\u2019s, input reduction keeps more irrelevant input tokens, and adversarial perturbations identify unimportant tokens as being highly important.","authors":["Junlin Wang","Jens Tuyls","Eric Wallace","Sameer Singh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.24","program":"findings","sessions":[],"similar_paper_uids":["findings.24"],"title":"Gradient-based Analysis of NLP Models is Manipulable","tldr":"Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they direc...","track":"Findings of EMNLP"},"forum":"findings.24","id":"findings.24","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.25.png","content":{"abstract":"Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained language models by leveraging world knowledge from pretrained models. Specifically, we present a universal training framework named Pretrain-KGE consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.","authors":["Zhiyuan Zhang","Xiaoqian Liu","Yi Zhang","Qi Su","Xu Sun","Bin He"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.25","program":"findings","sessions":[],"similar_paper_uids":["findings.25"],"title":"Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models","tldr":"Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained ...","track":"Findings of EMNLP"},"forum":"findings.25","id":"findings.25","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.26.png","content":{"abstract":"Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found that there is a non-negligible amount of \u201cnoise\u201d where errors were inappropriately edited or left uncorrected. To address this, we designed a self-refinement method where the key idea is to denoise these datasets by leveraging the prediction consistency of existing models, and outperformed strong denoising baseline methods. We further applied task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. We then analyzed the effect of the proposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance.","authors":["Masato Mita","Shun Kiyono","Masahiro Kaneko","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.26","program":"findings","sessions":[],"similar_paper_uids":["findings.26"],"title":"A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction","tldr":"Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality da...","track":"Findings of EMNLP"},"forum":"findings.26","id":"findings.26","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.27.png","content":{"abstract":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.","authors":["Julian Eisenschlos","Syrine Krichene","Thomas M\u00fcller"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.27","program":"findings","sessions":[],"similar_paper_uids":["findings.27"],"title":"Understanding tables with intermediate pre-training","tldr":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on t...","track":"Findings of EMNLP"},"forum":"findings.27","id":"findings.27","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.28.png","content":{"abstract":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification. However, it lacks sufficient contextual information enhancement and thus is less useful for sequence labelling tasks such as chunking and named entity recognition (NER). To address this limitation, we propose masked adversarial training (MAT) to improve robustness from contextual information in sequence labelling. MAT masks or replaces some words in the sentence when computing adversarial loss from perturbed inputs and consequently enhances model robustness using more context-level information. In our experiments, our method shows significant improvements on accuracy and robustness of sequence labelling. By further incorporating with ELMo embeddings, our model achieves better or comparable results to state-of-the-art on CoNLL 2000 and 2003 benchmarks using much less parameters.","authors":["Luoxin Chen","Xinyue Liu","Weitong Ruan","Jianhua Lu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.28","program":"findings","sessions":[],"similar_paper_uids":["findings.28"],"title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training","tldr":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input n...","track":"Findings of EMNLP"},"forum":"findings.28","id":"findings.28","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.29.png","content":{"abstract":"The growing interest in argument mining and computational argumentation brings with it a plethora of Natural Language Understanding (NLU) tasks and corresponding datasets. However, as with many other NLU tasks, the dominant language is English, with resources in other languages being few and far between. In this work, we explore the potential of transfer learning using the multilingual BERT model to address argument mining tasks in non-English languages, based on English datasets and the use of machine translation. We show that such methods are well suited for classifying the stance of arguments and detecting evidence, but less so for assessing the quality of arguments, presumably because quality is harder to preserve under translation. In addition, focusing on the translate-train approach, we show how the choice of languages for translation, and the relations among them, affect the accuracy of the resultant model. Finally, to facilitate evaluation of transfer learning on argument mining tasks, we provide a human-generated dataset with more than 10k arguments in multiple languages, as well as machine translation of the English datasets.","authors":["Orith Toledo-Ronen","Matan Orbach","Yonatan Bilu","Artem Spector","Noam Slonim"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.29","program":"findings","sessions":[],"similar_paper_uids":["findings.29"],"title":"Multilingual Argument Mining: Datasets and Analysis","tldr":"The growing interest in argument mining and computational argumentation brings with it a plethora of Natural Language Understanding (NLU) tasks and corresponding datasets. However, as with many other NLU tasks, the dominant language is English, with ...","track":"Findings of EMNLP"},"forum":"findings.29","id":"findings.29","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.30.png","content":{"abstract":"We propose a novel data synthesis method to generate diverse error-corrected sentence pairs for improving grammatical error correction, which is based on a pair of machine translation models (e.g., Chinese to English) of different qualities (i.e., poor and good). The poor translation model can resemble the ESL (English as a second language) learner and tends to generate translations of low quality in terms of fluency and grammaticality, while the good translation model generally generates fluent and grammatically correct translations. With the pair of translation models, we can generate unlimited numbers of poor to good English sentence pairs from text in the source language (e.g., Chinese) of the translators. Our approach can generate various error-corrected patterns and nicely complement the other data synthesis approaches for GEC. Experimental results demonstrate the data generated by our approach can effectively help a GEC model to improve the performance and achieve the state-of-the-art single-model performance in BEA-19 and CoNLL-14 benchmark datasets.","authors":["Wangchunshu Zhou","Tao Ge","Chang Mu","Ke Xu","Furu Wei","Ming Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.30","program":"findings","sessions":[],"similar_paper_uids":["findings.30"],"title":"Improving Grammatical Error Correction with Machine Translation Pairs","tldr":"We propose a novel data synthesis method to generate diverse error-corrected sentence pairs for improving grammatical error correction, which is based on a pair of machine translation models (e.g., Chinese to English) of different qualities (i.e., po...","track":"Findings of EMNLP"},"forum":"findings.30","id":"findings.30","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.31.png","content":{"abstract":"Modern dialog managers face the challenge of having to fulfill human-level conversational skills as part of common user expectations, including but not limited to discourse with no clear objective. Along with these requirements, agents are expected to extrapolate intent from the user\u2019s dialogue even when subjected to non-canonical forms of speech. This depends on the agent\u2019s comprehension of paraphrased forms of such utterances. Especially in low-resource languages, the lack of data is a bottleneck that prevents advancements of the comprehension performance for these types of agents. In this regard, here we demonstrate the necessity of extracting the intent argument of non-canonical directives in a natural language format, which may yield more accurate parsing, and suggest guidelines for building a parallel corpus for this purpose. Following the guidelines, we construct a Korean corpus of 50K instances of question/command-intent pairs, including the labels for classification of the utterance type. We also propose a method for mitigating class imbalance, demonstrating the potential applications of the corpus generation method and its multilingual extensibility.","authors":["Won Ik Cho","Youngki Moon","Sangwhan Moon","Seok Min Kim","Nam Soo Kim"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.31","program":"findings","sessions":[],"similar_paper_uids":["findings.31"],"title":"Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives","tldr":"Modern dialog managers face the challenge of having to fulfill human-level conversational skills as part of common user expectations, including but not limited to discourse with no clear objective. Along with these requirements, agents are expected t...","track":"Findings of EMNLP"},"forum":"findings.31","id":"findings.31","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.32.png","content":{"abstract":"Relation classification is one of the key topics in information extraction, which can be used to construct knowledge bases or to provide useful information for question answering. Current approaches for relation classification are mainly focused on the English language and require lots of training data with human annotations. Creating and annotating a large amount of training data for low-resource languages is impractical and expensive. To overcome this issue, we propose two cross-lingual relation classification models: a baseline model based on Multilingual BERT and a new multilingual pretraining setup, which significantly improves the baseline with distant supervision. For evaluation, we introduce a new public benchmark dataset for cross-lingual relation classification in English, French, German, Spanish, and Turkish, called RELX. We also provide the RELX-Distant dataset, which includes hundreds of thousands of sentences with relations from Wikipedia and Wikidata collected by distant supervision for these languages. Our code and data are available at: https://github.com/boun-tabi/RELX","authors":["Abdullatif K\u00f6ksal","Arzucan \u00d6zg\u00fcr"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.32","program":"findings","sessions":[],"similar_paper_uids":["findings.32"],"title":"The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual Relation Classification","tldr":"Relation classification is one of the key topics in information extraction, which can be used to construct knowledge bases or to provide useful information for question answering. Current approaches for relation classification are mainly focused on t...","track":"Findings of EMNLP"},"forum":"findings.32","id":"findings.32","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.33.png","content":{"abstract":"We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The scalability of our approach is ensured through a single discriminator, independently of the number of attributes. We show high quality, diversity and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a classification performance often comparable to adding same amount of additional real data.","authors":["Giuseppe Russo","Nora Hollenstein","Claudiu Cristian Musat","Ce Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.33","program":"findings","sessions":[],"similar_paper_uids":["findings.33"],"title":"Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation","tldr":"We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware ...","track":"Findings of EMNLP"},"forum":"findings.33","id":"findings.33","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.34.png","content":{"abstract":"We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to train a good target model. A straightforward solution is to fine-tune a pre-trained source model by using those limited labeled target data, but it usually cannot work well due to the considerable difference between the data distributions of the source and target domains. Moreover, the availability of multiple modalities (i.e., images, questions and answers) in VQA poses further challenges in modeling the transferability between various modalities. In this paper, we address the above issues by proposing a novel supervised multi-modal domain adaptation method for VQA to learn joint feature embeddings across different domains and modalities. Specifically, we align the data distributions of the source and target domains by considering those modalities both jointly and separately. Extensive experiments on the benchmark VQA 2.0 and VizWiz datasets demonstrate that our proposed method outperforms the existing state-of-the-art baselines for open-ended VQA in this challenging domain adaptation setting.","authors":["Yiming Xu","Lin Chen","Zhongwei Cheng","Lixin Duan","Jiebo Luo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.34","program":"findings","sessions":[],"similar_paper_uids":["findings.34"],"title":"Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation","tldr":"We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to trai...","track":"Findings of EMNLP"},"forum":"findings.34","id":"findings.34","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.35.png","content":{"abstract":"Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this paper proposes a novel multimodal fusion method called Fine-Grained Temporal Low-Rank Multimodal Fusion (FT-LMF). FT-LMF correlates the features of individual time steps between multiple modalities, while it involves multiplications of high-order tensors in its calculation. This paper further proposes Dual Low-Rank Multimodal Fusion (Dual-LMF) to reduce the computational complexity of FT-LMF through low-rank tensor approximation along dual dimensions of input features. Dual-LMF is conceptually simple and practically effective and efficient. Empirical studies on benchmark multimodal analysis tasks show that our proposed methods outperform the state-of-the-art tensor-based fusion methods with a similar computational complexity.","authors":["Tao Jin","Siyu Huang","Yingming Li","Zhongfei Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.35","program":"findings","sessions":[],"similar_paper_uids":["findings.35"],"title":"Dual Low-Rank Multimodal Fusion","tldr":"Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this pap...","track":"Findings of EMNLP"},"forum":"findings.35","id":"findings.35","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.36.png","content":{"abstract":"Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level identification by treating the task as either single-word classification or sequential labelling without explicitly modelling the interaction between the metaphor components. On the other hand, while existing relation-level approaches implicitly model this interaction, they ignore the context where the metaphor occurs. In this work, we address these limitations by introducing a novel architecture for identifying relation-level metaphoric expressions of certain grammatical relations based on contextual modulation. In a methodology inspired by works in visual reasoning, our approach is based on conditioning the neural network computation on the deep contextualised features of the candidate expressions using feature-wise linear modulation. We demonstrate that the proposed architecture achieves state-of-the-art results on benchmark datasets. The proposed methodology is generic and could be applied to other textual classification problems that benefit from contextual interaction.","authors":["Omnia Zayed","John P. McCrae","Paul Buitelaar"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.36","program":"findings","sessions":[],"similar_paper_uids":["findings.36"],"title":"Contextual Modulation for Relation-Level Metaphor Identification","tldr":"Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level ...","track":"Findings of EMNLP"},"forum":"findings.36","id":"findings.36","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.37.png","content":{"abstract":"Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings. On the contrary, humans can easily infer the corresponding correct words from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem, which only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperform the previous state-of-the-art result by 12.8% absolute F0.5 score.","authors":["Xiangci Li","Hairong Liu","Liang Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.37","program":"findings","sessions":[],"similar_paper_uids":["findings.37"],"title":"Context-aware Stand-alone Neural Spelling Correction","tldr":"Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings. On the contrary, humans can easily infer the corresponding correct words from their misspellings and surrounding context. Inspired by this, we ad...","track":"Findings of EMNLP"},"forum":"findings.37","id":"findings.37","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.38.png","content":{"abstract":"Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95% accuracy for predicate labels and 93% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56% to 14% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.","authors":["Youxuan Jiang","Huaiyu Zhu","Jonathan K. Kummerfeld","Yunyao Li","Walter Lasecki"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.38","program":"findings","sessions":[],"similar_paper_uids":["findings.38"],"title":"A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels","tldr":"Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd...","track":"Findings of EMNLP"},"forum":"findings.38","id":"findings.38","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.39.png","content":{"abstract":"Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in natural language understanding (NLU). Although several benchmark datasets for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the Korean language. Motivated by this, we construct and release new datasets for Korean NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous approaches, we machine-translate existing English training sets and manually translate development and test sets into Korean. To accelerate research on Korean NLU, we also establish baselines on KorNLI and KorSTS. Our datasets are publicly available at https://github.com/kakaobrain/KorNLUDatasets.","authors":["Jiyeon Ham","Yo Joong Choe","Kyubyong Park","Ilji Choi","Hyungjoon Soh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.39","program":"findings","sessions":[],"similar_paper_uids":["findings.39"],"title":"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding","tldr":"Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in natural language understanding (NLU). Although several benchmark datasets for those tasks have been released in English and a few other languages, there are no pu...","track":"Findings of EMNLP"},"forum":"findings.39","id":"findings.39","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.40.png","content":{"abstract":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.","authors":["Yifan Gao","Piji Li","Wei Bi","Xiaojiang Liu","Michael Lyu","Irwin King"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.40","program":"findings","sessions":[],"similar_paper_uids":["findings.40"],"title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning","tldr":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of...","track":"Findings of EMNLP"},"forum":"findings.40","id":"findings.40","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.41.png","content":{"abstract":"Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3% parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.","authors":["Zhaojiang Lin","Andrea Madotto","Pascale Fung"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.41","program":"findings","sessions":[],"similar_paper_uids":["findings.41"],"title":"Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning","tldr":"Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios...","track":"Findings of EMNLP"},"forum":"findings.41","id":"findings.41","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.42.png","content":{"abstract":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.","authors":["Federico L\u00f3pez","Michael Strube"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.42","program":"findings","sessions":[],"similar_paper_uids":["findings.42"],"title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification","tldr":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic ...","track":"Findings of EMNLP"},"forum":"findings.42","id":"findings.42","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.43.png","content":{"abstract":"As the first step of automatic fact checking, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem: check-worthiness ranking from political speeches and debates, rumour detection on Twitter, and citation needed detection from Wikipedia. To date, there has been no structured comparison of these various tasks to understand their relatedness, and no investigation into whether or not a unified approach to all of them is achievable. In this work, we illuminate a central challenge in claim check-worthiness detection underlying all of these tasks, being that they hinge upon detecting both how factual a sentence is, as well as how likely a sentence is to be believed without verification. As such, annotators only mark those instances they judge to be clear-cut check-worthy. Our best performing method is a unified approach which automatically corrects for this using a variant of positive unlabelled learning that finds instances which were incorrectly labelled as not check-worthy. In applying this, we out-perform the state of the art in two of the three tasks studied for claim check-worthiness detection in English.","authors":["Dustin Wright","Isabelle Augenstein"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.43","program":"findings","sessions":[],"similar_paper_uids":["findings.43"],"title":"Claim Check-Worthiness Detection as Positive Unlabelled Learning","tldr":"As the first step of automatic fact checking, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem: check-worthiness ranking from political speeches and debat...","track":"Findings of EMNLP"},"forum":"findings.43","id":"findings.43","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.44.png","content":{"abstract":"Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. A VQA model combines visual and textual features in order to answer questions grounded in an image. Current works in VQA focus on questions which are answerable by direct analysis of the question and image alone. We present a concept-aware algorithm, ConceptBert, for questions which require common sense, or basic factual knowledge from external structured content. Given an image and a question in natural language, ConceptBert requires visual elements of the image and a Knowledge Graph (KG) to infer the correct answer. We introduce a multi-modal representation which learns a joint Concept-Vision-Language embedding inspired by the popular BERT architecture. We exploit ConceptNet KG for encoding the common sense knowledge and evaluate our methodology on the Outside Knowledge-VQA (OK-VQA) and VQA datasets.","authors":["Fran\u00e7ois Gard\u00e8res","Maryam Ziaeefard","Baptiste Abeloos","Freddy Lecue"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.44","program":"findings","sessions":[],"similar_paper_uids":["findings.44"],"title":"ConceptBert: Concept-Aware Representation for Visual Question Answering","tldr":"Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. A VQA model combines visual and textual features in order to answer questions ...","track":"Findings of EMNLP"},"forum":"findings.44","id":"findings.44","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.45.png","content":{"abstract":"Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if machine translation is an adequate substitute for training data, and extend this to investigate bootstrapping using joint training with English, paraphrasing, and multilingual pre-trained models. We develop a Transformer-based parser combining paraphrases by ensembling attention over multiple encoders and present new versions of ATIS and Overnight in German and Chinese for evaluation. Experimental results indicate that MT can approximate training data in a new language for accurate parsing when augmented with paraphrasing through multiple MT engines. Considering when MT is inadequate, we also find that using our approach achieves parsing accuracy within 2% of complete translation using only 50% of training data.","authors":["Tom Sherborne","Yumo Xu","Mirella Lapata"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.45","program":"findings","sessions":[],"similar_paper_uids":["findings.45"],"title":"Bootstrapping a Crosslingual Semantic Parser","tldr":"Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple d...","track":"Findings of EMNLP"},"forum":"findings.45","id":"findings.45","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.46.png","content":{"abstract":"Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are likely to degenerate and lie in a narrow cone when training a language model. They call it the representation degeneration problem and propose a cosine regularization to solve it. Nevertheless, we prove that the cosine regularization is insufficient to solve the problem, as the degeneration is still likely to happen under certain conditions. In this paper, we revisit the representation degeneration problem and theoretically analyze the limitations of the previously proposed solution. Afterward, we propose an alternative regularization method called Laplacian regularization to tackle the problem. Experiments on language modeling demonstrate the effectiveness of the proposed Laplacian regularization.","authors":["Zhong Zhang","Chongming Gao","Cong Xu","Rui Miao","Qinli Yang","Junming Shao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.46","program":"findings","sessions":[],"similar_paper_uids":["findings.46"],"title":"Revisiting Representation Degeneration Problem in Language Modeling","tldr":"Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are l...","track":"Findings of EMNLP"},"forum":"findings.46","id":"findings.46","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.47.png","content":{"abstract":"Argument generation is a challenging task whose research is timely considering its potential impact on social media and the dissemination of information. Here we suggest a pipeline based on GPT-2 for generating coherent claims, and explore the types of claims that it produces, and their veracity, using an array of manual and automatic assessments. In addition, we explore the interplay between this task and the task of Claim Retrieval, showing how they can complement one another.","authors":["Shai Gretz","Yonatan Bilu","Edo Cohen-Karlik","Noam Slonim"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.47","program":"findings","sessions":[],"similar_paper_uids":["findings.47"],"title":"The workweek is the best time to start a family \u2013 A Study of GPT-2 Based Claim Generation","tldr":"Argument generation is a challenging task whose research is timely considering its potential impact on social media and the dissemination of information. Here we suggest a pipeline based on GPT-2 for generating coherent claims, and explore the types ...","track":"Findings of EMNLP"},"forum":"findings.47","id":"findings.47","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.48.png","content":{"abstract":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","authors":["John P. Lalor","Hong Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.48","program":"findings","sessions":[],"similar_paper_uids":["findings.48"],"title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation","tldr":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic...","track":"Findings of EMNLP"},"forum":"findings.48","id":"findings.48","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.49.png","content":{"abstract":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed \u2013 non-learnable \u2013 attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.","authors":["Alessandro Raganato","Yves Scherrer","J\u00f6rg Tiedemann"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.49","program":"findings","sessions":[],"similar_paper_uids":["findings.49"],"title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation","tldr":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of ...","track":"Findings of EMNLP"},"forum":"findings.49","id":"findings.49","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.50.png","content":{"abstract":"We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds\u2019 images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name zero-shot learning from text, focusing on learning to transfer knowledge about visual aspects of birds from seen classes to previously-unseen ones. Here, we suggest focusing on the textual description and distilling from the description the most relevant information to effectively match visual features to the parts of the text that discuss them. Specifically, (1) we propose to leverage the similarity between species, reflected in the similarity between text descriptions of the species. (2) we derive visual summaries of the texts, i.e., extractive summaries that focus on the visual features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition.","authors":["Tzuf Paz-Argaman","Reut Tsarfaty","Gal Chechik","Yuval Atzmon"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.50","program":"findings","sessions":[],"similar_paper_uids":["findings.50"],"title":"ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization","tldr":"We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds\u2019 images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on spec...","track":"Findings of EMNLP"},"forum":"findings.50","id":"findings.50","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.51.png","content":{"abstract":"Multi-hop relation reasoning over knowledge base is to generate effective and interpretable relation prediction through reasoning paths. The current methods usually require sufficient training data (fact triples) for each query relation, impairing their performances over few-shot relations (with limited triples) which are common in knowledge base. To this end, we propose FIRE, a novel few-shot multi-hop relation learning model. FIRE applies reinforcement learning to model the sequential steps of multi-hop reasoning, besides performs heterogeneous structure encoding and knowledge-aware search space pruning. The meta-learning technique is employed to optimize model parameters that could quickly adapt to few-shot relations. Empirical study on two datasets demonstrate that FIRE outperforms state-of-the-art methods.","authors":["Chuxu Zhang","Lu Yu","Mandana Saebi","Meng Jiang","Nitesh Chawla"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.51","program":"findings","sessions":[],"similar_paper_uids":["findings.51"],"title":"Few-Shot Multi-Hop Relation Reasoning over Knowledge Bases","tldr":"Multi-hop relation reasoning over knowledge base is to generate effective and interpretable relation prediction through reasoning paths. The current methods usually require sufficient training data (fact triples) for each query relation, impairing th...","track":"Findings of EMNLP"},"forum":"findings.51","id":"findings.51","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.52.png","content":{"abstract":"Syntactic information is essential for both sentiment analysis(SA) and aspect-based sentiment analysis(ABSA). Previous work has already achieved great progress utilizing Graph Convolutional Network(GCN) over dependency tree of a sentence. However, these models do not fully exploit the syntactic information obtained from dependency parsing such as the diversified types of dependency relations. The message passing process of GCN should be distinguished based on these syntactic information.To tackle this problem, we design a novel weighted graph convolutional network(WGCN) which can exploit rich syntactic information based on the feature combination. Furthermore, we utilize BERT instead of Bi-LSTM to generate contextualized representations as inputs for GCN and present an alignment method to keep word-level dependencies consistent with wordpiece unit of BERT. With our proposal, we are able to improve the state-of-the-art on four ABSA tasks out of six and two SA tasks out of three.","authors":["Fanyu Meng","Junlan Feng","Danping Yin","Si Chen","Min Hu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.52","program":"findings","sessions":[],"similar_paper_uids":["findings.52"],"title":"A structure-enhanced graph convolutional network for sentiment analysis","tldr":"Syntactic information is essential for both sentiment analysis(SA) and aspect-based sentiment analysis(ABSA). Previous work has already achieved great progress utilizing Graph Convolutional Network(GCN) over dependency tree of a sentence. However, th...","track":"Findings of EMNLP"},"forum":"findings.52","id":"findings.52","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.53.png","content":{"abstract":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.","authors":["Zhao Jinman","Shawn Zhong","Xiaomin Zhang","Yingyu Liang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.53","program":"findings","sessions":[],"similar_paper_uids":["findings.53"],"title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding","tldr":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the...","track":"Findings of EMNLP"},"forum":"findings.53","id":"findings.53","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.54.png","content":{"abstract":"In standard methodology for natural language processing, entities in text are typically embedded in dense vector spaces with pre-trained models. The embeddings produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to posterior probabilities over fine-grained entity types, indicating the confidence of a typing model\u2019s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al. 2018) or distantly-supervised examples from Wikipedia. On entity probing tasks involving recognizing entity identity, our embeddings used in parameter-free downstream models achieve competitive performance with ELMo- and BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance.","authors":["Yasumasa Onoe","Greg Durrett"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.54","program":"findings","sessions":[],"similar_paper_uids":["findings.54"],"title":"Interpretable Entity Representations through Large-Scale Typing","tldr":"In standard methodology for natural language processing, entities in text are typically embedded in dense vector spaces with pre-trained models. The embeddings produced this way are effective when fed into downstream models, but they require end-task...","track":"Findings of EMNLP"},"forum":"findings.54","id":"findings.54","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.55.png","content":{"abstract":"Federated learning has sparkled new interests in the deep learning society to make use of isolated data sources from independent institutes. With the development of novel training tools, we have successfully deployed federated natural language processing networks on GPU-enabled server clusters. This paper demonstrates federated training of a popular NLP model, TextCNN, with applications in sentence intent classification. Furthermore, differential privacy is introduced to protect participants in the training process, in a manageable manner. Distinguished from previous client-level privacy protection schemes, the proposed differentially private federated learning procedure is defined in the dataset sample level, inherent with the applications among institutions instead of individual users. Optimal settings of hyper-parameters for the federated TextCNN model are studied through comprehensive experiments. We also evaluated the performance of federated TextCNN model under imbalanced data load configuration. Experiments show that, the sampling ratio has a large impact on the performance of the FL models, causing up to 38.4% decrease in the test accuracy, while they are robust to different noise multiplier levels, with less than 3% variance in the test accuracy. It is also found that the FL models are sensitive to data load balancedness among client datasets. When the data load is imbalanced, model performance dropped by up to 10%.","authors":["Xinghua Zhu","Jianzong Wang","Zhenhou Hong","Jing Xiao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.55","program":"findings","sessions":[],"similar_paper_uids":["findings.55"],"title":"Empirical Studies of Institutional Federated Learning For Natural Language Processing","tldr":"Federated learning has sparkled new interests in the deep learning society to make use of isolated data sources from independent institutes. With the development of novel training tools, we have successfully deployed federated natural language proces...","track":"Findings of EMNLP"},"forum":"findings.55","id":"findings.55","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.56.png","content":{"abstract":"Mixed Boolean-Arithmetic (MBA) expressions involve both arithmetic calculation (e.g.,plus, minus, multiply) and bitwise computation (e.g., and, or, negate, xor). MBA expressions have been widely applied in software obfuscation, transforming programs from a simple form to a complex form. MBA expressions are challenging to be simplified, because the interleaving bitwise and arithmetic operations causing mathematical reduction laws to be ineffective. Our goal is to recover the original, simple form from an obfuscated MBA expression. In this paper, we first propose NeuReduce, a string to string method based on neural networks to automatically learn and reduce complex MBA expressions. We develop a comprehensive MBA dataset, including one million diversified MBA expression samples and corresponding simplified forms. After training on the dataset, NeuReduce can reduce MBA rules to homelier but mathematically equivalent forms. By comparing with three state-of-the-art MBA reduction methods, our evaluation result shows that NeuReduce outperforms all other tools in terms of accuracy, solving time, and performance overhead.","authors":["Weijie Feng","Binbin Liu","Dongpeng Xu","Qilong Zheng","Yun Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.56","program":"findings","sessions":[],"similar_paper_uids":["findings.56"],"title":"NeuReduce: Reducing Mixed Boolean-Arithmetic Expressions by Recurrent Neural Network","tldr":"Mixed Boolean-Arithmetic (MBA) expressions involve both arithmetic calculation (e.g.,plus, minus, multiply) and bitwise computation (e.g., and, or, negate, xor). MBA expressions have been widely applied in software obfuscation, transforming programs ...","track":"Findings of EMNLP"},"forum":"findings.56","id":"findings.56","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.57.png","content":{"abstract":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction to syntactically and semantically sound language stimuli. In this study, we asked: how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain\u2019s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain\u2019s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.","authors":["Maryam Hashemzadeh","Greta Kaufeld","Martha White","Andrea E. Martin","Alona Fyshe"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.57","program":"findings","sessions":[],"similar_paper_uids":["findings.57"],"title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?","tldr":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction t...","track":"Findings of EMNLP"},"forum":"findings.57","id":"findings.57","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.58.png","content":{"abstract":"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. https://github.com/ymcui/MacBERT","authors":["Yiming Cui","Wanxiang Che","Ting Liu","Bing Qin","Shijin Wang","Guoping Hu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.58","program":"findings","sessions":[],"similar_paper_uids":["findings.58"],"title":"Revisiting Pre-Trained Models for Chinese Natural Language Processing","tldr":"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper...","track":"Findings of EMNLP"},"forum":"findings.58","id":"findings.58","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.59.png","content":{"abstract":"Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies, and demonstrate the encouraging results compared with state of the art.","authors":["Juyong Jiang","Jie Zhang","Kai Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.59","program":"findings","sessions":[],"similar_paper_uids":["findings.59"],"title":"Cascaded Semantic and Positional Self-Attention Network for Document Classification","tldr":"Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (wor...","track":"Findings of EMNLP"},"forum":"findings.59","id":"findings.59","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.60.png","content":{"abstract":"In this work, we explore the way to quickly adjust an existing named entity recognition (NER) system to make it capable of recognizing entity types not defined in the system. As an illustrative example, consider the case that a NER system has been built to recognize person and organization names, and now it requires to additionally recognize job titles. Such a situation is common in the industrial areas, where the entity types required to recognize vary a lot in different products and keep changing. To avoid laborious data labeling and achieve fast adaptation, we propose to adjust the existing NER system using the previously labeled data and entity lexicons of the newly introduced entity types. We formulate such a task as a partially supervised learning problem and accordingly propose an effective algorithm to solve the problem. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our method.","authors":["Minlong Peng","Ruotian Ma","Qi Zhang","Lujun Zhao","Mengxi Wei","Changlong Sun","Xuanjing Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.60","program":"findings","sessions":[],"similar_paper_uids":["findings.60"],"title":"Toward Recognizing More Entity Types in NER: An Efficient Implementation using Only Entity Lexicons","tldr":"In this work, we explore the way to quickly adjust an existing named entity recognition (NER) system to make it capable of recognizing entity types not defined in the system. As an illustrative example, consider the case that a NER system has been bu...","track":"Findings of EMNLP"},"forum":"findings.60","id":"findings.60","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.61.png","content":{"abstract":"We present a method for creating parallel data to train Seq2Seq neural networks for sentiment transfer. Most systems for this task, which can be viewed as monolingual machine translation (MT), have relied on unsupervised methods, such as Generative Adversarial Networks (GANs)-inspired approaches, for coping with the lack of parallel corpora. Given that the literature shows that Seq2Seq methods have been consistently outperforming unsupervised methods in MT-related tasks, in this work we exploit the use of semantic similarity computation for converting non-parallel data onto a parallel corpus. That allows us to train a transformer neural network for the sentiment transfer task, and compare its performance against unsupervised approaches. With experiments conducted on two well-known public datasets, i.e. Yelp and Amazon, we demonstrate that the proposed methodology outperforms existing unsupervised methods very consistently in fluency, and presents competitive results in terms of sentiment conversion and content preservation. We believe that this works opens up an opportunity for seq2seq neural networks to be better exploited in problems for which they have not been applied owing to the lack of parallel training data.","authors":["Paulo Cavalin","Marisa Vasconcelos","Marcelo Grave","Claudio Pinhanez","Victor Henrique Alves Ribeiro"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.61","program":"findings","sessions":[],"similar_paper_uids":["findings.61"],"title":"From Disjoint Sets to Parallel Data to Train Seq2Seq Models for Sentiment Transfer","tldr":"We present a method for creating parallel data to train Seq2Seq neural networks for sentiment transfer. Most systems for this task, which can be viewed as monolingual machine translation (MT), have relied on unsupervised methods, such as Generative A...","track":"Findings of EMNLP"},"forum":"findings.61","id":"findings.61","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.62.png","content":{"abstract":"Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2Stop), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89% (absolute improvement) on Success weighted by Edit Distance (SED).","authors":["Jiannan Xiang","Xin Wang","William Yang Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.62","program":"findings","sessions":[],"similar_paper_uids":["findings.62"],"title":"Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation","tldr":"Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct...","track":"Findings of EMNLP"},"forum":"findings.62","id":"findings.62","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.63.png","content":{"abstract":"This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \u201ctarget tokens\u201d, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model\u2019s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.","authors":["Rodrigo Nogueira","Zhiying Jiang","Ronak Pradeep","Jimmy Lin"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.63","program":"findings","sessions":[],"similar_paper_uids":["findings.63"],"title":"Document Ranking with a Pretrained Sequence-to-Sequence Model","tldr":"This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures su...","track":"Findings of EMNLP"},"forum":"findings.63","id":"findings.63","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.64.png","content":{"abstract":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.","authors":["Zi Lin","Jeremiah Liu","Zi Yang","Nan Hua","Dan Roth"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.64","program":"findings","sessions":[],"similar_paper_uids":["findings.64"],"title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior","tldr":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which p...","track":"Findings of EMNLP"},"forum":"findings.64","id":"findings.64","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.65.png","content":{"abstract":"Attention mechanisms have improved the performance of NLP tasks while allowing models to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from label-specific information, while facilitating interpretation of predictions. We introduce the Label Attention Layer: a new form of self-attention where attention heads represent labels. We test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the Penn Treebank (PTB) and Chinese Treebank. Additionally, our model requires fewer self-attention layers compared to existing work. Finally, we find that the Label Attention heads learn relations between syntactic categories and show pathways to analyze errors.","authors":["Khalil Mrini","Franck Dernoncourt","Quan Hung Tran","Trung Bui","Walter Chang","Ndapa Nakashole"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.65","program":"findings","sessions":[],"similar_paper_uids":["findings.65"],"title":"Rethinking Self-Attention: Towards Interpretability in Neural Parsing","tldr":"Attention mechanisms have improved the performance of NLP tasks while allowing models to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has s...","track":"Findings of EMNLP"},"forum":"findings.65","id":"findings.65","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.66.png","content":{"abstract":"Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy document given a question. On the contrary, we argue that providing users with a short text span from policy documents reduces the burden of searching the target information from a lengthy text segment. In this paper, we present PolicyQA, a dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices. We evaluate two existing neural QA models and perform rigorous analysis to reveal the advantages and challenges offered by PolicyQA.","authors":["Wasi Ahmad","Jianfeng Chi","Yuan Tian","Kai-Wei Chang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.66","program":"findings","sessions":[],"similar_paper_uids":["findings.66"],"title":"PolicyQA: A Reading Comprehension Dataset for Privacy Policies","tldr":"Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text se...","track":"Findings of EMNLP"},"forum":"findings.66","id":"findings.66","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.67.png","content":{"abstract":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model\u2019s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues.","authors":["Takuma Udagawa","Takato Yamazaki","Akiko Aizawa"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.67","program":"findings","sessions":[],"similar_paper_uids":["findings.67"],"title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions","tldr":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize th...","track":"Findings of EMNLP"},"forum":"findings.67","id":"findings.67","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.68.png","content":{"abstract":"Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. To encode the dialogue context efficiently, we utilize the previous dialogue state (predicted) and the current dialogue utterance as the input for DST. To consider relations among different domain-slots, the schema graph involving prior knowledge is exploited. In this paper, a novel context and schema fusion network is proposed to encode the dialogue context and schema graph by using internal and external attention mechanisms. Experiment results show that our approach can outperform strong baselines, and the previous state-of-the-art method (SOM-DST) can also be improved by our proposed schema graph.","authors":["Su Zhu","Jieyu Li","Lu Chen","Kai Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.68","program":"findings","sessions":[],"similar_paper_uids":["findings.68"],"title":"Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking","tldr":"Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. T...","track":"Findings of EMNLP"},"forum":"findings.68","id":"findings.68","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.69.png","content":{"abstract":"One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of open domain corpora and the variety of natural language expressions further exacerbate this problem. In this paper, we propose a syntactic and semantic-driven learning approach, which can learn neural open IE models without any human-labelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervision. Specifically, we first employ syntactic patterns as data labelling functions and pretrain a base model using the generated labels. Then we propose a syntactic and semantic-driven reinforcement learning algorithm, which can effectively generalize the base model to open situations with high accuracy. Experimental results show that our approach significantly outperforms the supervised counterparts, and can even achieve competitive performance to supervised state-of-the-art (SoA) model.","authors":["Jialong Tang","Yaojie Lu","Hongyu Lin","Xianpei Han","Le Sun","Xinyan Xiao","Hua Wu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.69","program":"findings","sessions":[],"similar_paper_uids":["findings.69"],"title":"Syntactic and Semantic-driven Learning for Open Information Extraction","tldr":"One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of open domain corpora and the variety of natural language expressions further exacerbate this problem. In...","track":"Findings of EMNLP"},"forum":"findings.69","id":"findings.69","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.70.png","content":{"abstract":"Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the open-domain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multi-mapping relations prevalent in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches.","authors":["Hengyi Cai","Hongshen Chen","Yonghao Song","Zhuoye Ding","Yongjun Bao","Weipeng Yan","Xiaofang Zhao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.70","program":"findings","sessions":[],"similar_paper_uids":["findings.70"],"title":"Group-wise Contrastive Learning for Neural Dialogue Generation","tldr":"Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by th...","track":"Findings of EMNLP"},"forum":"findings.70","id":"findings.70","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.71.png","content":{"abstract":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors. The resulting entity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no expensive further pre-training of the BERT encoder. We evaluate E-BERT on unsupervised question answering (QA), supervised relation classification (RC) and entity linking (EL). On all three tasks, E-BERT outperforms BERT and other baselines. We also show quantitatively that the original BERT model is overly reliant on the surface form of entity names (e.g., guessing that someone with an Italian-sounding name speaks Italian), and that E-BERT mitigates this problem.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.71","program":"findings","sessions":[],"similar_paper_uids":["findings.71"],"title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT","tldr":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entit...","track":"Findings of EMNLP"},"forum":"findings.71","id":"findings.71","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.72.png","content":{"abstract":"The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are mainly based on either detecting aspect terms and their corresponding sentiment polarities, or co-extracting aspect and opinion terms. However, the extraction of aspect-sentiment pairs lacks opinion terms as a reference, while co-extraction of aspect and opinion terms would not lead to meaningful pairs without determining their sentiment dependencies. To address the issue, we present a novel view of ABSA as an opinion triplet extraction task, and propose a multi-task learning framework to jointly extract aspect terms and opinion terms, and simultaneously parses sentiment dependencies between them with a biaffine scorer. At inference phase, the extraction of triplets is facilitated by a triplet decoding method based on the above outputs. We evaluate the proposed framework on four SemEval benchmarks for ASBA. The results demonstrate that our approach significantly outperforms a range of strong baselines and state-of-the-art approaches.","authors":["Chen Zhang","Qiuchi Li","Dawei Song","Benyou Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.72","program":"findings","sessions":[],"similar_paper_uids":["findings.72"],"title":"A Multi-task Learning Framework for Opinion Triplet Extraction","tldr":"The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are mainly based on either detecting aspect terms and their corresponding sentiment polarities, or co-extracting aspect and opinion terms. However, the extraction of aspect-sentim...","track":"Findings of EMNLP"},"forum":"findings.72","id":"findings.72","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.73.png","content":{"abstract":"Event extraction, which aims to identify event triggers of pre-defined event types and their arguments of specific roles, is a challenging task in NLP. Most traditional approaches formulate this task as classification problems, with event types or argument roles taken as golden labels. Such approaches fail to model rich interactions among event types and arguments of different roles, and cannot generalize to new types or roles. This work proposes a new paradigm that formulates event extraction as multi-turn question answering. Our approach, MQAEE, casts the extraction task into a series of reading comprehension problems, by which it extracts triggers and arguments successively from a given sentence. A history answer embedding strategy is further adopted to model question answering history in the multi-turn process. By this new formulation, MQAEE makes full use of dependency among arguments and event types, and generalizes well to new types with new argument roles. Empirical results on ACE 2005 shows that MQAEE outperforms current state-of-the-art, pushing the final F1 of argument extraction to 53.4% (+2.0%). And it also has a good generalization ability, achieving competitive performance on 13 new event types even if trained only with a few samples of them.","authors":["Fayuan Li","Weihua Peng","Yuguang Chen","Quan Wang","Lu Pan","Yajuan Lyu","Yong Zhu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.73","program":"findings","sessions":[],"similar_paper_uids":["findings.73"],"title":"Event Extraction as Multi-turn Question Answering","tldr":"Event extraction, which aims to identify event triggers of pre-defined event types and their arguments of specific roles, is a challenging task in NLP. Most traditional approaches formulate this task as classification problems, with event types or ar...","track":"Findings of EMNLP"},"forum":"findings.73","id":"findings.73","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.74.png","content":{"abstract":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.","authors":["Mingzhu Wu","Nafise Sadat Moosavi","Andreas R\u00fcckl\u00e9","Iryna Gurevych"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.74","program":"findings","sessions":[],"similar_paper_uids":["findings.74"],"title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases","tldr":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge abo...","track":"Findings of EMNLP"},"forum":"findings.74","id":"findings.74","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.75.png","content":{"abstract":"In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than baselines by augmenting predicted future into a policy network, its complicated architecture introduces unwanted instability. In this work, we propose actor-double-critic (ADC) to improve the stability and overall performance of I2A. ADC simplifies the architecture of I2A to reduce excessive parameters and hyper-parameters. More importantly, a separate model-based critic shares parameters between actions and makes back-propagation explicit. In our experiments on Cambridge Restaurant Booking task, ADC enhances success rates considerably and shows robustness to imperfect environment models. In addition, ADC exhibits the stability and sample-efficiency as significantly reducing the baseline standard deviation of success rates and reaching the 80% success rate with half training data.","authors":["Yen-chen Wu","Bo-Hsiang Tseng","Milica Gasic"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.75","program":"findings","sessions":[],"similar_paper_uids":["findings.75"],"title":"Actor-Double-Critic: Incorporating Model-Based Critic for Task-Oriented Dialogue Systems","tldr":"In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than baselines by augmenting predicted futu...","track":"Findings of EMNLP"},"forum":"findings.75","id":"findings.75","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.76.png","content":{"abstract":"Neural text generation (data- or text-to-text) demonstrates remarkable performance when training data is abundant which for many applications is not the case. To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input. Consequently, models pick up on the noise and may hallucinate\u2013generate fluent but unsupported text. Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus (Lebret et al., 2016), a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation.","authors":["Katja Filippova"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.76","program":"findings","sessions":[],"similar_paper_uids":["findings.76"],"title":"Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data","tldr":"Neural text generation (data- or text-to-text) demonstrates remarkable performance when training data is abundant which for many applications is not the case. To collect a large corpus of parallel data, heuristic rules are often used but they inevita...","track":"Findings of EMNLP"},"forum":"findings.76","id":"findings.76","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.77.png","content":{"abstract":"Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers\u2019 information access in the biomedical domain. Conventional methods have regarded the task as a sequence labeling task based on sequential sentence classification, i.e., they assign a rhetorical label to each sentence by considering the context in the abstract. However, these methods have a critical problem: they are prone to mislabel longer continuous sentences with the same rhetorical label. To tackle the problem, we propose sequential span classification that assigns a rhetorical label, not to a single sentence but to a span that consists of continuous sentences. Accordingly, we introduce Neural Semi-Markov Conditional Random Fields to assign the labels to such spans by considering all possible spans of various lengths. Experimental results obtained from PubMed 20k RCT and NICTA-PIBOSO datasets demonstrate that our proposed method achieved the best micro sentence-F1 score as well as the best micro span-F1 score.","authors":["Kosuke Yamada","Tsutomu Hirao","Ryohei Sasano","Koichi Takeda","Masaaki Nagata"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.77","program":"findings","sessions":[],"similar_paper_uids":["findings.77"],"title":"Sequential Span Classification with Neural Semi-Markov CRFs for Biomedical Abstracts","tldr":"Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers\u2019 information access in the biomedical domain. Conventional methods have regarded the task as a sequence labeling task based on sequentia...","track":"Findings of EMNLP"},"forum":"findings.77","id":"findings.77","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.78.png","content":{"abstract":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keywords of a given paper. We adapt the TextCNN architecture and automatically analyze its predictions using the Integrated Gradients method to highlight words and phrases that led to the recommendation of a scientific venue. We train and test our method on publications from the fields of artificial intelligence (AI) and medicine, both derived from the Semantic Scholar dataset. WTS achieves an Accuracy@5 of approximately 83% for AI papers and 95% in the field of medicine. It is open source and available for testing on https://wheretosubmit.ml.","authors":["Konstantin Kobs","Tobias Koopmann","Albin Zehe","David Fernes","Philipp Krop","Andreas Hotho"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.78","program":"findings","sessions":[],"similar_paper_uids":["findings.78"],"title":"Where to Submit? Helping Researchers to Choose the Right Venue","tldr":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keyword...","track":"Findings of EMNLP"},"forum":"findings.78","id":"findings.78","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.79.png","content":{"abstract":"Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which cannot be simply encoded by neural approaches, such as memory network mechanisms. To alleviate the above problem, we propose , an end-to-end trainable text-to-SQL guided framework to learn a neural agent that interacts with KBs using the generated SQL queries. Specifically, the neural agent first learns to ask and confirm the customer\u2019s intent during the multi-turn interactions, then dynamically determining when to ground the user constraints into executable SQL queries so as to fetch relevant information from KBs. With the help of our method, the agent can use less but more accurate fetched results to generate useful responses efficiently, instead of incorporating the entire KBs. We evaluate the proposed method on the AirDialogue dataset, a large corpus released by Google, containing the conversations of customers booking flight tickets from the agent. The experimental results show that significantly improves over previous work in terms of accuracy and the BLEU score, which demonstrates not only the ability to achieve the given task but also the good quality of the generated dialogues.","authors":["Chieh-Yang Chen","Pei-Hsin Wang","Shih-Chieh Chang","Da-Cheng Juan","Wei Wei","Jia-Yu Pan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.79","program":"findings","sessions":[],"similar_paper_uids":["findings.79"],"title":"AirConcierge: Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval","tldr":"Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which cannot be simply encoded by neural approaches, such as memory network mechanisms. To alle...","track":"Findings of EMNLP"},"forum":"findings.79","id":"findings.79","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.80.png","content":{"abstract":"Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well, it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works cannot apply to all forms because of their requirements on formats. Therefore, we concentrate on the most elementary components, the key-value pairs, and adopt multimodal methods to extract features. We consider the form structure as a tree-like or graph-like hierarchy of text fragments. The parent-child relation corresponds to the key-value pairs in forms. We utilize the state-of-the-art models and design targeted extraction modules to extract multimodal features from semantic contents, layout information, and visual images. A hybrid fusion method of concatenation and feature shifting is designed to fuse the heterogeneous features and provide an informative joint representation. We adopt an asymmetric algorithm and negative sampling in our model as well. We validate our method on two benchmarks, MedForm and FUNSD, and extensive experiments demonstrate the effectiveness of our method.","authors":["Zilong Wang","Mingjie Zhan","Xuebo Liu","Ding Liang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.80","program":"findings","sessions":[],"similar_paper_uids":["findings.80"],"title":"DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding","tldr":"Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well, it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detec...","track":"Findings of EMNLP"},"forum":"findings.80","id":"findings.80","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.81.png","content":{"abstract":"Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned on multiple sources. Previous work simply concatenates all input sources or averages information from different input sources. In this work, we study dialogue models with multiple input sources adapted from the pretrained language model GPT2. We explore various methods to fuse multiple separate attention information corresponding to different sources. Our experimental results show that proper fusion methods deliver higher relevance with dialogue history than simple fusion baselines.","authors":["Yu Cao","Wei Bi","Meng Fang","Dacheng Tao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.81","program":"findings","sessions":[],"similar_paper_uids":["findings.81"],"title":"Pretrained Language Models for Dialogue Generation with Multiple Input Sources","tldr":"Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned...","track":"Findings of EMNLP"},"forum":"findings.81","id":"findings.81","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.82.png","content":{"abstract":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better coverage of the space of valid translations and thereby improve its correlation with human judgments. Our experiments on the into-English language directions of the WMT19 metrics task (at both the system and sentence level) show that using paraphrased references does generally improve BLEU, and when it does, the more diverse the better. However, we also show that better results could be achieved if those paraphrases were to specifically target the parts of the space most relevant to the MT outputs being evaluated. Moreover, the gains remain slight even when human paraphrases are used, suggesting inherent limitations to BLEU\u2019s capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentence-level BLEU.","authors":["Rachel Bawden","Biao Zhang","Lisa Yankovskaya","Andre T\u00e4ttar","Matt Post"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.82","program":"findings","sessions":[],"similar_paper_uids":["findings.82"],"title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing","tldr":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better cove...","track":"Findings of EMNLP"},"forum":"findings.82","id":"findings.82","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.83.png","content":{"abstract":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.","authors":["Saurabh Kulshreshtha","Jose Luis Redondo Garcia","Ching-Yun Chang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.83","program":"findings","sessions":[],"similar_paper_uids":["findings.83"],"title":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study","tldr":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved b...","track":"Findings of EMNLP"},"forum":"findings.83","id":"findings.83","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.84.png","content":{"abstract":"Recent studies have demonstrated the effectiveness of cross-lingual language model pre-training on different NLP tasks, such as natural language inference and machine translation. In our work, we test this approach on social media data, which are particularly challenging to process within this framework, since the limited length of the textual messages and the irregularity of the language make it harder to learn meaningful encodings. More specifically, we propose a hybrid emoji-based Masked Language Model (MLM) to leverage the common information conveyed by emojis across different languages and improve the learned cross-lingual representation of short text messages, with the goal to perform zero- shot abusive language detection. We compare the results obtained with the original MLM to the ones obtained by our method, showing improved performance on German, Italian and Spanish.","authors":["Michele Corazza","Stefano Menini","Elena Cabrio","Sara Tonelli","Serena Villata"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.84","program":"findings","sessions":[],"similar_paper_uids":["findings.84"],"title":"Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection","tldr":"Recent studies have demonstrated the effectiveness of cross-lingual language model pre-training on different NLP tasks, such as natural language inference and machine translation. In our work, we test this approach on social media data, which are par...","track":"Findings of EMNLP"},"forum":"findings.84","id":"findings.84","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.85.png","content":{"abstract":"Sensor metadata tagging, akin to the named entity recognition task, provides key contextual information (e.g., measurement type and location) about sensors for running smart building applications. Unfortunately, sensor metadata in different buildings often follows distinct naming conventions. Therefore, learning a tagger currently requires extensive annotations on a per building basis. In this work, we propose a novel framework, SeNsER, which learns a sensor metadata tagger for a new building based on its raw metadata and some existing fully annotated building. It leverages the commonality between different buildings: At the character level, it employs bidirectional neural language models to capture the shared underlying patterns between two buildings and thus regularizes the feature learning process; At the word level, it leverages as features the k-mers existing in the fully annotated building. During inference, we further incorporate the information obtained from sources such as Wikipedia as prior knowledge. As a result, SeNsER shows promising results in extensive experiments on multiple real-world buildings.","authors":["Yang Jiao","Jiacheng Li","Jiaman Wu","Dezhi Hong","Rajesh Gupta","Jingbo Shang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.85","program":"findings","sessions":[],"similar_paper_uids":["findings.85"],"title":"SeNsER: Learning Cross-Building Sensor Metadata Tagger","tldr":"Sensor metadata tagging, akin to the named entity recognition task, provides key contextual information (e.g., measurement type and location) about sensors for running smart building applications. Unfortunately, sensor metadata in different buildings...","track":"Findings of EMNLP"},"forum":"findings.85","id":"findings.85","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.86.png","content":{"abstract":"Ezafe is a grammatical particle in some Iranian languages that links two words together. Regardless of the important information it conveys, it is almost always not indicated in Persian script, resulting in mistakes in reading complex sentences and errors in natural language processing tasks. In this paper, we experiment with different machine learning methods to achieve state-of-the-art results in the task of ezafe recognition. Transformer-based methods, BERT and XLMRoBERTa, achieve the best results, the latter achieving 2.68% F1-score more than the previous state-of-the-art. We, moreover, use ezafe information to improve Persian part-of-speech tagging results and show that such information will not be useful to transformer-based methods and explain why that might be the case.","authors":["Ehsan Doostmohammadi","Minoo Nassajian","Adel Rahimi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.86","program":"findings","sessions":[],"similar_paper_uids":["findings.86"],"title":"Persian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging","tldr":"Ezafe is a grammatical particle in some Iranian languages that links two words together. Regardless of the important information it conveys, it is almost always not indicated in Persian script, resulting in mistakes in reading complex sentences and e...","track":"Findings of EMNLP"},"forum":"findings.86","id":"findings.86","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.87.png","content":{"abstract":"Structured representations like graphs and parse trees play a crucial role in many Natural Language Processing systems. In recent years, the advancements in multi-turn user interfaces necessitate the need for controlling and updating these structured representations given new sources of information. Although there have been many efforts focusing on improving the performance of the parsers that map text to graphs or parse trees, very few have explored the problem of directly manipulating these representations. In this paper, we explore the novel problem of graph modification, where the systems need to learn how to update an existing scene graph given a new user\u2019s command. Our novel models based on graph-based sparse transformer and cross attention information fusion outperform previous systems adapted from the machine translation and graph generation literature. We further contribute our large graph modification datasets to the research community to encourage future research for this new problem.","authors":["Xuanli He","Quan Hung Tran","Gholamreza Haffari","Walter Chang","Zhe Lin","Trung Bui","Franck Dernoncourt","Nhan Dam"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.87","program":"findings","sessions":[],"similar_paper_uids":["findings.87"],"title":"Scene Graph Modification Based on Natural Language Commands","tldr":"Structured representations like graphs and parse trees play a crucial role in many Natural Language Processing systems. In recent years, the advancements in multi-turn user interfaces necessitate the need for controlling and updating these structured...","track":"Findings of EMNLP"},"forum":"findings.87","id":"findings.87","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.88.png","content":{"abstract":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the LiMiT dataset and share it publicly as a new resource for the research community.","authors":["Irene Manotas","Ngoc Phuoc An Vo","Vadim Sheinin"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.88","program":"findings","sessions":[],"similar_paper_uids":["findings.88"],"title":"LiMiT: The Literal Motion in Text Dataset","tldr":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) datase...","track":"Findings of EMNLP"},"forum":"findings.88","id":"findings.88","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.89.png","content":{"abstract":"Modeling the parser state is key to good performance in transition-based parsing. Recurrent Neural Networks considerably improved the performance of transition-based systems by modelling the global state, e.g. stack-LSTM parsers, or local state modeling of contextualized features, e.g. Bi-LSTM parsers. Given the success of Transformer architectures in recent parsing systems, this work explores modifications of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data.","authors":["Ram\u00f3n Fernandez Astudillo","Miguel Ballesteros","Tahira Naseem","Austin Blodgett","Radu Florian"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.89","program":"findings","sessions":[],"similar_paper_uids":["findings.89"],"title":"Transition-based Parsing with Stack-Transformers","tldr":"Modeling the parser state is key to good performance in transition-based parsing. Recurrent Neural Networks considerably improved the performance of transition-based systems by modelling the global state, e.g. stack-LSTM parsers, or local state model...","track":"Findings of EMNLP"},"forum":"findings.89","id":"findings.89","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.90.png","content":{"abstract":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG\u02c6C, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG\u02c6C consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG\u02c6C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.","authors":["Yiben Yang","Chaitanya Malaviya","Jared Fernandez","Swabha Swayamdipta","Ronan Le Bras","Ji-Ping Wang","Chandra Bhagavatula","Yejin Choi","Doug Downey"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.90","program":"findings","sessions":[],"similar_paper_uids":["findings.90"],"title":"Generative Data Augmentation for Commonsense Reasoning","tldr":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models c...","track":"Findings of EMNLP"},"forum":"findings.90","id":"findings.90","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.91.png","content":{"abstract":"Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20%, while the hybrid model can achieve an EM over 40%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model\u2019s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.","authors":["Wenhu Chen","Hanwen Zha","Zhiyu Chen","Wenhan Xiong","Hong Wang","William Yang Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.91","program":"findings","sessions":[],"similar_paper_uids":["findings.91"],"title":"HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data","tldr":"Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone migh...","track":"Findings of EMNLP"},"forum":"findings.91","id":"findings.91","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.92.png","content":{"abstract":"We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT","authors":["Dat Quoc Nguyen","Anh Tuan Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.92","program":"findings","sessions":[],"similar_paper_uids":["findings.92"],"title":"PhoBERT: Pre-trained language models for Vietnamese","tldr":"We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained mul...","track":"Findings of EMNLP"},"forum":"findings.92","id":"findings.92","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.93.png","content":{"abstract":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on domain-specific, labeled data. In this paper, we propose ESTeR , an unsupervised model for identifying emotions using a novel similarity function based on random walks on graphs. Our model combines large-scale word co-occurrence information with word-associations from lexicons avoiding not only the dependence on labeled datasets, but also an explicit mapping of words to latent spaces used in emotion-enriched word embeddings. Our similarity function can also be computed efficiently. We study a range of datasets including recent tweets related to COVID-19 to illustrate the superior performance of our model and report insights on public emotions during the on-going pandemic.","authors":["Sujatha Das Gollapalli","Polina Rozenshtein","See-Kiong Ng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.93","program":"findings","sessions":[],"similar_paper_uids":["findings.93"],"title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection","tldr":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using comp...","track":"Findings of EMNLP"},"forum":"findings.93","id":"findings.93","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.94.png","content":{"abstract":"Neural network (NN) based data2text models achieve state-of-the-art (SOTA) performance in most metrics, but they sometimes drop or modify the information in the input, and it is hard to control the generation contents. Moreover, it requires paired training data that are usually expensive to collect. Template-based methods have good fidelity and controllability but require heavy human involvement. We propose a novel template-based data2text system powered by a text stitch model. It ensures fidelity and controllability by using templates to produce the main contents. In addition, it reduces human involvement in template design by using a text stitch model to automatically stitch adjacent template units, which is a step that usually requires careful template design and limits template reusability. The text stitch model can be trained in self-supervised fashion, which only requires free texts. The experiments on a benchmark dataset show that our system outperforms SOTA NN-based systems in fidelity and surpasses template-based systems in diversity and human involvement.","authors":["Bingfeng Luo","Zuo Bai","Kunfeng Lai","Jianping Shen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.94","program":"findings","sessions":[],"similar_paper_uids":["findings.94"],"title":"Make Templates Smarter: A Template Based Data2Text System Powered by Text Stitch Model","tldr":"Neural network (NN) based data2text models achieve state-of-the-art (SOTA) performance in most metrics, but they sometimes drop or modify the information in the input, and it is hard to control the generation contents. Moreover, it requires paired tr...","track":"Findings of EMNLP"},"forum":"findings.94","id":"findings.94","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.95.png","content":{"abstract":"As an essential component of task-oriented dialogue systems, Dialogue State Tracking (DST) takes charge of estimating user intentions and requests in dialogue contexts and extracting substantial goals (states) from user utterances to help the downstream modules to determine the next actions of dialogue systems. For practical usages, a major challenge to constructing a robust DST model is to process a conversation with multi-domain states. However, most existing approaches trained DST on a single domain independently, ignoring the information across domains. To tackle the multi-domain DST task, we first construct a dialogue state graph to transfer structured features among related domain-slot pairs across domains. Then, we encode the graph information of dialogue states by graph convolutional networks and utilize a hard copy mechanism to directly copy historical states from the previous conversation. Experimental results show that our model improves the performances of the multi-domain DST baseline (TRADE) with the absolute joint accuracy of 2.0% and 1.0% on the MultiWOZ 2.0 and 2.1 dialogue datasets, respectively.","authors":["Peng Wu","Bowei Zou","Ridong Jiang","AiTi Aw"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.95","program":"findings","sessions":[],"similar_paper_uids":["findings.95"],"title":"GCDST: A Graph-based and Copy-augmented Multi-domain Dialogue State Tracking","tldr":"As an essential component of task-oriented dialogue systems, Dialogue State Tracking (DST) takes charge of estimating user intentions and requests in dialogue contexts and extracting substantial goals (states) from user utterances to help the downstr...","track":"Findings of EMNLP"},"forum":"findings.95","id":"findings.95","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.96.png","content":{"abstract":"While recent advances in language modeling has resulted in powerful generation models, their generation style remains implicitly dependent on the training data and can not emulate a specific target style. Leveraging the generative capabilities of a transformer-based language models, we present an approach to induce certain target-author attributes by incorporating continuous multi-dimensional lexical preferences of an author into generative language models. We introduce rewarding strategies in a reinforcement learning framework that encourages the use of words across multiple categorical dimensions, to varying extents. Our experiments demonstrate that the proposed approach can generate text that distinctively aligns with a given target author\u2019s lexical style. We conduct quantitative and qualitative comparisons with competitive and relevant baselines to illustrate the benefits of the proposed approach.","authors":["Hrituraj Singh","Gaurav Verma","Balaji Vasan Srinivasan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.96","program":"findings","sessions":[],"similar_paper_uids":["findings.96"],"title":"Incorporating Stylistic Lexical Preferences in Generative Language Models","tldr":"While recent advances in language modeling has resulted in powerful generation models, their generation style remains implicitly dependent on the training data and can not emulate a specific target style. Leveraging the generative capabilities of a t...","track":"Findings of EMNLP"},"forum":"findings.96","id":"findings.96","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.97.png","content":{"abstract":"Evaluating the trustworthiness of a model\u2019s prediction is essential for differentiating between \u2018right for the right reasons\u2019 and \u2018right for the wrong reasons\u2019. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training\u2013framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart\u2019s performance in two cases. We further exploit the transparent decision\u2013making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale\u2013level.","authors":["Max Glockner","Ivan Habernal","Iryna Gurevych"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.97","program":"findings","sessions":[],"similar_paper_uids":["findings.97"],"title":"Why do you think that? Exploring Faithful Sentence-Level Rationales Without Supervision","tldr":"Evaluating the trustworthiness of a model\u2019s prediction is essential for differentiating between \u2018right for the right reasons\u2019 and \u2018right for the wrong reasons\u2019. Identifying textual spans that determine the target label, known as faithful rationales, ...","track":"Findings of EMNLP"},"forum":"findings.97","id":"findings.97","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.98.png","content":{"abstract":"Deep neural networks have made great success on video captioning in supervised learning setting. However, annotating videos with descriptions is very expensive and time-consuming. If the video captioning algorithm can benefit from a large number of unlabeled videos, the cost of annotation can be reduced. In the proposed study, we make the first attempt to train the video captioning model on labeled data and unlabeled data jointly, in a semi-supervised learning manner. For labeled data, we train them with the traditional cross-entropy loss. For unlabeled data, we leverage a self-critical policy gradient method with the difference between the scores obtained by Monte-Carlo sampling and greedy decoding as the reward function, while the scores are the negative K-L divergence between output distributions of original video data and augmented video data. The final loss is the weighted sum of losses obtained by labeled data and unlabeled data. Experiments conducted on VATEX, MSR-VTT and MSVD dataset demonstrate that the introduction of unlabeled data can improve the performance of the video captioning model. The proposed semi-supervised learning algorithm also outperforms several state-of-the-art semi-supervised learning approaches.","authors":["Ke Lin","Zhuoxin Gan","Liwei Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.98","program":"findings","sessions":[],"similar_paper_uids":["findings.98"],"title":"Semi-Supervised Learning for Video Captioning","tldr":"Deep neural networks have made great success on video captioning in supervised learning setting. However, annotating videos with descriptions is very expensive and time-consuming. If the video captioning algorithm can benefit from a large number of u...","track":"Findings of EMNLP"},"forum":"findings.98","id":"findings.98","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.99.png","content":{"abstract":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.","authors":["Youngbin Ro","Yukyung Lee","Pilsung Kang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.99","program":"findings","sessions":[],"similar_paper_uids":["findings.99"],"title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT","tldr":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query...","track":"Findings of EMNLP"},"forum":"findings.99","id":"findings.99","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.100.png","content":{"abstract":"Logic grid puzzle (LGP) is a type of word problem where the task is to solve a problem in logic. Constraints for the problem are given in the form of textual clues. Once these clues are transformed into formal logic, a deductive reasoning process provides the solution. Solving logic grid puzzles in a fully automatic manner has been a challenge since a precise understanding of clues is necessary to develop the corresponding formal logic representation. To meet this challenge, we propose a solution that uses a DistilBERT-based classifier to classify a clue into one of the predefined predicate types for logic grid puzzles. Another novelty of the proposed solution is the recognition of comparison structures in clues. By collecting comparative adjectives from existing dictionaries and utilizing a semantic framework to catch comparative quantifiers, the semantics of clues concerning comparison structures are better understood, ensuring conversion to correct logic representation. Our approach solves logic grid puzzles in a fully automated manner with 100% accuracy on the given puzzle datasets and outperforms state-of-the-art solutions by a large margin.","authors":["Elgun Jabrayilzade","Selma Tekir"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.100","program":"findings","sessions":[],"similar_paper_uids":["findings.100"],"title":"LGPSolver - Solving Logic Grid Puzzles Automatically","tldr":"Logic grid puzzle (LGP) is a type of word problem where the task is to solve a problem in logic. Constraints for the problem are given in the form of textual clues. Once these clues are transformed into formal logic, a deductive reasoning process pro...","track":"Findings of EMNLP"},"forum":"findings.100","id":"findings.100","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.101.png","content":{"abstract":"This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the new task. A key innovation of this proposed model is a novel parameter-gate (p-gate) mechanism that regulates the flow or transfer of the previously learned knowledge to the new task. Specifically, it can selectively use the network parameters (which represent the retained knowledge gained from the previous tasks) to assist the learning of the new task t. Knowledge distillation is also employed in the process to preserve the past knowledge by approximating the network output at the state when task t-1 was learned. Experimental results show that L2PG outperforms strong baselines, including even multiple task learning.","authors":["Qi Qin","Wenpeng Hu","Bing Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.101","program":"findings","sessions":[],"similar_paper_uids":["findings.101"],"title":"Using the Past Knowledge to Improve Sentiment Classification","tldr":"This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the ...","track":"Findings of EMNLP"},"forum":"findings.101","id":"findings.101","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.102.png","content":{"abstract":"Semantic role labeling is primarily used to identify predicates, arguments, and their semantic relationships. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the correlations between arguments at most, while the correlations between predicates have been neglected for a long time. High-order features and structure learning were very common in modeling such correlations before the neural network era. In this paper, we introduce a high-order graph structure for the neural semantic role labeling model, which enables the model to explicitly consider not only the isolated predicate-argument pairs but also the interaction between the predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009 benchmark show that the high-order structural learning techniques are beneficial to the strong performing SRL models and further boost our baseline to achieve new state-of-the-art results.","authors":["Zuchao Li","Hai Zhao","Rui Wang","Kevin Parnow"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.102","program":"findings","sessions":[],"similar_paper_uids":["findings.102"],"title":"High-order Semantic Role Labeling","tldr":"Semantic role labeling is primarily used to identify predicates, arguments, and their semantic relationships. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships b...","track":"Findings of EMNLP"},"forum":"findings.102","id":"findings.102","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.103.png","content":{"abstract":"Current reading comprehension methods generalise well to in-distribution test sets, yet perform poorly on adversarially selected data. Prior work on adversarial inputs typically studies model oversensitivity: semantically invariant text perturbations that cause a model\u2019s prediction to change. Here we focus on the complementary problem: excessive prediction undersensitivity, where input text is meaningfully changed but the model\u2019s prediction does not, even though it should. We formulate an adversarial attack which searches among semantic variations of the question for which a model erroneously predicts the same answer, and with even higher probability. We demonstrate that models trained on both SQuAD2.0 and NewsQA are vulnerable to this attack, and then investigate data augmentation and adversarial training as defences. Both substantially decrease adversarial vulnerability, which generalises to held-out data and held-out attack spaces. Addressing undersensitivity furthermore improves model robustness on the previously introduced ADDSENT and ADDONESENT datasets, and models generalise better when facing train / evaluation distribution mismatch: they are less prone to overly rely on shallow predictive cues present only in the training set, and outperform a conventional model by as much as 10.9% F1.","authors":["Johannes Welbl","Pasquale Minervini","Max Bartolo","Pontus Stenetorp","Sebastian Riedel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.103","program":"findings","sessions":[],"similar_paper_uids":["findings.103"],"title":"Undersensitivity in Neural Reading Comprehension","tldr":"Current reading comprehension methods generalise well to in-distribution test sets, yet perform poorly on adversarially selected data. Prior work on adversarial inputs typically studies model oversensitivity: semantically invariant text perturbations...","track":"Findings of EMNLP"},"forum":"findings.103","id":"findings.103","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.104.png","content":{"abstract":"Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchies precisely with limited representation capacity. Considering that hyperbolic space is naturally suitable for modelling tree-like hierarchical data, we propose a new model named HyperText for efficient text classification by endowing FastText with hyperbolic geometry. Empirically, we show that HyperText outperforms FastText on a range of text classification tasks with much reduced parameters.","authors":["Yudong Zhu","Di Zhou","Jinghui Xiao","Xin Jiang","Xiao Chen","Qun Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.104","program":"findings","sessions":[],"similar_paper_uids":["findings.104"],"title":"HyperText: Endowing FastText with Hyperbolic Geometry","tldr":"Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchie...","track":"Findings of EMNLP"},"forum":"findings.104","id":"findings.104","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.105.png","content":{"abstract":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.","authors":["Guanglin Niu","Bo Li","Yongfei Zhang","Shiliang Pu","Jingyang Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.105","program":"findings","sessions":[],"similar_paper_uids":["findings.105"],"title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding","tldr":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however ...","track":"Findings of EMNLP"},"forum":"findings.105","id":"findings.105","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.106.png","content":{"abstract":"Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages.","authors":["Kazuya Kawakami","Luyu Wang","Chris Dyer","Phil Blunsom","Aaron van den Oord"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.106","program":"findings","sessions":[],"similar_paper_uids":["findings.106"],"title":"Learning Robust and Multilingual Speech Representations","tldr":"Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating t...","track":"Findings of EMNLP"},"forum":"findings.106","id":"findings.106","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.107.png","content":{"abstract":"Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In an effort to track the progress of French Question Answering models we propose a leaderboard and we have made the 1.0 version of our dataset freely available at https://illuin-tech.github.io/FQuAD-explorer/.","authors":["Martin d\u2019Hoffschmidt","Wacim Belblidia","Quentin Heinrich","Tom Brendl\u00e9","Maxime Vidal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.107","program":"findings","sessions":[],"similar_paper_uids":["findings.107"],"title":"FQuAD: French Question Answering Dataset","tldr":"Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are rep...","track":"Findings of EMNLP"},"forum":"findings.107","id":"findings.107","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.108.png","content":{"abstract":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available.","authors":["Hoang Nguyen","Chenwei Zhang","Congying Xia","Philip Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.108","program":"findings","sessions":[],"similar_paper_uids":["findings.108"],"title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection","tldr":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel ...","track":"Findings of EMNLP"},"forum":"findings.108","id":"findings.108","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.109.png","content":{"abstract":"Pretrained language models achieve state-of-the-art results on many NLP tasks, but there are still many open questions about how and why they work so well. We investigate the contextualization of words in BERT. We quantify the amount of contextualization, i.e., how well words are interpreted in context, by studying the extent to which semantic classes of a word can be inferred from its contextualized embedding. Quantifying contextualization helps in understanding and utilizing pretrained language models. We show that the top layer representations support highly accurate inference of semantic classes; that the strongest contextualization effects occur in the lower layers; that local context is mostly sufficient for contextualizing words; and that top layer representations are more task-specific after finetuning while lower layer representations are more transferable. Finetuning uncovers task-related features, but pretrained knowledge about contextualization is still well preserved.","authors":["Mengjie Zhao","Philipp Dufter","Yadollah Yaghoobzadeh","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.109","program":"findings","sessions":[],"similar_paper_uids":["findings.109"],"title":"Quantifying the Contextualization of Word Representations with Semantic Class Probing","tldr":"Pretrained language models achieve state-of-the-art results on many NLP tasks, but there are still many open questions about how and why they work so well. We investigate the contextualization of words in BERT. We quantify the amount of contextualiza...","track":"Findings of EMNLP"},"forum":"findings.109","id":"findings.109","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.110.png","content":{"abstract":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinically incorrect radiology reports. In this work, we develop a radiology report generation model utilizing the transformer architecture that produces superior reports as measured by both standard language generation and clinical coherence metrics compared to competitive baselines. We then develop a method to differentiably extract clinical information from generated reports and utilize this differentiability to fine-tune our model to produce more clinically coherent reports.","authors":["Justin Lovelace","Bobak Mortazavi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.110","program":"findings","sessions":[],"similar_paper_uids":["findings.110"],"title":"Learning to Generate Clinically Coherent Chest X-Ray Reports","tldr":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinica...","track":"Findings of EMNLP"},"forum":"findings.110","id":"findings.110","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.111.png","content":{"abstract":"We present FELIX \u2013 a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining. In contrast to conventional sequenceto-sequence (seq2seq) models, FELIX is efficient in low-resource settings and fast at inference time, while being capable of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input. The tagging model employs a novel Pointer mechanism, while the insertion model is based on a Masked Language Model (MLM). Both of these models are chosen to be non-autoregressive to guarantee faster inference. FELIX performs favourably when compared to recent text-editing methods and strong seq2seq baselines when evaluated on four NLG tasks: Sentence Fusion, Machine Translation Automatic Post-Editing, Summarization, and Text Simplification","authors":["Jonathan Mallinson","Aliaksei Severyn","Eric Malmi","Guillermo Garrido"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.111","program":"findings","sessions":[],"similar_paper_uids":["findings.111"],"title":"FELIX: Flexible Text Editing Through Tagging and Insertion","tldr":"We present FELIX \u2013 a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining. In contrast to conventional sequenceto-sequence (seq2seq) ...","track":"Findings of EMNLP"},"forum":"findings.111","id":"findings.111","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.112.png","content":{"abstract":"Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.","authors":["Anna Rogers","Isabelle Augenstein"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.112","program":"findings","sessions":[],"similar_paper_uids":["findings.112"],"title":"What Can We Do to Improve Peer Review in NLP?","tldr":"Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges c...","track":"Findings of EMNLP"},"forum":"findings.112","id":"findings.112","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.113.png","content":{"abstract":"We show that state-of-the-art self-supervised language models can be readily used to extract relations from a corpus without the need to train a fine-tuned extractive head. We introduce RE-Flex, a simple framework that performs constrained cloze completion over pretrained language models to perform unsupervised relation extraction. RE-Flex uses contextual matching to ensure that language model predictions matches supporting evidence from the input corpus that is relevant to a target relation. We perform an extensive experimental study over multiple relation extraction benchmarks and demonstrate that RE-Flex outperforms competing unsupervised relation extraction methods based on pretrained language models by up to 27.8 F1 points compared to the next-best method. Our results show that constrained inference queries against a language model can enable accurate unsupervised relation extraction.","authors":["Ankur Goswami","Akshata Bhat","Hadar Ohana","Theodoros Rekatsinas"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.113","program":"findings","sessions":[],"similar_paper_uids":["findings.113"],"title":"Unsupervised Relation Extraction from Language Models using Constrained Cloze Completion","tldr":"We show that state-of-the-art self-supervised language models can be readily used to extract relations from a corpus without the need to train a fine-tuned extractive head. We introduce RE-Flex, a simple framework that performs constrained cloze comp...","track":"Findings of EMNLP"},"forum":"findings.113","id":"findings.113","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.114.png","content":{"abstract":"Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to incorporate domain knowledge from Unified Medical Language System (UMLS) to a pre-trained language model via Graph Edge-conditioned Attention Networks (GEANet) and hierarchical graph representation. To better recognize the trigger words, each sentence is first grounded to a sentence graph based on a jointly modeled hierarchical knowledge graph from UMLS. The grounded graphs are then propagated by GEANet, a novel graph neural networks for enhanced capabilities in inferring complex events. On BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41% F1 and 3.19% F1 improvements on all events and complex events, respectively. Ablation studies confirm the importance of GEANet and hierarchical KG.","authors":["Kung-Hsiang Huang","Mu Yang","Nanyun Peng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.114","program":"findings","sessions":[],"similar_paper_uids":["findings.114"],"title":"Biomedical Event Extraction with Hierarchical Knowledge Graphs","tldr":"Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to...","track":"Findings of EMNLP"},"forum":"findings.114","id":"findings.114","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.115.png","content":{"abstract":"Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMC, an efficient method to generate high likelihood sentences with respect to a pre-trained language model while satisfying the constraints. Our approach is highly flexible, requires no task-specific train- ing, and leverages efficient constraint satisfaction solving techniques. To better handle the combinatorial constraints, a tree search algorithm is embedded into the proposal process of the Markov Chain Monte Carlo (MCMC) to explore candidates that satisfy more constraints. Compared to existing MCMC approaches, our sampling approach has a better mixing performance. Experiments show that TSMC achieves consistent and significant improvement on multiple language generation tasks.","authors":["Maosen Zhang","Nan Jiang","Lei Li","Yexiang Xue"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.115","program":"findings","sessions":[],"similar_paper_uids":["findings.115"],"title":"Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced Monte-Carlo Approach","tldr":"Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMC, an efficient ...","track":"Findings of EMNLP"},"forum":"findings.115","id":"findings.115","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.116.png","content":{"abstract":"Interpreting how persuasive language influences audiences has implications across many domains like advertising, argumentation, and propaganda. Persuasion relies on more than a message\u2019s content. Arranging the order of the message itself (i.e., ordering specific rhetorical strategies) also plays an important role. To examine how strategy orderings contribute to persuasiveness, we first utilize a Variational Autoencoder model to disentangle content and rhetorical strategies in textual requests from a large-scale loan request corpus. We then visualize interplay between content and strategy through an attentional LSTM that predicts the success of textual requests. We find that specific (orderings of) strategies interact uniquely with a request\u2019s content to impact success rate, and thus the persuasiveness of a request.","authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Polo Chau","Diyi Yang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.116","program":"findings","sessions":[],"similar_paper_uids":["findings.116"],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests","tldr":"Interpreting how persuasive language influences audiences has implications across many domains like advertising, argumentation, and propaganda. Persuasion relies on more than a message\u2019s content. Arranging the order of the message itself (i.e., order...","track":"Findings of EMNLP"},"forum":"findings.116","id":"findings.116","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.117.png","content":{"abstract":"Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model\u2019s decision boundary, which can be used to more accurately evaluate a model\u2019s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets\u2014up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.","authors":["Matt Gardner","Yoav Artzi","Victoria Basmov","Jonathan Berant","Ben Bogin","Sihao Chen","Pradeep Dasigi","Dheeru Dua","Yanai Elazar","Ananth Gottumukkala","Nitish Gupta","Hannaneh Hajishirzi","Gabriel Ilharco","Daniel Khashabi","Kevin Lin","Jiangming Liu","Nelson F. Liu","Phoebe Mulcaire","Qiang Ning","Sameer Singh","Noah A. Smith","Sanjay Subramanian","Reut Tsarfaty","Eric Wallace","Ally Zhang","Ben Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.117","program":"findings","sessions":[],"similar_paper_uids":["findings.117"],"title":"Evaluating Models\u2019 Local Decision Boundaries via Contrast Sets","tldr":"Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform...","track":"Findings of EMNLP"},"forum":"findings.117","id":"findings.117","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.118.png","content":{"abstract":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models\u2019 pretraining data and target language varieties.","authors":["Ethan C. Chau","Lucy H. Lin","Noah A. Smith"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.118","program":"findings","sessions":[],"similar_paper_uids":["findings.118"],"title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank","tldr":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar t...","track":"Findings of EMNLP"},"forum":"findings.118","id":"findings.118","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.119.png","content":{"abstract":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but they are seldom available and expensive to hire. This has lead to the thriving of crowdsourcing platforms such as Amazon Mechanical Turk (AMT). However, the annotations provided by one worker cannot be used directly to train the model due to the lack of expertise. Existing literature in annotation aggregation focuses on binary and multi-choice problems. In contrast, little work has been done on complex tasks such as sequence labeling with imbalanced classes, a ubiquitous task in Natural Language Processing (NLP), and Bio-Informatics. We propose OptSLA, an Optimization-based Sequential Label Aggregation method, that jointly considers the characteristics of sequential labeling tasks, workers reliabilities, and advanced deep learning techniques to conquer the challenge. We evaluate our model on crowdsourced data for named entity recognition task. Our results show that the proposed OptSLA outperforms the state-of-the-art aggregation methods, and the results are easier to interpret.","authors":["Nasim Sabetpour","Adithya Kulkarni","Qi Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.119","program":"findings","sessions":[],"similar_paper_uids":["findings.119"],"title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation","tldr":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but th...","track":"Findings of EMNLP"},"forum":"findings.119","id":"findings.119","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.120.png","content":{"abstract":"In traditional NLP, we tokenize a given sentence as a preprocessing, and thus the tokenization is unrelated to a target downstream task. To address this issue, we propose a novel method to explore a tokenization which is appropriate for the downstream task. Our proposed method, optimizing tokenization (OpTok), is trained to assign a high probability to such appropriate tokenization based on the downstream task loss. OpTok can be used for any downstream task which uses a vector representation of a sentence such as text classification. Experimental results demonstrate that OpTok improves the performance of sentiment analysis and textual entailment. In addition, we introduce OpTok into BERT, the state-of-the-art contextualized embeddings and report a positive effect.","authors":["Tatsuya Hiraoka","Sho Takase","Kei Uchiumi","Atsushi Keyaki","Naoaki Okazaki"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.120","program":"findings","sessions":[],"similar_paper_uids":["findings.120"],"title":"Optimizing Word Segmentation for Downstream Task","tldr":"In traditional NLP, we tokenize a given sentence as a preprocessing, and thus the tokenization is unrelated to a target downstream task. To address this issue, we propose a novel method to explore a tokenization which is appropriate for the downstrea...","track":"Findings of EMNLP"},"forum":"findings.120","id":"findings.120","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.121.png","content":{"abstract":"Temporal relation classification is the pair-wise task for identifying the relation of a temporal link (TLINKs) between two mentions, i.e. event, time and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two strong transfer learning baselines on both the English and Japanese data.","authors":["Fei Cheng","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.121","program":"findings","sessions":[],"similar_paper_uids":["findings.121"],"title":"Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning","tldr":"Temporal relation classification is the pair-wise task for identifying the relation of a temporal link (TLINKs) between two mentions, i.e. event, time and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common ...","track":"Findings of EMNLP"},"forum":"findings.121","id":"findings.121","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.122.png","content":{"abstract":"Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce noise in the KS processing. In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce noise (before and during decoding). In addition, we propose two metrics for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.","authors":["Longxuan Ma","Wei-Nan Zhang","Runxin Sun","Ting Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.122","program":"findings","sessions":[],"similar_paper_uids":["findings.122"],"title":"A Compare Aggregate Transformer for Understanding Document-grounded Dialogue","tldr":"Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to t...","track":"Findings of EMNLP"},"forum":"findings.122","id":"findings.122","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.123.png","content":{"abstract":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.","authors":["Yangsibo Huang","Zhao Song","Danqi Chen","Kai Li","Sanjeev Arora"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.123","program":"findings","sessions":[],"similar_paper_uids":["findings.123"],"title":"TextHide: Tackling Data Privacy in Language Understanding Tasks","tldr":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language unders...","track":"Findings of EMNLP"},"forum":"findings.123","id":"findings.123","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.124.png","content":{"abstract":"Sarcasm is a pervasive phenomenon in today\u2019s social media platforms such as Twitter and Reddit. These platforms allow users to create multi-modal messages, including texts, images, and videos. Existing multi-modal sarcasm detection methods either simply concatenate the features from multi modalities or fuse the multi modalities information in a designed manner. However, they ignore the incongruity character in sarcastic utterance, which is often manifested between modalities or within modalities. Inspired by this, we propose a BERT architecture-based model, which concentrates on both intra and inter-modality incongruity for multi-modal sarcasm detection. To be specific, we are inspired by the idea of self-attention mechanism and design inter-modality attention to capturing inter-modality incongruity. In addition, the co-attention mechanism is applied to model the contradiction within the text. The incongruity information is then used for prediction. The experimental results demonstrate that our model achieves state-of-the-art performance on a public multi-modal sarcasm detection dataset.","authors":["Hongliang Pan","Zheng Lin","Peng Fu","Yatao Qi","Weiping Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.124","program":"findings","sessions":[],"similar_paper_uids":["findings.124"],"title":"Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection","tldr":"Sarcasm is a pervasive phenomenon in today\u2019s social media platforms such as Twitter and Reddit. These platforms allow users to create multi-modal messages, including texts, images, and videos. Existing multi-modal sarcasm detection methods either sim...","track":"Findings of EMNLP"},"forum":"findings.124","id":"findings.124","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.125.png","content":{"abstract":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.","authors":["Alex Tamkin","Trisha Singh","Davide Giovanardi","Noah Goodman"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.125","program":"findings","sessions":[],"similar_paper_uids":["findings.125"],"title":"Investigating Transferability in Pretrained Language Models","tldr":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different ...","track":"Findings of EMNLP"},"forum":"findings.125","id":"findings.125","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.126.png","content":{"abstract":"Incorporating commonsense knowledge can alleviate the issue of generating generic responses in open-domain generative dialogue systems. However, selecting knowledge facts for the dialogue context is still a challenge. The widely used approach Entity Name Matching always retrieves irrelevant facts from the view of local entity words. This paper proposes a novel knowledge selection approach, Prototype-KR, and a knowledge-aware generative model, Prototype-KRG. Given a query, our approach first retrieves a set of prototype dialogues that are relevant to the query. We find knowledge facts used in prototype dialogues usually are highly relevant to the current query; thus, Prototype-KR ranks such knowledge facts based on the semantic similarity and then selects the most appropriate facts. Subsequently, Prototype-KRG can generate an informative response using the selected knowledge facts. Experiments demonstrate that our approach has achieved notable improvements on the most metrics, compared to generative baselines. Meanwhile, compared to IR(Retrieval)-based baselines, responses generated by our approach are more relevant to the context and have comparable informativeness.","authors":["Sixing Wu","Ying Li","Dawei Zhang","Zhonghai Wu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.126","program":"findings","sessions":[],"similar_paper_uids":["findings.126"],"title":"Improving Knowledge-Aware Dialogue Response Generation by Using Human-Written Prototype Dialogues","tldr":"Incorporating commonsense knowledge can alleviate the issue of generating generic responses in open-domain generative dialogue systems. However, selecting knowledge facts for the dialogue context is still a challenge. The widely used approach Entity ...","track":"Findings of EMNLP"},"forum":"findings.126","id":"findings.126","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.127.png","content":{"abstract":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring (FIRE) for this task. In this method, a context filter and a knowledge filter are first built, which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. Besides, the entries irrelevant to the conversation are discarded by the knowledge filter. After that, iteratively referring is performed between context and response representations as well as between knowledge and response representations, in order to collect deep matching features for scoring response candidates. Experimental results show that FIRE outperforms previous methods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with original and revised personas respectively, and margins larger than 3.1% on the CMU_DoG dataset in terms of top-1 accuracy. We also show that FIRE is more interpretable by visualizing the knowledge grounding process.","authors":["Jia-Chen Gu","Zhenhua Ling","Quan Liu","Zhigang Chen","Xiaodan Zhu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.127","program":"findings","sessions":[],"similar_paper_uids":["findings.127"],"title":"Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots","tldr":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method n...","track":"Findings of EMNLP"},"forum":"findings.127","id":"findings.127","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.128.png","content":{"abstract":"News recommendation aims to display news articles to users based on their personal interest. Existing news recommendation methods rely on centralized storage of user behavior data for model training, which may lead to privacy concerns and risks due to the privacy-sensitive nature of user behaviors. In this paper, we propose a privacy-preserving method for news recommendation model training based on federated learning, where the user behavior data is locally stored on user devices. Our method can leverage the useful information in the behaviors of massive number users to train accurate news recommendation models and meanwhile remove the need of centralized storage of them. More specifically, on each user device we keep a local copy of the news recommendation model, and compute gradients of the local model based on the user behaviors in this device. The local gradients from a group of randomly selected users are uploaded to server, which are further aggregated to update the global model in the server. Since the model gradients may contain some implicit private information, we apply local differential privacy (LDP) to them before uploading for better privacy protection. The updated global model is then distributed to each user device for local model update. We repeat this process for multiple rounds. Extensive experiments on a real-world dataset show the effectiveness of our method in news recommendation model training with privacy protection.","authors":["Tao Qi","Fangzhao Wu","Chuhan Wu","Yongfeng Huang","Xing Xie"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.128","program":"findings","sessions":[],"similar_paper_uids":["findings.128"],"title":"Privacy-Preserving News Recommendation Model Learning","tldr":"News recommendation aims to display news articles to users based on their personal interest. Existing news recommendation methods rely on centralized storage of user behavior data for model training, which may lead to privacy concerns and risks due t...","track":"Findings of EMNLP"},"forum":"findings.128","id":"findings.128","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.129.png","content":{"abstract":"We introduce exBERT, a training method to extend BERT pre-trained models from a general domain to a new pre-trained model for a specific domain with a new additive vocabulary under constrained training resources (i.e., constrained computation and data). exBERT uses a small extension module to learn to adapt an augmenting embedding for the new domain in the context of the original BERT\u2019s embedding of a general vocabulary. The exBERT training method is novel in learning the new vocabulary and the extension module while keeping the weights of the original BERT model fixed, resulting in a substantial reduction in required training resources. We pre-train exBERT with biomedical articles from ClinicalKey and PubMed Central, and study its performance on biomedical downstream benchmark tasks using the MTL-Bioinformatics-2016 datasets. We demonstrate that exBERT consistently outperforms prior approaches when using limited corpus and pre-training computation resources.","authors":["Wen Tai","H. T. Kung","Xin Dong","Marcus Comiter","Chang-Fu Kuo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.129","program":"findings","sessions":[],"similar_paper_uids":["findings.129"],"title":"exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources","tldr":"We introduce exBERT, a training method to extend BERT pre-trained models from a general domain to a new pre-trained model for a specific domain with a new additive vocabulary under constrained training resources (i.e., constrained computation and dat...","track":"Findings of EMNLP"},"forum":"findings.129","id":"findings.129","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.130.png","content":{"abstract":"Data balancing is a known technique for improving the performance of classification tasks. In this work we define a novel balancing-viageneration framework termed BalaGen. BalaGen consists of a flexible balancing policy coupled with a text generation mechanism. Combined, these two techniques can be used to augment a dataset for more balanced distribution. We evaluate BalaGen on three publicly available semantic utterance classification (SUC) datasets. One of these is a new COVID-19 Q&A dataset published here for the first time. Our work demonstrates that optimal balancing policies can significantly improve classifier performance, while augmenting just part of the classes and under-sampling others. Furthermore, capitalizing on the advantages of balancing, we show its usefulness in all relevant BalaGen framework components. We validate the superiority of BalaGen on ten semantic utterance datasets taken from real-life goaloriented dialogue systems. Based on our results we encourage using data balancing prior to training for text classification tasks.","authors":["Naama Tepper","Esther Goldbraich","Naama Zwerdling","George Kour","Ateret Anaby Tavor","Boaz Carmeli"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.130","program":"findings","sessions":[],"similar_paper_uids":["findings.130"],"title":"Balancing via Generation for Multi-Class Text Classification Improvement","tldr":"Data balancing is a known technique for improving the performance of classification tasks. In this work we define a novel balancing-viageneration framework termed BalaGen. BalaGen consists of a flexible balancing policy coupled with a text generation...","track":"Findings of EMNLP"},"forum":"findings.130","id":"findings.130","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.131.png","content":{"abstract":"Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neural framework that can flexibly control summary generation by introducing a set of sub-aspect functions (i.e. importance, diversity, position). These sub-aspect functions are regulated by a set of control codes to decide which sub-aspect to focus on during summary generation. We demonstrate that extracted summaries with minimal position bias is comparable with those generated by standard models that take advantage of position preference. We also show that news summaries generated with a focus on diversity can be more preferred by human raters. These results suggest that a more flexible neural summarization framework providing more control options could be desirable in tailoring to different user preferences, which is useful since it is often impractical to articulate such preferences for different applications a priori.","authors":["Zhengyuan Liu","Ke Shi","Nancy Chen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.131","program":"findings","sessions":[],"similar_paper_uids":["findings.131"],"title":"Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization","tldr":"Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inver...","track":"Findings of EMNLP"},"forum":"findings.131","id":"findings.131","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.132.png","content":{"abstract":"Knowing whether a published research result can be replicated is important. Carrying out direct replication of published research incurs a high cost. There are efforts tried to use machine learning aided methods to predict scientific claims\u2019 replicability. However, existing machine learning aided approaches use only hand-extracted statistics features such as p-value, sample size, etc. without utilizing research papers\u2019 text information and train only on a very small size of annotated data without making the most use of a large number of unlabeled articles. Therefore, it is desirable to develop effective machine learning aided automatic methods which can automatically extract text information as features so that we can benefit from Natural Language Processing techniques. Besides, we aim for an approach that benefits from both labeled and the large number of unlabeled data. In this paper, we propose two weakly supervised learning approaches that use automatically extracted text information of research papers to improve the prediction accuracy of research replication using both labeled and unlabeled datasets. Our experiments over real-world datasets show that our approaches obtain much better prediction performance compared to the supervised models utilizing only statistic features and a small size of labeled dataset. Further, we are able to achieve an accuracy of 75.76% for predicting the replicability of research.","authors":["Tianyi Luo","Xingyu Li","Hainan Wang","Yang Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.132","program":"findings","sessions":[],"similar_paper_uids":["findings.132"],"title":"Research Replication Prediction Using Weakly Supervised Learning","tldr":"Knowing whether a published research result can be replicated is important. Carrying out direct replication of published research incurs a high cost. There are efforts tried to use machine learning aided methods to predict scientific claims\u2019 replicab...","track":"Findings of EMNLP"},"forum":"findings.132","id":"findings.132","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.133.png","content":{"abstract":"The incompleteness of knowledge base (KB) is a vital factor limiting the performance of question answering (QA). This paper proposes a novel QA method by leveraging text information to enhance the incomplete KB. The model enriches the entity representation through semantic information contained in the text, and employs graph convolutional networks to update the entity status. Furthermore, to exploit the latent structural information of text, we treat the text as hyperedges connecting entities among it to complement the deficient relations in KB, and hypergraph convolutional networks are further applied to reason on the hypergraph-formed text. Extensive experiments on the WebQuestionsSP benchmark with different KB settings prove the effectiveness of our model.","authors":["Jiale Han","Bo Cheng","Xu Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.133","program":"findings","sessions":[],"similar_paper_uids":["findings.133"],"title":"Open Domain Question Answering based on Text Enhanced Knowledge Graph with Hyperedge Infusion","tldr":"The incompleteness of knowledge base (KB) is a vital factor limiting the performance of question answering (QA). This paper proposes a novel QA method by leveraging text information to enhance the incomplete KB. The model enriches the entity represen...","track":"Findings of EMNLP"},"forum":"findings.133","id":"findings.133","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.134.png","content":{"abstract":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT - BERT F1 delta, at 5% of BioBERT\u2019s CO 2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.134","program":"findings","sessions":[],"similar_paper_uids":["findings.134"],"title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","tldr":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper...","track":"Findings of EMNLP"},"forum":"findings.134","id":"findings.134","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.135.png","content":{"abstract":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders models from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the model with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over state-of-the-art models.","authors":["Eyal Ben-David","Orgad Keller","Eric Malmi","Idan Szpektor","Roi Reichart"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.135","program":"findings","sessions":[],"similar_paper_uids":["findings.135"],"title":"Semantically Driven Sentence Fusion: Modeling and Evaluation","tldr":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders mod...","track":"Findings of EMNLP"},"forum":"findings.135","id":"findings.135","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.136.png","content":{"abstract":"Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as grammatical error correction and spell or OCR correction. Motivated by this characteristic of LST tasks, we propose Pseudo-Bidirectional Decoding (PBD), a simple but versatile approach for LST tasks. PBD copies the representation of source tokens to the decoder as pseudo future context that enables the decoder self-attention to attends to its bi-directional context. In addition, the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of LST models. Our approach provides right-side context information for the decoder, reduces the number of parameters by half, and provides good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks.","authors":["Wangchunshu Zhou","Tao Ge","Ke Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.136","program":"findings","sessions":[],"similar_paper_uids":["findings.136"],"title":"Pseudo-Bidirectional Decoding for Local Sequence Transduction","tldr":"Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as grammatical error correction and spell or OCR correction. Motivated by this characteristic...","track":"Findings of EMNLP"},"forum":"findings.136","id":"findings.136","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.137.png","content":{"abstract":"Psychologists routinely assess people\u2019s emotions and traits, such as their personality, by collecting their responses to survey questionnaires. Such assessments can be costly in terms of both time and money, and often lack generalizability, as existing data cannot be used to predict responses for new survey questions or participants. In this study, we propose a method for predicting a participant\u2019s questionnaire response using their social media texts and the text of the survey question they are asked. Specifically, we use Natural Language Processing (NLP) tools such as BERT embeddings to represent both participants (via the text they write) and survey questions as embeddings vectors, allowing us to predict responses for out-of-sample participants and questions. Our novel approach can be used by researchers to integrate new participants or new questions into psychological studies without the constraint of costly data collection, facilitating novel practical applications and furthering the development of psychological theory. Finally, as a side contribution, the success of our model also suggests a new approach to study survey questions using NLP tools such as text embeddings rather than response data used in traditional methods.","authors":["Huy Vu","Suhaib Abdurahman","Sudeep Bhatia","Lyle Ungar"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.137","program":"findings","sessions":[],"similar_paper_uids":["findings.137"],"title":"Predicting Responses to Psychological Questionnaires from Participants\u2019 Social Media Posts and Question Text Embeddings","tldr":"Psychologists routinely assess people\u2019s emotions and traits, such as their personality, by collecting their responses to survey questionnaires. Such assessments can be costly in terms of both time and money, and often lack generalizability, as existi...","track":"Findings of EMNLP"},"forum":"findings.137","id":"findings.137","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.138.png","content":{"abstract":"Natural language processing systems often struggle with out-of-vocabulary (OOV) terms, which do not appear in training data. Blends, such as \u201cinnoventor\u201d, are one particularly challenging class of OOV, as they are formed by fusing together two or more bases that relate to the intended meaning in unpredictable manners and degrees. In this work, we run experiments on a novel dataset of English OOV blends to quantify the difficulty of interpreting the meanings of blends by large-scale contextual language models such as BERT. We first show that BERT\u2019s processing of these blends does not fully access the component meanings, leaving their contextual representations semantically impoverished. We find this is mostly due to the loss of characters resulting from blend formation. Then, we assess how easily different models can recognize the structure and recover the origin of blends, and find that context-aware embedding systems outperform character-level and context-free embeddings, although their results are still far from satisfactory.","authors":["Yuval Pinter","Cassandra L. Jacobs","Jacob Eisenstein"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.138","program":"findings","sessions":[],"similar_paper_uids":["findings.138"],"title":"Will it Unblend?","tldr":"Natural language processing systems often struggle with out-of-vocabulary (OOV) terms, which do not appear in training data. Blends, such as \u201cinnoventor\u201d, are one particularly challenging class of OOV, as they are formed by fusing together two or mor...","track":"Findings of EMNLP"},"forum":"findings.138","id":"findings.138","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.139.png","content":{"abstract":"We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \u201cbimodal\u201d data of NL-PL pairs and \u201cunimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.","authors":["Zhangyin Feng","Daya Guo","Duyu Tang","Nan Duan","Xiaocheng Feng","Ming Gong","Linjun Shou","Bing Qin","Ting Liu","Daxin Jiang","Ming Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.139","program":"findings","sessions":[],"similar_paper_uids":["findings.139"],"title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages","tldr":"We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentat...","track":"Findings of EMNLP"},"forum":"findings.139","id":"findings.139","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.140.png","content":{"abstract":"Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained language models that have brought breakthrough to various natural language tasks. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform state-of-the-art methods in terms of both style consistency and contextual coherence.","authors":["Ze Yang","Wei Wu","Can Xu","Xinnian Liang","Jiaqi Bai","Liran Wang","Wei Wang","Zhoujun Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.140","program":"findings","sessions":[],"similar_paper_uids":["findings.140"],"title":"StyleDGPT: Stylized Response Generation with Pre-trained Language Models","tldr":"Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained lang...","track":"Findings of EMNLP"},"forum":"findings.140","id":"findings.140","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.141.png","content":{"abstract":"Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Language Models via fusing representations from different layers, constructing an auxiliary sentence, using multi-task learning, etc. However, to solve the AES task, previous works utilize shallow neural networks to learn essay representations and constrain calculated scores with regression loss or ranking loss, respectively. Since shallow neural networks trained on limited samples show poor performance to capture deep semantic of texts. And without an accurate scoring function, ranking loss and regression loss measures two different aspects of the calculated scores. To improve AES\u2019s performance, we find a new way to fine-tune pre-trained language models with multiple losses of the same task. In this paper, we propose to utilize a pre-trained language model to learn text representations first. With scores calculated from the representations, mean square error loss and the batch-wise ListNet loss with dynamic weights constrain the scores simultaneously. We utilize Quadratic Weighted Kappa to evaluate our model on the Automated Student Assessment Prize dataset. Our model outperforms not only state-of-the-art neural models near 3 percent but also the latest statistic model. Especially on the two narrative prompts, our model performs much better than all other state-of-the-art models.","authors":["Ruosong Yang","Jiannong Cao","Zhiyuan Wen","Youzheng Wu","Xiaodong He"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.141","program":"findings","sessions":[],"similar_paper_uids":["findings.141"],"title":"Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of Regression and Ranking","tldr":"Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Languag...","track":"Findings of EMNLP"},"forum":"findings.141","id":"findings.141","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.142.png","content":{"abstract":"Dialogue state tracking (DST) is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work, we propose Temporally Expressive Networks (TEN) to jointly model the two types of temporal dependencies in DST. The TEN model utilizes the power of recurrent networks and probabilistic graphical models. Evaluating on standard datasets, TEN is demonstrated to improve the accuracy of turn-level-state prediction and the state aggregation.","authors":["Junfan Chen","Richong Zhang","Yongyi Mao","Jie Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.142","program":"findings","sessions":[],"similar_paper_uids":["findings.142"],"title":"Neural Dialogue State Tracking with Temporally Expressive Networks","tldr":"Dialogue state tracking (DST) is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work,...","track":"Findings of EMNLP"},"forum":"findings.142","id":"findings.142","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.143.png","content":{"abstract":"Public works procurements move US$ 10 billion yearly in Brazil and are a preferred field for collusion and fraud. Federal Police and audit agencies investigate collusion (bid-rigging), over-pricing, and delivery fraud in this field and efforts have been employed to early detect fraud and collusion on public works procurements. The current automatic methods of fraud detection use structured data to classification and usually do not involve annotated data. The use of NLP for this kind of application is rare. Our work introduces a new dataset formed by public procurement calls available on Brazilian official journal (Di\u00e1rio Oficial da Uni\u00e3o), using by 15,132,968 textual entries of which 1,907 are annotated risky entries. Both bottleneck deep neural network and BiLSTM shown competitive compared with classical classifiers and achieved better precision (93.0% and 92.4%, respectively), which signs improvements in a criminal fraud investigation.","authors":["Marcos Lima","Roberta Silva","Felipe Lopes de Souza Mendes","Leonardo R. de Carvalho","Aleteia Araujo","Flavio de Barros Vidal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.143","program":"findings","sessions":[],"similar_paper_uids":["findings.143"],"title":"Inferring about fraudulent collusion risk on Brazilian public works contracts in official texts using a Bi-LSTM approach","tldr":"Public works procurements move US$ 10 billion yearly in Brazil and are a preferred field for collusion and fraud. Federal Police and audit agencies investigate collusion (bid-rigging), over-pricing, and delivery fraud in this field and efforts have b...","track":"Findings of EMNLP"},"forum":"findings.143","id":"findings.143","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.144.png","content":{"abstract":"Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional systems use templates to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a template acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of stylistic control by using existing sentences as \u201csoft\u201d templates. That is, a model learns to imitate the writing style of any given exemplar sentence, with automatic adaptions to faithfully describe the record. The problem is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in restaurants and sports domains. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.","authors":["Shuai Lin","Wentao Wang","Zichao Yang","Xiaodan Liang","Frank F. Xu","Eric Xing","Zhiting Hu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.144","program":"findings","sessions":[],"similar_paper_uids":["findings.144"],"title":"Data-to-Text Generation with Style Imitation","tldr":"Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional systems use templates to determine th...","track":"Findings of EMNLP"},"forum":"findings.144","id":"findings.144","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.145.png","content":{"abstract":"Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying world knowledge, linguistic sophistication, and/or simply superior deductive powers. In this paper, we focus on \u201cteaching\u201d machines reading comprehension, using a small number of semi-structured explanations that explicitly inform machines why answer spans are correct. We extract structured variables and rules from explanations and compose neural module teachers that annotate instances for training downstream MRC models. We use learnable neural modules and soft logic to handle linguistic variation and overcome sparse coverage; the modules are jointly optimized with the MRC model to improve final performance. On the SQuAD dataset, our proposed method achieves 70.14% F1 score with supervision from 26 explanations, comparable to plain supervised learning using 1,100 labeled instances, yielding a 12x speed up.","authors":["Qinyuan Ye","Xiao Huang","Elizabeth Boschee","Xiang Ren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.145","program":"findings","sessions":[],"similar_paper_uids":["findings.145"],"title":"Teaching Machine Comprehension with Compositional Explanations","tldr":"Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples...","track":"Findings of EMNLP"},"forum":"findings.145","id":"findings.145","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.146.png","content":{"abstract":"Classifying and resolving coreferences of objects (e.g., product names) and attributes (e.g., product aspects) in opinionated reviews is crucial for improving the opinion mining performance. However, the task is challenging as one often needs to consider domain-specific knowledge (e.g., iPad is a tablet and has aspect resolution) to identify coreferences in opinionated reviews. Also, compiling a handcrafted and curated domain-specific knowledge base for each domain is very time consuming and arduous. This paper proposes an approach to automatically mine and leverage domain-specific knowledge for classifying objects and attribute coreferences. The approach extracts domain-specific knowledge from unlabeled review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach","authors":["Jiahua Chen","Shuai Wang","Sahisnu Mazumder","Bing Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.146","program":"findings","sessions":[],"similar_paper_uids":["findings.146"],"title":"A Knowledge-Driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining","tldr":"Classifying and resolving coreferences of objects (e.g., product names) and attributes (e.g., product aspects) in opinionated reviews is crucial for improving the opinion mining performance. However, the task is challenging as one often needs to cons...","track":"Findings of EMNLP"},"forum":"findings.146","id":"findings.146","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.147.png","content":{"abstract":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings \u2013 both static and contextualized \u2013 for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners \u2013 even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.","authors":["Masoud Jalili Sabet","Philipp Dufter","Fran\u00e7ois Yvon","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.147","program":"findings","sessions":[],"similar_paper_uids":["findings.147"],"title":"SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings","tldr":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. Howeve...","track":"Findings of EMNLP"},"forum":"findings.147","id":"findings.147","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.148.png","content":{"abstract":"The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.","authors":["Francesco Barbieri","Jose Camacho-Collados","Luis Espinosa Anke","Leonardo Neves"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.148","program":"findings","sessions":[],"similar_paper_uids":["findings.148"],"title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification","tldr":"The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it i...","track":"Findings of EMNLP"},"forum":"findings.148","id":"findings.148","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.149.png","content":{"abstract":"Sentiments in opinionated text are often determined by both aspects and target words (or targets). We observe that targets and aspects interrelate in subtle ways, often yielding conflicting sentiments. Thus, a naive aggregation of sentiments from aspects and targets treated separately, as in existing sentiment analysis models, impairs performance. We propose Octa, an approach that jointly considers aspects and targets when inferring sentiments. To capture and quantify relationships between targets and context words, Octa uses a selective self-attention mechanism that handles implicit or missing targets. Specifically, Octa involves two layers of attention mechanisms for, respectively, selective attention between targets and context words and attention over words based on aspects. On benchmark datasets, Octa outperforms leading models by a large margin, yielding (absolute) gains in accuracy of 1.6% to 4.3%.","authors":["Zhe Zhang","Chung-Wei Hang","Munindar Singh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.149","program":"findings","sessions":[],"similar_paper_uids":["findings.149"],"title":"Octa: Omissions and Conflicts in Target-Aspect Sentiment Analysis","tldr":"Sentiments in opinionated text are often determined by both aspects and target words (or targets). We observe that targets and aspects interrelate in subtle ways, often yielding conflicting sentiments. Thus, a naive aggregation of sentiments from asp...","track":"Findings of EMNLP"},"forum":"findings.149","id":"findings.149","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.150.png","content":{"abstract":"Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.","authors":["Jind\u0159ich Libovick\u00fd","Rudolf Rosa","Alexander Fraser"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.150","program":"findings","sessions":[],"similar_paper_uids":["findings.150"],"title":"On the Language Neutrality of Pre-trained Multilingual Representations","tldr":"Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morpholog...","track":"Findings of EMNLP"},"forum":"findings.150","id":"findings.150","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.151.png","content":{"abstract":"Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when models are pretrained on in-domain data. Often, the pretraining data used in these models are selected based on their subject matter, e.g., biology or computer science. Given the range of applications using social media text, and its unique language variety, we pretrain two models on tweets and forum text respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how similarity measures can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0.","authors":["Xiang Dai","Sarvnaz Karimi","Ben Hachey","Cecile Paris"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.151","program":"findings","sessions":[],"similar_paper_uids":["findings.151"],"title":"Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media","tldr":"Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when models are pretrained on in-domain data. Often, the pretraining data used in these models are selected based on their subject matter, e.g.,...","track":"Findings of EMNLP"},"forum":"findings.151","id":"findings.151","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.152.png","content":{"abstract":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations \u2013 a main performance bottleneck. Consequently, our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets.","authors":["Yatin Chaudhary","Pankaj Gupta","Khushbu Saxena","Vivek Kulkarni","Thomas Runkler","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.152","program":"findings","sessions":[],"similar_paper_uids":["findings.152"],"title":"TopicBERT for Energy Efficient Document Classification","tldr":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre...","track":"Findings of EMNLP"},"forum":"findings.152","id":"findings.152","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.153.png","content":{"abstract":"Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and thus could also be beneficial for constituency parsing if they are appropriately modeled. In this paper, we propose span attention for neural chart-based constituency parsing to leverage n-gram information. Considering that current chart-based parsers with Transformer-based encoder represent spans by subtraction of the hidden states at the span boundaries, which may cause information loss especially for long spans, we incorporate n-grams into span representations by weighting them according to their contributions to the parsing process. Moreover, we propose categorical span attention to further enhance the model by weighting n-grams within different length categories, and thus benefit long-sentence parsing. Experimental results on three widely used benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.","authors":["Yuanhe Tian","Yan Song","Fei Xia","Tong Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.153","program":"findings","sessions":[],"similar_paper_uids":["findings.153"],"title":"Improving Constituency Parsing with Span Attention","tldr":"Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have b...","track":"Findings of EMNLP"},"forum":"findings.153","id":"findings.153","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.154.png","content":{"abstract":"Language models that utilize extensive self-supervised pre-training from unlabeled text, have recently shown to significantly advance the state-of-the-art performance in a variety of language understanding tasks. However, it is yet unclear if and how these recent models can be harnessed for conducting text-based recommendations. In this work, we introduce RecoBERT, a BERT-based approach for learning catalog-specialized language models for text-based item recommendations. We suggest novel training and inference procedures for scoring similarities between pairs of items, that don\u2019t require item similarity labels. Both the training and the inference techniques were designed to utilize the unlabeled structure of textual catalogs, and minimize the discrepancy between them. By incorporating four scores during inference, RecoBERT can infer text-based item-to-item similarities more accurately than other techniques. In addition, we introduce a new language understanding task for wine recommendations using similarities based on professional wine reviews. As an additional contribution, we publish annotated recommendations dataset crafted by human wine experts. Finally, we evaluate RecoBERT and compare it to various state-of-the-art NLP models on wine and fashion recommendations tasks.","authors":["Itzik Malkiel","Oren Barkan","Avi Caciularu","Noam Razin","Ori Katz","Noam Koenigstein"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.154","program":"findings","sessions":[],"similar_paper_uids":["findings.154"],"title":"RecoBERT: A Catalog Language Model for Text-Based Recommendations","tldr":"Language models that utilize extensive self-supervised pre-training from unlabeled text, have recently shown to significantly advance the state-of-the-art performance in a variety of language understanding tasks. However, it is yet unclear if and how...","track":"Findings of EMNLP"},"forum":"findings.154","id":"findings.154","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.155.png","content":{"abstract":"Mutual learning, where multiple agents learn collaboratively and teach one another, has been shown to be an effective way to distill knowledge for image classification tasks. In this paper, we extend mutual learning to the machine translation task and operate at both the sentence-level and the token-level. Firstly, we co-train multiple agents by using the same parallel corpora. After convergence, each agent selects and learns its poorly predicted tokens from other agents. The poorly predicted tokens are determined by the acceptance-rejection sampling algorithm. Our experiments show that sequential mutual learning at the sentence-level and the token-level improves the results cumulatively. Absolute improvements compared to strong baselines are obtained on various translation tasks. On the IWSLT\u201914 German-English task, we get a new state-of-the-art BLEU score of 37.0. We also report a competitive result, 29.9 BLEU score, on the WMT\u201914 English-German task.","authors":["Baohao Liao","Yingbo Gao","Hermann Ney"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.155","program":"findings","sessions":[],"similar_paper_uids":["findings.155"],"title":"Multi-Agent Mutual Learning at Sentence-Level and Token-Level for Neural Machine Translation","tldr":"Mutual learning, where multiple agents learn collaboratively and teach one another, has been shown to be an effective way to distill knowledge for image classification tasks. In this paper, we extend mutual learning to the machine translation task an...","track":"Findings of EMNLP"},"forum":"findings.155","id":"findings.155","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.156.png","content":{"abstract":"This paper focuses on learning domain-oriented language models driven by end tasks, which aims to combine the worlds of both general-purpose language models (such as ELMo and BERT) and domain-specific language understanding. We propose DomBERT, an extension of BERT to learn from both in-domain corpus and relevant domain corpora. This helps in learning domain language models with low-resources. Experiments are conducted on an assortment of tasks in aspect-based sentiment analysis (ABSA), demonstrating promising results.","authors":["Hu Xu","Bing Liu","Lei Shu","Philip Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.156","program":"findings","sessions":[],"similar_paper_uids":["findings.156"],"title":"DomBERT: Domain-oriented Language Model for Aspect-based Sentiment Analysis","tldr":"This paper focuses on learning domain-oriented language models driven by end tasks, which aims to combine the worlds of both general-purpose language models (such as ELMo and BERT) and domain-specific language understanding. We propose DomBERT, an ex...","track":"Findings of EMNLP"},"forum":"findings.156","id":"findings.156","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.157.png","content":{"abstract":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent RMM where robots need to both ask and answer questions.","authors":["Homero Roman Roman","Yonatan Bisk","Jesse Thomason","Asli Celikyilmaz","Jianfeng Gao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.157","program":"findings","sessions":[],"similar_paper_uids":["findings.157"],"title":"RMM: A Recursive Mental Model for Dialogue Navigation","tldr":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and ask...","track":"Findings of EMNLP"},"forum":"findings.157","id":"findings.157","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.158.png","content":{"abstract":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we study translational research at the level of scientific concepts for all scientific fields. We do this through text mining and predictive modeling using three corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28 million clinical trials. We extract scientific concepts (i.e., phrases) from corpora as instantiations of \u201cresearch ideas\u201d, create concept-level features as motivated by literature, and then follow the trajectories of over 450,000 new concepts (emerged from 1995-2014) to identify factors that lead only a small proportion of these ideas to be used in inventions and drug trials. Results from our analysis suggest several mechanisms that distinguish which scientific concept will be adopted in practice, and which will not. We also demonstrate that our derived features can be used to explain and predict knowledge transfer with high accuracy. Our work provides greater understanding of knowledge transfer for researchers, practitioners, and government agencies interested in encouraging translational research.","authors":["Hancheng Cao","Mengjie Cheng","Zhepeng Cen","Daniel McFarland","Xiang Ren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.158","program":"findings","sessions":[],"similar_paper_uids":["findings.158"],"title":"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora","tldr":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into ...","track":"Findings of EMNLP"},"forum":"findings.158","id":"findings.158","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.159.png","content":{"abstract":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.","authors":["Ning Shi","Ziheng Zeng","Haotian Zhang","Yichen Gong"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.159","program":"findings","sessions":[],"similar_paper_uids":["findings.159"],"title":"Recurrent Inference in Text Editing","tldr":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying dec...","track":"Findings of EMNLP"},"forum":"findings.159","id":"findings.159","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.160.png","content":{"abstract":"Recently, pre-training contextualized encoders with language model (LM) objectives has been shown an effective semi-supervised method for structured prediction. In this work, we empirically explore an alternative pre-training method for contextualized encoders. Instead of predicting words in LMs, we \u201cmask out\u201d and predict word order information, with a local ordering strategy and word-selecting objectives. With evaluations on three typical structured prediction tasks (dependency parsing, POS tagging, and NER) over four languages (English, Finnish, Czech, and Italian), we show that our method is consistently beneficial. We further conduct detailed error analysis, including one that examines a specific type of parsing error where the head is misidentified. The results show that pre-trained contextual encoders can bring improvements in a structured way, suggesting that they may be able to capture higher-order patterns and feature combinations from unlabeled data.","authors":["Zhisong Zhang","Xiang Kong","Lori Levin","Eduard Hovy"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.160","program":"findings","sessions":[],"similar_paper_uids":["findings.160"],"title":"An Empirical Exploration of Local Ordering Pre-training for Structured Prediction","tldr":"Recently, pre-training contextualized encoders with language model (LM) objectives has been shown an effective semi-supervised method for structured prediction. In this work, we empirically explore an alternative pre-training method for contextualize...","track":"Findings of EMNLP"},"forum":"findings.160","id":"findings.160","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.161.png","content":{"abstract":"Unsupervised extractive document summarization aims to select important sentences from a document without using labeled summaries during training. Existing methods are mostly graph-based with sentences as nodes and edge weights measured by sentence similarities. In this work, we find that transformer attentions can be used to rank sentences for unsupervised extractive summarization. Specifically, we first pre-train a hierarchical transformer model using unlabeled documents only. Then we propose a method to rank sentences using sentence-level self-attentions and pre-training objectives. Experiments on CNN/DailyMail and New York Times datasets show our model achieves state-of-the-art performance on unsupervised summarization. We also find in experiments that our model is less dependent on sentence positions. When using a linear combination of our model and a recent unsupervised model explicitly modeling sentence positions, we obtain even better results.","authors":["Shusheng Xu","Xingxing Zhang","Yi Wu","Furu Wei","Ming Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.161","program":"findings","sessions":[],"similar_paper_uids":["findings.161"],"title":"Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers","tldr":"Unsupervised extractive document summarization aims to select important sentences from a document without using labeled summaries during training. Existing methods are mostly graph-based with sentences as nodes and edge weights measured by sentence s...","track":"Findings of EMNLP"},"forum":"findings.161","id":"findings.161","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.162.png","content":{"abstract":"Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer learning and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined method steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for NMT. Specifically, with a human translation budget of only 20% of the original parallel corpus, we manage to surpass Transformer trained on the entire parallel corpus in three language pairs.","authors":["Yuekai Zhao","Haoran Zhang","Shuchang Zhou","Zhihua Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.162","program":"findings","sessions":[],"similar_paper_uids":["findings.162"],"title":"Active Learning Approaches to Enhancing Neural Machine Translation","tldr":"Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer...","track":"Findings of EMNLP"},"forum":"findings.162","id":"findings.162","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.163.png","content":{"abstract":"In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (SLU) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context vector for all tokens, ignoring the fine-grained multiple intents information integration for token-level slot prediction. In this paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint multiple intent detection and slot filling, where we introduce an intent-slot graph interaction layer to model the strong correlation between the slot and intents. Such an interaction layer is applied to each token adaptively, which has the advantage to automatically extract the relevant intents information, making a fine-grained intent information integration for the token-level slot prediction. Experimental results on three multi-intent datasets show that our framework obtains substantial improvement and achieves the state-of-the-art performance. In addition, our framework achieves new state-of-the-art performance on two single-intent datasets.","authors":["Libo Qin","Xiao Xu","Wanxiang Che","Ting Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.163","program":"findings","sessions":[],"similar_paper_uids":["findings.163"],"title":"AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling","tldr":"In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (SLU) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context v...","track":"Findings of EMNLP"},"forum":"findings.163","id":"findings.163","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.164.png","content":{"abstract":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task\u2019s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.","authors":["Xin Guo","Yu Tian","Qinghan Xue","Panos Lampropoulos","Steven Eliuk","Kenneth Barner","Xiaolong Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.164","program":"findings","sessions":[],"similar_paper_uids":["findings.164"],"title":"Continual Learning Long Short Term Memory","tldr":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell i...","track":"Findings of EMNLP"},"forum":"findings.164","id":"findings.164","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.165.png","content":{"abstract":"Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., \u201ca man throws a frisbee and his dog catches it\u201d). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6% v.s. 63.5% in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9% to 78.4 in dev accuracy) by generating additional context.","authors":["Bill Yuchen Lin","Wangchunshu Zhou","Ming Shen","Pei Zhou","Chandra Bhagavatula","Yejin Choi","Xiang Ren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.165","program":"findings","sessions":[],"similar_paper_uids":["findings.165"],"title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning","tldr":"Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challengi...","track":"Findings of EMNLP"},"forum":"findings.165","id":"findings.165","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.166.png","content":{"abstract":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in F1 that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both PyTorch and TensorFlow.","authors":["Brian Lester","Daniel Pressel","Amy Hemmeter","Sagnik Ray Choudhury","Srinivas Bangalore"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.166","program":"findings","sessions":[],"similar_paper_uids":["findings.166"],"title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers","tldr":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. C...","track":"Findings of EMNLP"},"forum":"findings.166","id":"findings.166","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.167.png","content":{"abstract":"Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL, a dataset that enriches 11,276 WIKITABLEQUESTIONS English-language questions with manually created SQL equivalents plus alignments between SQL and question fragments. Our annotation enables new training possibilities for encoderdecoder models, including approaches from machine translation previously precluded by the absence of alignments. We propose and test two methods: (1) supervised attention; (2) adopting an auxiliary objective of disambiguating references in the input queries to table columns. In 5-fold cross validation, these strategies improve over strong baselines by 4.4% execution accuracy. Oracle experiments suggest that annotated alignments can support further accuracy gains of up to 23.9%.","authors":["Tianze Shi","Chen Zhao","Jordan Boyd-Graber","Hal Daum\u00e9 III","Lillian Lee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.167","program":"findings","sessions":[],"similar_paper_uids":["findings.167"],"title":"On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries","tldr":"Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL...","track":"Findings of EMNLP"},"forum":"findings.167","id":"findings.167","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.168.png","content":{"abstract":"Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose TED, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in news articles to pretrain the model on millions of unlabeled corpora. Next, we finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of generated summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN/DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by TED are highly abstractive, and each component in the objective function of TED is highly effective.","authors":["Ziyi Yang","Chenguang Zhu","Robert Gmyr","Michael Zeng","Xuedong Huang","Eric Darve"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.168","program":"findings","sessions":[],"similar_paper_uids":["findings.168"],"title":"TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising","tldr":"Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently propos...","track":"Findings of EMNLP"},"forum":"findings.168","id":"findings.168","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.169.png","content":{"abstract":"Automatic speech recognition systems usually require large annotated speech corpus for training. The manual annotation of a large corpus is very difficult. It can be very helpful to use unsupervised and semi-supervised learning methods in addition to supervised learning. In this work, we focus on using a semi-supervised training approach for Bangla Speech Recognition that can exploit large unpaired audio and text data. We encode speech and text data in an intermediate domain and propose a novel loss function based on the global encoding distance between encoded data to guide the semi-supervised training. Our proposed method reduces the Word Error Rate (WER) of the system from 37% to 31.9%.","authors":["Nafis Sadeq","Nafis Tahmid Chowdhury","Farhan Tanvir Utshaw","Shafayat Ahmed","Muhammad Abdullah Adnan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.169","program":"findings","sessions":[],"similar_paper_uids":["findings.169"],"title":"Improving End-to-End Bangla Speech Recognition with Semi-supervised Training","tldr":"Automatic speech recognition systems usually require large annotated speech corpus for training. The manual annotation of a large corpus is very difficult. It can be very helpful to use unsupervised and semi-supervised learning methods in addition to...","track":"Findings of EMNLP"},"forum":"findings.169","id":"findings.169","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.170.png","content":{"abstract":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle","authors":["Chaitanya Ahuja","Dong Won Lee","Ryo Ishii","Louis-Philippe Morency"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.170","program":"findings","sessions":[],"similar_paper_uids":["findings.170"],"title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures","tldr":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made a...","track":"Findings of EMNLP"},"forum":"findings.170","id":"findings.170","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.171.png","content":{"abstract":"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.","authors":["Daniel Khashabi","Sewon Min","Tushar Khot","Ashish Sabharwal","Oyvind Tafjord","Peter Clark","Hannaneh Hajishirzi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.171","program":"findings","sessions":[],"similar_paper_uids":["findings.171"],"title":"UNIFIEDQA: Crossing Format Boundaries with a Single QA System","tldr":"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such bou...","track":"Findings of EMNLP"},"forum":"findings.171","id":"findings.171","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.172.png","content":{"abstract":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","authors":["Tsung-Yen Yang","Andrew Lan","Karthik Narasimhan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.172","program":"findings","sessions":[],"similar_paper_uids":["findings.172"],"title":"Robust and Interpretable Grounding of Spatial References with Relation Networks","tldr":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for...","track":"Findings of EMNLP"},"forum":"findings.172","id":"findings.172","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.173.png","content":{"abstract":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a partition that places images into equivalence classes based on player position. To model this task, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these models generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.","authors":["Allen Nie","Reuben Cohn-Gordon","Christopher Potts"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.173","program":"findings","sessions":[],"similar_paper_uids":["findings.173"],"title":"Pragmatic Issue-Sensitive Image Captioning","tldr":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the playe...","track":"Findings of EMNLP"},"forum":"findings.173","id":"findings.173","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.174.png","content":{"abstract":"User modeling is critical for many personalized web services. Many existing methods model users based on their behaviors and the labeled data of target tasks. However, these methods cannot exploit useful information in unlabeled user behavior data, and their performance may be not optimal when labeled data is scarce. Motivated by pre-trained language models which are pre-trained on large-scale unlabeled corpus to empower many downstream tasks, in this paper we propose to pre-train user models from large-scale unlabeled user behaviors data. We propose two self-supervision tasks for user model pre-training. The first one is masked behavior prediction, which can model the relatedness between historical behaviors. The second one is next K behavior prediction, which can model the relatedness between past and future behaviors. The pre-trained user models are finetuned in downstream tasks to learn task-specific user representations. Experimental results on two real-world datasets validate the effectiveness of our proposed user model pre-training method.","authors":["Chuhan Wu","Fangzhao Wu","Tao Qi","Jianxun Lian","Yongfeng Huang","Xing Xie"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.174","program":"findings","sessions":[],"similar_paper_uids":["findings.174"],"title":"PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision","tldr":"User modeling is critical for many personalized web services. Many existing methods model users based on their behaviors and the labeled data of target tasks. However, these methods cannot exploit useful information in unlabeled user behavior data, a...","track":"Findings of EMNLP"},"forum":"findings.174","id":"findings.174","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.175.png","content":{"abstract":"Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of machine translation as NMT models can experience various subword candidates. However, the diversification of subword segmentations mostly relies on the pre-trained subword language models from which erroneous segmentations of unseen words are less likely to be sampled. In this paper, we present adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations. We experimentally show that our model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of NMT models in low-resource and out-domain datasets.","authors":["Jungsoo Park","Mujeen Sung","Jinhyuk Lee","Jaewoo Kang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.175","program":"findings","sessions":[],"similar_paper_uids":["findings.175"],"title":"Adversarial Subword Regularization for Robust Neural Machine Translation","tldr":"Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of machine translation as NMT models can experience various subword candidates. However, the diversification of subword segmentations most...","track":"Findings of EMNLP"},"forum":"findings.175","id":"findings.175","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.176.png","content":{"abstract":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.","authors":["Jianmo Ni","Chun-Nan Hsu","Amilcare Gentili","Julian McAuley"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.176","program":"findings","sessions":[],"similar_paper_uids":["findings.176"],"title":"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays","tldr":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such model...","track":"Findings of EMNLP"},"forum":"findings.176","id":"findings.176","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.177.png","content":{"abstract":"In this paper, we study a new task of synonym expansion using transitivity, and propose a novel approach named SynET, which considers both the contexts of two given synonym pairs. It introduces an auxiliary task to reduce the impact of noisy sentences, and proposes a Multi-Perspective Entity Matching Network to match entities from multiple perspectives. Extensive experiments on a real-world dataset show the effectiveness of our approach.","authors":["Jiale Yu","Yongliang Shen","Xinyin Ma","Chenghao Jia","Chen Chen","Weiming Lu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.177","program":"findings","sessions":[],"similar_paper_uids":["findings.177"],"title":"SynET: Synonym Expansion using Transitivity","tldr":"In this paper, we study a new task of synonym expansion using transitivity, and propose a novel approach named SynET, which considers both the contexts of two given synonym pairs. It introduces an auxiliary task to reduce the impact of noisy sentence...","track":"Findings of EMNLP"},"forum":"findings.177","id":"findings.177","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.178.png","content":{"abstract":"We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of transformer. In contrast to the conventional dropout mechanism which randomly drops units or connections, DropHead drops entire attention heads during training to prevent the multi-head attention model from being dominated by a small portion of attention heads. It can help reduce the risk of overfitting and allow the models to better benefit from the multi-head attention. Given the interaction between multi-headedness and training dynamics, we further propose a novel dropout rate scheduler to adjust the dropout rate of DropHead throughout training, which results in a better regularization effect. Experimental results demonstrate that our proposed approach can improve transformer models by 0.9 BLEU score on WMT14 En-De translation task and around 1.0 accuracy for various text classification tasks.","authors":["Wangchunshu Zhou","Tao Ge","Furu Wei","Ming Zhou","Ke Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.178","program":"findings","sessions":[],"similar_paper_uids":["findings.178"],"title":"Scheduled DropHead: A Regularization Method for Transformer Models","tldr":"We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of transformer. In contrast to the conventional dropout mechanism which randomly drops units or conn...","track":"Findings of EMNLP"},"forum":"findings.178","id":"findings.178","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.179.png","content":{"abstract":"As an important research topic, customer service dialogue generation tends to generate generic seller responses by leveraging current dialogue information. In this study, we propose a novel and extensible dialogue generation method by leveraging sellers\u2019 historical dialogue information, which can be both accessible and informative. By utilizing innovative historical dialogue representation learning and historical dialogue selection mechanism, the proposed model is capable of detecting most related responses from sellers\u2019 historical dialogues, which can further enhance the current dialogue generation quality. Unlike prior dialogue generation efforts, we treat each seller\u2019s historical dialogues as a list of Customer-Seller utterance pairs and allow the model to measure their different importance, and copy words directly from most relevant pairs. Extensive experimental results show that the proposed approach can generate high-quality responses that cater to specific sellers\u2019 characteristics and exhibit consistent superiority over baselines on a real-world multi-turn customer service dialogue dataset.","authors":["WeiSheng Zhang","Kaisong Song","Yangyang Kang","Zhongqing Wang","Changlong Sun","Xiaozhong Liu","Shoushan Li","Min Zhang","Luo Si"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.179","program":"findings","sessions":[],"similar_paper_uids":["findings.179"],"title":"Multi-Turn Dialogue Generation in E-Commerce Platform with the Context of Historical Dialogue","tldr":"As an important research topic, customer service dialogue generation tends to generate generic seller responses by leveraging current dialogue information. In this study, we propose a novel and extensible dialogue generation method by leveraging sell...","track":"Findings of EMNLP"},"forum":"findings.179","id":"findings.179","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.180.png","content":{"abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","authors":["Hila Gonen","Kellie Webster"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.180","program":"findings","sessions":[],"similar_paper_uids":["findings.180"],"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","tldr":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Whil...","track":"Findings of EMNLP"},"forum":"findings.180","id":"findings.180","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.181.png","content":{"abstract":"Data programming aims to reduce the cost of curating training data by encoding domain knowledge as labeling functions over source data. As such it not only requires domain expertise but also programming experience, a skill that many subject matter experts lack. Additionally, generating functions by enumerating rules is not only time consuming but also inherently difficult, even for people with programming experience. In this paper we introduce Ruler, an interactive system that synthesizes labeling rules using span-level interactive demonstrations over document examples. Ruler is a first-of-a-kind implementation of data programming by demonstration (DPBD). This new framework aims to relieve users from the burden of writing labeling functions, enabling them to focus on higher-level semantic analysis, such as identifying relevant signals for the labeling task. We compare Ruler with conventional data programming through a user study conducted with 10 data scientists who were asked to create labeling functions for sentiment and spam classification tasks. Results show Ruler is easier to learn and to use, and that it offers higher overall user-satisfaction while providing model performances comparable to those achieved by conventional data programming.","authors":["Sara Evensen","Chang Ge","Cagatay Demiralp"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.181","program":"findings","sessions":[],"similar_paper_uids":["findings.181"],"title":"Ruler: Data Programming by Demonstration for Document Labeling","tldr":"Data programming aims to reduce the cost of curating training data by encoding domain knowledge as labeling functions over source data. As such it not only requires domain expertise but also programming experience, a skill that many subject matter ex...","track":"Findings of EMNLP"},"forum":"findings.181","id":"findings.181","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.182.png","content":{"abstract":"While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.","authors":["Weijia Xu","Xing Niu","Marine Carpuat"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.182","program":"findings","sessions":[],"similar_paper_uids":["findings.182"],"title":"Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation","tldr":"While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared....","track":"Findings of EMNLP"},"forum":"findings.182","id":"findings.182","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.183.png","content":{"abstract":"To model diverse responses for a given post, one promising way is to introduce a latent variable into Seq2Seq models. The latent variable is supposed to capture the discourse-level information and encourage the informativeness of target responses. However, such discourse-level information is often too coarse for the decoder to be utilized. To tackle it, our idea is to transform the coarse-grained discourse-level information into fine-grained word-level information. Specifically, we firstly measure the semantic concentration of corresponding target response on the post words by introducing a fine-grained focus signal. Then, we propose a focus-constrained attention mechanism to take full advantage of focus in well aligning the input to the target response. The experimental results demonstrate that by exploiting the fine-grained signal, our model can generate more diverse and informative responses compared with several state-of-the-art models.","authors":["Zhi Cui","Yanran Li","Jiayi Zhang","Jianwei Cui","Chen Wei","Bin Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.183","program":"findings","sessions":[],"similar_paper_uids":["findings.183"],"title":"Focus-Constrained Attention Mechanism for CVAE-based Response Generation","tldr":"To model diverse responses for a given post, one promising way is to introduce a latent variable into Seq2Seq models. The latent variable is supposed to capture the discourse-level information and encourage the informativeness of target responses. Ho...","track":"Findings of EMNLP"},"forum":"findings.183","id":"findings.183","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.184.png","content":{"abstract":"Chinese spelling check is a challenging task due to the characteristics of the Chinese language, such as the large character set, no word boundary, and short word length. On the one hand, most of the previous works only consider corrections with similar character pronunciation or shape, failing to correct visually and phonologically irrelevant typos. On the other hand, pipeline-style architectures are widely adopted to deal with different types of spelling errors in individual modules, which is difficult to optimize. In order to handle these issues, in this work, 1) we extend the traditional confusion sets with semantical candidates to cover different types of errors; 2) we propose a chunk-based framework to correct single-character and multi-character word errors uniformly; and 3) we adopt a global optimization strategy to enable a sentence-level correction selection. The experimental results show that the proposed approach achieves a new state-of-the-art performance on three benchmark datasets, as well as an optical character recognition dataset.","authors":["Zuyi Bao","Chen Li","Rui Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.184","program":"findings","sessions":[],"similar_paper_uids":["findings.184"],"title":"Chunk-based Chinese Spelling Check with Global Optimization","tldr":"Chinese spelling check is a challenging task due to the characteristics of the Chinese language, such as the large character set, no word boundary, and short word length. On the one hand, most of the previous works only consider corrections with simi...","track":"Findings of EMNLP"},"forum":"findings.184","id":"findings.184","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.185.png","content":{"abstract":"Deep neural network-based pretraining methods have achieved impressive results in many natural language processing tasks including text classification. However, their applicability to large-scale text classification with numerous categories (e.g., several thousands) is yet to be well-studied, where the training data is insufficient and skewed in terms of categories. In addition, existing pretraining methods usually involve excessive computation and memory overheads. In this paper, we develop a novel multi-pretraining framework for large-scale text classification. This multi-pretraining framework includes both a self-supervised pretraining and a weakly supervised pretraining. We newly introduce an out-of-context words detection task on the unlabeled data as the self-supervised pretraining. It captures the topic-consistency of words used in sentences, which is proven to be useful for text classification. In addition, we propose a weakly supervised pretraining, where labels for text classification are obtained automatically from an existing approach. Experimental results clearly show that both pretraining approaches are effective for large-scale text classification task. The proposed scheme exhibits significant improvements as much as 3.8% in terms of macro-averaging F1-score over strong pretraining methods, while being computationally efficient.","authors":["Kang-Min Kim","Bumsu Hyeon","Yeachan Kim","Jun-Hyung Park","SangKeun Lee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.185","program":"findings","sessions":[],"similar_paper_uids":["findings.185"],"title":"Multi-pretraining for Large-scale Text Classification","tldr":"Deep neural network-based pretraining methods have achieved impressive results in many natural language processing tasks including text classification. However, their applicability to large-scale text classification with numerous categories (e.g., se...","track":"Findings of EMNLP"},"forum":"findings.185","id":"findings.185","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.186.png","content":{"abstract":"Disfluency detection is usually an intermediate step between an automatic speech recognition (ASR) system and a downstream task. By contrast, this paper aims to investigate the task of end-to-end speech recognition and disfluency removal. We specifically explore whether it is possible to train an ASR model to directly map disfluent speech into fluent transcripts, without relying on a separate disfluency detection model. We show that end-to-end models do learn to directly generate fluent transcripts; however, their performance is slightly worse than a baseline pipeline approach consisting of an ASR system and a specialized disfluency detection model. We also propose two new metrics for evaluating integrated ASR and disfluency removal models. The findings of this paper can serve as a benchmark for further research on the task of end-to-end speech recognition and disfluency removal in the future.","authors":["Paria Jamshid Lou","Mark Johnson"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.186","program":"findings","sessions":[],"similar_paper_uids":["findings.186"],"title":"End-to-End Speech Recognition and Disfluency Removal","tldr":"Disfluency detection is usually an intermediate step between an automatic speech recognition (ASR) system and a downstream task. By contrast, this paper aims to investigate the task of end-to-end speech recognition and disfluency removal. We specific...","track":"Findings of EMNLP"},"forum":"findings.186","id":"findings.186","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.187.png","content":{"abstract":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical notes only provide additional predictive power over structured information in readmission prediction. We further propose a probing framework to select parts of notes that enable more accurate predictions than using all notes, despite that the selected information leads to a distribution shift from the training data (\u201call notes\u201d). Finally, we demonstrate that models trained on the selected valuable information achieve even better predictive performance, with only 6.8%of all the tokens for readmission prediction.","authors":["Chao-Chun Hsu","Shantanu Karnwal","Sendhil Mullainathan","Ziad Obermeyer","Chenhao Tan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.187","program":"findings","sessions":[],"similar_paper_uids":["findings.187"],"title":"Characterizing the Value of Information in Medical Notes","tldr":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmis...","track":"Findings of EMNLP"},"forum":"findings.187","id":"findings.187","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.188.png","content":{"abstract":"The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to background knowledge. In contrast, this work puts background knowledge in the foreground. Building on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge, we develop and compare techniques for inferring background knowledge from summarization data. Based on this framework, we define summary scoring functions that explicitly model background knowledge, and show that these scoring functions fit human judgments significantly better than baselines. We illustrate some of the many potential applications of our framework. First, we provide insights into human information importance priors. Second, we demonstrate that averaging the background knowledge of multiple, potentially biased annotators or corpora greatly improves summaryscoring performance. Finally, we discuss potential applications of our framework beyond summarization.","authors":["Maxime Peyrard","Robert West"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.188","program":"findings","sessions":[],"similar_paper_uids":["findings.188"],"title":"KLearn: Background Knowledge Inference from Summarization Data","tldr":"The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to ba...","track":"Findings of EMNLP"},"forum":"findings.188","id":"findings.188","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.189.png","content":{"abstract":"The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network (DNN) models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the model reliability. To estimate the data uncertainty and improve the reliability, \u201ccalibration\u201d techniques have been applied to deep learning models. In this study, to extract chemical\u2013protein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our model first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods: mixup training and addition of a confidence penalty loss. Finally, the model is re-trained with augmented data that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.","authors":["Dongha Choi","Hyunju Lee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.189","program":"findings","sessions":[],"similar_paper_uids":["findings.189"],"title":"Extracting Chemical-Protein Interactions via Calibrated Deep Neural Network and Self-training","tldr":"The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing method...","track":"Findings of EMNLP"},"forum":"findings.189","id":"findings.189","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.190.png","content":{"abstract":"Previous studies on Natural Language Generation (NLG) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an NLG system to describe interesting facts from logical inferences across records. If only provided with the table, it is hard for existing models to produce controllable and high-fidelity logical generations. In this work, we formulate high-fidelity NLG as generation from logical forms in order to obtain controllable and faithful generations. We present a new large-scale dataset, Logic2Text, with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which pose great challenges on the model\u2019s ability to understand the semantics. We experiment on (1) Fully-supervised training with the full datasets, and (2) Few-shot setting, provided with hundreds of paired examples; We compare several popular generation models and analyze their performances. We hope our dataset can encourage research towards building an advanced NLG system capable of natural, faithful, and human-like generation. The dataset and code is available at https://github.com/czyssrs/Logic2Text.","authors":["Zhiyu Chen","Wenhu Chen","Hanwen Zha","Xiyou Zhou","Yunkai Zhang","Sairam Sundaresan","William Yang Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.190","program":"findings","sessions":[],"similar_paper_uids":["findings.190"],"title":"Logic2Text: High-Fidelity Natural Language Generation from Logical Forms","tldr":"Previous studies on Natural Language Generation (NLG) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an NLG system...","track":"Findings of EMNLP"},"forum":"findings.190","id":"findings.190","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.191.png","content":{"abstract":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in scientific papers focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of medical images in context. MedICaT consists of 217K images from 131K open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.","authors":["Sanjay Subramanian","Lucy Lu Wang","Ben Bogin","Sachin Mehta","Madeleine van Zuylen","Sravanthi Parasa","Sameer Singh","Matt Gardner","Hannaneh Hajishirzi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.191","program":"findings","sessions":[],"similar_paper_uids":["findings.191"],"title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References","tldr":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describin...","track":"Findings of EMNLP"},"forum":"findings.191","id":"findings.191","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.192.png","content":{"abstract":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete response at a stroke. This tends to generate high-frequency function words with less semantic information rather than low-frequency content words with more semantic information. To address this issue, we propose a content-aware model with two-stage decoding process named Two-stage Dialogue Generation (TSDG). We separate the decoding process of content words and function words so that content words can be generated independently without the interference of function words. Experimental results on two datasets indicate that our model significantly outperforms several competitive generative models in terms of automatic and human evaluation.","authors":["Junsheng Kong","Zhicheng Zhong","Yi Cai","Xin Wu","Da Ren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.192","program":"findings","sessions":[],"similar_paper_uids":["findings.192"],"title":"TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process","tldr":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete r...","track":"Findings of EMNLP"},"forum":"findings.192","id":"findings.192","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.193.png","content":{"abstract":"We consider the task of cross-lingual adaptation of dependency parsers without annotated target corpora and parallel corpora. Previous work either directly applies a discriminative source parser to the target language, ignoring unannotated target corpora, or employs an unsupervised generative parser that can leverage unannotated target data but has weaker representational power than discriminative parsers. In this paper, we propose to utilize unsupervised discriminative parsers based on the CRF autoencoder framework for this task. We train a source parser and use it to initialize and regularize a target parser that is trained on unannotated target data. We conduct experiments that transfer an English parser to 20 target languages. The results show that our method significantly outperforms previous methods.","authors":["Zhao Li","Kewei Tu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.193","program":"findings","sessions":[],"similar_paper_uids":["findings.193"],"title":"Unsupervised Cross-Lingual Adaptation of Dependency Parsers Using CRF Autoencoders","tldr":"We consider the task of cross-lingual adaptation of dependency parsers without annotated target corpora and parallel corpora. Previous work either directly applies a discriminative source parser to the target language, ignoring unannotated target cor...","track":"Findings of EMNLP"},"forum":"findings.193","id":"findings.193","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.194.png","content":{"abstract":"Generating questions based on answers and relevant contexts is a challenging task. Recent work mainly pays attention to the quality of a single generated question. However, question generation is actually a one-to-many problem, as it is possible to raise questions with different focuses on contexts and various means of expression. In this paper, we explore the diversity of question generation and come up with methods from these two aspects. Specifically, we relate contextual focuses with content selectors, which are modeled by a continuous latent variable with the technique of conditional variational auto-encoder (CVAE). In the realization of CVAE, a multimodal prior distribution is adopted to allow for more diverse content selectors. To take into account various means of expression, question types are explicitly modeled and a diversity-promoting algorithm is proposed further. Experimental results on public datasets show that our proposed method can significantly improve the diversity of generated questions, especially from the perspective of using different question types. Overall, our proposed method achieves a better trade-off between generation quality and diversity compared with existing approaches.","authors":["Zhen Wang","Siwei Rao","Jie Zhang","Zhen Qin","Guangjian Tian","Jun Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.194","program":"findings","sessions":[],"similar_paper_uids":["findings.194"],"title":"Diversify Question Generation with Continuous Content Selectors and Question Type Modeling","tldr":"Generating questions based on answers and relevant contexts is a challenging task. Recent work mainly pays attention to the quality of a single generated question. However, question generation is actually a one-to-many problem, as it is possible to r...","track":"Findings of EMNLP"},"forum":"findings.194","id":"findings.194","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.195.png","content":{"abstract":"Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved. \u2018Low-resourced\u2019-ness is a complex problem going beyond data availability and reflects systemic problems in society. In this paper, we focus on the task of Machine Translation (MT), that plays a crucial role for information accessibility and communication worldwide. Despite immense improvements in MT over the past decade, MT is centered around a few high-resourced languages. As MT researchers cannot solve the problem of low-resourcedness alone, we propose participatory research as a means to involve all necessary agents required in the MT development process. We demonstrate the feasibility and scalability of participatory research with a case study on MT for African languages. Its implementation leads to a collection of novel translation datasets, MT benchmarks for over 30 languages, with human evaluations for a third of them, and enables participants without formal training to make a unique scientific contribution. Benchmarks, models, data, code, and evaluation results are released at https://github.com/masakhane-io/masakhane-mt.","authors":["Wilhelmina Nekoto","Vukosi Marivate","Tshinondiwa Matsila","Timi Fasubaa","Taiwo Fagbohungbe","Solomon Oluwole Akinola","Shamsuddeen Muhammad","Salomon Kabongo Kabenamualu","Salomey Osei","Freshia Sackey","Rubungo Andre Niyongabo","Ricky Macharm","Perez Ogayo","Orevaoghene Ahia","Musie Meressa Berhe","Mofetoluwa Adeyemi","Masabata Mokgesi-Selinga","Lawrence Okegbemi","Laura Martinus","Kolawole Tajudeen","Kevin Degila","Kelechi Ogueji","Kathleen Siminyu","Julia Kreutzer","Jason Webster","Jamiil Toure Ali","Jade Abbott","Iroro Orife","Ignatius Ezeani","Idris Abdulkadir Dangana","Herman Kamper","Hady Elsahar","Goodness Duru","Ghollah Kioko","Murhabazi Espoir","Elan van Biljon","Daniel Whitenack","Christopher Onyefuluchi","Chris Chinenye Emezue","Bonaventure F. P. Dossou","Blessing Sibanda","Blessing Bassey","Ayodele Olabiyi","Arshath Ramkilowan","Alp \u00d6ktem","Adewale Akinfaderin","Abdallah Bashir"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.195","program":"findings","sessions":[],"similar_paper_uids":["findings.195"],"title":"Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages","tldr":"Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved. \u2018Low-resourced\u2019-ness is a complex problem going beyond data availability and reflects systemic proble...","track":"Findings of EMNLP"},"forum":"findings.195","id":"findings.195","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.196.png","content":{"abstract":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse data sets. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications.","authors":["Matthew Henderson","I\u00f1igo Casanueva","Nikola Mrk\u0161i\u0107","Pei-Hao Su","Tsung-Hsien Wen","Ivan Vuli\u0107"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.196","program":"findings","sessions":[],"similar_paper_uids":["findings.196"],"title":"ConveRT: Efficient and Accurate Conversational Representations from Transformers","tldr":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers)...","track":"Findings of EMNLP"},"forum":"findings.196","id":"findings.196","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.197.png","content":{"abstract":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an end-to-end deep learning framework of the quality estimation and automatic post-editing of the machine translation output. Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model. To imitate the behavior of human translators, we design three efficient delegation modules \u2013 quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them. We examine this approach with the English\u2013German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance. We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation.","authors":["Ke Wang","Jiayi Wang","Niyu Ge","Yangbin Shi","Yu Zhao","Kai Fan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.197","program":"findings","sessions":[],"similar_paper_uids":["findings.197"],"title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing","tldr":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by p...","track":"Findings of EMNLP"},"forum":"findings.197","id":"findings.197","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.198.png","content":{"abstract":"Extracting rationales can help human understand which information the model utilizes and how it makes the prediction towards better interpretability. However, annotating rationales requires much effort and only few datasets contain such labeled rationales, making supervised learning for rationalization difficult. In this paper, we propose a novel approach that leverages the benefits of both multi-task learning and transfer learning for generating rationales through question answering in a zero-shot fashion. For two benchmark rationalization datasets, the proposed method achieves comparable or even better performance of rationalization without any supervised signal, demonstrating the great potential of zero-shot rationalization for better interpretability.","authors":["Po-Nien Kung","Tse-Hsuan Yang","Yi-Cheng Chen","Sheng-Siang Yin","Yun-Nung Chen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.198","program":"findings","sessions":[],"similar_paper_uids":["findings.198"],"title":"Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering","tldr":"Extracting rationales can help human understand which information the model utilizes and how it makes the prediction towards better interpretability. However, annotating rationales requires much effort and only few datasets contain such labeled ratio...","track":"Findings of EMNLP"},"forum":"findings.198","id":"findings.198","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.199.png","content":{"abstract":"Abstract Meaning Representation (AMR) parsing aims at converting sentences into AMR representations. These are graphs and not trees because AMR supports reentrancies (nodes with more than one parent). Following previous findings on the importance of reen- trancies for AMR, we empirically find and discuss several linguistic phenomena respon- sible for reentrancies in AMR, some of which have not received attention before. We cate- gorize the types of errors AMR parsers make with respect to reentrancies. Furthermore, we find that correcting these errors provides an in- crease of up to 5% Smatch in parsing perfor- mance and 20% in reentrancy prediction","authors":["Marco Damonte","Ida Szubert","Shay B. Cohen","Mark Steedman"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.199","program":"findings","sessions":[],"similar_paper_uids":["findings.199"],"title":"The Role of Reentrancies in Abstract Meaning Representation Parsing","tldr":"Abstract Meaning Representation (AMR) parsing aims at converting sentences into AMR representations. These are graphs and not trees because AMR supports reentrancies (nodes with more than one parent). Following previous findings on the importance of ...","track":"Findings of EMNLP"},"forum":"findings.199","id":"findings.199","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.200.png","content":{"abstract":"Early intervention for suicide risks with social media data has increasingly received great attention. Using a suicide dictionary created by mental health experts is one of the effective ways to detect suicidal ideation. However, little attention has been paid to validate whether and how the existing dictionaries for other languages (i.e., English and Chinese) can be used for predicting suicidal ideation for a low-resource language (i.e., Korean) where a knowledge-based suicide dictionary has not yet been developed. To this end, we propose a cross-lingual suicidal ideation detection model that can identify whether a given social media post includes suicidal ideation or not. To utilize the existing suicide dictionaries developed for other languages (i.e., English and Chinese) in word embedding, our model translates a post written in the target language (i.e., Korean) into English and Chinese, and then uses the separate suicidal-oriented word embeddings developed for English and Chinese, respectively. By applying an ensemble approach for different languages, the model achieves high accuracy, over 87%. We believe our model is useful in accessing suicidal ideation using social media data for preventing potential suicide risk in an early stage.","authors":["Daeun Lee","Soyoung Park","Jiwon Kang","Daejin Choi","Jinyoung Han"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.200","program":"findings","sessions":[],"similar_paper_uids":["findings.200"],"title":"Cross-Lingual Suicidal-Oriented Word Embedding toward Suicide Prevention","tldr":"Early intervention for suicide risks with social media data has increasingly received great attention. Using a suicide dictionary created by mental health experts is one of the effective ways to detect suicidal ideation. However, little attention has...","track":"Findings of EMNLP"},"forum":"findings.200","id":"findings.200","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.201.png","content":{"abstract":"The information retrieval from relational database requires professionals who has an understanding of structural query language such as SQL. TEXT2SQL models apply natural language inference to enable user interacting the database via natural language utterance. Current TEXT2SQL models normally focus on generating complex SQL query in a precise and complete fashion while certain features of real-world application in the production environment is not fully addressed. This paper is aimed to develop a service-oriented Text-to-SQL parser that translates natural language utterance to structural and executable SQL query. We introduce a algorithmic framework named Semantic-Enriched SQL generator (SE-SQL) that enables flexibly access database than rigid API in the application while keeping the performance quality for the most commonly used cases. The qualitative result shows that the proposed model achieves 88.3% execution accuracy on WikiSQL task, outperforming baseline by 13% error reduction. Moreover, the framework considers several service-oriented needs including low-complexity inference, out-of-table rejection, and text normalization.","authors":["Wangsu Hu","Jilei Tian"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.201","program":"findings","sessions":[],"similar_paper_uids":["findings.201"],"title":"Service-oriented Text-to-SQL Parsing","tldr":"The information retrieval from relational database requires professionals who has an understanding of structural query language such as SQL. TEXT2SQL models apply natural language inference to enable user interacting the database via natural language...","track":"Findings of EMNLP"},"forum":"findings.201","id":"findings.201","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.202.png","content":{"abstract":"Automated generation of medical reports that describe the findings in the medical images helps radiologists by alleviating their workload. Medical report generation system should generate correct and concise reports. However, data imbalance makes it difficult to train models accurately. Medical datasets are commonly imbalanced in their finding labels because incidence rates differ among diseases; moreover, the ratios of abnormalities to normalities are significantly imbalanced. We propose a novel reinforcement learning method with a reconstructor to improve the clinical correctness of generated reports to train the data-to-text module with a highly imbalanced dataset. Moreover, we introduce a novel data augmentation strategy for reinforcement learning to additionally train the model on infrequent findings. From the perspective of a practical use, we employ a Two-Stage Medical Report Generator (TS-MRGen) for controllable report generation from input images. TS-MRGen consists of two separated stages: an image diagnosis module and a data-to-text module. Radiologists can modify the image diagnosis module results to control the reports that the data-to-text module generates. We conduct an experiment with two medical datasets to assess the data-to-text module and the entire two-stage model. Results demonstrate that the reports generated by our model describe the findings in the input image more correctly.","authors":["Toru Nishino","Ryota Ozaki","Yohei Momoki","Tomoki Taniguchi","Ryuji Kano","Norihisa Nakano","Yuki Tagawa","Motoki Taniguchi","Tomoko Ohkuma","Keigo Nakamura"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.202","program":"findings","sessions":[],"similar_paper_uids":["findings.202"],"title":"Reinforcement Learning with Imbalanced Dataset for Data-to-Text Medical Report Generation","tldr":"Automated generation of medical reports that describe the findings in the medical images helps radiologists by alleviating their workload. Medical report generation system should generate correct and concise reports. However, data imbalance makes it ...","track":"Findings of EMNLP"},"forum":"findings.202","id":"findings.202","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.203.png","content":{"abstract":"It is well-known that abstractive summaries are subject to hallucination\u2014including material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to be very informative. Alternatively, one can try to avoid hallucinations by verifying that any specific entities in the summary appear in the original text in a similar context. This is the approach taken by our system, Herman. The system learns to recognize and verify quantity entities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive summaries produced by state-of-the-art models, in order to up-rank those summaries whose quantity terms are supported by the original text. Experimental results demonstrate that the ROUGE scores of such up-ranked summaries have a higher Precision than summaries that have not been up-ranked, without a comparable loss in Recall, resulting in higher F1. Preliminary human evaluation of up-ranked vs. original summaries shows people\u2019s preference for the former.","authors":["Zheng Zhao","Shay B. Cohen","Bonnie Webber"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.203","program":"findings","sessions":[],"similar_paper_uids":["findings.203"],"title":"Reducing Quantity Hallucinations in Abstractive Summarization","tldr":"It is well-known that abstractive summaries are subject to hallucination\u2014including material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to b...","track":"Findings of EMNLP"},"forum":"findings.203","id":"findings.203","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.204.png","content":{"abstract":"This paper problematizes the reliance on documents as the basic notion for defining term interactions in standard topic models. As an alternative to this practice, we reformulate topic distributions as latent factors in term similarity space. We exemplify the idea using a number of standard word embeddings built with very wide context windows. The embedding spaces are transformed to sparse similarity spaces, and topics are extracted in standard fashion by factorizing to a lower-dimensional space. We use a number of different factorization techniques, and evaluate the various models using a large set of evaluation metrics, including previously published coherence measures, as well as a number of novel measures that we suggest better correspond to real-world applications of topic models. Our results clearly demonstrate that term-based models outperform standard document-based models by a large margin.","authors":["Magnus Sahlgren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.204","program":"findings","sessions":[],"similar_paper_uids":["findings.204"],"title":"Rethinking Topic Modelling: From Document-Space to Term-Space","tldr":"This paper problematizes the reliance on documents as the basic notion for defining term interactions in standard topic models. As an alternative to this practice, we reformulate topic distributions as latent factors in term similarity space. We exem...","track":"Findings of EMNLP"},"forum":"findings.204","id":"findings.204","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.205.png","content":{"abstract":"Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model degenerates by decoding non-English texts into English regardless of the target specifier token. We present that enforcing both sparsity and decorrelation on encoder intermediate representations with the SLNI regularizer (Aljundi et al., 2019) efficiently mitigates this problem, without performance loss in supervised directions. Notably, effects of SLNI turns out to be irrelevant to promoting language-invariance in encoder representations.","authors":["Bokyung Son","Sungwon Lyu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.205","program":"findings","sessions":[],"similar_paper_uids":["findings.205"],"title":"Sparse and Decorrelated Representations for Stable Zero-shot NMT","tldr":"Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model dege...","track":"Findings of EMNLP"},"forum":"findings.205","id":"findings.205","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.206.png","content":{"abstract":"Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural language processing (NLP) applications. A potential solution is to either manually create or crowd-source the code-mixed labelled data for the task at hand, but that requires much human efforts and often not feasible because of the language specific diversity in the code-mixed text. To circumvent the data scarcity issue, we propose an effective deep learning approach for automatically generating the code-mixed text from English to multiple languages without any parallel data. In order to train the neural network, we create synthetic code-mixed texts from the available parallel corpus by modelling various linguistic properties of code-mixing. Our codemixed text generator is built upon the encoder-decoder framework, where the encoder is augmented with the linguistic and task-agnostic features obtained from the transformer based language model. We also transfer the knowledge from a neural machine translation (NMT) to warm-start the training of code-mixed generator. Experimental results and in-depth analysis show the effectiveness of our proposed code-mixed text generation on eight diverse language pairs.","authors":["Deepak Gupta","Asif Ekbal","Pushpak Bhattacharyya"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.206","program":"findings","sessions":[],"similar_paper_uids":["findings.206"],"title":"A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning","tldr":"Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based mo...","track":"Findings of EMNLP"},"forum":"findings.206","id":"findings.206","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.207.png","content":{"abstract":"Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.","authors":["Bin He","Di Zhou","Jinghui Xiao","Xin Jiang","Qun Liu","Nicholas Jing Yuan","Tong Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.207","program":"findings","sessions":[],"similar_paper_uids":["findings.207"],"title":"BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models","tldr":"Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat...","track":"Findings of EMNLP"},"forum":"findings.207","id":"findings.207","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.208.png","content":{"abstract":"We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks: SCAN, where it outperforms previous models on the LENGTH split, and English question formation, where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset, and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.","authors":["Shawn Tan","Yikang Shen","Alessandro Sordoni","Aaron Courville","Timothy J. O\u2019Donnell"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.208","program":"findings","sessions":[],"similar_paper_uids":["findings.208"],"title":"Recursive Top-Down Production for Sentence Generation with Latent Trees","tldr":"We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to comput...","track":"Findings of EMNLP"},"forum":"findings.208","id":"findings.208","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.209.png","content":{"abstract":"Reinforcement learning methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed by human experts, which requires domain knowledge. Recently, a number of adversarial learning methods have been proposed to learn the reward function together with the dialogue policy. However, to alternatively update the dialogue policy and the reward model on the fly, we are limited to policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the alternating training of a dialogue agent and the reward model can easily get stuck in local optima or result in mode collapse. To overcome the listed issues, we propose to decompose the adversarial training into two steps. First, we train the discriminator with an auxiliary dialogue generator and then incorporate a derived reward model into a common reinforcement learning method to guide the dialogue policy learning. This approach is applicable to both on-policy and off-policy reinforcement learning methods. Based on our extensive experimentation, we can conclude the proposed method: (1) achieves a remarkable task success rate using both on-policy and off-policy reinforcement learning methods; and (2) has potential to transfer knowledge from existing domains to a new domain.","authors":["Ziming Li","Sungjin Lee","Baolin Peng","Jinchao Li","Julia Kiseleva","Maarten de Rijke","Shahin Shayandeh","Jianfeng Gao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.209","program":"findings","sessions":[],"similar_paper_uids":["findings.209"],"title":"Guided Dialogue Policy Learning without Adversarial Learning in the Loop","tldr":"Reinforcement learning methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes....","track":"Findings of EMNLP"},"forum":"findings.209","id":"findings.209","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.210.png","content":{"abstract":"In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue system, the user is guided by the different aspects of a product or service that regulates the conversation towards selecting the product or service. In this work, we present a multi-modal conversational framework for a task-oriented dialogue setup that generates the responses following the different aspects of a product or service to cater to the user\u2019s needs. We show that the responses guided by the aspect information provide more interactive and informative responses for better communication between the agent and the user. We first create a Multi-domain Multi-modal Dialogue (MDMMD) dataset having conversations involving both text and images belonging to the three different domains, such as restaurants, electronics, and furniture. We implement a Graph Convolutional Network (GCN) based framework that generates appropriate textual responses from the multi-modal inputs. The multi-modal information having both textual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation.","authors":["Mauajama Firdaus","Nidhi Thakur","Asif Ekbal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.210","program":"findings","sessions":[],"similar_paper_uids":["findings.210"],"title":"MultiDM-GCN: Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network","tldr":"In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue s...","track":"Findings of EMNLP"},"forum":"findings.210","id":"findings.210","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.211.png","content":{"abstract":"Event detection (ED), a key subtask of information extraction, aims to recognize instances of specific event types in text. Previous studies on the task have verified the effectiveness of integrating syntactic dependency into graph convolutional networks. However, these methods usually ignore dependency label information, which conveys rich and useful linguistic knowledge for ED. In this paper, we propose a novel architecture named Edge-Enhanced Graph Convolution Networks (EE-GCN), which simultaneously exploits syntactic structure and typed dependency label information to perform ED. Specifically, an edge-aware node update module is designed to generate expressive word representations by aggregating syntactically-connected words through specific dependency types. Furthermore, to fully explore clues hidden from dependency edges, a node-aware edge update module is introduced, which refines the relation representations with contextual information.These two modules are complementary to each other and work in a mutual promotion way. We conduct experiments on the widely used ACE2005 dataset and the results show significant improvement over competitive baseline methods.","authors":["Shiyao Cui","Bowen Yu","Tingwen Liu","Zhenyu Zhang","Xuebin Wang","Jinqiao Shi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.211","program":"findings","sessions":[],"similar_paper_uids":["findings.211"],"title":"Edge-Enhanced Graph Convolution Networks for Event Detection with Syntactic Relation","tldr":"Event detection (ED), a key subtask of information extraction, aims to recognize instances of specific event types in text. Previous studies on the task have verified the effectiveness of integrating syntactic dependency into graph convolutional netw...","track":"Findings of EMNLP"},"forum":"findings.211","id":"findings.211","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.212.png","content":{"abstract":"Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.","authors":["Kunal Chawla","Diyi Yang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.212","program":"findings","sessions":[],"similar_paper_uids":["findings.212"],"title":"Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization","tldr":"Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfe...","track":"Findings of EMNLP"},"forum":"findings.212","id":"findings.212","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.213.png","content":{"abstract":"It has been demonstrated that hidden representation learned by deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.","authors":["Lingjuan Lyu","Xuanli He","Yitong Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.213","program":"findings","sessions":[],"similar_paper_uids":["findings.213"],"title":"Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness","tldr":"It has been demonstrated that hidden representation learned by deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach cal...","track":"Findings of EMNLP"},"forum":"findings.213","id":"findings.213","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.214.png","content":{"abstract":"When interacting with each other, we motivate, advise, inform, show love or power towards our peers. However, the way we interact may also hold some indication on how successful we are, as people often try to help each other to achieve their goals. We study the chat interactions of thousands of aspiring entrepreneurs who discuss and develop business models. We manually annotate a set of about 5,500 chat interactions with four dimensions of interaction styles (motivation, cooperation, equality, advice). We find that these styles can be reliably predicted, and that the communication styles can be used to predict a number of indices of business success. Our findings indicate that successful communicators are also successful in other domains.","authors":["Farzana Rashid","Tommaso Fornaciari","Dirk Hovy","Eduardo Blanco","Fernando Vega-Redondo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.214","program":"findings","sessions":[],"similar_paper_uids":["findings.214"],"title":"Helpful or Hierarchical? Predicting the Communicative Strategies of Chat Participants, and their Impact on Success","tldr":"When interacting with each other, we motivate, advise, inform, show love or power towards our peers. However, the way we interact may also hold some indication on how successful we are, as people often try to help each other to achieve their goals. W...","track":"Findings of EMNLP"},"forum":"findings.214","id":"findings.214","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.215.png","content":{"abstract":"Task-oriented dialogue systems are either modularized with separate dialogue state tracking (DST) and management steps or end-to-end trainable. In either case, the knowledge base (KB) plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems, instead, use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets.","authors":["Andrea Madotto","Samuel Cahyawijaya","Genta Indra Winata","Yan Xu","Zihan Liu","Zhaojiang Lin","Pascale Fung"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.215","program":"findings","sessions":[],"similar_paper_uids":["findings.215"],"title":"Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems","tldr":"Task-oriented dialogue systems are either modularized with separate dialogue state tracking (DST) and management steps or end-to-end trainable. In either case, the knowledge base (KB) plays an essential role in fulfilling user requests. Modularized s...","track":"Findings of EMNLP"},"forum":"findings.215","id":"findings.215","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.216.png","content":{"abstract":"With the epidemic of COVID-19, verifying the scientifically false online information, such as fake news and maliciously fabricated statements, has become crucial. However, the lack of training data in the scientific domain limits the performance of fact verification models. This paper proposes an in-domain language modeling method for fact extraction and verification systems. We come up with SciKGAT to combine the advantages of open-domain literature search, state-of-the-art fact verification systems and in-domain medical knowledge through language modeling. Our experiments on SCIFACT, a dataset of expert-written scientific fact verification, show that SciKGAT achieves 30% absolute improvement on precision. Our analyses show that such improvement thrives from our in-domain language model by picking up more related evidence pieces and accurate fact verification. Our codes and data are released via Github.","authors":["Zhenghao Liu","Chenyan Xiong","Zhuyun Dai","Si Sun","Maosong Sun","Zhiyuan Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.216","program":"findings","sessions":[],"similar_paper_uids":["findings.216"],"title":"Adapting Open Domain Fact Extraction and Verification to COVID-FACT through In-Domain Language Modeling","tldr":"With the epidemic of COVID-19, verifying the scientifically false online information, such as fake news and maliciously fabricated statements, has become crucial. However, the lack of training data in the scientific domain limits the performance of f...","track":"Findings of EMNLP"},"forum":"findings.216","id":"findings.216","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.217.png","content":{"abstract":"This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.","authors":["Weizhen Qi","Yu Yan","Yeyun Gong","Dayiheng Liu","Nan Duan","Jiusheng Chen","Ruofei Zhang","Ming Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.217","program":"findings","sessions":[],"similar_paper_uids":["findings.217"],"title":"ProphetNet: Predicting Future N-gram for Sequence-to-SequencePre-training","tldr":"This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-...","track":"Findings of EMNLP"},"forum":"findings.217","id":"findings.217","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.218.png","content":{"abstract":"Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on paraphrase generation mainly focus on the fidelity while ignoring the diversity of outputs. In this paper, we propose a deep generative model to generate diverse paraphrases. We build our model based on the conditional generative adversarial network, and propose to incorporate a simple yet effective diversity loss term into the model in order to improve the diversity of outputs. The proposed diversity loss maximizes the ratio of pairwise distance between the generated texts and their corresponding latent codes, forcing the generator to focus more on the latent codes and produce diverse samples. Experimental results on benchmarks of paraphrase generation show that our proposed model can generate more diverse paraphrases compared with baselines.","authors":["Yue Cao","Xiaojun Wan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.218","program":"findings","sessions":[],"similar_paper_uids":["findings.218"],"title":"DivGAN: Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network","tldr":"Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on paraphrase generation mainly focus on the fidelity while ignoring the diversity of outputs. In this paper, we propose a deep ...","track":"Findings of EMNLP"},"forum":"findings.218","id":"findings.218","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.219.png","content":{"abstract":"There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.","authors":["Andrea Madotto","Etsuko Ishii","Zhaojiang Lin","Sumanth Dathathri","Pascale Fung"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.219","program":"findings","sessions":[],"similar_paper_uids":["findings.219"],"title":"Plug-and-Play Conversational Models","tldr":"There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational mo...","track":"Findings of EMNLP"},"forum":"findings.219","id":"findings.219","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.220.png","content":{"abstract":"It is reported that financial news, especially financial events expressed in news, provide information to investors\u2019 long/short decisions and influence the movements of stock markets. Motivated by this, we leverage financial event streams to train a classification neural network that detects latent event-stock linkages and stock markets\u2019 systematic behaviours in the U.S. stock market. Our proposed pipeline includes (1) a combined event extraction method that utilizes Open Information Extraction and neural co-reference resolution, (2) a BERT/ALBERT enhanced representation of events, and (3) an extended hierarchical attention network that includes attentions on event, news and temporal levels. Our pipeline achieves significantly better accuracies and higher simulated annualized returns than state-of-the-art models when being applied to predicting Standard&Poor 500, Dow Jones, Nasdaq indices and 10 individual stocks.","authors":["Xianchao Wu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.220","program":"findings","sessions":[],"similar_paper_uids":["findings.220"],"title":"Event-Driven Learning of Systematic Behaviours in Stock Markets","tldr":"It is reported that financial news, especially financial events expressed in news, provide information to investors\u2019 long/short decisions and influence the movements of stock markets. Motivated by this, we leverage financial event streams to train a ...","track":"Findings of EMNLP"},"forum":"findings.220","id":"findings.220","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.221.png","content":{"abstract":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator\u2019s goal is to convert the feedback into a response that answers the user\u2019s previous utterance and to fool the discriminator which distinguishes feedback from natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94%to 75.96% in ranking correct responses on the PERSONACHATdataset, a large improvement given that the original model is already trained on 131k samples.","authors":["Makesh Narsimhan Sreedhar","Kun Ni","Siva Reddy"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.221","program":"findings","sessions":[],"similar_paper_uids":["findings.221"],"title":"Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback","tldr":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissati...","track":"Findings of EMNLP"},"forum":"findings.221","id":"findings.221","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.222.png","content":{"abstract":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining different portions of OntoNotes with Twitter data show that selecting text genres for the training data can beat the mere maximization of training data amount. In addition, we inspect several phenomena such as the role of deictic pronouns in conversational data, and present additional results for variant settings. Our best configuration improves the performance of the\u201dout of the box\u201d system by 21.6%.","authors":["Berfin Akta\u015f","Veronika Solopova","Annalena Kohnert","Manfred Stede"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.222","program":"findings","sessions":[],"similar_paper_uids":["findings.222"],"title":"Adapting Coreference Resolution to Twitter Conversations","tldr":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter con...","track":"Findings of EMNLP"},"forum":"findings.222","id":"findings.222","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.223.png","content":{"abstract":"Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.","authors":["Chantal Amrhein","Rico Sennrich"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.223","program":"findings","sessions":[],"similar_paper_uids":["findings.223"],"title":"On Romanization for Model Transfer Between Scripts in Neural Machine Translation","tldr":"Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case whe...","track":"Findings of EMNLP"},"forum":"findings.223","id":"findings.223","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.224.png","content":{"abstract":"In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation. Current state-of-theart methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, COSMIC addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at https://github.com/declare-lab/conv-emotion.","authors":["Deepanway Ghosal","Navonil Majumder","Alexander Gelbukh","Rada Mihalcea","Soujanya Poria"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.224","program":"findings","sessions":[],"similar_paper_uids":["findings.224"],"title":"COSMIC: COmmonSense knowledge for eMotion Identification in Conversations","tldr":"In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal r...","track":"Findings of EMNLP"},"forum":"findings.224","id":"findings.224","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.225.png","content":{"abstract":"Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.","authors":["Inbar Oren","Jonathan Herzig","Nitish Gupta","Matt Gardner","Jonathan Berant"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.225","program":"findings","sessions":[],"similar_paper_uids":["findings.225"],"title":"Improving Compositional Generalization in Semantic Parsing","tldr":"Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has s...","track":"Findings of EMNLP"},"forum":"findings.225","id":"findings.225","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.226.png","content":{"abstract":"Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the \u201canswerability\u201d of the question given the extracted answer. Here we address a different problem: the tendency of existing MRC systems to produce partially correct answers when presented with answerable questions. We explore the nature of such errors and propose a post-processing correction method that yields statistically significant performance improvements over state-of-the-art MRC systems in both monolingual and multilingual evaluation.","authors":["Revanth Gangi Reddy","Md Arafat Sultan","Efsun Sarioglu Kayi","Rong Zhang","Vittorio Castelli","Avi Sil"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.226","program":"findings","sessions":[],"similar_paper_uids":["findings.226"],"title":"Answer Span Correction in Machine Reading Comprehension","tldr":"Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the \u201canswerability\u201d of the question given the extracted answer. He...","track":"Findings of EMNLP"},"forum":"findings.226","id":"findings.226","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.227.png","content":{"abstract":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.","authors":["Marius Mosbach","Anna Khokhlova","Michael A. Hedderich","Dietrich Klakow"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.227","program":"findings","sessions":[],"similar_paper_uids":["findings.227"],"title":"On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers","tldr":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, u...","track":"Findings of EMNLP"},"forum":"findings.227","id":"findings.227","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.228.png","content":{"abstract":"This paper considers the problem of zero-shot entity linking, in which a link in the test time may not present in training. Following the prevailing BERT-based research efforts, we find a simple yet effective way is to expand the long-range sequence modeling. Unlike many previous methods, our method does not require expensive pre-training of BERT with long position embeddings. Instead, we propose an efficient position embeddings initialization method called Embedding-repeat, which initializes larger position embeddings based on BERT-Base. On the zero-shot entity linking dataset, our method improves the STOA from 76.06% to 79.08%, and for its long data, the corresponding improvement is from 74.57% to 82.14%. Our experiments suggest the effectiveness of long-range sequence modeling without retraining the BERT model.","authors":["Zonghai Yao","Liangliang Cao","Huapu Pan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.228","program":"findings","sessions":[],"similar_paper_uids":["findings.228"],"title":"Zero-shot Entity Linking with Efficient Long Range Sequence Modeling","tldr":"This paper considers the problem of zero-shot entity linking, in which a link in the test time may not present in training. Following the prevailing BERT-based research efforts, we find a simple yet effective way is to expand the long-range sequence ...","track":"Findings of EMNLP"},"forum":"findings.228","id":"findings.228","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.229.png","content":{"abstract":"Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED model. This paper aims to fill this research gap by stressing the importance of robustness modeling in ED models. We first pinpoint three stark cases demonstrating the brittleness of the existing ED models. After analyzing the underlying reason, we propose a new training mechanism, called context-selective mask generalization for ED, which can effectively mine context-specific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our model regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use full context for feature learning.","authors":["Jian Liu","Yubo Chen","Kang Liu","Yantao Jia","Zhicheng Sheng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.229","program":"findings","sessions":[],"similar_paper_uids":["findings.229"],"title":"How Does Context Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization","tldr":"Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED mo...","track":"Findings of EMNLP"},"forum":"findings.229","id":"findings.229","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.230.png","content":{"abstract":"Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to ASR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech EnFr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84% temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation).","authors":["Biao Zhang","Ivan Titov","Barry Haddow","Rico Sennrich"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.230","program":"findings","sessions":[],"similar_paper_uids":["findings.230"],"title":"Adaptive Feature Selection for End-to-End Speech Translation","tldr":"Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-...","track":"Findings of EMNLP"},"forum":"findings.230","id":"findings.230","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.231.png","content":{"abstract":"Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the encoder and decoder and utilizing a decoding controller to aggregate the decoder\u2019s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets: Multi-News and DUC-04. Experimental results show the efficacy of our approach, and it can substantially outperform several strong baselines. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.","authors":["Hanqi Jin","Xiaojun Wan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.231","program":"findings","sessions":[],"similar_paper_uids":["findings.231"],"title":"Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization","tldr":"Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-docu...","track":"Findings of EMNLP"},"forum":"findings.231","id":"findings.231","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.232.png","content":{"abstract":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.","authors":["Jiezhong Qiu","Hao Ma","Omer Levy","Wen-tau Yih","Sinong Wang","Jie Tang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.232","program":"findings","sessions":[],"similar_paper_uids":["findings.232"],"title":"Blockwise Self-Attention for Long Document Understanding","tldr":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/infere...","track":"Findings of EMNLP"},"forum":"findings.232","id":"findings.232","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.233.png","content":{"abstract":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of severe information loss. In this paper, we present a simple but effective unsupervised neural generative semantic hashing method with a focus on few-bits hashing. Our model is built upon variational autoencoder and represents each hash bit as a Bernoulli variable, which allows the model to be end-to-end trainable. To address the issue of information loss, we introduce a set of auxiliary implicit topic vectors. With the aid of these topic vectors, the generated hash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves significant improvements over state-of-the-art semantic hashing methods in few-bits hashing.","authors":["Fanghua Ye","Jarana Manotumruksa","Emine Yilmaz"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.233","program":"findings","sessions":[],"similar_paper_uids":["findings.233"],"title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling","tldr":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guarante...","track":"Findings of EMNLP"},"forum":"findings.233","id":"findings.233","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.234.png","content":{"abstract":"Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of containing several opinion factors, the complete AFOE task is usually divided into multiple subtasks and achieved in the pipeline. However, pipeline approaches easily suffer from error propagation and inconvenience in real-world scenarios. To this end, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end fashion only with one unified grid tagging task. Additionally, we design an effective inference strategy on GTS to exploit mutual indication between different opinion factors for more accurate extractions. To validate the feasibility and compatibility of GTS, we implement three different GTS models respectively based on CNN, BiLSTM, and BERT, and conduct experiments on the aspect-oriented opinion pair extraction and opinion triplet extraction datasets. Extensive experimental results indicate that GTS models outperform strong baselines significantly and achieve state-of-the-art performance.","authors":["Zhen Wu","Chengcan Ying","Fei Zhao","Zhifang Fan","Xinyu Dai","Rui Xia"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.234","program":"findings","sessions":[],"similar_paper_uids":["findings.234"],"title":"Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction","tldr":"Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of contain...","track":"Findings of EMNLP"},"forum":"findings.234","id":"findings.234","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.235.png","content":{"abstract":"Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.","authors":["Chengyue Jiang","Zhonglin Nian","Kaihao Guo","Shanbo Chu","Yinggong Zhao","Libin Shen","Kewei Tu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.235","program":"findings","sessions":[],"similar_paper_uids":["findings.235"],"title":"Learning Numeral Embedding","tldr":"Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words,...","track":"Findings of EMNLP"},"forum":"findings.235","id":"findings.235","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.236.png","content":{"abstract":"The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the representations of the contextual words as input. Our extensive experiments show that the decomposed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance.","authors":["Zechuan Hu","Yong Jiang","Nguyen Bach","Tao Wang","Zhongqiang Huang","Fei Huang","Kewei Tu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.236","program":"findings","sessions":[],"similar_paper_uids":["findings.236"],"title":"An Investigation of Potential Function Designs for Neural CRF","tldr":"The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and tra...","track":"Findings of EMNLP"},"forum":"findings.236","id":"findings.236","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.237.png","content":{"abstract":"Recently, end-to-end neural network-based approaches have shown significant improvements over traditional pipeline-based models in English coreference resolution. However, such advancements came at a cost of computational complexity and recent works have not focused on tackling this problem. Hence, in this paper, to cope with this issue, we propose BERT-SRU-based Pointer Networks that leverages the linguistic property of head-final languages. Applying this model to the Korean coreference resolution, we significantly reduce the coreference linking search space. Combining this with Ensemble Knowledge Distillation, we maintain state-of-the-art performance 66.9% of CoNLL F1 on ETRI test set while achieving 2x speedup (30 doc/sec) in document processing time.","authors":["Cheoneum Park","Jamin Shin","Sungjoon Park","Joonho Lim","Changki Lee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.237","program":"findings","sessions":[],"similar_paper_uids":["findings.237"],"title":"Fast End-to-end Coreference Resolution for Korean","tldr":"Recently, end-to-end neural network-based approaches have shown significant improvements over traditional pipeline-based models in English coreference resolution. However, such advancements came at a cost of computational complexity and recent works ...","track":"Findings of EMNLP"},"forum":"findings.237","id":"findings.237","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.238.png","content":{"abstract":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-based representation (e.g. \u201cI have two cats.\u201d). We argue that these representations remain superficial w.r.t. the complexity of human personality. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, values, and beliefs to drive language generation. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.","authors":["Thomas Scialom","Serra Sinem Tekiro\u011flu","Jacopo Staiano","Marco Guerini"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.238","program":"findings","sessions":[],"similar_paper_uids":["findings.238"],"title":"Toward Stance-based Personas for Opinionated Dialogues","tldr":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-ba...","track":"Findings of EMNLP"},"forum":"findings.238","id":"findings.238","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.239.png","content":{"abstract":"Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE). SILICONE is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.","authors":["Emile Chapuis","Pierre Colombo","Matteo Manica","Matthieu Labeau","Chlo\u00e9 Clavel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.239","program":"findings","sessions":[],"similar_paper_uids":["findings.239"],"title":"Hierarchical Pre-training for Sequence Labelling in Spoken Dialog","tldr":"Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new ...","track":"Findings of EMNLP"},"forum":"findings.239","id":"findings.239","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.240.png","content":{"abstract":"Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT E-MBERT so it can benefit any new language, and show that our approach aids languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6% F1 on M-BERT languages and 23% F1 increase on new languages. We release models and code at http://cogcomp.org/page/publication_view/912.","authors":["Zihan Wang","Karthikeyan K","Stephen Mayhew","Dan Roth"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.240","program":"findings","sessions":[],"similar_paper_uids":["findings.240"],"title":"Extending Multilingual BERT to Low-Resource Languages","tldr":"Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but...","track":"Findings of EMNLP"},"forum":"findings.240","id":"findings.240","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.241.png","content":{"abstract":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.","authors":["Marjan Albooyeh","Rishab Goel","Seyed Mehran Kazemi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.241","program":"findings","sessions":[],"similar_paper_uids":["findings.241"],"title":"Out-of-Sample Representation Learning for Knowledge Graphs","tldr":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for ...","track":"Findings of EMNLP"},"forum":"findings.241","id":"findings.241","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.242.png","content":{"abstract":"Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as adjectives and verbs. In this paper, we propose a model that uses finer-grained visual information from different parts of the image, using automatic object proposals. In experiments on the Flickr8K Audio Captions Corpus, we find that our model improves over approaches that use global visual features, that the proposals enable the model to recover entities and other related words, such as adjectives, and that improvements are due to the model\u2019s ability to localize the correct proposals.","authors":["Tejas Srinivasan","Ramon Sanabria","Florian Metze","Desmond Elliott"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.242","program":"findings","sessions":[],"similar_paper_uids":["findings.242"],"title":"Fine-Grained Grounding for Multimodal Speech Recognition","tldr":"Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that hav...","track":"Findings of EMNLP"},"forum":"findings.242","id":"findings.242","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.243.png","content":{"abstract":"Approaching new data can be quite deterrent; you do not know how your categories of interest are realized in it, commonly, there is no labeled data at hand, and the performance of domain adaptation methods is unsatisfactory. Aiming to assist domain experts in their first steps into a new task over a new corpus, we present an unsupervised approach to reveal complex rules which cluster the unexplored corpus by its prominent categories (or facets). These rules are human-readable, thus providing an important ingredient which has become in short supply lately - explainability. Each rule provides an explanation for the commonality of all the texts it clusters together. The experts can then identify which rules best capture texts of their categories of interest, and utilize them to deepen their understanding of these categories. These rules can also bootstrap the process of data labeling by pointing at a subset of the corpus which is enriched with texts demonstrating the target categories. We present an extensive evaluation of the usefulness of these rules in identifying target categories, as well as a user study which assesses their interpretability.","authors":["Eyal Shnarch","Leshem Choshen","Guy Moshkowich","Ranit Aharonov","Noam Slonim"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.243","program":"findings","sessions":[],"similar_paper_uids":["findings.243"],"title":"Unsupervised Expressive Rules Provide Explainability and Assist Human Experts Grasping New Domains","tldr":"Approaching new data can be quite deterrent; you do not know how your categories of interest are realized in it, commonly, there is no labeled data at hand, and the performance of domain adaptation methods is unsatisfactory. Aiming to assist domain e...","track":"Findings of EMNLP"},"forum":"findings.243","id":"findings.243","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.244.png","content":{"abstract":"Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.","authors":["Bertrand Higy","Desmond Elliott","Grzegorz Chrupa\u0142a"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.244","program":"findings","sessions":[],"similar_paper_uids":["findings.244"],"title":"Textual Supervision for Visually Grounded Spoken Language Understanding","tldr":"Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. ...","track":"Findings of EMNLP"},"forum":"findings.244","id":"findings.244","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.245.png","content":{"abstract":"This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions. Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly comparing BERT relations to Universal Dependencies (UD) annotations, showing that they often do not match one-to-one. We suggest a method for relation identification and syntactic tree construction. Our approach produces significantly more consistent dependency trees than previous work, showing that it better explains the syntactic abstractions in BERT. At the same time, it can be successfully applied with only a minimal amount of supervision and generalizes well across languages.","authors":["Tomasz Limisiewicz","David Mare\u010dek","Rudolf Rosa"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.245","program":"findings","sessions":[],"similar_paper_uids":["findings.245"],"title":"Universal Dependencies According to BERT: Both More Specific and More General","tldr":"This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions. Previous work showed that individual BERT heads tend to encode particular dependency relation ty...","track":"Findings of EMNLP"},"forum":"findings.245","id":"findings.245","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.246.png","content":{"abstract":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH assumes that a context surrounding a hyponym is more informative than that of a hypernym, it has never been tested on visual objects. Since our perception is tightly associated with language, it is meaningful to explore whether the DIH holds on visual objects. To this end, we consider visual objects as the context of a word and represent a word as a bag of visual objects found in images associated with the word. This allows us to test the feasibility of the visual DIH. To better distinguish word pairs in a hypernym relation from other relations such as co-hypernyms, we also propose a new measurable function that takes into account both the difference in the generality of meaning and similarity of meaning between words. Our experimental results show that the DIH holds on visual objects and that the proposed method combined with the proposed function outperforms existing unsupervised representation methods.","authors":["Masayasu Muraoka","Tetsuya Nasukawa","Bishwaranjan Bhattacharjee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.246","program":"findings","sessions":[],"similar_paper_uids":["findings.246"],"title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment","tldr":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH as...","track":"Findings of EMNLP"},"forum":"findings.246","id":"findings.246","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.247.png","content":{"abstract":"Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectures that do not adequately model human generation processes. To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach.","authors":["Sashank Santhanam","Zhuo Cheng","Brodie Mather","Bonnie Dorr","Archna Bhatia","Bryanna Hebenstreit","Alan Zemel","Adam Dalton","Tomek Strzalkowski","Samira Shaikh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.247","program":"findings","sessions":[],"similar_paper_uids":["findings.247"],"title":"Learning to Plan and Realize Separately for Open-Ended Dialogue Systems","tldr":"Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectu...","track":"Findings of EMNLP"},"forum":"findings.247","id":"findings.247","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.248.png","content":{"abstract":"This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results.","authors":["Sandro Pezzelle","Claudio Greco","Greta Gandolfi","Eleonora Gualdoni","Raffaella Bernardi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.248","program":"findings","sessions":[],"similar_paper_uids":["findings.248"],"title":"Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision","tldr":"This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitabl...","track":"Findings of EMNLP"},"forum":"findings.248","id":"findings.248","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.249.png","content":{"abstract":"We tackle the challenge of cross-lingual training of neural document ranking models for mono-lingual retrieval, specifically leveraging relevance judgments in English to improve search in non-English languages. Our work successfully applies multi-lingual BERT (mBERT) to document ranking and additionally compares against a number of alternatives: translating the training data, translating documents, multi-stage hybrids, and ensembles. Experiments on test collections in six different languages from diverse language families reveal many interesting findings: model-based relevance transfer using mBERT can significantly improve search quality in (non-English) mono-lingual retrieval, but other \u201clow resource\u201d approaches are competitive as well.","authors":["Peng Shi","He Bai","Jimmy Lin"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.249","program":"findings","sessions":[],"similar_paper_uids":["findings.249"],"title":"Cross-Lingual Training of Neural Models for Document Ranking","tldr":"We tackle the challenge of cross-lingual training of neural document ranking models for mono-lingual retrieval, specifically leveraging relevance judgments in English to improve search in non-English languages. Our work successfully applies multi-lin...","track":"Findings of EMNLP"},"forum":"findings.249","id":"findings.249","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.250.png","content":{"abstract":"Word-embeddings are vital components of Natural Language Processing (NLP) models and have been extensively explored. However, they consume a lot of memory which poses a challenge for edge deployment. Embedding matrices, typically, contain most of the parameters for language models and about a third for machine translation systems. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition and knowledge distillation. First, we initialize the weights of our decomposed matrices by learning to reconstruct the full pre-trained word-embedding and then fine-tune end-to-end, employing knowledge distillation on the factorized embedding. We conduct extensive experiments with various compression rates on machine translation and language modeling, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique is simple to replicate, with one fixed parameter controlling compression size, has higher BLEU score on translation and lower perplexity on language modeling compared to complex, difficult to tune state-of-the-art methods.","authors":["Vasileios Lioutas","Ahmad Rashid","Krtin Kumar","Md. Akmal Haidar","Mehdi Rezagholizadeh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.250","program":"findings","sessions":[],"similar_paper_uids":["findings.250"],"title":"Improving Word Embedding Factorization for Compression Using Distilled Nonlinear Neural Decomposition","tldr":"Word-embeddings are vital components of Natural Language Processing (NLP) models and have been extensively explored. However, they consume a lot of memory which poses a challenge for edge deployment. Embedding matrices, typically, contain most of the...","track":"Findings of EMNLP"},"forum":"findings.250","id":"findings.250","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.251.png","content":{"abstract":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce additional errors that can lead to potentially severe health outcomes. We propose a novel machine translation-based approach, PharmMT, to automatically and reliably simplify prescription directions into patient-friendly language, thereby significantly reducing pharmacist workload. We evaluate the proposed approach over a dataset consisting of over 530K prescriptions obtained from a large mail-order pharmacy. The end-to-end system achieves a BLEU score of 60.27 against the reference directions generated by pharmacists, a 39.6% relative improvement over the rule-based normalization. Pharmacists judged 94.3% of the simplified directions as usable as-is or with minimal changes. This work demonstrates the feasibility of a machine translation-based tool for simplifying prescription directions in real-life.","authors":["Jiazhao Li","Corey Lester","Xinyan Zhao","Yuting Ding","Yun Jiang","V.G.Vinod Vydiswaran"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.251","program":"findings","sessions":[],"similar_paper_uids":["findings.251"],"title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions","tldr":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce ...","track":"Findings of EMNLP"},"forum":"findings.251","id":"findings.251","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.252.png","content":{"abstract":"Recent work in NLP shows that LSTM language models capture compositional structure in language data. In contrast to existing work, we consider the learning process that leads to compositional behavior. For a closer look at how an LSTM\u2019s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence (DI) between word meanings in an LSTM, based on their gate interactions. We support this measure with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than on learning the longer-range relations independently.","authors":["Naomi Saphra","Adam Lopez"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.252","program":"findings","sessions":[],"similar_paper_uids":["findings.252"],"title":"LSTMs Compose\u2014and Learn\u2014Bottom-Up","tldr":"Recent work in NLP shows that LSTM language models capture compositional structure in language data. In contrast to existing work, we consider the learning process that leads to compositional behavior. For a closer look at how an LSTM\u2019s sequential re...","track":"Findings of EMNLP"},"forum":"findings.252","id":"findings.252","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.253.png","content":{"abstract":"Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale\u02c6VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. In addition, we find that integration of richer semantic and pragmatic visual features improves visual fidelity of rationales.","authors":["Ana Marasovi\u0107","Chandra Bhagavatula","Jae sung Park","Ronan Le Bras","Noah A. Smith","Yejin Choi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.253","program":"findings","sessions":[],"similar_paper_uids":["findings.253"],"title":"Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs","tldr":"Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first stu...","track":"Findings of EMNLP"},"forum":"findings.253","id":"findings.253","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.254.png","content":{"abstract":"Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other datasets. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization_bias.git","authors":["Alvin Dey","Tanya Chowdhury","Yash Kumar","Tanmoy Chakraborty"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.254","program":"findings","sessions":[],"similar_paper_uids":["findings.254"],"title":"Corpora Evaluation and System Bias Detection in Multi-document Summarization","tldr":"Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard...","track":"Findings of EMNLP"},"forum":"findings.254","id":"findings.254","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.255.png","content":{"abstract":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.","authors":["Shucheng Li","Lingfei Wu","Shiwei Feng","Fangli Xu","Fengyuan Xu","Sheng Zhong"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.255","program":"findings","sessions":[],"similar_paper_uids":["findings.255"],"title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem","tldr":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as se...","track":"Findings of EMNLP"},"forum":"findings.255","id":"findings.255","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.256.png","content":{"abstract":"Neural Machine Translation (NMT) models often lack diversity in their generated translations, even when paired with search algorithm, like beam search. A challenge is that the diversity in translations are caused by the variability in the target language, and cannot be inferred from the source sentence alone. In this paper, we propose to explicitly model this one-to-many mapping by conditioning the decoder of a NMT model on a latent variable that represents the domain of target sentences. The domain is a discrete variable generated by a target encoder that is jointly trained with the NMT model.The predicted domain of target sentences are given as input to the decoder during training. At inference, we can generate diverse translations by decoding with different domains. Unlike our strongest baseline (Shen et al., 2019), our method can scale to any number of domains without affecting the performance or the training time. We assess the quality and diversity of translations generated by our model with several metrics, on three different datasets.","authors":["Marie-Anne Lachaux","Armand Joulin","Guillaume Lample"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.256","program":"findings","sessions":[],"similar_paper_uids":["findings.256"],"title":"Target Conditioning for One-to-Many Generation","tldr":"Neural Machine Translation (NMT) models often lack diversity in their generated translations, even when paired with search algorithm, like beam search. A challenge is that the diversity in translations are caused by the variability in the target lang...","track":"Findings of EMNLP"},"forum":"findings.256","id":"findings.256","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.257.png","content":{"abstract":"Rephrasings or paraphrases are sentences with similar meanings expressed in different ways. Visual Question Answering (VQA) models are closing the gap with the oracle performance for datasets like VQA2.0. However, these models fail to perform well on rephrasings of a question, which raises some important questions like Are these models robust towards linguistic variations? Is it the architecture or the dataset that we need to optimize? In this paper, we analyzed VQA models in the space of paraphrasing. We explored the role of language & cross-modal pre-training to investigate the robustness of VQA models towards lexical variations. Our experiments find that pre-trained language encoders generate efficient representations of question rephrasings, which help VQA models correctly infer these samples. We empirically determine why pre-training language encoders improve lexical robustness. Finally, we observe that although pre-training all VQA components obtain state-of-the-art results on the VQA-Rephrasings dataset, it still fails to completely close the performance gap between original and rephrasing validation splits.","authors":["Shailza Jolly","Shubham Kapoor"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.257","program":"findings","sessions":[],"similar_paper_uids":["findings.257"],"title":"Can Pre-training help VQA with Lexical Variations?","tldr":"Rephrasings or paraphrases are sentences with similar meanings expressed in different ways. Visual Question Answering (VQA) models are closing the gap with the oracle performance for datasets like VQA2.0. However, these models fail to perform well on...","track":"Findings of EMNLP"},"forum":"findings.257","id":"findings.257","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.258.png","content":{"abstract":"Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using a weight sharing-based approach called Efficient Neural Architecture Search (ENAS). In this work, we propose a novel architecture search algorithm called Flexible and Expressible Neural Architecture Search (FENAS), with more flexible and expressible search space than ENAS, in terms of more activation functions, input edges, and atomic operations. Also, our FENAS approach is able to reproduce the well-known LSTM and GRU architectures (unlike ENAS), and is also able to initialize with them for finding architectures more efficiently. We explore this extended search space via evolutionary search and show that FENAS performs significantly better on several popular text classification tasks and performs similar to ENAS on standard language model benchmark. Further, we present ablations and analyses on our FENAS approach.","authors":["Ramakanth Pasunuru","Mohit Bansal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.258","program":"findings","sessions":[],"similar_paper_uids":["findings.258"],"title":"FENAS: Flexible and Expressive Neural Architecture Search","tldr":"Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image cl...","track":"Findings of EMNLP"},"forum":"findings.258","id":"findings.258","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.259.png","content":{"abstract":"We present a methodological framework for inferring symmetry of verb predicates in natural language. Empirical work on predicate symmetry has taken two main approaches. The feature-based approach focuses on linguistic features pertaining to symmetry. The context-based approach denies the existence of absolute symmetry but instead argues that such inference is context dependent. We develop methods that formalize these approaches and evaluate them against a novel symmetry inference sentence (SIS) dataset comprised of 400 naturalistic usages of literature-informed verbs spanning the spectrum of symmetry-asymmetry. Our results show that a hybrid transfer learning model that integrates linguistic features with contextualized language models most faithfully predicts the empirical data. Our work integrates existing approaches to symmetry in natural language and suggests how symmetry inference can improve systematicity in state-of-the-art language models.","authors":["Chelsea Tanchip","Lei Yu","Aotao Xu","Yang Xu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.259","program":"findings","sessions":[],"similar_paper_uids":["findings.259"],"title":"Inferring symmetry in natural language","tldr":"We present a methodological framework for inferring symmetry of verb predicates in natural language. Empirical work on predicate symmetry has taken two main approaches. The feature-based approach focuses on linguistic features pertaining to symmetry....","track":"Findings of EMNLP"},"forum":"findings.259","id":"findings.259","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.260.png","content":{"abstract":"Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different tasks, which are learned together under the multi-task learning framework. In this paper, we propose a concise but effective unified model for MCCWS, which is fully-shared for all the criteria. By leveraging the powerful ability of the Transformer encoder, the proposed unified model can segment Chinese text according to a unique criterion-token indicating the output criterion. Besides, the proposed unified model can segment both simplified and traditional Chinese and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github.","authors":["Xipeng Qiu","Hengzhi Pei","Hang Yan","Xuanjing Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.260","program":"findings","sessions":[],"similar_paper_uids":["findings.260"],"title":"A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder","tldr":"Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different tasks...","track":"Findings of EMNLP"},"forum":"findings.260","id":"findings.260","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.261.png","content":{"abstract":"BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.","authors":["Ilias Chalkidis","Manos Fergadiotis","Prodromos Malakasiotis","Nikolaos Aletras","Ion Androutsopoulos"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.261","program":"findings","sessions":[],"similar_paper_uids":["findings.261"],"title":"LEGAL-BERT: The Muppets straight out of Law School","tldr":"BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT...","track":"Findings of EMNLP"},"forum":"findings.261","id":"findings.261","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.262.png","content":{"abstract":"Neural table-to-text models, which select and order salient data, as well as verbalizing them fluently via surface realization, have achieved promising progress. Based on results from previous work, the performance bottleneck of current models lies in the stage of content planing (selecting and ordering salient content from the input). That is, performance drops drastically when an oracle content plan is replaced by a model-inferred one during surface realization. In this paper, we propose to enhance neural content planning by (1) understanding data values with contextual numerical value representations that bring the sense of value comparison into content planning; (2) verifying the importance and ordering of the selected sequence of records with policy gradient. We evaluated our model on ROTOWIRE and MLB, two datasets on this task, and results show that our model outperforms existing systems with respect to content planning metrics.","authors":["Heng Gong","Wei Bi","Xiaocheng Feng","Bing Qin","Xiaojiang Liu","Ting Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.262","program":"findings","sessions":[],"similar_paper_uids":["findings.262"],"title":"Enhancing Content Planning for Table-to-Text Generation with Data Understanding and Verification","tldr":"Neural table-to-text models, which select and order salient data, as well as verbalizing them fluently via surface realization, have achieved promising progress. Based on results from previous work, the performance bottleneck of current models lies i...","track":"Findings of EMNLP"},"forum":"findings.262","id":"findings.262","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.263.png","content":{"abstract":"We introduce a new task, Contextual Text Style Transfer - translating a sentence into a desired style with its surrounding context taken into account. This brings two key challenges to existing style transfer approaches: (I) how to preserve the semantic meaning of target sentence and its consistency with surrounding context during transfer; (ii) how to train a robust model with limited labeled data accompanied by context. To realize high-quality style transfer with natural context preservation, we propose a Context-Aware Style Transfer (CAST) model, which uses two separate encoders for each input sentence and its surrounding context. A classifier is further trained to ensure contextual consistency of the generated sentence. To compensate for the lack of parallel data, additional self-reconstruction and back-translation losses are introduced to leverage non-parallel data in a semi-supervised fashion. Two new benchmarks, Enron-Context and Reddit-Context, are introduced for formality and offensiveness style transfer. Experimental results on these datasets demonstrate the effectiveness of the proposed CAST model over state-of-the-art methods across style accuracy, content preservation and contextual consistency metrics.","authors":["Yu Cheng","Zhe Gan","Yizhe Zhang","Oussama Elachqar","Dianqi Li","Jingjing Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.263","program":"findings","sessions":[],"similar_paper_uids":["findings.263"],"title":"Contextual Text Style Transfer","tldr":"We introduce a new task, Contextual Text Style Transfer - translating a sentence into a desired style with its surrounding context taken into account. This brings two key challenges to existing style transfer approaches: (I) how to preserve the seman...","track":"Findings of EMNLP"},"forum":"findings.263","id":"findings.263","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.264.png","content":{"abstract":"Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.","authors":["Jiecao Chen","Liu Yang","Karthik Raman","Michael Bendersky","Jung-Jung Yeh","Yun Zhou","Marc Najork","Danyang Cai","Ehsan Emadzadeh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.264","program":"findings","sessions":[],"similar_paper_uids":["findings.264"],"title":"DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling","tldr":"Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial d...","track":"Findings of EMNLP"},"forum":"findings.264","id":"findings.264","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.265.png","content":{"abstract":"We propose a novel approach to cross-lingual dependency parsing based on word reordering. The words in each sentence of a source language corpus are rearranged to meet the word order in a target language under the guidance of a part-of-speech based language model (LM). To obtain the highest reordering score under the LM, a population-based optimization algorithm and its genetic operators are designed to deal with the combinatorial nature of such word reordering. A parser trained on the reordered corpus then can be used to parse sentences in the target language. We demonstrate through extensive experimentation that our approach achieves better or comparable results across 25 target languages (1.73% increase in average), and outperforms a baseline by a significant margin on the languages that are greatly different from the source one. For example, when transferring the English parser to Hindi and Latin, our approach outperforms the baseline by 15.3% and 6.7% respectively.","authors":["Lu Liu","Yi Zhou","Jianhan Xu","Xiaoqing Zheng","Kai-Wei Chang","Xuanjing Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.265","program":"findings","sessions":[],"similar_paper_uids":["findings.265"],"title":"Cross-Lingual Dependency Parsing by POS-Guided Word Reordering","tldr":"We propose a novel approach to cross-lingual dependency parsing based on word reordering. The words in each sentence of a source language corpus are rearranged to meet the word order in a target language under the guidance of a part-of-speech based l...","track":"Findings of EMNLP"},"forum":"findings.265","id":"findings.265","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.266.png","content":{"abstract":"Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.","authors":["Emanuele La Malfa","Min Wu","Luca Laurenti","Benjie Wang","Anthony Hartshorn","Marta Kwiatkowska"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.266","program":"findings","sessions":[],"similar_paper_uids":["findings.266"],"title":"Assessing Robustness of Text Classification through Maximal Safe Radius Computation","tldr":"Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to p...","track":"Findings of EMNLP"},"forum":"findings.266","id":"findings.266","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.267.png","content":{"abstract":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes semi-structured commonsense inference rules and learns to incorporate them in a transformer-based reasoning cell.We assess the model\u2019s performance on two tasks that require different reasoning skills: Abductive Natural Language Inference and Counterfactual Invariance Prediction as a new task. We show that our proposed model improves performance over strong state-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we are, to the best of our knowledge, the first to demonstrate that a model that learns to perform counterfactual reasoning helps predicting the best explanation in an abductive reasoning task. We validate the robustness of the model\u2019s reasoning capabilities by perturbing the knowledge and provide qualitative analysis on the model\u2019s knowledge incorporation capabilities.","authors":["Debjit Paul","Anette Frank"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.267","program":"findings","sessions":[],"similar_paper_uids":["findings.267"],"title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention","tldr":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes se...","track":"Findings of EMNLP"},"forum":"findings.267","id":"findings.267","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.268.png","content":{"abstract":"Syntactic and pragmatic completeness is known to be important for turn-taking prediction, but so far machine learning models of turn-taking have used such linguistic information in a limited way. In this paper, we introduce TurnGPT, a transformer-based language model for predicting turn-shifts in spoken dialog. The model has been trained and evaluated on a variety of written and spoken dialog datasets. We show that the model outperforms two baselines used in prior work. We also report on an ablation study, as well as attention and gradient analyses, which show that the model is able to utilize the dialog context and pragmatic completeness for turn-taking prediction. Finally, we explore the model\u2019s potential in not only detecting, but also projecting, turn-completions.","authors":["Erik Ekstedt","Gabriel Skantze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.268","program":"findings","sessions":[],"similar_paper_uids":["findings.268"],"title":"TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog","tldr":"Syntactic and pragmatic completeness is known to be important for turn-taking prediction, but so far machine learning models of turn-taking have used such linguistic information in a limited way. In this paper, we introduce TurnGPT, a transformer-bas...","track":"Findings of EMNLP"},"forum":"findings.268","id":"findings.268","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.269.png","content":{"abstract":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classification has not been fully explored. We present the first systematic study on how data augmentation techniques impact performance across toxic language classifiers, ranging from shallow logistic regression architectures to BERT \u2013 a state-of-the-art pretrained Transformer network. We compare the performance of eight techniques on very scarce seed datasets. We show that while BERT performed the best, shallow classifiers performed comparably when trained on data augmented with a combination of three techniques, including GPT-2-generated sentences. We discuss the interplay of performance and computational overhead, which can inform the choice of techniques under different constraints.","authors":["Mika Juuti","Tommi Gr\u00f6ndahl","Adrian Flanagan","N. Asokan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.269","program":"findings","sessions":[],"similar_paper_uids":["findings.269"],"title":"A little goes a long way: Improving toxic language classification despite data scarcity","tldr":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classifi...","track":"Findings of EMNLP"},"forum":"findings.269","id":"findings.269","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.270.png","content":{"abstract":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.","authors":["Daivik Swarup","Ahsaas Bajaj","Sheshera Mysore","Tim O\u2019Gorman","Rajarshi Das","Andrew McCallum"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.270","program":"findings","sessions":[],"similar_paper_uids":["findings.270"],"title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text","tldr":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different way...","track":"Findings of EMNLP"},"forum":"findings.270","id":"findings.270","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.271.png","content":{"abstract":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational cost during inference can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an encoder and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this encoder, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher accuracy and lower computational cost once the system is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the computational cost when multiple tasks are performed on the same text.","authors":["Jingfei Du","Myle Ott","Haoran Li","Xing Zhou","Veselin Stoyanov"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.271","program":"findings","sessions":[],"similar_paper_uids":["findings.271"],"title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference","tldr":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a sin...","track":"Findings of EMNLP"},"forum":"findings.271","id":"findings.271","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.272.png","content":{"abstract":"Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences contain the word \u201cnot\u201d, and image recognition datasets can have tell-tale object-background correlations if dogs are always indoors. In this paper, we propose a method that can automatically detect and ignore these kinds of dataset-specific patterns, which we call dataset biases. Our method trains a lower capacity model in an ensemble with a higher capacity model. During training, the lower capacity model learns to capture relatively shallow correlations, which we hypothesize are likely to reflect dataset bias. This frees the higher capacity model to focus on patterns that should generalize better. We ensure the models learn non-overlapping approaches by introducing a novel method to make them conditionally independent. Importantly, our approach does not require the bias to be known in advance. We evaluate performance on synthetic datasets, and four datasets built to penalize models that exploit known biases on textual entailment, visual question answering, and image recognition tasks. We show improvement in all settings, including a 10 point gain on the visual question answering dataset.","authors":["Christopher Clark","Mark Yatskar","Luke Zettlemoyer"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.272","program":"findings","sessions":[],"similar_paper_uids":["findings.272"],"title":"Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles","tldr":"Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences conta...","track":"Findings of EMNLP"},"forum":"findings.272","id":"findings.272","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.273.png","content":{"abstract":"We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7% on in-domain problems and 24% on out-of-domain problems.","authors":["Xusen Yin","Ralph Weischedel","Jonathan May"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.273","program":"findings","sessions":[],"similar_paper_uids":["findings.273"],"title":"Learning to Generalize for Sequential Decision Making","tldr":"We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied ...","track":"Findings of EMNLP"},"forum":"findings.273","id":"findings.273","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.274.png","content":{"abstract":"The search for Participants, Interventions, and Outcomes (PIO) in clinical trial reports is a critical task in Evidence Based Medicine. For an automatic PIO extraction, high-quality corpora are needed. Obtaining such a corpus from crowdworkers, however, has been shown to be ineffective since (i) workers usually lack domain-specific expertise to conduct the task with sufficient quality, and (ii) the standard approach of annotating entire abstracts of trial reports as one task-instance (i.e. HIT) leads to an uneven distribution in task effort. In this paper, we switch from entire abstract to sentence annotation, referred to as the SenBase approach. We build upon SenBase in SenSupport, where we compensate the lack of domain-specific expertise of crowdworkers by showing for each task-instance similar sentences that are already annotated by experts. Such tailored task-instance examples are retrieved via unsupervised semantic short-text similarity (SSTS) method \u2013 and we evaluate nine methods to find an effective solution for SenSupport. We compute the Cohen\u2019s Kappa agreement between crowd-annotations and gold standard annotations and show that (i) both sentence-based approaches outperform a Baseline approach where entire abstracts are annotated; (ii) supporting annotators with tailored task-instance examples is the best performing approach with Kappa agreements of 0.78/0.75/0.69 for P, I, and O respectively.","authors":["Markus Zlabinger","Marta Sabou","Sebastian Hofst\u00e4tter","Allan Hanbury"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.274","program":"findings","sessions":[],"similar_paper_uids":["findings.274"],"title":"Effective Crowd-Annotation of Participants, Interventions, and Outcomes in the Text of Clinical Trial Reports","tldr":"The search for Participants, Interventions, and Outcomes (PIO) in clinical trial reports is a critical task in Evidence Based Medicine. For an automatic PIO extraction, high-quality corpora are needed. Obtaining such a corpus from crowdworkers, howev...","track":"Findings of EMNLP"},"forum":"findings.274","id":"findings.274","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.275.png","content":{"abstract":"Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs) have been successful in generating realistic texts across many different tasks by learning to directly minimize the difference between human-generated and synthetic text. In this work, we present an adversarial learning approach to GEC, using the generator-discriminator framework. The generator is a Transformer model, trained to produce grammatically correct sentences given grammatically incorrect ones. The discriminator is a sentence-pair classification model, trained to judge a given pair of grammatically incorrect-correct sentences on the quality of grammatical correction. We pre-train both the discriminator and the generator on parallel texts and then fine-tune them further using a policy gradient method that assigns high rewards to sentences which could be true corrections of the grammatically incorrect text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that Adversarial-GEC can achieve competitive GEC quality compared to NMT-based baselines.","authors":["Vipul Raheja","Dimitris Alikaniotis"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.275","program":"findings","sessions":[],"similar_paper_uids":["findings.275"],"title":"Adversarial Grammatical Error Correction","tldr":"Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the...","track":"Findings of EMNLP"},"forum":"findings.275","id":"findings.275","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.276.png","content":{"abstract":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.","authors":["Vikas Raunak","Siddharth Dalmia","Vivek Gupta","Florian Metze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.276","program":"findings","sessions":[],"similar_paper_uids":["findings.276"],"title":"On Long-Tailed Phenomena in Neural Machine Translation","tldr":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered ...","track":"Findings of EMNLP"},"forum":"findings.276","id":"findings.276","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.277.png","content":{"abstract":"The ability to accurately track what happens during a conversation is essential for the performance of a dialogue system. Current state-of-the-art multi-domain dialogue state trackers achieve just over 55% accuracy on the current go-to benchmark, which means that in almost every second dialogue turn they place full confidence in an incorrect dialogue state. Belief trackers, on the other hand, maintain a distribution over possible dialogue states. However, they lack in performance compared to dialogue state trackers, and do not produce well calibrated distributions. In this work we present state-of-the-art performance in calibration for multi-domain dialogue belief trackers using a calibrated ensemble of models. Our resulting dialogue belief tracker also outperforms previous dialogue belief tracking models in terms of accuracy.","authors":["Carel van Niekerk","Michael Heck","Christian Geishauser","Hsien-chin Lin","Nurul Lubis","Marco Moresi","Milica Gasic"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.277","program":"findings","sessions":[],"similar_paper_uids":["findings.277"],"title":"Knowing What You Know: Calibrating Dialogue Belief State Distributions via Ensembles","tldr":"The ability to accurately track what happens during a conversation is essential for the performance of a dialogue system. Current state-of-the-art multi-domain dialogue state trackers achieve just over 55% accuracy on the current go-to benchmark, whi...","track":"Findings of EMNLP"},"forum":"findings.277","id":"findings.277","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.278.png","content":{"abstract":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.","authors":["Giorgos Vernikos","Katerina Margatina","Alexandra Chronopoulou","Ion Androutsopoulos"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.278","program":"findings","sessions":[],"similar_paper_uids":["findings.278"],"title":"Domain Adversarial Fine-Tuning as an Effective Regularizer","tldr":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations...","track":"Findings of EMNLP"},"forum":"findings.278","id":"findings.278","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.279.png","content":{"abstract":"Semantic role labeling (SRL) identifies predicate-argument structure(s) in a given sentence. Although different languages have different argument annotations, polyglot training, the idea of training one model on multiple languages, has previously been shown to outperform monolingual baselines, especially for low resource languages. In fact, even a simple combination of data has been shown to be effective with polyglot training by representing the distant vocabularies in a shared representation space. Meanwhile, despite the dissimilarity in argument annotations between languages, certain argument labels do share common semantic meaning across languages (e.g. adjuncts have more or less similar semantic meaning across languages). To leverage such similarity in annotation space across languages, we propose a method called Cross-Lingual Argument Regularizer (CLAR). CLAR identifies such linguistic annotation similarity across languages and exploits this information to map the target language arguments using a transformation of the space on which source language arguments lie. By doing so, our experimental results show that CLAR consistently improves SRL performance on multiple languages over monolingual and polyglot baselines for low resource languages.","authors":["Ishan Jindal","Yunyao Li","Siddhartha Brahma","Huaiyu Zhu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.279","program":"findings","sessions":[],"similar_paper_uids":["findings.279"],"title":"CLAR: A Cross-Lingual Argument Regularizer for Semantic Role Labeling","tldr":"Semantic role labeling (SRL) identifies predicate-argument structure(s) in a given sentence. Although different languages have different argument annotations, polyglot training, the idea of training one model on multiple languages, has previously bee...","track":"Findings of EMNLP"},"forum":"findings.279","id":"findings.279","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.280.png","content":{"abstract":"Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a Latent Disentanglement method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a Counterfactual Generation to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.","authors":["Seungjae Shin","Kyungwoo Song","JoonHo Jang","Hyemi Kim","Weonyoung Joo","Il-Chul Moon"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.280","program":"findings","sessions":[],"similar_paper_uids":["findings.280"],"title":"Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation","tldr":"Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous...","track":"Findings of EMNLP"},"forum":"findings.280","id":"findings.280","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.281.png","content":{"abstract":"Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring and ordering a (possibly large) set of EDUs. We then present a solution for this task that combines neural dependency tree induction with pointer networks, and can be trained on large discourse treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new task compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures.","authors":["Grigorii Guz","Giuseppe Carenini"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.281","program":"findings","sessions":[],"similar_paper_uids":["findings.281"],"title":"Towards Domain-Independent Text Structuring Trainable on Large Discourse Treebanks","tldr":"Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring...","track":"Findings of EMNLP"},"forum":"findings.281","id":"findings.281","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.282.png","content":{"abstract":"There is a huge performance gap between formal and informal language understanding tasks. The recent pre-trained models that improved formal language understanding tasks did not achieve a comparable result on informal language. We propose data annealing transfer learning procedure to bridge the performance gap on informal natural language understanding tasks. It successfully utilizes a pre-trained model such as BERT in informal language. In the data annealing procedure, the training set contains mainly formal text data at first; then, the proportion of the informal text data is gradually increased during the training process. Our data annealing procedure is model-independent and can be applied to various tasks. We validate its effectiveness in exhaustive experiments. When BERT is implemented with our learning procedure, it outperforms all the state-of-the-art models on the three common informal language tasks.","authors":["Jing Gu","Zhou Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.282","program":"findings","sessions":[],"similar_paper_uids":["findings.282"],"title":"Data Annealing for Informal Language Understanding Tasks","tldr":"There is a huge performance gap between formal and informal language understanding tasks. The recent pre-trained models that improved formal language understanding tasks did not achieve a comparable result on informal language. We propose data anneal...","track":"Findings of EMNLP"},"forum":"findings.282","id":"findings.282","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.283.png","content":{"abstract":"We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT\u201914 English-French, WMT\u201916 English-German, and WMT\u201916 English-Romanian datasets in most directions.","authors":["Xavier Garcia","Pierre Foret","Thibault Sellam","Ankur Parikh"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.283","program":"findings","sessions":[],"similar_paper_uids":["findings.283"],"title":"A Multilingual View of Unsupervised Machine Translation","tldr":"We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data ...","track":"Findings of EMNLP"},"forum":"findings.283","id":"findings.283","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.284.png","content":{"abstract":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedical data, and a range of evaluation measures suited to the task. The approach is applied to two recent DWSI systems, thus demonstrating its relevance and providing an in-depth analysis of the models.","authors":["Ashjan Alsulaimani","Erwan Moreau","Carl Vogel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.284","program":"findings","sessions":[],"similar_paper_uids":["findings.284"],"title":"An Evaluation Method for Diachronic Word Sense Induction","tldr":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedi...","track":"Findings of EMNLP"},"forum":"findings.284","id":"findings.284","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.285.png","content":{"abstract":"Pretrained Language Models (PLMs) have improved the performance of natural language understanding in recent years. Such models are pretrained on large corpora, which encode the general prior knowledge of natural languages but are agnostic to information characteristic of downstream tasks. This often results in overfitting when fine-tuned with low resource datasets where task-specific information is limited. In this paper, we integrate label information as a task-specific prior into the self-attention component of pretrained BERT models. Experiments on several benchmarks and real-word datasets suggest that the proposed approach can largely improve the performance of pretrained models when fine-tuning with small datasets.","authors":["Rui Wang","Shijing Si","Guoyin Wang","Lei Zhang","Lawrence Carin","Ricardo Henao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.285","program":"findings","sessions":[],"similar_paper_uids":["findings.285"],"title":"Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning","tldr":"Pretrained Language Models (PLMs) have improved the performance of natural language understanding in recent years. Such models are pretrained on large corpora, which encode the general prior knowledge of natural languages but are agnostic to informat...","track":"Findings of EMNLP"},"forum":"findings.285","id":"findings.285","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.286.png","content":{"abstract":"Pretrained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pretrained models, especially in the era of edge computing. In this work, we propose an efficient transformer-based large-scale language representation using hardware-friendly block structure pruning. We incorporate the reweighted group Lasso into block-structured pruning for optimization. Besides the significantly reduced weight storage and computation, the proposed approach achieves high compression rates. Experimental results on different models (BERT, RoBERTa, and DistilBERT) on the General Language Understanding Evaluation (GLUE) benchmark tasks show that we achieve up to 5.0x with zero or minor accuracy degradation on certain task(s). Our proposed method is also orthogonal to existing compact pretrained language models such as DistilBERT using knowledge distillation, since a further 1.79x average compression rate can be achieved on top of DistilBERT with zero or minor accuracy degradation. It is suitable to deploy the final compressed model on resource-constrained edge devices.","authors":["Bingbing Li","Zhenglun Kong","Tianyun Zhang","Ji Li","Zhengang Li","Hang Liu","Caiwen Ding"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.286","program":"findings","sessions":[],"similar_paper_uids":["findings.286"],"title":"Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning","tldr":"Pretrained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pret...","track":"Findings of EMNLP"},"forum":"findings.286","id":"findings.286","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.287.png","content":{"abstract":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.","authors":["Zorik Gekhman","Roee Aharoni","Genady Beryozkin","Markus Freitag","Wolfgang Macherey"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.287","program":"findings","sessions":[],"similar_paper_uids":["findings.287"],"title":"KoBE: Knowledge-Based Machine Translation Evaluation","tldr":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a la...","track":"Findings of EMNLP"},"forum":"findings.287","id":"findings.287","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.288.png","content":{"abstract":"Abstract Meaning Representation (AMR) parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance boundaries of other natural language processing applications, such as machine translation or question answering. In this paper, we explore different ways in which trained models can be applied to improve AMR parsing performance, including generation of synthetic text and AMR annotations as well as refinement of actions oracle. We show that, without any additional human annotations, these techniques improve an already performant parser and achieve state-of-the-art results on AMR 1.0 and AMR 2.0.","authors":["Young-Suk Lee","Ram\u00f3n Fernandez Astudillo","Tahira Naseem","Revanth Gangi Reddy","Radu Florian","Salim Roukos"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.288","program":"findings","sessions":[],"similar_paper_uids":["findings.288"],"title":"Pushing the Limits of AMR Parsing with Self-Learning","tldr":"Abstract Meaning Representation (AMR) parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learni...","track":"Findings of EMNLP"},"forum":"findings.288","id":"findings.288","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.289.png","content":{"abstract":"Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-based literature discovery. In this paper we study the problem of conditional summarization in which content selection and surface realization are explicitly conditioned on an ad-hoc natural language question or topic description. Because of the difficulty in obtaining sufficient reference summaries to support arbitrary conditional summarization, we explore the use of multi-task fine-tuning (MTFT) on twenty-one natural language tasks to enable zero-shot conditional summarization on five tasks. We present four new summarization datasets, two novel \u201conline\u201d or adaptive task-mixing strategies, and report zero-shot performance using T5 and BART, demonstrating that MTFT can improve zero-shot summarization quality.","authors":["Travis Goodwin","Max Savery","Dina Demner-Fushman"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.289","program":"findings","sessions":[],"similar_paper_uids":["findings.289"],"title":"Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning","tldr":"Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-ba...","track":"Findings of EMNLP"},"forum":"findings.289","id":"findings.289","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.290.png","content":{"abstract":"Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on data quality and coverage. This is quite challenging since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and inconsistency of described facts. In this paper, we propose kens, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs.KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to com-bine prediction results from multiple language-specific embeddings, for which multiple en-semble techniques are investigated. Experiments on the basis of five real-world language-specific KGs show that, by effectively identifying and leveraging complementary knowledge, KEnS consistently improves state-of-the-art methods on KG completion.","authors":["Xuelu Chen","Muhao Chen","Changjun Fan","Ankith Uppunda","Yizhou Sun","Carlo Zaniolo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.290","program":"findings","sessions":[],"similar_paper_uids":["findings.290"],"title":"Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer","tldr":"Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn a...","track":"Findings of EMNLP"},"forum":"findings.290","id":"findings.290","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.291.png","content":{"abstract":"We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios: 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former scenario enables us to detect the types of biases present in the model. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second scenario is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.","authors":["Emily Sheng","Kai-Wei Chang","Prem Natarajan","Nanyun Peng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.291","program":"findings","sessions":[],"similar_paper_uids":["findings.291"],"title":"Towards Controllable Biases in Language Generation","tldr":"We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentio...","track":"Findings of EMNLP"},"forum":"findings.291","id":"findings.291","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.292.png","content":{"abstract":"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model\u2019s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","authors":["Pieter Delobelle","Thomas Winters","Bettina Berendt"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.292","program":"findings","sessions":[],"similar_paper_uids":["findings.292"],"title":"RobBERT: a Dutch RoBERTa-based Language Model","tldr":"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models...","track":"Findings of EMNLP"},"forum":"findings.292","id":"findings.292","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.293.png","content":{"abstract":"Unsupervised question answering (UQA) has been proposed to avoid the high cost of creating high-quality datasets for QA. One approach to UQA is to train a QA model with questions generated automatically. However, the generated questions are either too similar to a word sequence in the context or too drifted from the semantics of the context, thereby making it difficult to train a robust QA model. We propose a novel regularization method based on teacher-student architecture to avoid bias toward a particular question generation strategy and modulate the process of generating individual words when a question is generated. Our experiments demonstrate that we have achieved the goal of generating higher-quality questions for UQA across diverse QA datasets and tasks. We also show that this method can be useful for creating a QA model with few-shot learning.","authors":["Junmo Kang","Giwon Hong","Haritz Puerto San Roman","Sung-Hyon Myaeng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.293","program":"findings","sessions":[],"similar_paper_uids":["findings.293"],"title":"Regularization of Distinct Strategies for Unsupervised Question Generation","tldr":"Unsupervised question answering (UQA) has been proposed to avoid the high cost of creating high-quality datasets for QA. One approach to UQA is to train a QA model with questions generated automatically. However, the generated questions are either to...","track":"Findings of EMNLP"},"forum":"findings.293","id":"findings.293","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.294.png","content":{"abstract":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.","authors":["Alireza Mohammadshahi","James Henderson"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.294","program":"findings","sessions":[],"similar_paper_uids":["findings.294"],"title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing","tldr":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dep...","track":"Findings of EMNLP"},"forum":"findings.294","id":"findings.294","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.295.png","content":{"abstract":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB\u201905 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.","authors":["Piotr Szyma\u0144ski","Piotr \u017belasko","Mikolaj Morzy","Adrian Szymczak","Marzena \u017by\u0142a-Hoppe","Joanna Banaszczak","Lukasz Augustyniak","Jan Mizgajski","Yishay Carmiel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.295","program":"findings","sessions":[],"similar_paper_uids":["findings.295"],"title":"WER we are and WER we think we are","tldr":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Re...","track":"Findings of EMNLP"},"forum":"findings.295","id":"findings.295","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.296.png","content":{"abstract":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cMistaken scientists claim [...].\" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","authors":["Yiwei Luo","Dallas Card","Dan Jurafsky"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.296","program":"findings","sessions":[],"similar_paper_uids":["findings.296"],"title":"DeSMOG: Detecting Stance in Media On Global Warming","tldr":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat gl...","track":"Findings of EMNLP"},"forum":"findings.296","id":"findings.296","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.297.png","content":{"abstract":"One of the primary tasks of morphological parsers is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exist sufficient examples of the minority analyses in order to properly evaluate performance, nor to train effective classifiers. In this paper we address the issue of unbalanced morphological ambiguities in Hebrew. We offer a challenge set for Hebrew homographs \u2014 the first of its kind \u2014 containing substantial attestation of each analysis of 21 Hebrew homographs. We show that the current SOTA of Hebrew disambiguation performs poorly on cases of unbalanced ambiguity. Leveraging our new dataset, we achieve a new state-of-the-art for all 21 words, improving the overall average F1 score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research.","authors":["Avi Shmidman","Joshua Guedalia","Shaltiel Shmidman","Moshe Koppel","Reut Tsarfaty"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.297","program":"findings","sessions":[],"similar_paper_uids":["findings.297"],"title":"A Novel Challenge Set for Hebrew Morphological Disambiguation and Diacritics Restoration","tldr":"One of the primary tasks of morphological parsers is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exi...","track":"Findings of EMNLP"},"forum":"findings.297","id":"findings.297","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.298.png","content":{"abstract":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.","authors":["Zhiheng Huang","Davis Liang","Peng Xu","Bing Xiang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.298","program":"findings","sessions":[],"similar_paper_uids":["findings.298"],"title":"Improve Transformer Models with Better Relative Position Embeddings","tldr":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal...","track":"Findings of EMNLP"},"forum":"findings.298","id":"findings.298","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.299.png","content":{"abstract":"Generating a vivid, novel, and diverse essay with only several given topic words is a promising task of natural language generation. Previous work in this task exists two challenging problems: neglect of sentiment beneath the text and insufficient utilization of topic-related knowledge. Therefore, we propose a novel Sentiment Controllable topic-to- essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. We firstly inject the sentiment information into the generator for controlling sentiment for each sentence, which leads to various generated essays. Then we design a Topic Knowledge Graph enhanced decoder. Unlike existing models that use knowledge entities separately, our model treats knowledge graph as a whole and encodes more structured, connected semantic information in the graph to generate a more relevant essay. Experimental results show that our SCTKG can generate sentiment controllable essays and outperform the state-of-the-art approach in terms of topic relevance, fluency, and diversity on both automatic and human evaluation.","authors":["Lin Qiao","Jianhao Yan","Fandong Meng","Zhendong Yang","Jie Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.299","program":"findings","sessions":[],"similar_paper_uids":["findings.299"],"title":"A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph","tldr":"Generating a vivid, novel, and diverse essay with only several given topic words is a promising task of natural language generation. Previous work in this task exists two challenging problems: neglect of sentiment beneath the text and insufficient ut...","track":"Findings of EMNLP"},"forum":"findings.299","id":"findings.299","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.300.png","content":{"abstract":"Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit\u2019s life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to solve the original prediction task well (e.g., illness results in less rabbits), the explanation task - identifying the causal chain of events from perturbation to effect - remains largely unaddressed, and is the goal of this research. We present QUARTET, a system that constructs such explanations from paragraphs, by modeling the explanation task as a multitask learning problem. QUARTET constructs explanations from the sentences in the procedural text, achieving ~18 points better on explanation accuracy compared to several strong baselines on a recent process comprehension benchmark. On an end task on this benchmark, we show a surprising finding that good explanations do not have to come at the expense of end task performance, in fact leading to a 7% F1 improvement over SOTA.","authors":["Dheeraj Rajagopal","Niket Tandon","Peter Clark","Bhavana Dalvi","Eduard Hovy"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.300","program":"findings","sessions":[],"similar_paper_uids":["findings.300"],"title":"What-if I ask you to explain: Explaining the effects of perturbations in procedural text","tldr":"Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit\u2019s life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to ...","track":"Findings of EMNLP"},"forum":"findings.300","id":"findings.300","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.301.png","content":{"abstract":"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning \u201cbad\u201d words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.","authors":["Samuel Gehman","Suchin Gururangan","Maarten Sap","Yejin Choi","Noah A. Smith"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.301","program":"findings","sessions":[],"similar_paper_uids":["findings.301"],"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models","tldr":"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the eff...","track":"Findings of EMNLP"},"forum":"findings.301","id":"findings.301","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.302.png","content":{"abstract":"End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R-PRED); and the other predicts the exact duration value (E-PRED). Our best model \u2013 E-PRED, substantially outperforms previous work, and captures duration information more accurately than R-PRED. We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines.","authors":["Zonglin Yang","Xinya Du","Alexander Rush","Claire Cardie"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.302","program":"findings","sessions":[],"similar_paper_uids":["findings.302"],"title":"Improving Event Duration Prediction via Time-aware Pre-training","tldr":"End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training)...","track":"Findings of EMNLP"},"forum":"findings.302","id":"findings.302","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.303.png","content":{"abstract":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an action, and propose a composed variational natural language generator (CLANG), a transformer-based conditional variational autoencoder. CLANG utilizes two latent variables to represent the utterances corresponding to two different independent parts (domain and action) in the intent, and the latent variables are composed together to generate natural examples. Additionally, to improve the generator learning, we adopt the contrastive regularization loss that contrasts the in-class with the out-of-class utterance generation given the intent. To evaluate the quality of the generated utterances, experiments are conducted on the generalized few-shot intent detection task. Empirical results show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets.","authors":["Congying Xia","Caiming Xiong","Philip Yu","Richard Socher"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.303","program":"findings","sessions":[],"similar_paper_uids":["findings.303"],"title":"Composed Variational Natural Language Generation for Few-shot Intents","tldr":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an...","track":"Findings of EMNLP"},"forum":"findings.303","id":"findings.303","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.304.png","content":{"abstract":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets (e.g., disease, mutation) that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: (a). document-query matching (b). keyword extraction and (c). facet-conditioned abstractive summarization. The outcomes of (b) and (c) are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component (a) directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST\u2019s TREC-PM track datasets (2017\u20132019) show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: https://github.com/bionlproc/text-summ-for-doc-retrieval.","authors":["Jiho Noh","Ramakanth Kavuluru"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.304","program":"findings","sessions":[],"similar_paper_uids":["findings.304"],"title":"Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization","tldr":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patie...","track":"Findings of EMNLP"},"forum":"findings.304","id":"findings.304","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.305.png","content":{"abstract":"Many pairwise classification tasks, such as paraphrase detection and open-domain question answering, naturally have extreme label imbalance (e.g., 99.99% of examples are negatives). In contrast, many recent datasets heuristically choose examples to ensure label balance. We show that these heuristics lead to trained models that generalize poorly: State-of-the art models trained on QQP and WikiQA each have only 2.4% average precision when evaluated on realistically imbalanced test data. We instead collect training data with active learning, using a BERT-based embedding model to efficiently retrieve uncertain points from a very large pool of unlabeled utterance pairs. By creating balanced training data with more informative negative examples, active learning greatly improves average precision to 32.5% on QQP and 20.1% on WikiQA.","authors":["Stephen Mussmann","Robin Jia","Percy Liang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.305","program":"findings","sessions":[],"similar_paper_uids":["findings.305"],"title":"On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks","tldr":"Many pairwise classification tasks, such as paraphrase detection and open-domain question answering, naturally have extreme label imbalance (e.g., 99.99% of examples are negatives). In contrast, many recent datasets heuristically choose examples to e...","track":"Findings of EMNLP"},"forum":"findings.305","id":"findings.305","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.306.png","content":{"abstract":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited efforts leveraging inter-dependencies among the two granularities. We instead propose a multi-grained joint deep network to concurrently learn the ADE entity recognition and ADE sentence classification tasks. Our joint approach takes advantage of their symbiotic relationship, with a transfer of knowledge between the two levels of granularity. Our dual-attention mechanism constructs multiple distinct representations of a sentence that capture both task-specific and semantic information in the sentence, providing stronger emphasis on the key elements essential for sentence classification. Our model improves state-of- art F1-score for both tasks: (i) entity recognition of ADE words (12.5% increase) and (ii) ADE sentence classification (13.6% increase) on MADE 1.0 benchmark of EHR notes.","authors":["Susmitha Wunnava","Xiao Qin","Tabassum Kakar","Xiangnan Kong","Elke Rundensteiner"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.306","program":"findings","sessions":[],"similar_paper_uids":["findings.306"],"title":"A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events","tldr":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited ...","track":"Findings of EMNLP"},"forum":"findings.306","id":"findings.306","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.307.png","content":{"abstract":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional information retrieval step (IR) and a kNN search over a large datastore of an embedded text collection. Our contributions are as follows: i) BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training. ii) We show that BERT often identifies the correct response category (e.g., US city), but only kNN recovers the factually correct answer (e.g.,\u201cMiami\u201d). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN can easily handle facts not covered by BERT\u2019s training set, e.g., recent events.","authors":["Nora Kassner","Hinrich Sch\u00fctze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.307","program":"findings","sessions":[],"similar_paper_uids":["findings.307"],"title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA","tldr":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we comb...","track":"Findings of EMNLP"},"forum":"findings.307","id":"findings.307","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.308.png","content":{"abstract":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from \u201cgenuine\u201d ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.","authors":["Zhao Wang","Aron Culotta"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.308","program":"findings","sessions":[],"similar_paper_uids":["findings.308"],"title":"Identifying Spurious Correlations for Robust Text Classification","tldr":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we pr...","track":"Findings of EMNLP"},"forum":"findings.308","id":"findings.308","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.309.png","content":{"abstract":"We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is supported or not-supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification.","authors":["Yichen Jiang","Shikha Bordia","Zheng Zhong","Charles Dognin","Maneesh Singh","Mohit Bansal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.309","program":"findings","sessions":[],"similar_paper_uids":["findings.309"],"title":"HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification","tldr":"We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is supported ...","track":"Findings of EMNLP"},"forum":"findings.309","id":"findings.309","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.310.png","content":{"abstract":"Natural language generation (NLG) is an essential component of task-oriented dialog systems. Despite the recent success of neural approaches for NLG, they are typically developed in an offline manner for particular domains. To better fit real-life applications where new data come in a stream, we study NLG in a \u201ccontinual learning\u201d setting to expand its knowledge to new domains or functionalities incrementally. The major challenge towards this goal is catastrophic forgetting, meaning that a continually trained model tends to forget the knowledge it has learned before. To this end, we propose a method called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying prioritized historical exemplars, together with an adaptive regularization technique based on Elastic Weight Consolidation. Extensive experiments to continually learn new domains and intents are conducted on MultiWoZ-2.0 to benchmark ARPER with a wide range of techniques. Empirical results demonstrate that ARPER significantly outperforms other methods by effectively mitigating the detrimental catastrophic forgetting issue.","authors":["Fei Mi","Liangwei Chen","Mengjie Zhao","Minlie Huang","Boi Faltings"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.310","program":"findings","sessions":[],"similar_paper_uids":["findings.310"],"title":"Continual Learning for Natural Language Generation in Task-oriented Dialog Systems","tldr":"Natural language generation (NLG) is an essential component of task-oriented dialog systems. Despite the recent success of neural approaches for NLG, they are typically developed in an offline manner for particular domains. To better fit real-life ap...","track":"Findings of EMNLP"},"forum":"findings.310","id":"findings.310","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.311.png","content":{"abstract":"While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.","authors":["Tao Li","Daniel Khashabi","Tushar Khot","Ashish Sabharwal","Vivek Srikumar"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.311","program":"findings","sessions":[],"similar_paper_uids":["findings.311"],"title":"UNQOVERing Stereotyping Biases via Underspecified Questions","tldr":"While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified q...","track":"Findings of EMNLP"},"forum":"findings.311","id":"findings.311","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.312.png","content":{"abstract":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and social networks, as well as the power of modern AI to synthesize and gain insights from this data. This paper presents an approach to detect emotional and informational self-disclosure in natural language. We hypothesize that identifying frame semantics can meaningfully support this task. Specifically, we use Semantic Role Labeling to identify the lexical units and their semantic roles that signal self-disclosure. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.","authors":["Chandan Akiti","Anna Squicciarini","Sarah Rajtmajer"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.312","program":"findings","sessions":[],"similar_paper_uids":["findings.312"],"title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content","tldr":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal infor...","track":"Findings of EMNLP"},"forum":"findings.312","id":"findings.312","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.313.png","content":{"abstract":"Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI: a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in Wikipedia. We show that we can improve strong baselines such as BERT and RoBERTa by pretraining them on WikiNLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on WikiNLI gives the best performance. In addition, we construct WikiNLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.","authors":["Mingda Chen","Zewei Chu","Karl Stratos","Kevin Gimpel"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.313","program":"findings","sessions":[],"similar_paper_uids":["findings.313"],"title":"Mining Knowledge for Natural Language Inference from Wikipedia Categories","tldr":"Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI: a resource for improving model performance on NLI and LE tasks. ...","track":"Findings of EMNLP"},"forum":"findings.313","id":"findings.313","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.314.png","content":{"abstract":"Despite the tremendous recent progress on natural language inference (NLI), driven largely by large-scale investment in new datasets (e.g.,SNLI, MNLI) and advances in modeling, most progress has been limited to English due to a lack of reliable datasets for most of the world\u2019s languages. In this paper, we present the first large-scale NLI dataset (consisting of ~56,000 annotated sentence pairs) for Chinese called the Original Chinese Natural Language Inference dataset (OCNLI). Unlike recent attempts at extending NLI to other languages, our dataset does not rely on any automatic translation or non-expert annotation. Instead, we elicit annotations from native speakers specializing in linguistics. We follow closely the annotation protocol used for MNLI, but create new strategies for eliciting diverse hypotheses. We establish several baseline results on our dataset using state-of-the-art pre-trained models for Chinese, and find even the best performing models to be far outpaced by human performance (~12% absolute performance gap), making it a challenging new resource that we hope will help to accelerate progress in Chinese NLU. To the best of our knowledge, this is the first human-elicited MNLI-style corpus for a non-English language.","authors":["Hai Hu","Kyle Richardson","Liang Xu","Lu Li","Sandra K\u00fcbler","Lawrence Moss"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.314","program":"findings","sessions":[],"similar_paper_uids":["findings.314"],"title":"OCNLI: Original Chinese Natural Language Inference","tldr":"Despite the tremendous recent progress on natural language inference (NLI), driven largely by large-scale investment in new datasets (e.g.,SNLI, MNLI) and advances in modeling, most progress has been limited to English due to a lack of reliable datas...","track":"Findings of EMNLP"},"forum":"findings.314","id":"findings.314","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.315.png","content":{"abstract":"Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al., 2019b) and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach advances the state-of-the-art results by a large margin. Specifically, we improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between theory and the loss function used in the original work Zhang et al.(2019b), and thereby significantly boosts the performance. Our numerical results also indicate that VAT can remarkably improve the generalization performance of both domains for various domain adaptation approaches.","authors":["Dejiao Zhang","Ramesh Nallapati","Henghui Zhu","Feng Nan","Cicero Nogueira dos Santos","Kathleen McKeown","Bing Xiang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.315","program":"findings","sessions":[],"similar_paper_uids":["findings.315"],"title":"Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling","tldr":"Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al...","track":"Findings of EMNLP"},"forum":"findings.315","id":"findings.315","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.316.png","content":{"abstract":"Dialogue policy learning for task-oriented dialogue systems has enjoyed great progress recently mostly through employing reinforcement learning methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on reinforcement learning? We demonstrate how (1) traditional supervised learning together with (2) a simulator-free adversarial learning method can be used to achieve performance comparable to state-of-the-art reinforcement learning-based methods. First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using reinforcement learning. Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat RL with supervised learning, but to demonstrate the value of rethinking the role of reinforcement learning and supervised learning in optimizing task-oriented dialogue systems.","authors":["Ziming Li","Julia Kiseleva","Maarten de Rijke"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.316","program":"findings","sessions":[],"similar_paper_uids":["findings.316"],"title":"Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems","tldr":"Dialogue policy learning for task-oriented dialogue systems has enjoyed great progress recently mostly through employing reinforcement learning methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we re...","track":"Findings of EMNLP"},"forum":"findings.316","id":"findings.316","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.317.png","content":{"abstract":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good performance on a dataset might not indicate performance that correlates well with human\u2019s expectations from models that \u201cunderstand\u201d language. In this work we consider a top performing model on several Multiple Choice Question Answering (MCQA) datasets, and evaluate it against a set of expectations one might have from such a model, using a series of zero-information perturbations of the model\u2019s inputs. Our results show that the model clearly falls short of our expectations, and motivates a modified training approach that forces the model to better attend to the inputs. We show that the new training paradigm leads to a model that performs on par with the original model while better satisfying our expectations.","authors":["Krunal Shah","Nitish Gupta","Dan Roth"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.317","program":"findings","sessions":[],"similar_paper_uids":["findings.317"],"title":"What do we expect from Multiple-choice QA Systems?","tldr":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good perf...","track":"Findings of EMNLP"},"forum":"findings.317","id":"findings.317","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.318.png","content":{"abstract":"Event argument extraction (EAE) aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument. (3) Integrating event trigger information into candidate argument representation. For (1), we explore using unlabeled data. For (2), we use Transformer that uses dependency parses to guide the attention mechanism. For (3), we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE 2005 benchmark show that our approach achieves a new state-of-the-art.","authors":["Jie Ma","Shuai Wang","Rishita Anubhai","Miguel Ballesteros","Yaser Al-Onaizan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.318","program":"findings","sessions":[],"similar_paper_uids":["findings.318"],"title":"Resource-Enhanced Neural Model for Event Argument Extraction","tldr":"Event argument extraction (EAE) aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the long-range depen...","track":"Findings of EMNLP"},"forum":"findings.318","id":"findings.318","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.319.png","content":{"abstract":"To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.","authors":["Luyu Gao","Xinyi Wang","Graham Neubig"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.319","program":"findings","sessions":[],"similar_paper_uids":["findings.319"],"title":"Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation","tldr":"To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficia...","track":"Findings of EMNLP"},"forum":"findings.319","id":"findings.319","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.320.png","content":{"abstract":"Semantic parses are directed acyclic graphs (DAGs), but in practice most parsers treat them as strings or trees, mainly because models that predict graphs are far less understood. This simplification, however, comes at a cost: there is no guarantee that the output is a well-formed graph. A recent work by Fancellu et al. (2019) addressed this problem by proposing a graph-aware sequence model that utilizes a DAG grammar to guide graph generation. We significantly improve upon this work, by proposing a simpler architecture as well as more efficient training and inference algorithms that can always guarantee the well-formedness of the generated graphs. Importantly, unlike Fancellu et al., our model does not require language-specific features, and hence can harness the inherent ability of DAG-grammar parsing in multilingual settings. We perform monolingual as well as multilingual experiments on the Parallel Meaning Bank (Abzianidze et al., 2017). Our parser outperforms previous graph-aware models by a large margin, and closes the performance gap between string-based and DAG-grammar parsing.","authors":["Federico Fancellu","\u00c1kos K\u00e1d\u00e1r","Ran Zhang","Afsaneh Fazly"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.320","program":"findings","sessions":[],"similar_paper_uids":["findings.320"],"title":"Accurate polyglot semantic parsing with DAG grammars","tldr":"Semantic parses are directed acyclic graphs (DAGs), but in practice most parsers treat them as strings or trees, mainly because models that predict graphs are far less understood. This simplification, however, comes at a cost: there is no guarantee t...","track":"Findings of EMNLP"},"forum":"findings.320","id":"findings.320","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.321.png","content":{"abstract":"This paper is concerned with improving dialogue generation models through injection of knowledge, e.g., content relevant to the post that can increase the quality of responses. Past research extends the training of the generative models by incorporating statistical properties of posts, responses and related knowledge, without explicitly assessing the knowledge quality. In our work, we demonstrate the importance of knowledge relevance and adopt a two-phase approach. We first apply a novel method, Transformer & Post based Posterior Approximation (TPPA) to select knowledge, and then use the Transformer with Expanded Decoder (TED) model to generate responses from both the post and the knowledge. TPPA method processes posts, post related knowledge, and response related knowledge at both word and sentence level. Our experiments with the TED generative model demonstrate the effectiveness of TPPA as it outperforms a set of strong baseline models. Our TPPA method is extendable and supports further optimization of knowledge retrieval and injection.","authors":["Wen Zheng","Natasa Milic-Frayling","Ke Zhou"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.321","program":"findings","sessions":[],"similar_paper_uids":["findings.321"],"title":"Approximation of Response Knowledge Retrieval in Knowledge-grounded Dialogue Generation","tldr":"This paper is concerned with improving dialogue generation models through injection of knowledge, e.g., content relevant to the post that can increase the quality of responses. Past research extends the training of the generative models by incorporat...","track":"Findings of EMNLP"},"forum":"findings.321","id":"findings.321","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.322.png","content":{"abstract":"Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to identify factual errors; however, these sentence-level entailment models are trained to solve a different problem than generation filtering and they do not localize which part of a generation is non-factual. In this paper, we propose a new formulation of entailment that decomposes it at the level of dependency arcs. Rather than focusing on aggregate decisions, we instead ask whether the semantic relationship manifested by individual dependency arcs in the generated output is supported by the input. Human judgments on this task are difficult to obtain; we therefore propose a method to automatically create data based on existing entailment or paraphrase corpora. Experiments show that our dependency arc entailment model trained on this data can identify factual inconsistencies in paraphrasing and summarization better than sentence-level methods or those based on question generation, while additionally localizing the erroneous parts of the generation.","authors":["Tanya Goyal","Greg Durrett"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.322","program":"findings","sessions":[],"similar_paper_uids":["findings.322"],"title":"Evaluating Factuality in Generation with Dependency-level Entailment","tldr":"Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to ide...","track":"Findings of EMNLP"},"forum":"findings.322","id":"findings.322","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.323.png","content":{"abstract":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train classifiers without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates \u201cweak\u201d supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages: by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier: leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of word translations.","authors":["Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.323","program":"findings","sessions":[],"similar_paper_uids":["findings.323"],"title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher","tldr":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-...","track":"Findings of EMNLP"},"forum":"findings.323","id":"findings.323","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.324.png","content":{"abstract":"Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a chatbot can be developed to help train human counselors. Such a system needs to simulate intimate situations across multiple practice sessions. Open-domain dialogue systems frequently suffer from generic responses that do not characterize personal stories, so we look to infuse conversations with persona information by mimicking prototype conversations. Towards building a \u201cCrisisbot\u201d hotline visitor simulation, we propose a counseling strategy annotation scheme and a multi-task framework that leverages these counselor strategies to retrieve similar examples, generate diverse sub-utterances, and interleave prototype and generated sub-utterances into complex responses. We evaluate this framework with crowdworkers and experienced hotline counselors. The framework considerably increases response diversity and specificity, with limited impact to coherence. Our results also show a considerable discrepancy between crowdworker and counselor judgements, which emphasizes the importance of including target populations in system development and evaluation.","authors":["Orianna Demasi","Yu Li","Zhou Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.324","program":"findings","sessions":[],"similar_paper_uids":["findings.324"],"title":"A Multi-Persona Chatbot for Hotline Counselor Training","tldr":"Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a chatbot can be developed to help train human counselors. Such a sy...","track":"Findings of EMNLP"},"forum":"findings.324","id":"findings.324","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.325.png","content":{"abstract":"Past work on story generation has demonstrated the usefulness of conditioning on a generation plan to generate coherent stories. However, these approaches have used heuristics or off-the-shelf models to first tag training stories with the desired type of plan, and then train generation models in a supervised fashion. In this paper, we propose a deep latent variable model that first samples a sequence of anchor words, one per sentence in the story, as part of its generative process. During training, our model treats the sequence of anchor words as a latent variable and attempts to induce anchoring sequences that help guide generation in an unsupervised fashion. We conduct experiments with several types of sentence decoder distributions \u2013 left-to-right and non-monotonic, with different degrees of restriction. Further, since we use amortized variational inference to train our model, we introduce two corresponding types of inference network for predicting the posterior on anchor words. We conduct human evaluations which demonstrate that the stories produced by our model are rated better in comparison with baselines which do not consider story plans, and are similar or better in quality relative to baselines which use external supervision for plans. Additionally, the proposed model gets favorable scores when evaluated on perplexity, diversity, and control of story via discrete plan","authors":["Harsh Jhamtani","Taylor Berg-Kirkpatrick"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.325","program":"findings","sessions":[],"similar_paper_uids":["findings.325"],"title":"Narrative Text Generation with a Latent Discrete Plan","tldr":"Past work on story generation has demonstrated the usefulness of conditioning on a generation plan to generate coherent stories. However, these approaches have used heuristics or off-the-shelf models to first tag training stories with the desired typ...","track":"Findings of EMNLP"},"forum":"findings.325","id":"findings.325","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.326.png","content":{"abstract":"The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks (GTNs) to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.","authors":["Amir Pouran Ben Veyseh","Tuan Ngo Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.326","program":"findings","sessions":[],"similar_paper_uids":["findings.326"],"title":"Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction","tldr":"The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for...","track":"Findings of EMNLP"},"forum":"findings.326","id":"findings.326","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.327.png","content":{"abstract":"Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless/contextual syntactic ambiguity that requires commonsense knowledge to resolve. We manually create 1,200 triples, each of which contain a source sentence and two contrastive translations, involving 7 different common sense types. Language models pretrained on large-scale corpora, such as BERT, GPT-2, achieve a commonsense reasoning accuracy of lower than 72% on target translations of this test suite. We conduct extensive experiments on the test suite to evaluate commonsense reasoning in neural machine translation and investigate factors that have impact on this capability. Our experiments and analyses demonstrate that neural machine translation performs poorly on commonsense reasoning of the three ambiguity types in terms of both reasoning accuracy ( 6 60.1%) and reasoning consistency (6 31%). We will release our test suite as a machine translation commonsense reasoning testbed to promote future work in this direction.","authors":["Jie He","Tao Wang","Deyi Xiong","Qun Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.327","program":"findings","sessions":[],"similar_paper_uids":["findings.327"],"title":"The Box is in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation","tldr":"Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets...","track":"Findings of EMNLP"},"forum":"findings.327","id":"findings.327","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.328.png","content":{"abstract":"Our work aims to leverage visual feature space to pass information across languages. We show that models trained to generate textual captions in more than one language conditioned on an input image can leverage their jointly trained feature space during inference to pivot across languages. We particularly demonstrate improved quality on a caption generated from an input image, by leveraging a caption in a second language. More importantly, we demonstrate that even without conditioning on any visual input, the model demonstrates to have learned implicitly to perform to some extent machine translation from one language to another in their shared visual feature space. We show results in German-English, and Japanese-English language pairs that pave the way for using the visual world to learn a common representation for language.","authors":["Ziyan Yang","Leticia Pinto-Alva","Franck Dernoncourt","Vicente Ordonez"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.328","program":"findings","sessions":[],"similar_paper_uids":["findings.328"],"title":"Using Visual Feature Space as a Pivot Across Languages","tldr":"Our work aims to leverage visual feature space to pass information across languages. We show that models trained to generate textual captions in more than one language conditioned on an input image can leverage their jointly trained feature space dur...","track":"Findings of EMNLP"},"forum":"findings.328","id":"findings.328","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.329.png","content":{"abstract":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways (i.e. abstractive and extractive) on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in https://github.com/zide05/CDEvalSumm.","authors":["Yiran Chen","Pengfei Liu","Ming Zhong","Zi-Yi Dou","Danqing Wang","Xipeng Qiu","Xuanjing Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.329","program":"findings","sessions":[],"similar_paper_uids":["findings.329"],"title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems","tldr":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and...","track":"Findings of EMNLP"},"forum":"findings.329","id":"findings.329","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.330.png","content":{"abstract":"We present in this work a method for incorporating global context in long documents when making local decisions in sequence labeling problems like NER. Inspired by work in featurized log-linear models (Chieu and Ng, 2002; Sutton and McCallum, 2004), our model learns to attend to multiple mentions of the same word type in generating a representation for each token in context, extending that work to learning representations that can be incorporated into modern neural models. Attending to broader context at test time provides complementary information to pretraining (Gururangan et al., 2020), yields strong gains over equivalently parameterized models lacking such context, and performs best at recognizing entities with high TF-IDF scores (i.e., those that are important within a document).","authors":["Matthew J\u00f6rke","Jon Gillick","Matthew Sims","David Bamman"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.330","program":"findings","sessions":[],"similar_paper_uids":["findings.330"],"title":"Attending to Long-Distance Document Context for Sequence Labeling","tldr":"We present in this work a method for incorporating global context in long documents when making local decisions in sequence labeling problems like NER. Inspired by work in featurized log-linear models (Chieu and Ng, 2002; Sutton and McCallum, 2004), ...","track":"Findings of EMNLP"},"forum":"findings.330","id":"findings.330","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.331.png","content":{"abstract":"Bootstrapping for entity set expansion (ESE) has been studied for a long period, which expands new entities using only a few seed entities as supervision. Recent end-to-end bootstrapping approaches have shown their advantages in information capturing and bootstrapping process modeling. However, due to the sparse supervision problem, previous end-to-end methods often only leverage information from near neighborhoods (local semantics) rather than those propagated from the co-occurrence structure of the whole corpus (global semantics). To address this issue, this paper proposes Global Bootstrapping Network (GBN) with the \u201cpre-training and fine-tuning\u201d strategies for effective learning. Specifically, it contains a global-sighted encoder to capture and encode both local and global semantics into entity embedding, and an attention-guided decoder to sequentially expand new entities based on these embeddings. The experimental results show that the GBN learned by \u201cpre-training and fine-tuning\u201d strategies achieves state-of-the-art performance on two bootstrapping datasets.","authors":["Lingyong Yan","Xianpei Han","Ben He","Le Sun"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.331","program":"findings","sessions":[],"similar_paper_uids":["findings.331"],"title":"Global Bootstrapping Neural Network for Entity Set Expansion","tldr":"Bootstrapping for entity set expansion (ESE) has been studied for a long period, which expands new entities using only a few seed entities as supervision. Recent end-to-end bootstrapping approaches have shown their advantages in information capturing...","track":"Findings of EMNLP"},"forum":"findings.331","id":"findings.331","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.332.png","content":{"abstract":"The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86% and 75% respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub.","authors":["Bernal Jimenez Gutierrez","Jucheng Zeng","Dongdong Zhang","Ping Zhang","Yu Su"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.332","program":"findings","sessions":[],"similar_paper_uids":["findings.332"],"title":"Document Classification for COVID-19 Literature","tldr":"The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document clas...","track":"Findings of EMNLP"},"forum":"findings.332","id":"findings.332","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.333.png","content":{"abstract":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.","authors":["Adyasha Maharana","Mohit Bansal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.333","program":"findings","sessions":[],"similar_paper_uids":["findings.333"],"title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension","tldr":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models....","track":"Findings of EMNLP"},"forum":"findings.333","id":"findings.333","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.334.png","content":{"abstract":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at https://github.com/weakrules/Denoise-multi-weak-sources.","authors":["Wendi Ren","Yinghao Li","Hanting Su","David Kartchner","Cassie Mitchell","Chao Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.334","program":"findings","sessions":[],"similar_paper_uids":["findings.334"],"title":"Denoising Multi-Source Weak Supervision for Neural Text Classification","tldr":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete....","track":"Findings of EMNLP"},"forum":"findings.334","id":"findings.334","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.335.png","content":{"abstract":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient\u2019s medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","authors":["Anirudh Joshi","Namit Katariya","Xavier Amatriain","Anitha Kannan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.335","program":"findings","sessions":[],"similar_paper_uids":["findings.335"],"title":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures.","tldr":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and ...","track":"Findings of EMNLP"},"forum":"findings.335","id":"findings.335","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.336.png","content":{"abstract":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either medical domain-specific knowledge, or patients\u2019 prior diagnoses and clinical encounters. In this paper, we propose a novel model for automated clinical assessment generation (MCAG). MCAG is built on an innovative graph neural network, where rich clinical knowledge is incorporated into an end-to-end corpus-learning system. Our evaluation results against physician generated gold standard show that MCAG significantly improves the BLEU and rouge score compared with competitive baseline models. Further, physicians\u2019 evaluation showed that MCAG could generate high-quality assessments.","authors":["Zhichao Yang","Hong Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.336","program":"findings","sessions":[],"similar_paper_uids":["findings.336"],"title":"Generating Accurate Electronic Health Assessment from Medical Graph","tldr":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic syste...","track":"Findings of EMNLP"},"forum":"findings.336","id":"findings.336","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.337.png","content":{"abstract":"Proxy-based methods for annotating mental health status in social media have grown popular in computational research due to their ability to gather large training samples. However, an emerging body of literature has raised new concerns regarding the validity of these types of methods for use in clinical applications. To further understand the robustness of distantly supervised mental health models, we explore the generalization ability of machine learning classifiers trained to detect depression in individuals across multiple social media platforms. Our experiments not only reveal that substantial loss occurs when transferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction.","authors":["Keith Harrigian","Carlos Aguirre","Mark Dredze"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.337","program":"findings","sessions":[],"similar_paper_uids":["findings.337"],"title":"Do Models of Mental Health Based on Social Media Data Generalize?","tldr":"Proxy-based methods for annotating mental health status in social media have grown popular in computational research due to their ability to gather large training samples. However, an emerging body of literature has raised new concerns regarding the ...","track":"Findings of EMNLP"},"forum":"findings.337","id":"findings.337","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.338.png","content":{"abstract":"Pre-trained language models that learn contextualized word representations from a large un-annotated corpus have become a standard component for many state-of-the-art NLP systems. Despite their successful applications in various downstream NLP tasks, the extent of contextual impact on the word representation has not been explored. In this paper, we present a detailed analysis of contextual impact in Transformer- and BiLSTM-based masked language models. We follow two different approaches to evaluate the impact of context: a masking based approach that is architecture agnostic, and a gradient based approach that requires back-propagation through networks. The findings suggest significant differences on the contextual impact between the two model architectures. Through further breakdown of analysis by syntactic categories, we find the contextual impact in Transformer-based MLM aligns well with linguistic intuition. We further explore the Transformer attention pruning based on our findings in contextual analysis.","authors":["Yi-An Lai","Garima Lalwani","Yi Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.338","program":"findings","sessions":[],"similar_paper_uids":["findings.338"],"title":"Context Analysis for Pre-trained Masked Language Models","tldr":"Pre-trained language models that learn contextualized word representations from a large un-annotated corpus have become a standard component for many state-of-the-art NLP systems. Despite their successful applications in various downstream NLP tasks,...","track":"Findings of EMNLP"},"forum":"findings.338","id":"findings.338","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.339.png","content":{"abstract":"This work introduces Focused-Variation Network (FVN), a novel model to control language generation. The main problems in previous controlled language generation models range from the difficulty of generating text according to the given attributes, to the lack of diversity of the generated texts. FVN addresses these issues by learning disjoint discrete latent spaces for each attribute inside codebooks, which allows for both controllability and diversity, while at the same time generating fluent text. We evaluate FVN on two text generation datasets with annotated content and style, and show state-of-the-art performance as assessed by automatic and human evaluations.","authors":["Lei Shu","Alexandros Papangelis","Yi-Chia Wang","Gokhan Tur","Hu Xu","Zhaleh Feizollahi","Bing Liu","Piero Molino"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.339","program":"findings","sessions":[],"similar_paper_uids":["findings.339"],"title":"Controllable Text Generation with Focused Variation","tldr":"This work introduces Focused-Variation Network (FVN), a novel model to control language generation. The main problems in previous controlled language generation models range from the difficulty of generating text according to the given attributes, to...","track":"Findings of EMNLP"},"forum":"findings.339","id":"findings.339","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.340.png","content":{"abstract":"Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been hampered in part due to the lack of large scale labeled data grounded in text. This paper introduces PeKo, a crowd-sourced annotation of preconditions between event pairs in newswire, an order of magnitude larger than prior text annotations. To complement this new corpus, we also introduce two challenge tasks aimed at modeling preconditions: (i) Precondition Identification \u2013 a standard classification task defined over pairs of event mentions, and (ii) Precondition Generation \u2013 a generative task aimed at testing a more general ability to reason about a given event. Evaluation on both tasks shows that modeling preconditions is challenging even for today\u2019s large language models (LM). This suggests that precondition knowledge is not easily accessible in LM-derived representations alone. Our generation results show that fine-tuning an LM on PeKo yields better conditional relations than when trained on raw text or temporally-ordered corpora.","authors":["Heeyoung Kwon","Mahnaz Koupaee","Pratyush Singh","Gargi Sawhney","Anmol Shukla","Keerthi Kumar Kallur","Nathanael Chambers","Niranjan Balasubramanian"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.340","program":"findings","sessions":[],"similar_paper_uids":["findings.340"],"title":"Modeling Preconditions in Text with a Crowd-sourced Dataset","tldr":"Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse ...","track":"Findings of EMNLP"},"forum":"findings.340","id":"findings.340","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.341.png","content":{"abstract":"State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.","authors":["John Morris","Eli Lifland","Jack Lanchantin","Yangfeng Ji","Yanjun Qi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.341","program":"findings","sessions":[],"similar_paper_uids":["findings.341"],"title":"Reevaluating Adversarial Examples in Natural Language","tldr":"State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the mo...","track":"Findings of EMNLP"},"forum":"findings.341","id":"findings.341","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.342.png","content":{"abstract":"Answering questions in many real-world applications often requires complex and precise information excerpted from texts spanned across a long document. However, currently no such annotated dataset is publicly available, which hinders the development of neural question-answering (QA) systems. To this end, we present MASH-QA, a Multiple Answer Spans Healthcare Question Answering dataset from the consumer health domain, where answers may need to be excerpted from multiple, non-consecutive parts of text spanned across a long document. We also propose MultiCo, a neural architecture that is able to capture the relevance among multiple answer spans, by using a query-based contextualized sentence selection approach, for forming the answer to the given question. We also demonstrate that conventional QA models are not suitable for this type of task and perform poorly in this setting. Extensive experiments are conducted, and the experimental results confirm the proposed model significantly outperforms the state-of-the-art QA models in this multi-span QA setting.","authors":["Ming Zhu","Aman Ahuja","Da-Cheng Juan","Wei Wei","Chandan K. Reddy"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.342","program":"findings","sessions":[],"similar_paper_uids":["findings.342"],"title":"Question Answering with Long Multiple-Span Answers","tldr":"Answering questions in many real-world applications often requires complex and precise information excerpted from texts spanned across a long document. However, currently no such annotated dataset is publicly available, which hinders the development ...","track":"Findings of EMNLP"},"forum":"findings.342","id":"findings.342","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.343.png","content":{"abstract":"Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.","authors":["Zhiying Jiang","Raphael Tang","Ji Xin","Jimmy Lin"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.343","program":"findings","sessions":[],"similar_paper_uids":["findings.343"],"title":"Inserting Information Bottlenecks for Attribution in Transformers","tldr":"Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this ...","track":"Findings of EMNLP"},"forum":"findings.343","id":"findings.343","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.344.png","content":{"abstract":"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.","authors":["Salvador Medina Maza","Evangelia Spiliopoulou","Eduard Hovy","Alexander Hauptmann"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.344","program":"findings","sessions":[],"similar_paper_uids":["findings.344"],"title":"Event-Related Bias Removal for Real-time Disaster Events","tldr":"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. Th...","track":"Findings of EMNLP"},"forum":"findings.344","id":"findings.344","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.345.png","content":{"abstract":"As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for modern MT systems using 17 translation directions as test bed. Through thorough analysis, we find that indeed the presence of negation can significantly impact downstream quality, in some cases resulting in quality reductions of more than 60%. We also provide a linguistically motivated analysis that directly explains the majority of our findings. We release our annotations and code to replicate our analysis here: https://github.com/mosharafhossain/negation-mt.","authors":["Md Mosharaf Hossain","Antonios Anastasopoulos","Eduardo Blanco","Alexis Palmer"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.345","program":"findings","sessions":[],"similar_paper_uids":["findings.345"],"title":"It\u2019s not a Non-Issue: Negation as a Source of Error in Machine Translation","tldr":"As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigat...","track":"Findings of EMNLP"},"forum":"findings.345","id":"findings.345","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.346.png","content":{"abstract":"Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years, where neural methods became capable of producing audios with high naturalness. However, these efforts still suffer from two types of latencies: (a) the computational latency (synthesizing time), which grows linearly with the sentence length, and (b) the input latency in scenarios where the input text is incrementally available (such as in simultaneous translation, dialog generation, and assistive technologies). To reduce these latencies, we propose a neural incremental TTS approach using the prefix-to-prefix framework from simultaneous translation. We synthesize speech in an online fashion, playing a segment of audio while generating the next, resulting in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence TTS, but only with a constant (1-2 words) latency.","authors":["Mingbo Ma","Baigong Zheng","Kaibo Liu","Renjie Zheng","Hairong Liu","Kainan Peng","Kenneth Church","Liang Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.346","program":"findings","sessions":[],"similar_paper_uids":["findings.346"],"title":"Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework","tldr":"Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years, where neural methods became capable of producing audios with high naturalness. However, these efforts still suffer from two types of latencies: (a) the computational latency...","track":"Findings of EMNLP"},"forum":"findings.346","id":"findings.346","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.347.png","content":{"abstract":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn\u2019s contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features. On dialogues sampled from 28 Alexa domains, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27% (0.43 -> 0.70) and 7% (0.63 -> 0.70) improvement in linear correlation performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.","authors":["Praveen Kumar Bodigutla","Aditya Tiwari","Spyros Matsoukas","Josep Valls-Vargas","Lazaros Polymenakos"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.347","program":"findings","sessions":[],"similar_paper_uids":["findings.347"],"title":"Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations","tldr":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which redu...","track":"Findings of EMNLP"},"forum":"findings.347","id":"findings.347","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.348.png","content":{"abstract":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ARRAMON. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language (English) instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work.","authors":["Hyounghun Kim","Abhaysinh Zala","Graham Burri","Hao Tan","Mohit Bansal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.348","program":"findings","sessions":[],"similar_paper_uids":["findings.348"],"title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments","tldr":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We ...","track":"Findings of EMNLP"},"forum":"findings.348","id":"findings.348","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.349.png","content":{"abstract":"Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches will accumulate more and more latencies in later sentences when the speaker talks faster and introduce unnatural pauses into translated speech when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech latency than the baseline, in both Zh<->En directions.","authors":["Renjie Zheng","Mingbo Ma","Baigong Zheng","Kaibo Liu","Jiahong Yuan","Kenneth Church","Liang Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.349","program":"findings","sessions":[],"similar_paper_uids":["findings.349"],"title":"Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training","tldr":"Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speec...","track":"Findings of EMNLP"},"forum":"findings.349","id":"findings.349","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.350.png","content":{"abstract":"Code comments are vital for software maintenance and comprehension, but many software projects suffer from the lack of meaningful and up-to-date comments in practice. This paper presents a novel approach to automatically generate code comments at a function level by targeting object-oriented programming languages. Unlike prior work that only uses information locally available within the target function, our approach leverages broader contextual information by considering all other functions of the same class. To propagate and integrate information beyond the scope of the target function, we design a novel learning framework based on the bidirectional gated recurrent unit and a graph attention network with a pointer mechanism. We apply our approach to produce code comments for Java methods and compare it against four strong baseline methods. Experimental results show that our approach outperforms most methods by a large margin and achieves a comparable result with the state-of-the-art method.","authors":["Xiaohan Yu","Quzhe Huang","Zheng Wang","Yansong Feng","Dongyan Zhao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.350","program":"findings","sessions":[],"similar_paper_uids":["findings.350"],"title":"Towards Context-Aware Code Comment Generation","tldr":"Code comments are vital for software maintenance and comprehension, but many software projects suffer from the lack of meaningful and up-to-date comments in practice. This paper presents a novel approach to automatically generate code comments at a f...","track":"Findings of EMNLP"},"forum":"findings.350","id":"findings.350","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.351.png","content":{"abstract":"Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains. To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop (MCMH) rules result in superior results compared to the standard single-chain approaches, justifying both our formulation of generalized rules and the effectiveness of the proposed learning framework.","authors":["Lu Zhang","Mo Yu","Tian Gao","Yue Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.351","program":"findings","sessions":[],"similar_paper_uids":["findings.351"],"title":"MCMH: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning","tldr":"Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where...","track":"Findings of EMNLP"},"forum":"findings.351","id":"findings.351","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.352.png","content":{"abstract":"We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.","authors":["Thamme Gowda","Jonathan May"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.352","program":"findings","sessions":[],"similar_paper_uids":["findings.352"],"title":"Finding the Optimal Vocabulary Size for Neural Machine Translation","tldr":"We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions...","track":"Findings of EMNLP"},"forum":"findings.352","id":"findings.352","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.353.png","content":{"abstract":"For many prediction tasks, stakeholders desire not only predictions but also supporting evidence that a human can use to verify its correctness. However, in practice, evidence annotations may only be available for a minority of training examples (if available at all). In this paper, we propose new methods to combine few evidence annotations (strong semi-supervision) with abundant document-level labels (weak supervision) for the task of evidence extraction. Evaluating on two classification tasks that feature evidence annotations, we find that our methods outperform baselines adapted from the interpretability literature to our task. Our approach yields gains with as few as hundred evidence annotations.","authors":["Danish Pruthi","Bhuwan Dhingra","Graham Neubig","Zachary C. Lipton"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.353","program":"findings","sessions":[],"similar_paper_uids":["findings.353"],"title":"Weakly- and Semi-supervised Evidence Extraction","tldr":"For many prediction tasks, stakeholders desire not only predictions but also supporting evidence that a human can use to verify its correctness. However, in practice, evidence annotations may only be available for a minority of training examples (if ...","track":"Findings of EMNLP"},"forum":"findings.353","id":"findings.353","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.354.png","content":{"abstract":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing the first-stage of passage retrieval. Given an input question, it uses a BERT-based classifier (trained with weak supervision) to de-contextualize the input by selecting relevant terms from the dialog history. Using the question and the selected terms, it issues a query to a search engine to perform the first-stage of passage retrieval. On the other hand, MVR is responsible for contextualized passage reranking. It first constructs multiple views of the information need embedded within an input question. The views are based on the dialog history and the top documents obtained in the first-stage of retrieval. It then uses each view to rerank passages using BERT (fine-tuned for passage ranking). Finally, MVR performs a fusion over the rankings produced by the individual views. Experiments show that the above combination improves first-state retrieval as well as the overall accuracy in a reranking pipeline. On the key metric of NDCG@3, the proposed combination achieves a relative performance improvement of 14.8% over the state-of-the-art baseline and is also able to surpass the Oracle.","authors":["Vaibhav Kumar","Jamie Callan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.354","program":"findings","sessions":[],"similar_paper_uids":["findings.354"],"title":"Making Information Seeking Easier: An Improved Pipeline for Conversational Search","tldr":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing ...","track":"Findings of EMNLP"},"forum":"findings.354","id":"findings.354","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.355.png","content":{"abstract":"Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two objectives. Such an approach relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted. To address this issue, we propose to learn natural language actions that represent utterances as a span of words. This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.","authors":["Xinting Huang","Jianzhong Qi","Yu Sun","Rui Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.355","program":"findings","sessions":[],"similar_paper_uids":["findings.355"],"title":"Generalizable and Explainable Dialogue Generation via Explicit Action Learning","tldr":"Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two ob...","track":"Findings of EMNLP"},"forum":"findings.355","id":"findings.355","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.356.png","content":{"abstract":"Recent work proposes a family of contextual embeddings that significantly improves the accuracy of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of embeddings in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the accuracy of sequence labeling with various embedding concatenations and make three observations: (1) concatenating more embedding variants leads to better accuracy in rich-resource and cross-domain settings and some conditions of low-resource settings; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the accuracy in extremely low-resource settings; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings cannot lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.","authors":["Xinyu Wang","Yong Jiang","Nguyen Bach","Tao Wang","Zhongqiang Huang","Fei Huang","Kewei Tu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.356","program":"findings","sessions":[],"similar_paper_uids":["findings.356"],"title":"More Embeddings, Better Sequence Labelers?","tldr":"Recent work proposes a family of contextual embeddings that significantly improves the accuracy of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combinin...","track":"Findings of EMNLP"},"forum":"findings.356","id":"findings.356","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.357.png","content":{"abstract":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving millions of clients. They cannot afford traditional fine tuning for individual clients. Many clients cannot even afford significant fine tuning, and own little or no labeled data. Recognizing that word usage and salience diversity across clients leads to reduced accuracy, we initiate a study of practical and lightweight adaptation of centralized NLP services to clients. Each client uses an unsupervised, corpus-based sketch to register to the service. The server modifies its network mildly to accommodate client sketches, and occasionally trains the augmented network over existing clients. When a new client registers with its sketch, it gets immediate accuracy benefits. We demonstrate the proposed architecture using sentiment labeling, NER, and predictive language modeling.","authors":["Sahil Shah","Vihari Piratla","Soumen Chakrabarti","Sunita Sarawagi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.357","program":"findings","sessions":[],"similar_paper_uids":["findings.357"],"title":"NLP Service APIs and Models for Efficient Registration of New Clients","tldr":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving ...","track":"Findings of EMNLP"},"forum":"findings.357","id":"findings.357","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.358.png","content":{"abstract":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.","authors":["Jatin Ganhotra","Robert Moore","Sachindra Joshi","Kahini Wadhawan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.358","program":"findings","sessions":[],"similar_paper_uids":["findings.358"],"title":"Effects of Naturalistic Variation in Goal-Oriented Dialog","tldr":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fix...","track":"Findings of EMNLP"},"forum":"findings.358","id":"findings.358","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.359.png","content":{"abstract":"This paper targets the task of determining event outcomes in social media. We work with tweets containing either #cookingFail or #bakingFail, and show that many of the events described in them resulted in something edible. Tweets that contain images are more likely to result in edible albeit imperfect outcomes. Experimental results show that edibility is easier to predict than outcome quality.","authors":["Srikala Murugan","Dhivya Chinnappa","Eduardo Blanco"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.359","program":"findings","sessions":[],"similar_paper_uids":["findings.359"],"title":"Determining Event Outcomes: The Case of #fail","tldr":"This paper targets the task of determining event outcomes in social media. We work with tweets containing either #cookingFail or #bakingFail, and show that many of the events described in them resulted in something edible. Tweets that contain images ...","track":"Findings of EMNLP"},"forum":"findings.359","id":"findings.359","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.360.png","content":{"abstract":"We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct cross-lingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.","authors":["Faisal Ladhak","Esin Durmus","Claire Cardie","Kathleen McKeown"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.360","program":"findings","sessions":[],"similar_paper_uids":["findings.360"],"title":"WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization","tldr":"We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to gu...","track":"Findings of EMNLP"},"forum":"findings.360","id":"findings.360","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.361.png","content":{"abstract":"Code retrieval is a key task aiming to match natural and programming languages. In this work, we propose adversarial learning for code retrieval, that is regularized by question-description relevance. First, we adapt a simple adversarial learning technique to generate difficult code snippets given the input question, which can help the learning of code retrieval that faces bi-modal and data-scarce challenges. Second, we propose to leverage question-description relevance to regularize adversarial learning, such that a generated code snippet should contribute more to the code retrieval training loss, only if its paired natural language description is predicted to be less relevant to the user given question. Experiments on large-scale code retrieval datasets of two programming languages show that our adversarial learning method is able to improve the performance of state-of-the-art models. Moreover, using an additional duplicated question detection model to regularize adversarial learning further improves the performance, and this is more effective than using the duplicated questions in strong multi-task learning baselines.","authors":["Jie Zhao","Huan Sun"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.361","program":"findings","sessions":[],"similar_paper_uids":["findings.361"],"title":"Adversarial Training for Code Retrieval with Question-Description Relevance Regularization","tldr":"Code retrieval is a key task aiming to match natural and programming languages. In this work, we propose adversarial learning for code retrieval, that is regularized by question-description relevance. First, we adapt a simple adversarial learning tec...","track":"Findings of EMNLP"},"forum":"findings.361","id":"findings.361","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.362.png","content":{"abstract":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of pretrained language models (PLMs), we investigate how to incorporate large PKM into PLMs that can be finetuned for a wide variety of downstream NLP tasks. We define a new memory usage metric, and careful observation using this metric reveals that most memory slots remain outdated during the training of PKM-augmented models. To train better PLMs by tackling this issue, we propose simple but effective solutions: (1) initialization from the model weights pretrained without memory and (2) augmenting PKM by addition rather than replacing a feed-forward network. We verify that both of them are crucial for the pretraining of PKM-augmented PLMs, enhancing memory utilization and downstream performance. Code and pretrained weights are available at https://github.com/clovaai/pkm-transformers.","authors":["Gyuwan Kim","Tae Hwan Jung"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.362","program":"findings","sessions":[],"similar_paper_uids":["findings.362"],"title":"Large Product Key Memory for Pretrained Language Models","tldr":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal langua...","track":"Findings of EMNLP"},"forum":"findings.362","id":"findings.362","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.363.png","content":{"abstract":"We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event duration\u2014how long an event lasts\u2014and event ordering\u2014how events are temporally arranged\u2014into more than one million NLI examples. We use these datasets to investigate how well neural models trained on a popular NLI corpus capture these forms of temporal reasoning.","authors":["Siddharth Vashishtha","Adam Poliak","Yash Kumar Lal","Benjamin Van Durme","Aaron Steven White"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.363","program":"findings","sessions":[],"similar_paper_uids":["findings.363"],"title":"Temporal Reasoning in Natural Language Inference","tldr":"We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event duration\u2014how long an event lasts\u2014and event ordering\u2014how events are temporally arranged\u2014into more than ...","track":"Findings of EMNLP"},"forum":"findings.363","id":"findings.363","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.364.png","content":{"abstract":"Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020).","authors":["Anh Tuan Nguyen","Mai Hoang Dao","Dat Quoc Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.364","program":"findings","sessions":[],"similar_paper_uids":["findings.364"],"title":"A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese","tldr":"Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two st...","track":"Findings of EMNLP"},"forum":"findings.364","id":"findings.364","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.365.png","content":{"abstract":"We present a new challenging news dataset that targets both stance detection (SD) and fine-grained evidence retrieval (ER). With its 3,291 expert-annotated articles, the dataset constitutes a high-quality benchmark for future research in SD and multi-task learning. We provide a detailed description of the corpus collection methodology and carry out an extensive analysis on the sources of disagreement between annotators, observing a correlation between their disagreement and the diffusion of uncertainty around a target in the real world. Our experiments show that the dataset poses a strong challenge to recent state-of-the-art models. Notably, our dataset aligns with an existing Twitter SD dataset: their union thus addresses a key shortcoming of previous works, by providing the first dedicated resource to study multi-genre SD as well as the interplay of signals from social media and news sources in rumour verification.","authors":["Costanza Conforti","Jakob Berndt","Mohammad Taher Pilehvar","Chryssi Giannitsarou","Flavio Toxvaerd","Nigel Collier"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.365","program":"findings","sessions":[],"similar_paper_uids":["findings.365"],"title":"STANDER: An Expert-Annotated Dataset for News Stance Detection and Evidence Retrieval","tldr":"We present a new challenging news dataset that targets both stance detection (SD) and fine-grained evidence retrieval (ER). With its 3,291 expert-annotated articles, the dataset constitutes a high-quality benchmark for future research in SD and multi...","track":"Findings of EMNLP"},"forum":"findings.365","id":"findings.365","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.366.png","content":{"abstract":"In times of crisis, identifying essential needs is crucial to providing appropriate resources and services to affected entities. Social media platforms such as Twitter contain a vast amount of information about the general public\u2019s needs. However, the sparsity of information and the amount of noisy content present a challenge for practitioners to effectively identify relevant information on these platforms. This study proposes two novel methods for two needs detection tasks: 1) extracting a list of needed resources, such as masks and ventilators, and 2) detecting sentences that specify who-needs-what resources (e.g., we need testing). We evaluate our methods on a set of tweets about the COVID-19 crisis. For extracting a list of needs, we compare our results against two official lists of resources, achieving 0.64 precision. For detecting who-needs-what sentences, we compared our results against a set of 1,000 annotated tweets and achieved a 0.68 F1-score.","authors":["M. Janina Sarol","Ly Dinh","Rezvaneh Rezapour","Chieh-Li Chin","Pingjing Yang","Jana Diesner"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.366","program":"findings","sessions":[],"similar_paper_uids":["findings.366"],"title":"An Empirical Methodology for Detecting and Prioritizing Needs during Crisis Events","tldr":"In times of crisis, identifying essential needs is crucial to providing appropriate resources and services to affected entities. Social media platforms such as Twitter contain a vast amount of information about the general public\u2019s needs. However, th...","track":"Findings of EMNLP"},"forum":"findings.366","id":"findings.366","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.367.png","content":{"abstract":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.","authors":["Umanga Bista","Alexander Mathews","Aditya Menon","Lexing Xie"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.367","program":"findings","sessions":[],"similar_paper_uids":["findings.367"],"title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy","tldr":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information pres...","track":"Findings of EMNLP"},"forum":"findings.367","id":"findings.367","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.368.png","content":{"abstract":"In this paper, we propose a meta-learning based semi-supervised explicit dialogue state tracker (SEDST) for neural dialogue generation, denoted as MEDST. Our main motivation is to further bridge the chasm between the need for high accuracy dialogue state tracker and the common reality that only scarce annotated data is available for most real-life dialogue tasks. Specifically, MEDST has two core steps: meta-training with adequate unlabelled data in an automatic way and meta-testing with a few annotated data by supervised learning. In particular, we enhance SEDST via entropy regularization, and investigate semi-supervised learning frameworks based on model-agnostic meta-learning (MAML) that are able to reduce the amount of required intermediate state labelling. We find that by leveraging un-annotated data in meta-way instead, the amount of dialogue state annotations can be reduced below 10% while maintaining equivalent system performance. Experimental results show MEDST outperforms SEDST substantially by 18.7% joint goal accuracy and 14.3% entity match rate on the KVRET corpus with 2% labelled data in semi-supervision.","authors":["Yi Huang","Junlan Feng","Shuo Ma","Xiaoyu Du","Xiaoting Wu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.368","program":"findings","sessions":[],"similar_paper_uids":["findings.368"],"title":"Towards Low-Resource Semi-Supervised Dialogue Generation with Meta-Learning","tldr":"In this paper, we propose a meta-learning based semi-supervised explicit dialogue state tracker (SEDST) for neural dialogue generation, denoted as MEDST. Our main motivation is to further bridge the chasm between the need for high accuracy dialogue s...","track":"Findings of EMNLP"},"forum":"findings.368","id":"findings.368","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.369.png","content":{"abstract":"Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may not suffice, considering their limited coverage and the contextual dependence of their knowledge. In this paper, we augment a general commonsense QA framework with a knowledgeable path generator. By extrapolating over existing paths in a KG with a state-of-the-art language model, our generator learns to connect a pair of entities in text with a dynamic, and potentially novel, multi-hop relational path. Such paths can provide structured evidence for solving commonsense questions without fine-tuning the path generator. Experiments on two datasets show the superiority of our method over previous works which fully rely on knowledge from KGs (with up to 6% improvement in accuracy), across various amounts of training data. Further evaluation suggests that the generated paths are typically interpretable, novel, and relevant to the task.","authors":["Peifeng Wang","Nanyun Peng","Filip Ilievski","Pedro Szekely","Xiang Ren"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.369","program":"findings","sessions":[],"similar_paper_uids":["findings.369"],"title":"Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering","tldr":"Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may...","track":"Findings of EMNLP"},"forum":"findings.369","id":"findings.369","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.370.png","content":{"abstract":"The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short), but also of richer types (including no-answer, yes/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard, with F1 scores of 77.2 and 64.1, respectively.","authors":["Xuguang Wang","Linjun Shou","Ming Gong","Nan Duan","Daxin Jiang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.370","program":"findings","sessions":[],"similar_paper_uids":["findings.370"],"title":"No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension","tldr":"The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short), but also of richer types (including no-answer, yes/no, single-span and mult...","track":"Findings of EMNLP"},"forum":"findings.370","id":"findings.370","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.371.png","content":{"abstract":"Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel corpus. The rise of unsupervised neural machine translation (UNMT) almost completely relieves the parallel corpus curse, though UNMT is still subject to unsatisfactory performance due to the vagueness of the clues available for its core back-translation training. Further enriching the idea of pivot translation by extending the use of parallel corpora beyond the source-target paradigm, we propose a new reference language-based framework for UNMT, RUNMT, in which the reference language only shares a parallel corpus with the source, but this corpus still indicates a signal clear enough to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline that uses only one auxiliary language, demonstrating the usefulness of the proposed reference language-based UNMT and establishing a good start for the community.","authors":["Zuchao Li","Hai Zhao","Rui Wang","Masao Utiyama","Eiichiro Sumita"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.371","program":"findings","sessions":[],"similar_paper_uids":["findings.371"],"title":"Reference Language based Unsupervised Neural Machine Translation","tldr":"Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a...","track":"Findings of EMNLP"},"forum":"findings.371","id":"findings.371","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.372.png","content":{"abstract":"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.","authors":["Xiaoqi Jiao","Yichun Yin","Lifeng Shang","Xin Jiang","Xiao Chen","Linlin Li","Fang Wang","Qun Liu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.372","program":"findings","sessions":[],"similar_paper_uids":["findings.372"],"title":"TinyBERT: Distilling BERT for Natural Language Understanding","tldr":"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute th...","track":"Findings of EMNLP"},"forum":"findings.372","id":"findings.372","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.373.png","content":{"abstract":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier\u2019s predictions can be steered to the poison target class with success rates of >80\\% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.","authors":["Alvin Chan","Yi Tay","Yew-Soon Ong","Aston Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.373","program":"findings","sessions":[],"similar_paper_uids":["findings.373"],"title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder","tldr":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regula...","track":"Findings of EMNLP"},"forum":"findings.373","id":"findings.373","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.374.png","content":{"abstract":"#TurkihTweets is a benchmark dataset for the task of correcting the user misspellings, with the purpose of introducing the first public Turkish dataset in this area. \\#TurkihTweets provides correct/incorrect word annotations with a detailed misspelling category formulation based on the real user data. We evaluated four state-of-the-art approaches on our dataset to present a preliminary analysis for the sake of reproducibility.","authors":["Asiye Tuba Koksal","Ozge Bozal","Emre Y\u00fcrekli","Gizem Gezici"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.374","program":"findings","sessions":[],"similar_paper_uids":["findings.374"],"title":"#Turki$hTweets: A Benchmark Dataset for Turkish Text Correction","tldr":"#TurkihTweets is a benchmark dataset for the task of correcting the user misspellings, with the purpose of introducing the first public Turkish dataset in this area. \\#TurkihTweets provides correct/incorrect word annotations with a detailed misspelli...","track":"Findings of EMNLP"},"forum":"findings.374","id":"findings.374","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.375.png","content":{"abstract":"Recent machine translation shared tasks have shown top-performing systems to tie or in some cases even outperform human translation. Such conclusions about system and human performance are, however, based on estimates aggregated from scores collected over large test sets of translations and unfortunately leave some remaining questions unanswered. For instance, simply because a system significantly outperforms the human translator on average may not necessarily mean that it has done so for every translation in the test set. Firstly, are there remaining source segments present in evaluation test sets that cause significant challenges for top-performing systems and can such challenging segments go unnoticed due to the opacity of current human evaluation procedures? To provide insight into these issues we carefully inspect the outputs of top-performing systems in the most recent WMT-19 news translation shared task for all language pairs in which a system either tied or outperformed human translation. Our analysis provides a new method of identifying the remaining segments for which either machine or human perform poorly. For example, in our close inspection of WMT-19 English to German and German to English we discover the segments that disjointly proved a challenge for human and machine. For English to Russian, there were no segments included in our sample of translations that caused a significant challenge for the human translator, while we again identify the set of segments that caused issues for the top-performing system.","authors":["Yvette Graham","Christian Federmann","Maria Eskevich","Barry Haddow"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.375","program":"findings","sessions":[],"similar_paper_uids":["findings.375"],"title":"Assessing Human-Parity in Machine Translation on the Segment Level","tldr":"Recent machine translation shared tasks have shown top-performing systems to tie or in some cases even outperform human translation. Such conclusions about system and human performance are, however, based on estimates aggregated from scores collected...","track":"Findings of EMNLP"},"forum":"findings.375","id":"findings.375","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.376.png","content":{"abstract":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.","authors":["Harris Chan","Jamie Kiros","William Chan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.376","program":"findings","sessions":[],"similar_paper_uids":["findings.376"],"title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels","tldr":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work,...","track":"Findings of EMNLP"},"forum":"findings.376","id":"findings.376","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.377.png","content":{"abstract":"Multi-Domain Neural Machine Translation (NMT) aims at building a single system that performs well on a range of target domains. However, along with the extreme diversity of cross-domain wording and phrasing style, the imperfections of training data distribution and the inherent defects of the current sequential learning process all contribute to making the task of multi-domain NMT very challenging. To mitigate these problems, we propose the Factorized Transformer, which consists of an in-depth factorization of the parameters of an NMT model, namely Transformer in this paper, into two categories: domain-shared ones that encode common cross-domain knowledge and domain-specific ones that are private for each constituent domain. We experiment with various designs of our model and conduct extensive validations on English to French open multi-domain dataset. Our approach achieves state-of-the-art performance and opens up new perspectives for multi-domain and open-domain applications.","authors":["Yongchao Deng","Hongfei Yu","Heng Yu","Xiangyu Duan","Weihua Luo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.377","program":"findings","sessions":[],"similar_paper_uids":["findings.377"],"title":"Factorized Transformer for Multi-Domain Neural Machine Translation","tldr":"Multi-Domain Neural Machine Translation (NMT) aims at building a single system that performs well on a range of target domains. However, along with the extreme diversity of cross-domain wording and phrasing style, the imperfections of training data d...","track":"Findings of EMNLP"},"forum":"findings.377","id":"findings.377","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.378.png","content":{"abstract":"Named entity recognition (NER) is highly sensitive to sentential syntactic and semantic properties where entities may be extracted according to how they are used and placed in the running text. To model such properties, one could rely on existing resources to providing helpful knowledge to the NER task; some existing studies proved the effectiveness of doing so, and yet are limited in appropriately leveraging the knowledge such as distinguishing the important ones for particular context. In this paper, we improve NER by leveraging different types of syntactic information through attentive ensemble, which functionalizes by the proposed key-value memory networks, syntax attention, and the gate mechanism for encoding, weighting and aggregating such syntactic information, respectively. Experimental results on six English and Chinese benchmark datasets suggest the effectiveness of the proposed model and show that it outperforms previous studies on all experiment datasets.","authors":["Yuyang Nie","Yuanhe Tian","Yan Song","Xiang Ao","Xiang Wan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.378","program":"findings","sessions":[],"similar_paper_uids":["findings.378"],"title":"Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information","tldr":"Named entity recognition (NER) is highly sensitive to sentential syntactic and semantic properties where entities may be extracted according to how they are used and placed in the running text. To model such properties, one could rely on existing res...","track":"Findings of EMNLP"},"forum":"findings.378","id":"findings.378","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.379.png","content":{"abstract":"Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer\u2019s normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply l2-normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT\u201915.","authors":["Alex Henry","Prudhvi Raj Dachapally","Shubham Shantaram Pawar","Yuxuan Chen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.379","program":"findings","sessions":[],"similar_paper_uids":["findings.379"],"title":"Query-Key Normalization for Transformers","tldr":"Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer\u2019s normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to...","track":"Findings of EMNLP"},"forum":"findings.379","id":"findings.379","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.380.png","content":{"abstract":"We propose a new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed \u2013 where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The task differs substantially from conventional NLI and shared tasks on legal information extraction (e.g., one has to identify text span instead of a single document, page, or paragraph). The specification of the proposed task is followed by an evaluation of multiple solutions within the unified framework proposed for this branch of methods. It is shown that state-of-the-art pretrained encoders fail to provide satisfactory results on the task proposed. In contrast, Language Model-based solutions perform better, especially when unsupervised fine-tuning is applied. Besides the ablation studies, we addressed questions regarding detection accuracy for relevant text fragments depending on the number of examples available. In addition to the dataset and reference results, LMs specialized in the legal domain were made publicly available.","authors":["\u0141ukasz Borchmann","Dawid Wisniewski","Andrzej Gretkowski","Izabela Kosmala","Dawid Jurkiewicz","\u0141ukasz Sza\u0142kiewicz","Gabriela Pa\u0142ka","Karol Kaczmarek","Agnieszka Kaliska","Filip Grali\u0144ski"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.380","program":"findings","sessions":[],"similar_paper_uids":["findings.380"],"title":"Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines","tldr":"We propose a new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed \u2013 where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The tas...","track":"Findings of EMNLP"},"forum":"findings.380","id":"findings.380","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.381.png","content":{"abstract":"Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between distant domains (e.g., movie subtitles and research papers), however, cannot be performed effectively due to mismatches in vocabulary; it will encounter many domain-specific words (e.g., \u201cangstrom\u201d) and words whose meanings shift across domains (e.g., \u201cconductor\u201d). In this study, aiming to solve these vocabulary mismatches in domain adaptation for neural machine translation (NMT), we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pretrained NMT model to the target domain. Prior to fine-tuning, our method replaces the embedding layers of the NMT model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space. Experimental results indicate that our method improves the performance of conventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En translation, respectively.","authors":["Shoetsu Sato","Jin Sakuma","Naoki Yoshinaga","Masashi Toyoda","Masaru Kitsuregawa"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.381","program":"findings","sessions":[],"similar_paper_uids":["findings.381"],"title":"Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation","tldr":"Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between dist...","track":"Findings of EMNLP"},"forum":"findings.381","id":"findings.381","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.382.png","content":{"abstract":"Target sentiment analysis aims to detect opinion targets along with recognizing their sentiment polarities from a sentence. Some models with span-based labeling have achieved promising results in this task. However, the relation between the target extraction task and the target classification task has not been well exploited. Besides, the span-based target extraction algorithm has a poor performance on target phrases due to the maximum target length setting or length penalty factor. To address these problems, we propose a novel framework of Shared-Private Representation Model (SPRM) with a coarse-to-fine extraction algorithm. For jointly learning target extraction and classification, we design a Shared-Private Network, which encodes not only shared information for both tasks but also private information for each task. To avoid missing correct target phrases, we also propose a heuristic coarse-to-fine extraction algorithm that first gets the approximate interval of the targets by matching the nearest predicted start and end indexes and then extracts the targets by adopting an extending strategy. Experimental results show that our model achieves state-of-the-art performance.","authors":["Peiqin Lin","Meng Yang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.382","program":"findings","sessions":[],"similar_paper_uids":["findings.382"],"title":"A Shared-Private Representation Model with Coarse-to-Fine Extraction for Target Sentiment Analysis","tldr":"Target sentiment analysis aims to detect opinion targets along with recognizing their sentiment polarities from a sentence. Some models with span-based labeling have achieved promising results in this task. However, the relation between the target ex...","track":"Findings of EMNLP"},"forum":"findings.382","id":"findings.382","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.383.png","content":{"abstract":"Media plays an important role in shaping public opinion. Biased media can influence people in undesirable directions and hence should be unmasked as such. We observe that feature-based and neural text classification approaches which rely only on the distribution of low-level lexical information fail to detect media bias. This weakness becomes most noticeable for articles on new events, where words appear in new contexts and hence their \u201cbias predictiveness\u201d is unclear. In this paper, we therefore study how second-order information about biased statements in an article helps to improve detection effectiveness. In particular, we utilize the probability distributions of the frequency, positions, and sequential order of lexical and informational sentence-level bias in a Gaussian Mixture Model. On an existing media bias dataset, we find that the frequency and positions of biased statements strongly impact article-level bias, whereas their exact sequential order is secondary. Using a standard model for sentence-level bias detection, we provide empirical evidence that article-level bias detectors that use second-order information clearly outperform those without.","authors":["Wei-Fan Chen","Khalid Al Khatib","Benno Stein","Henning Wachsmuth"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.383","program":"findings","sessions":[],"similar_paper_uids":["findings.383"],"title":"Detecting Media Bias in News Articles using Gaussian Bias Distributions","tldr":"Media plays an important role in shaping public opinion. Biased media can influence people in undesirable directions and hence should be unmasked as such. We observe that feature-based and neural text classification approaches which rely only on the ...","track":"Findings of EMNLP"},"forum":"findings.383","id":"findings.383","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.384.png","content":{"abstract":"We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that SA- completely breaks down on long sequences whereas the accuracy of SA+ is 58.82%. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.","authors":["Javid Ebrahimi","Dhruv Gelda","Wei Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.384","program":"findings","sessions":[],"similar_paper_uids":["findings.384"],"title":"How Can Self-Attention Networks Recognize Dyck-n Languages?","tldr":"We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one withou...","track":"Findings of EMNLP"},"forum":"findings.384","id":"findings.384","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.385.png","content":{"abstract":"The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method\u2014\u2014LayerDrop.","authors":["Qiang Wang","Tong Xiao","Jingbo Zhu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.385","program":"findings","sessions":[],"similar_paper_uids":["findings.385"],"title":"Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation","tldr":"The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditio...","track":"Findings of EMNLP"},"forum":"findings.385","id":"findings.385","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.386.png","content":{"abstract":"A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of a preposition or a free paraphrase. Prepositional paraphrasing refers to the use of preposition to explain the semantic relation, whereas free paraphrasing refers to invoking an appropriate predicate denoting the semantic relation. In this paper, we propose an unsupervised methodology for these two types of paraphrasing. We use pre-trained contextualized language models to uncover the \u2018missing\u2019 words (preposition or predicate). These language models are usually trained to uncover the missing word/words in a given input sentence. Our approach uses templates to prepare the input sequence for the language model. The template uses a special token to indicate the missing predicate. As the model has already been pre-trained to uncover a missing word (or a sequence of words), we exploit it to predict missing words for the input sequence. Our experiments using four datasets show that our unsupervised approach (a) performs comparably to supervised approaches for prepositional paraphrasing, and (b) outperforms supervised approaches for free paraphrasing. Paraphrasing (prepositional or free) using our unsupervised approach is potentially helpful for NLP tasks like machine translation and information extraction.","authors":["Girishkumar Ponkiya","Rudra Murthy","Pushpak Bhattacharyya","Girish Palshikar"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.386","program":"findings","sessions":[],"similar_paper_uids":["findings.386"],"title":"Looking inside Noun Compounds: Unsupervised Prepositional and Free Paraphrasing","tldr":"A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of...","track":"Findings of EMNLP"},"forum":"findings.386","id":"findings.386","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.387.png","content":{"abstract":"Large-scale pretrained language models have become ubiquitous in Natural Language Processing. However, most of these models are available either in high-resource languages, in particular English, or as multilingual models that compromise performance on individual languages for coverage. This paper introduces Romanian BERT, the first purely Romanian transformer-based language model, pretrained on a large text corpus. We discuss corpus com-position and cleaning, the model training process, as well as an extensive evaluation of the model on various Romanian datasets. We opensource not only the model itself, but also a repository that contains information on how to obtain the corpus, fine-tune and use this model in production (with practical examples), and how to fully replicate the evaluation process.","authors":["Stefan Dumitrescu","Andrei-Marius Avram","Sampo Pyysalo"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.387","program":"findings","sessions":[],"similar_paper_uids":["findings.387"],"title":"The birth of Romanian BERT","tldr":"Large-scale pretrained language models have become ubiquitous in Natural Language Processing. However, most of these models are available either in high-resource languages, in particular English, or as multilingual models that compromise performance ...","track":"Findings of EMNLP"},"forum":"findings.387","id":"findings.387","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.388.png","content":{"abstract":"Reverse dictionary is the task to find the proper target word given the word description. In this paper, we tried to incorporate BERT into this task. However, since BERT is based on the byte-pair-encoding (BPE) subword encoding, it is nontrivial to make BERT generate a word given the description. We propose a simple but effective method to make BERT generate the target word for this specific task. Besides, the cross-lingual reverse dictionary is the task to find the proper target word described in another language. Previous models have to keep two different word embeddings and learn to align these embeddings. Nevertheless, by using the Multilingual BERT (mBERT), we can efficiently conduct the cross-lingual reverse dictionary with one subword embedding, and the alignment between languages is not necessary. More importantly, mBERT can achieve remarkable cross-lingual reverse dictionary performance even without the parallel corpus, which means it can conduct the cross-lingual reverse dictionary with only corresponding monolingual data. Code is publicly available at https://github.com/yhcc/BertForRD.git.","authors":["Hang Yan","Xiaonan Li","Xipeng Qiu","Bocao Deng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.388","program":"findings","sessions":[],"similar_paper_uids":["findings.388"],"title":"BERT for Monolingual and Cross-Lingual Reverse Dictionary","tldr":"Reverse dictionary is the task to find the proper target word given the word description. In this paper, we tried to incorporate BERT into this task. However, since BERT is based on the byte-pair-encoding (BPE) subword encoding, it is nontrivial to m...","track":"Findings of EMNLP"},"forum":"findings.388","id":"findings.388","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.389.png","content":{"abstract":"Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of part-of-speech tagging, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.","authors":["Wietse de Vries","Andreas van Cranenburgh","Malvina Nissim"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.389","program":"findings","sessions":[],"similar_paper_uids":["findings.389"],"title":"What\u2019s so special about BERT\u2019s layers? A closer look at the NLP pipeline in monolingual and multilingual models","tldr":"Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language othe...","track":"Findings of EMNLP"},"forum":"findings.389","id":"findings.389","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.390.png","content":{"abstract":"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model\u2019s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.","authors":["Peter Hase","Shiyue Zhang","Harry Xie","Mohit Bansal"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.390","program":"findings","sessions":[],"similar_paper_uids":["findings.390"],"title":"Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?","tldr":"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to ...","track":"Findings of EMNLP"},"forum":"findings.390","id":"findings.390","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.391.png","content":{"abstract":"Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), i.e., the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed as a simple pipeline, where segmentation is followed by sequence tagging, or as an end-to-end model, predicting morphemes from raw tokens. Both approaches are sub-optimal; the former is heavily prone to error propagation, and the latter does not enjoy explicit access to the basic processing units called morphemes. This paper offers MD architecture that combines the symbolic knowledge of morphemes with the learning capacity of neural end-to-end modeling. We propose a new, general and easy-to-implement Pointer Network model where the input is a morphological lattice and the output is a sequence of indices pointing at a single disambiguated path of morphemes. We demonstrate the efficacy of the model on segmentation and tagging, for Hebrew and Turkish texts, based on their respective Universal Dependencies (UD) treebanks. Our experiments show that with complete lattices, our model outperforms all shared-task results on segmenting and tagging these languages. On the SPMRL treebank, our model outperforms all previously reported results for Hebrew MD in realistic scenarios.","authors":["Amit Seker","Reut Tsarfaty"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.391","program":"findings","sessions":[],"similar_paper_uids":["findings.391"],"title":"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging","tldr":"Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), i.e., the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed ...","track":"Findings of EMNLP"},"forum":"findings.391","id":"findings.391","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.392.png","content":{"abstract":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process.","authors":["Wanqing Cui","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.392","program":"findings","sessions":[],"similar_paper_uids":["findings.392"],"title":"Beyond Language: Learning Commonsense from Images for Reasoning","tldr":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thous...","track":"Findings of EMNLP"},"forum":"findings.392","id":"findings.392","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.393.png","content":{"abstract":"In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, the existing DG designs are mainly for single distractor generation. However, for practical MCQ preparation, multiple distractors are desired. Aiming at these goals, in this paper, we present a new distractor generation scheme with multi-tasking and negative answer training strategies for effectively generating multiple distractors. The experimental results show that (1) our model advances the state-of-the-art result from 28.65 to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse and shows strong distracting power for multiple choice question.","authors":["Ho-Lam Chung","Ying-Hong Chan","Yao-Chung Fan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.393","program":"findings","sessions":[],"similar_paper_uids":["findings.393"],"title":"A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies.","tldr":"In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, ...","track":"Findings of EMNLP"},"forum":"findings.393","id":"findings.393","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.394.png","content":{"abstract":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei andZou, 2019) and Back-Translation (Sennrichet al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XLNet, and RoBERTa. We observe a negative result, finding that techniques which previously reported strong improvements for non-pretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.","authors":["Shayne Longpre","Yu Wang","Chris DuBois"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.394","program":"findings","sessions":[],"similar_paper_uids":["findings.394"],"title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?","tldr":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models...","track":"Findings of EMNLP"},"forum":"findings.394","id":"findings.394","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.395.png","content":{"abstract":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Currently, the best-performing models are able to complete less than 1% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information, the starting location in the virtual environment, is incorporated, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases, suggesting contextualized language models may provide strong planning modules for grounded virtual agents.","authors":["Peter Jansen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.395","program":"findings","sessions":[],"similar_paper_uids":["findings.395"],"title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions","tldr":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Curre...","track":"Findings of EMNLP"},"forum":"findings.395","id":"findings.395","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.396.png","content":{"abstract":"We propose a method to control the specificity of responses while maintaining the consistency with the utterances. We first design a metric based on pointwise mutual information, which measures the co-occurrence degree between an utterance and a response. To control the specificity of generated responses, we add the distant supervision based on the co-occurrence degree and a PMI-based word prediction mechanism to a sequence-to-sequence model. With these mechanisms, our model outputs the words with optimal specificity for a given specificity control variable. In experiments with open-domain dialogue corpora, automatic and human evaluation results confirm that our model controls the specificity of the response more sensitively than the conventional model and can generate highly consistent responses.","authors":["Junya Takayama","Yuki Arase"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.396","program":"findings","sessions":[],"similar_paper_uids":["findings.396"],"title":"Consistent Response Generation with Controlled Specificity","tldr":"We propose a method to control the specificity of responses while maintaining the consistency with the utterances. We first design a metric based on pointwise mutual information, which measures the co-occurrence degree between an utterance and a resp...","track":"Findings of EMNLP"},"forum":"findings.396","id":"findings.396","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.397.png","content":{"abstract":"In previous work, artificial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural languages, such as compositionality. In this paper, we propose some realistic sources of pressure on communication that avert this outcome. More specifically, we formalise the principle of least effort through an auxiliary objective. Moreover, we explore several game variants, inspired by the principle of object constancy, in which we alter the frequency, position, and luminosity of the objects in the images. We perform an extensive analysis on their effect through compositionality metrics, diagnostic classifiers, and zero-shot evaluation. Our findings reveal that the proposed sources of pressure result in emerging languages with less redundancy, more focus on high-level conceptual information, and better abilities of generalisation. Overall, our contributions reduce the gap between emergent and natural languages.","authors":["Diana Rodr\u00edguez Luna","Edoardo Maria Ponti","Dieuwke Hupkes","Elia Bruni"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.397","program":"findings","sessions":[],"similar_paper_uids":["findings.397"],"title":"Internal and external pressures on language emergence: least effort, object constancy and frequency","tldr":"In previous work, artificial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural l...","track":"Findings of EMNLP"},"forum":"findings.397","id":"findings.397","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.398.png","content":{"abstract":"Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax and semantics may be represented in either span (constituent/phrase) or dependency, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of neural network and benefits from two representation formalisms in a uniform way. The experiments show that semantics and syntax can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.","authors":["Junru Zhou","Zuchao Li","Hai Zhao"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.398","program":"findings","sessions":[],"similar_paper_uids":["findings.398"],"title":"Parsing All: Syntax and Semantics, Dependencies and Spans","tldr":"Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As...","track":"Findings of EMNLP"},"forum":"findings.398","id":"findings.398","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.399.png","content":{"abstract":"In this paper, we present Linguistics Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistics tasks by Multi-Task Learning. LIMIT-BERT includes five key linguistics tasks: Part-Of-Speech (POS) tags, constituent and dependency syntactic parsing, span and dependency semantic role labeling (SRL). Different from recent Multi-Task Deep Neural Networks (MT-DNN), our LIMIT-BERT is fully linguistics motivated and thus is capable of adopting an improved masked training objective according to syntactic and semantic constituents. Besides, LIMIT-BERT takes a semi-supervised learning strategy to offer the same large amount of linguistics task data as that for the language model training. As a result, LIMIT-BERT not only improves linguistics tasks performance but also benefits from a regularization effect and linguistics information that leads to more general representations to help adapt to new tasks and domains. LIMIT-BERT outperforms the strong baseline Whole Word Masking BERT on both dependency and constituent syntactic/semantic parsing, GLUE benchmark, and SNLI task. Our practice on the proposed LIMIT-BERT also enables us to release a well pre-trained model for multi-purpose of natural language processing tasks once for all.","authors":["Junru Zhou","Zhuosheng Zhang","Hai Zhao","Shuailiang Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.399","program":"findings","sessions":[],"similar_paper_uids":["findings.399"],"title":"LIMIT-BERT : Linguistics Informed Multi-Task BERT","tldr":"In this paper, we present Linguistics Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistics tasks by Multi-Task Learning. LIMIT-BERT includes five key linguistics tasks: Part-Of-Speech (POS) tags, con...","track":"Findings of EMNLP"},"forum":"findings.399","id":"findings.399","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.400.png","content":{"abstract":"Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives: preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve joint goal accuracy by 8.95% when only 1% labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76% improvement if some unlabeled data is jointly trained as semi-supervised learning. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.","authors":["Chien-Sheng Wu","Steven C.H. Hoi","Caiming Xiong"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.400","program":"findings","sessions":[],"similar_paper_uids":["findings.400"],"title":"Improving Limited Labeled Dialogue State Tracking with Self-Supervision","tldr":"Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discuss...","track":"Findings of EMNLP"},"forum":"findings.400","id":"findings.400","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.401.png","content":{"abstract":"Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will inflate the performances on languages with the same branch it biases to. In this work, we propose quantitatively measuring the branching bias by comparing the performance gap on a language and its reversed language, which is agnostic to both language models and extracting methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely feature definitions, parsing algorithms, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias.","authors":["Huayang Li","Lemao Liu","Guoping Huang","Shuming Shi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.401","program":"findings","sessions":[],"similar_paper_uids":["findings.401"],"title":"On the Branching Bias of Syntax Extracted from Pre-trained Language Models","tldr":"Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will infla...","track":"Findings of EMNLP"},"forum":"findings.401","id":"findings.401","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.402.png","content":{"abstract":"There has been an increased interest in modelling political discourse within the natural language processing (NLP) community, in tasks such as political bias and misinformation detection, among others. Metaphor-rich and emotion-eliciting communication strategies are ubiquitous in political rhetoric, according to social science research. Yet, none of the existing computational models of political discourse has incorporated these phenomena. In this paper, we present the first joint models of metaphor, emotion and political rhetoric, and demonstrate that they advance performance in three tasks: predicting political perspective of news articles, party affiliation of politicians and framing of policy issues.","authors":["Pere-Llu\u00eds Huguet Cabot","Verna Dankers","David Abadi","Agneta Fischer","Ekaterina Shutova"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.402","program":"findings","sessions":[],"similar_paper_uids":["findings.402"],"title":"The Pragmatics behind Politics: Modelling Metaphor, Framing and Emotion in Political Discourse","tldr":"There has been an increased interest in modelling political discourse within the natural language processing (NLP) community, in tasks such as political bias and misinformation detection, among others. Metaphor-rich and emotion-eliciting communicatio...","track":"Findings of EMNLP"},"forum":"findings.402","id":"findings.402","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.403.png","content":{"abstract":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves over a strong Transformer baseline as measured by human and automatic quality scores and lexical diversity. We also find SMRT is comparable to pretraining in human evaluation quality, and outperforms pretraining on automatic quality and lexical diversity, without requiring related-domain dialog data.","authors":["Huda Khayrallah","Jo\u00e3o Sedoc"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.403","program":"findings","sessions":[],"similar_paper_uids":["findings.403"],"title":"SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training","tldr":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple re...","track":"Findings of EMNLP"},"forum":"findings.403","id":"findings.403","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.404.png","content":{"abstract":"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacy-aware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users\u2019 privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.","authors":["Guangneng Hu","Qiang Yang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.404","program":"findings","sessions":[],"similar_paper_uids":["findings.404"],"title":"PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation","tldr":"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the sour...","track":"Findings of EMNLP"},"forum":"findings.404","id":"findings.404","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.405.png","content":{"abstract":"The success of deep learning methods hinges on the availability of large training datasets annotated for the task of interest. In contrast to human intelligence, these methods lack versatility and struggle to learn and adapt quickly to new tasks, where labeled data is scarce. Meta-learning aims to solve this problem by training a model on a large number of few-shot tasks, with an objective to learn new tasks quickly from a small number of examples. In this paper, we propose a meta-learning framework for few-shot word sense disambiguation (WSD), where the goal is to learn to disambiguate unseen words from only a few labeled instances. Meta-learning approaches have so far been typically tested in an N-way, K-shot classification setting where each task has N classes with K examples per class. Owing to its nature, WSD deviates from this controlled setup and requires the models to handle a large number of highly unbalanced classes. We extend several popular meta-learning approaches to this scenario, and analyze their strengths and weaknesses in this new challenging setting.","authors":["Nithin Holla","Pushkar Mishra","Helen Yannakoudakis","Ekaterina Shutova"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.405","program":"findings","sessions":[],"similar_paper_uids":["findings.405"],"title":"Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense Disambiguation","tldr":"The success of deep learning methods hinges on the availability of large training datasets annotated for the task of interest. In contrast to human intelligence, these methods lack versatility and struggle to learn and adapt quickly to new tasks, whe...","track":"Findings of EMNLP"},"forum":"findings.405","id":"findings.405","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.406.png","content":{"abstract":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2018) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness.","authors":["Renato Negrinho","Matthew R. Gormley","Geoff Gordon"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.406","program":"findings","sessions":[],"similar_paper_uids":["findings.406"],"title":"An Empirical Investigation of Beam-Aware Training in Supertagging","tldr":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and ...","track":"Findings of EMNLP"},"forum":"findings.406","id":"findings.406","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.407.png","content":{"abstract":"Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to compute the hidden/representation vectors without considering the aspect terms and fail to benefit from the overall contextual importance scores of the words that can be obtained from the dependency tree for ABSA. In this work, we propose a novel graph-based deep learning model to overcome these two issues of the prior work on ABSA. In our model, gate vectors are generated from the representation vectors of the aspect terms to customize the hidden vectors of the graph-based models toward the aspect terms. In addition, we propose a mechanism to obtain the importance scores for each word in the sentences based on the dependency trees that are then injected into the model to improve the representation vectors for ABSA. The proposed model achieves the state-of-the-art performance on three benchmark datasets.","authors":["Amir Pouran Ben Veyseh","Nasim Nouri","Franck Dernoncourt","Quan Hung Tran","Dejing Dou","Thien Huu Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.407","program":"findings","sessions":[],"similar_paper_uids":["findings.407"],"title":"Improving Aspect-based Sentiment Analysis with Gated Graph Convolutional Networks and Syntax-based Regulation","tldr":"Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art perfo...","track":"Findings of EMNLP"},"forum":"findings.407","id":"findings.407","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.408.png","content":{"abstract":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our method with a user study, validating that our generated spatial arrangements align with human expectation.","authors":["Gorjan Radevski","Guillem Collell","Marie-Francine Moens","Tinne Tuytelaars"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.408","program":"findings","sessions":[],"similar_paper_uids":["findings.408"],"title":"Decoding Language Spatial Relations to 2D Spatial Arrangements","tldr":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-...","track":"Findings of EMNLP"},"forum":"findings.408","id":"findings.408","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.409.png","content":{"abstract":"The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for representation learning. However, this model does not capture the representations for the nodes in the graphs, thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue, we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets.","authors":["Hieu Minh Tran","Minh Trung Nguyen","Thien Huu Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.409","program":"findings","sessions":[],"similar_paper_uids":["findings.409"],"title":"The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction","tldr":"The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model w...","track":"Findings of EMNLP"},"forum":"findings.409","id":"findings.409","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.410.png","content":{"abstract":"Pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. However, the reasons for their enhanced performance are largely unexamined. In this work, we examine three commonly used pooling techniques (mean-pooling, max-pooling, and attention, and propose *max-attention*, a novel variant that captures interactions among predictive tokens in a sentence. Using novel experiments, we demonstrate that pooling architectures substantially differ from their non-pooling equivalents in their learning ability and positional biases: (i) pooling facilitates better gradient flow than BiLSTMs in initial training epochs, and (ii) BiLSTMs are biased towards tokens at the beginning and end of the input, whereas pooling alleviates this bias. Consequently, we find that pooling yields large gains in low resource scenarios, and instances when salient words lie towards the middle of the input. Across several text classification tasks, we find max-attention to frequently outperform other pooling techniques.","authors":["Pratyush Maini","Keshav Kolluru","Danish Pruthi","Mausam"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.410","program":"findings","sessions":[],"similar_paper_uids":["findings.410"],"title":"Why and when should you pool? Analyzing Pooling in Recurrent Architectures","tldr":"Pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. However, the reasons for their enhanced performance are largely unexamined. In this work, we examine three commo...","track":"Findings of EMNLP"},"forum":"findings.410","id":"findings.410","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.411.png","content":{"abstract":"Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions while the listener encourages the generated captions to contain discriminative information about the input images and personality traits. In this way, we expect that the generated captions can be improved to naturally represent the images and express the traits. In addition, we propose to adapt the language model GPT2 to perform caption generation for PIC. This enables the speaker and listener to benefit from the language encoding capacity of GPT2. Our experiments show that the proposed model achieves the state-of-the-art performance for PIC.","authors":["Minh Thu Nguyen","Duy Phung","Minh Hoai","Thien Huu Nguyen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.411","program":"findings","sessions":[],"similar_paper_uids":["findings.411"],"title":"Structural and Functional Decomposition for Personality Image Captioning in a Communication Game","tldr":"Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker...","track":"Findings of EMNLP"},"forum":"findings.411","id":"findings.411","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.412.png","content":{"abstract":"The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer, enforces the principle properties desired in ranking: local contextualization, hierarchical representation, and query-oriented proximity matching, while it also enjoys efficiency from sparsity. Experiments on four fully supervised and few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the computing complexity and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected self-attention. All source codes, trained model, and predictions of this work are available at https://github.com/hallogameboy/QDS-Transformer.","authors":["Jyun-Yu Jiang","Chenyan Xiong","Chia-Jung Lee","Wei Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.412","program":"findings","sessions":[],"similar_paper_uids":["findings.412"],"title":"Long Document Ranking with Query-Directed Sparse Transformer","tldr":"The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transf...","track":"Findings of EMNLP"},"forum":"findings.412","id":"findings.412","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.413.png","content":{"abstract":"Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/.","authors":["Shailaja Keyur Sampat","Yezhou Yang","Chitta Baral"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.413","program":"findings","sessions":[],"similar_paper_uids":["findings.413"],"title":"Visuo-Linguistic Question Answering (VLQA) Challenge","tldr":"Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoni...","track":"Findings of EMNLP"},"forum":"findings.413","id":"findings.413","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.414.png","content":{"abstract":"The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE\u2019s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.","authors":["Kaj Bostrom","Greg Durrett"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.414","program":"findings","sessions":[],"similar_paper_uids":["findings.414"],"title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining","tldr":"The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (B...","track":"Findings of EMNLP"},"forum":"findings.414","id":"findings.414","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.415.png","content":{"abstract":"Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows \u201cpriming\u201d, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.","authors":["Kanishka Misra","Allyson Ettinger","Julia Rayz"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.415","program":"findings","sessions":[],"similar_paper_uids":["findings.415"],"title":"Exploring BERT\u2019s Sensitivity to Lexical Cues using Tests from Semantic Priming","tldr":"Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analy...","track":"Findings of EMNLP"},"forum":"findings.415","id":"findings.415","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.416.png","content":{"abstract":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed model is able to generate fluent questions with high completeness and outperforms the strongest baseline by 20.8% in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU","authors":["Dan Su","Yan Xu","Wenliang Dai","Ziwei Ji","Tiezheng Yu","Pascale Fung"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.416","program":"findings","sessions":[],"similar_paper_uids":["findings.416"],"title":"Multi-hop Question Generation with Graph Convolutional Network","tldr":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop...","track":"Findings of EMNLP"},"forum":"findings.416","id":"findings.416","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.417.png","content":{"abstract":"We present MMFT-BERT(MultiModal FusionTransformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator\u2019s judgment. This set of questions helps us to study the model\u2019s behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.","authors":["Aisha Urooj","Amir Mazaheri","Niels Da vitoria lobo","Mubarak Shah"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.417","program":"findings","sessions":[],"similar_paper_uids":["findings.417"],"title":"MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering","tldr":"We present MMFT-BERT(MultiModal FusionTransformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video an...","track":"Findings of EMNLP"},"forum":"findings.417","id":"findings.417","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.418.png","content":{"abstract":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \\delta-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.","authors":["Rachel Rudinger","Vered Shwartz","Jena D. Hwang","Chandra Bhagavatula","Maxwell Forbes","Ronan Le Bras","Noah A. Smith","Yejin Choi"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.418","program":"findings","sessions":[],"similar_paper_uids":["findings.418"],"title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language","tldr":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference ha...","track":"Findings of EMNLP"},"forum":"findings.418","id":"findings.418","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.419.png","content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"findings","sessions":[],"similar_paper_uids":["findings.419"],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"Findings of EMNLP"},"forum":"findings.419","id":"findings.419","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.420.png","content":{"abstract":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with multiple visual features with different sizes of receptive fields, though the proper size of the receptive field of visual features intuitively varies depending on expressions. In this paper, we introduce a neural network architecture that modulates visual features with varying sizes of receptive field by linguistic features. We evaluate our architecture on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our architecture. Source code is available at https://github.com/Alab-NII/lcfp .","authors":["Taichi Iki","Akiko Aizawa"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.420","program":"findings","sessions":[],"similar_paper_uids":["findings.420"],"title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks","tldr":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models cons...","track":"Findings of EMNLP"},"forum":"findings.420","id":"findings.420","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.421.png","content":{"abstract":"We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision.","authors":["Haibo Ding","Zhe Feng"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.421","program":"findings","sessions":[],"similar_paper_uids":["findings.421"],"title":"Learning to Classify Events from Human Needs Category Descriptions","tldr":"We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle...","track":"Findings of EMNLP"},"forum":"findings.421","id":"findings.421","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.422.png","content":{"abstract":"Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that reflect the gene function are still named by the experts. To fill this gap, we propose a novel task, namely term name generation for GO, and build a large-scale benchmark dataset. Furthermore, we present a graph-based generative model that incorporates the relations between genes, words and terms for term name generation, which exhibits great advantages over the strong baselines.","authors":["Yanjian Zhang","Qin Chen","Yiteng Zhang","Zhongyu Wei","Yixu Gao","Jiajie Peng","Zengfeng Huang","Weijian Sun","Xuanjing Huang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.422","program":"findings","sessions":[],"similar_paper_uids":["findings.422"],"title":"Automatic Term Name Generation for Gene Ontology: Task and Dataset","tldr":"Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that reflect the gene function are still named by the experts. To fill this gap, w...","track":"Findings of EMNLP"},"forum":"findings.422","id":"findings.422","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.423.png","content":{"abstract":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google Assistant on edge devices with limited memory budgets. We propose to learn compositional code embeddings to greatly reduce the sizes of BERT-base and RoBERTa-base. We also apply the technique to DistilBERT, ALBERT-base, and ALBERT-large, three already compressed BERT variants which attain similar state-of-the-art performances on semantic parsing with much smaller model sizes. We observe 95.15% 98.46% embedding compression rates and 20.47% 34.22% encoder compression rates, while preserving >97.5% semantic parsing performances. We provide the recipe for training and analyze the trade-off between code embedding sizes and downstream performances.","authors":["Prafull Prakash","Saurabh Kumar Shashidhar","Wenlong Zhao","Subendhu Rongali","Haidar Khan","Michael Kayser"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.423","program":"findings","sessions":[],"similar_paper_uids":["findings.423"],"title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings","tldr":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google A...","track":"Findings of EMNLP"},"forum":"findings.423","id":"findings.423","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.424.png","content":{"abstract":"Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models.","authors":["Zhi Zheng","Kai Hui","Ben He","Xianpei Han","Le Sun","Andrew Yates"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.424","program":"findings","sessions":[],"similar_paper_uids":["findings.424"],"title":"BERT-QE: Contextualized Query Expansion for Document Re-ranking","tldr":"Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by rece...","track":"Findings of EMNLP"},"forum":"findings.424","id":"findings.424","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.425.png","content":{"abstract":"The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encoders cannot easily adapt to certain combinations of characters. This leads to a loss of important semantic information, which is especially problematic for Chinese because the language does not have explicit word boundaries. In this paper, we propose ZEN, a BERT-based Chinese text encoder enhanced by n-gram representations, where different combinations of characters are considered during training, thus potential word or phrase boundaries are explicitly pre-trained and fine-tuned with the character encoder (BERT). Therefore ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains. Experimental results illustrated the effectiveness of ZEN on a series of Chinese NLP tasks, where state-of-the-art results is achieved on most tasks with requiring less resource than other published encoders. It is also shown that reasonable performance is obtained when ZEN is trained on a small corpus, which is important for applying pre-training techniques to scenarios with limited data. The code and pre-trained models of ZEN are available at https://github.com/sinovation/ZEN.","authors":["Shizhe Diao","Jiaxin Bai","Yan Song","Tong Zhang","Yonggang Wang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.425","program":"findings","sessions":[],"similar_paper_uids":["findings.425"],"title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations","tldr":"The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encod...","track":"Findings of EMNLP"},"forum":"findings.425","id":"findings.425","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.426.png","content":{"abstract":"Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained BERT model, and find that the automatically-enhanced training sets result in more accurate models.","authors":["Hannah Chen","Yangfeng Ji","David Evans"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.426","program":"findings","sessions":[],"similar_paper_uids":["findings.426"],"title":"Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory","tldr":"Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from th...","track":"Findings of EMNLP"},"forum":"findings.426","id":"findings.426","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.427.png","content":{"abstract":"A case-based reasoning (CBR) system solves a new problem by retrieving \u2018cases\u2019 that are similar to the given problem. If such a system can achieve high accuracy, it is appealing owing to its simplicity, interpretability, and scalability. In this paper, we demonstrate that such a system is achievable for reasoning in knowledge-bases (KBs). Our approach predicts attributes for an entity by gathering reasoning paths from similar entities in the KB. Our probabilistic model estimates the likelihood that a path is effective at answering a query about the given entity. The parameters of our model can be efficiently computed using simple path statistics and require no iterative optimization. Our model is non-parametric, growing dynamically as new entities and relations are added to the KB. On several benchmark datasets our approach significantly outperforms other rule learning approaches and performs comparably to state-of-the-art embedding-based approaches. Furthermore, we demonstrate the effectiveness of our model in an \u201copen-world\u201d setting where new entities arrive in an online fashion, significantly outperforming state-of-the-art approaches and nearly matching the best offline method.","authors":["Rajarshi Das","Ameya Godbole","Nicholas Monath","Manzil Zaheer","Andrew McCallum"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.427","program":"findings","sessions":[],"similar_paper_uids":["findings.427"],"title":"Probabilistic Case-based Reasoning for Open-World Knowledge Graph Completion","tldr":"A case-based reasoning (CBR) system solves a new problem by retrieving \u2018cases\u2019 that are similar to the given problem. If such a system can achieve high accuracy, it is appealing owing to its simplicity, interpretability, and scalability. In this pape...","track":"Findings of EMNLP"},"forum":"findings.427","id":"findings.427","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.428.png","content":{"abstract":"We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.","authors":["Isabel Cachola","Kyle Lo","Arman Cohan","Daniel Weld"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.428","program":"findings","sessions":[],"similar_paper_uids":["findings.428"],"title":"TLDR: Extreme Summarization of Scientific Documents","tldr":"We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate s...","track":"Findings of EMNLP"},"forum":"findings.428","id":"findings.428","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.429.png","content":{"abstract":"The training process of scientific NER models is commonly performed in two steps: i) Pre-training a language model by self-supervised tasks on huge data and ii) fine-tune training with small labelled data. The success of the strategy depends on the relevance between the data domains and between the tasks. However, gaps are found in practice when the target domains are specific and small. We propose a novel framework to introduce a \u201cpre-fine tuning\u201d step between pre-training and fine-tuning. It constructs a corpus by selecting sentences from unlabeled documents that are the most relevant with the labelled training data. Instead of predicting tokens in random spans, the pre-fine tuning task is to predict tokens in entity candidates identified by text mining methods. Pre-fine tuning is automatic and light-weight because the corpus size can be much smaller than pre-training data to achieve a better performance. Experiments on seven benchmarks demonstrate the effectiveness.","authors":["Qingkai Zeng","Wenhao Yu","Mengxia Yu","Tianwen Jiang","Tim Weninger","Meng Jiang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.429","program":"findings","sessions":[],"similar_paper_uids":["findings.429"],"title":"Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER","tldr":"The training process of scientific NER models is commonly performed in two steps: i) Pre-training a language model by self-supervised tasks on huge data and ii) fine-tune training with small labelled data. The success of the strategy depends on the r...","track":"Findings of EMNLP"},"forum":"findings.429","id":"findings.429","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.430.png","content":{"abstract":"Named Entity Recognition (NER) is deeply explored and widely used in various tasks. Usually, some entity mentions are nested in other entities, which leads to the nested NER problem. Leading region based models face both the efficiency and effectiveness challenge due to the high subsequence enumeration complexity. To tackle these challenges, we propose a hierarchical region learning framework to automatically generate a tree hierarchy of candidate regions with nearly linear complexity and incorporate structure information into the region representation for better classification. Experiments on benchmark datasets ACE-2005, GENIA and JNLPBA demonstrate competitive or better results than state-of-the-art baselines.","authors":["Xinwei Long","Shuzi Niu","Yucheng Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.430","program":"findings","sessions":[],"similar_paper_uids":["findings.430"],"title":"Hierarchical Region Learning for Nested Named Entity Recognition","tldr":"Named Entity Recognition (NER) is deeply explored and widely used in various tasks. Usually, some entity mentions are nested in other entities, which leads to the nested NER problem. Leading region based models face both the efficiency and effectiven...","track":"Findings of EMNLP"},"forum":"findings.430","id":"findings.430","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.431.png","content":{"abstract":"Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader\u2019s strategies and paid little attention to the persuadee (user). However, understanding and addressing users\u2019 resistance strategies is an essential job of a persuasive dialog system. So, we adopt a preliminary framework on persuasion resistance in psychology and design a fine-grained resistance strategy annotation scheme. We annotate the PersuasionForGood dataset with the scheme. With the enriched annotations, we build a classifier to predict the resistance strategies. Furthermore, we analyze the relationships between persuasion strategies and persuasion resistance strategies. Our work lays the ground for developing a persuasive dialogue system that can understand and address user resistance strategy appropriately. The code and data will be released.","authors":["Youzhi Tian","Weiyan Shi","Chen Li","Zhou Yu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.431","program":"findings","sessions":[],"similar_paper_uids":["findings.431"],"title":"Understanding User Resistance Strategies in Persuasive Conversations","tldr":"Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader\u2019s strategies and paid little attention to the persuadee...","track":"Findings of EMNLP"},"forum":"findings.431","id":"findings.431","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.432.png","content":{"abstract":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages \u2013 developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance \u2013 a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.","authors":["Yilin Yang","Longyue Wang","Shuming Shi","Prasad Tadepalli","Stefan Lee","Zhaopeng Tu"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.432","program":"findings","sessions":[],"similar_paper_uids":["findings.432"],"title":"On the Sub-layer Functionalities of Transformer Decoder","tldr":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the...","track":"Findings of EMNLP"},"forum":"findings.432","id":"findings.432","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.433.png","content":{"abstract":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8\u00d7 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3\u00d7 reduction in run-time memory footprints and 3.5\u00d7 speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.","authors":["Insoo Chung","Byeongwook Kim","Yoonjung Choi","Se Jung Kwon","Yongkweon Jeon","Baeseong Park","Sangha Kim","Dongsoo Lee"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.433","program":"findings","sessions":[],"similar_paper_uids":["findings.433"],"title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation","tldr":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quan...","track":"Findings of EMNLP"},"forum":"findings.433","id":"findings.433","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.434.png","content":{"abstract":"Out-of-vocabulary (oov) words cause serious troubles in solving natural language tasks with a neural network. Existing approaches to this problem resort to using subwords, which are shorter and more ambiguous units than words, in order to represent oov words with a bag of subwords. In this study, inspired by the processes for creating words from known words, we propose a robust method of estimating oov word embeddings by referring to pre-trained word embeddings for known words with similar surfaces to target oov words. We collect known words by segmenting oov words and by approximate string matching, and we then aggregate their pre-trained embeddings. Experimental results show that the obtained oov word embeddings improve not only word similarity tasks but also downstream tasks in Twitter and biomedical domains where oov words often appear, even when the computed oov embeddings are integrated into a bert-based strong baseline.","authors":["Nobukazu Fukuda","Naoki Yoshinaga","Masaru Kitsuregawa"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.434","program":"findings","sessions":[],"similar_paper_uids":["findings.434"],"title":"Robust Backed-off Estimation of Out-of-Vocabulary Embeddings","tldr":"Out-of-vocabulary (oov) words cause serious troubles in solving natural language tasks with a neural network. Existing approaches to this problem resort to using subwords, which are shorter and more ambiguous units than words, in order to represent o...","track":"Findings of EMNLP"},"forum":"findings.434","id":"findings.434","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.435.png","content":{"abstract":"Emotion Recognition in Conversations (ERC) aims to predict the emotional state of speakers in conversations, which is essentially a text classification task. Unlike the sentence-level text classification problem, the available supervised data for the ERC task is limited, which potentially prevents the models from playing their maximum effect. In this paper, we propose a novel approach to leverage unsupervised conversation data, which is more accessible. Specifically, we propose the Conversation Completion (ConvCom) task, which attempts to select the correct answer from candidate answers to fill a masked utterance in a conversation. Then, we Pre-train a basic COntext-Dependent Encoder (Pre-CODE) on the ConvCom task. Finally, we fine-tune the Pre-CODE on the datasets of ERC. Experimental results demonstrate that pre-training on unsupervised data achieves significant improvement of performance on the ERC datasets, particularly on the minority emotion classes.","authors":["Wenxiang Jiao","Michael Lyu","Irwin King"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.435","program":"findings","sessions":[],"similar_paper_uids":["findings.435"],"title":"Exploiting Unsupervised Data for Emotion Recognition in Conversations","tldr":"Emotion Recognition in Conversations (ERC) aims to predict the emotional state of speakers in conversations, which is essentially a text classification task. Unlike the sentence-level text classification problem, the available supervised data for the...","track":"Findings of EMNLP"},"forum":"findings.435","id":"findings.435","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.436.png","content":{"abstract":"The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parameterizing embedding layers based on the Tensor Train decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers.","authors":["Oleksii Hrinchuk","Valentin Khrulkov","Leyla Mirvakhabova","Elena Orlova","Ivan Oseledets"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.436","program":"findings","sessions":[],"similar_paper_uids":["findings.436"],"title":"Tensorized Embedding Layers","tldr":"The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which preclu...","track":"Findings of EMNLP"},"forum":"findings.436","id":"findings.436","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.437.png","content":{"abstract":"For decades, chitchat bots are designed as a listener to passively answer what people ask. This passive and relatively simple dialogue mechanism gains less attention from humans and consumes the interests of human beings rapidly. Therefore some recent researches attempt to endow the bots with proactivity through external knowledge to transform the role from a listener to a speaker with a hypothesis that the speaker expresses more just like a knowledge disseminator. However, along with the proactive manner introduced into a dialogue agent, an issue arises that, with too many knowledge facts to express, the agent starts to talks endlessly, and even completely ignores what the other expresses in dialogue sometimes, which greatly harms the interest of the other chatter to continue the conversation. To the end, we propose a novel model named Initiative-Imitate to interact with adaptive initiative throughout a dialogue. It forces the agent to express in parallel with the appropriate role during the whole conversation. The corresponding experiments show the proposed Initiative-Imitate obtains competitive results both on the automatic and manual metrics. And the fluency and engagement of the chatbot have also been improved significantly. Besides, the case study indicates the Initiative-Imitate can constantly transfer to appropriate role timely and response more properly during the whole continuous conversation.","authors":["Yafei Liu","Hongjin Qian","Hengpeng Xu","Jinmao Wei"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.437","program":"findings","sessions":[],"similar_paper_uids":["findings.437"],"title":"Speaker or Listener? The Role of a Dialog Agent","tldr":"For decades, chitchat bots are designed as a listener to passively answer what people ask. This passive and relatively simple dialogue mechanism gains less attention from humans and consumes the interests of human beings rapidly. Therefore some recen...","track":"Findings of EMNLP"},"forum":"findings.437","id":"findings.437","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.438.png","content":{"abstract":"We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by BERT with minimal subsequent layers and the text-DB contextualization is realized via the fine-tuned deep attention in BERT. Combined with a pointer-generator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on the well-studied Spider benchmark (65.5% dev, 59.2% test), despite being much simpler than most recently proposed models for this task. Our analysis shows that BRIDGE effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-DB related tasks. Our model implementation is available at https://github.com/ salesforce/TabularSemanticParsing.","authors":["Xi Victoria Lin","Richard Socher","Caiming Xiong"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.438","program":"findings","sessions":[],"similar_paper_uids":["findings.438"],"title":"Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing","tldr":"We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset...","track":"Findings of EMNLP"},"forum":"findings.438","id":"findings.438","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.439.png","content":{"abstract":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results.","authors":["Xikun Zhang","Deepak Ramachandran","Ian Tenney","Yanai Elazar","Dan Roth"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.439","program":"findings","sessions":[],"similar_paper_uids":["findings.439"],"title":"Do Language Embeddings capture Scales?","tldr":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show...","track":"Findings of EMNLP"},"forum":"findings.439","id":"findings.439","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.440.png","content":{"abstract":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model\u2019s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other.","authors":["Yehudit Meged","Avi Caciularu","Vered Shwartz","Ido Dagan"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.440","program":"findings","sessions":[],"similar_paper_uids":["findings.440"],"title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin","tldr":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as dista...","track":"Findings of EMNLP"},"forum":"findings.440","id":"findings.440","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.441.png","content":{"abstract":"Active learning for sentence understanding aims at discovering informative unlabeled data for annotation and therefore reducing the demand for labeled data. We argue that the typical uncertainty sampling method for active learning is time-consuming and can hardly work in real-time, which may lead to ineffective sample selection. We propose adversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popular pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness.","authors":["Dongyu Ru","Jiangtao Feng","Lin Qiu","Hao Zhou","Mingxuan Wang","Weinan Zhang","Yong Yu","Lei Li"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.441","program":"findings","sessions":[],"similar_paper_uids":["findings.441"],"title":"Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space","tldr":"Active learning for sentence understanding aims at discovering informative unlabeled data for annotation and therefore reducing the demand for labeled data. We argue that the typical uncertainty sampling method for active learning is time-consuming a...","track":"Findings of EMNLP"},"forum":"findings.441","id":"findings.441","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.442.png","content":{"abstract":"Spoken languages are ever-changing, with new words entering them all the time. However, coming up with new words (neologisms) today relies exclusively on human creativity. In this paper we propose a system to automatically suggest neologisms. We focus on the Hebrew language as a test case due to the unusual regularity of its noun formation. User studies comparing our algorithm to experts and non-experts demonstrate that our algorithm is capable of generating high-quality outputs, as well as enhance human creativity. More broadly, we seek to inspire more computational work around the topic of linguistic creativity, which we believe offers numerous unexplored opportunities.","authors":["Moran Mizrahi","Stav Yardeni Seelig","Dafna Shahaf"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.442","program":"findings","sessions":[],"similar_paper_uids":["findings.442"],"title":"Coming to Terms: Automatic Formation of Neologisms in Hebrew","tldr":"Spoken languages are ever-changing, with new words entering them all the time. However, coming up with new words (neologisms) today relies exclusively on human creativity. In this paper we propose a system to automatically suggest neologisms. We focu...","track":"Findings of EMNLP"},"forum":"findings.442","id":"findings.442","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.443.png","content":{"abstract":"Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance. However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models. To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining. The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.","authors":["Shang-Yu Su","Yung-Sung Chuang","Yun-Nung Chen"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.443","program":"findings","sessions":[],"similar_paper_uids":["findings.443"],"title":"Dual Inference for Improving Language Understanding and Generation","tldr":"Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focus...","track":"Findings of EMNLP"},"forum":"findings.443","id":"findings.443","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.444.png","content":{"abstract":"Continuous efforts have been devoted to language understanding (LU) for conversational queries with the fast and wide-spread popularity of voice assistants. In this paper, we first study the LU problem in the spatial domain, which is a critical problem for providing location-based services by voice assistants but is without in-depth investigation in existing studies. Spatial domain queries have several unique properties making them be more challenging for language understanding than common conversational queries, including lexical-similar but diverse intents and highly ambiguous words. Thus, a special tailored LU framework for spatial domain queries is necessary. To the end, a dataset was extracted and annotated based on the real-life queries from a voice assistant service. We then proposed a new multi-task framework that jointly learns the intent detection and entity linking tasks on the with invented hierarchical intent detection method and triple-scoring mechanism for entity linking. A specially designed spatial GCN is also utilized to model spatial context information among entities. We have conducted extensive experimental evaluations with state-of-the-art entity linking and intent detection methods, which demonstrated that can outperform all baselines with a significant margin.","authors":["Lei Zhang","Runze Wang","Jingbo Zhou","Jingsong Yu","Zhenhua Ling","Hui Xiong"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.444","program":"findings","sessions":[],"similar_paper_uids":["findings.444"],"title":"Joint Intent Detection and Entity Linking on Spatial Domain Queries","tldr":"Continuous efforts have been devoted to language understanding (LU) for conversational queries with the fast and wide-spread popularity of voice assistants. In this paper, we first study the LU problem in the spatial domain, which is a critical probl...","track":"Findings of EMNLP"},"forum":"findings.444","id":"findings.444","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.445.png","content":{"abstract":"In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on FastText, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (IndicGLUE benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, etc. Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in NLP over a more diverse pool of languages. The data and models are available at https://indicnlp.ai4bharat.org.","authors":["Divyanshu Kakwani","Anoop Kunchukuttan","Satish Golla","Gokul N.C.","Avik Bhattacharyya","Mitesh M. Khapra","Pratyush Kumar"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.445","program":"findings","sessions":[],"similar_paper_uids":["findings.445"],"title":"IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages","tldr":"In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, a...","track":"Findings of EMNLP"},"forum":"findings.445","id":"findings.445","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.446.png","content":{"abstract":"Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrative graph and learn contextualized event representations over it using a relational graph neural network model. We train our model to capture event relations, derived from the Penn Discourse Tree Bank, on a huge corpus, and show that our multi-relational contextualized event representation can improve performance when learning script knowledge without direct supervision and provide a better representation for the implicit discourse sense classification task.","authors":["I-Ta Lee","Maria Leonor Pacheco","Dan Goldwasser"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.446","program":"findings","sessions":[],"similar_paper_uids":["findings.446"],"title":"Weakly-Supervised Modeling of Contextualized Event Embedding for Discourse Relations","tldr":"Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrative graph and learn cont...","track":"Findings of EMNLP"},"forum":"findings.446","id":"findings.446","presentation_id":null},{"card_image_path":"https://raw.githubusercontent.com/acl-org/emnlp-2020-virtual-conference-images/master/paper_images//findings.447.png","content":{"abstract":"Pre-trained language models such as BERT have achieved the state-of-the-art performance on natural language inference (NLI). However, it has been shown that such models can be tricked by variations of surface patterns such as syntax. We investigate the use of dependency trees to enhance the generalization of BERT in the NLI task, leveraging on a graph convolutional network to represent a syntax-based matching graph with heterogeneous matching patterns. Experimental results show that, our syntax-based method largely enhance generalization of BERT on a test set where the sentence pair has high lexical overlap but diverse syntactic structures, and do not degrade performance on the standard test set. In other words, the proposed method makes BERT more robust on syntactic changes.","authors":["Qi He","Han Wang","Yue Zhang"],"demo_url":"","keywords":[""],"material":null,"paper_type":"Findings","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.447","program":"findings","sessions":[],"similar_paper_uids":["findings.447"],"title":"Enhancing Generalization in Natural Language Inference by Syntax","tldr":"Pre-trained language models such as BERT have achieved the state-of-the-art performance on natural language inference (NLI). However, it has been shown that such models can be tricked by variations of surface patterns such as syntax. We investigate t...","track":"Findings of EMNLP"},"forum":"findings.447","id":"findings.447","presentation_id":null},{"content":{"abstract":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.","authors":["Francois Meyer","Martha Lewis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modelling Lexical Ambiguity with Density Matrices","tldr":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelate...","track":"CoNLL 2020"},"id":"WS-1.100","presentation_id":"38939483","rocketchat_channel":"paper-conll-100","speakers":"Francois Meyer|Martha Lewis","title":"Modelling Lexical Ambiguity with Density Matrices"},{"content":{"abstract":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network\u2019s architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","authors":["William Havard","Laurent Besacier","Jean-Pierre Chevrot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech","tldr":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and the...","track":"CoNLL 2020"},"id":"WS-1.101","presentation_id":"38939484","rocketchat_channel":"paper-conll-101","speakers":"William Havard|Laurent Besacier|Jean-Pierre Chevrot","title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech"},{"content":{"abstract":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the \u201cLarge-scale online biomedical semantic indexing\u201d track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach.","authors":["Dusan Grujicic","Gorjan Radevski","Tinne Tuytelaars","Matthew Blaschko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to ground medical text in a 3D human atlas","tldr":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to r...","track":"CoNLL 2020"},"id":"WS-1.108","presentation_id":"38939485","rocketchat_channel":"paper-conll-108","speakers":"Dusan Grujicic|Gorjan Radevski|Tinne Tuytelaars|Matthew Blaschko","title":"Learning to ground medical text in a 3D human atlas"},{"content":{"abstract":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional types. The multilinear maps of words compose with each other to form sentence representations. We extend the skipgram algorithm from vectors to multi- linear maps to learn these representations and instantiate it on unary and binary maps for transitive verbs. These are evaluated on verb and sentence similarity and disambiguation tasks and a subset of the SICK relatedness dataset. Our model performs better than previous type- driven models and is competitive with state of the art representation learning methods such as BERT and neural sentence encoders.","authors":["Gijs Wijnholds","Mehrnoosh Sadrzadeh","Stephen Clark"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Representation Learning for Type-Driven Composition","tldr":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional...","track":"CoNLL 2020"},"id":"WS-1.109","presentation_id":"38939486","rocketchat_channel":"paper-conll-109","speakers":"Gijs Wijnholds|Mehrnoosh Sadrzadeh|Stephen Clark","title":"Representation Learning for Type-Driven Composition"},{"content":{"abstract":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.","authors":["Romain Couillet","Yagmur Gizem Cinar","Eric Gaussier","Muhammad Imran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word Representations Concentrate and This is Good News!","tldr":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p ...","track":"CoNLL 2020"},"id":"WS-1.113","presentation_id":"38939487","rocketchat_channel":"paper-conll-113","speakers":"Romain Couillet|Yagmur Gizem Cinar|Eric Gaussier|Muhammad Imran","title":"Word Representations Concentrate and This is Good News!"},{"content":{"abstract":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, \u201cLazImpa\u201d, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.","authors":["Mathieu Rita","Rahma Chaabouni","Emmanuel Dupoux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently","tldr":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission o...","track":"CoNLL 2020"},"id":"WS-1.115","presentation_id":"38939488","rocketchat_channel":"paper-conll-115","speakers":"Mathieu Rita|Rahma Chaabouni|Emmanuel Dupoux","title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently"},{"content":{"abstract":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.","authors":["Alex Tamkin","Trisha Singh","Davide Giovanardi","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.125","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Transferability in Pretrained Language Models","tldr":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different ...","track":"CoNLL 2020"},"id":"WS-1.1165_F","presentation_id":"38940643","rocketchat_channel":"paper-conll-1165_F","speakers":"Alex Tamkin|Trisha Singh|Davide Giovanardi|Noah Goodman","title":"Investigating Transferability in Pretrained Language Models"},{"content":{"abstract":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic competence, which includes basic linguistic skills encompassing both referential phenomena and generic knowledge, in particular a) the ability to denote, b) the mastery of the lexicon, or c) the ability to model one\u2019s language use on others. Even though each of those faculties has been extensively tested individually, there is still no computational model that would account for their joint acquisition under the conditions experienced by a human. In this paper, I focus on one particular aspect of this problem: the amount of linguistic data available to the child or machine. I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-the-art performance. I argue that both the nature of the data and the way it is presented to the system matter to acquisition.","authors":["Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Re-solve it: simulating the acquisition of core semantic competences from small data","tldr":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic comp...","track":"CoNLL 2020"},"id":"WS-1.127","presentation_id":"38939489","rocketchat_channel":"paper-conll-127","speakers":"Aur\u00e9lie Herbelot","title":"Re-solve it: simulating the acquisition of core semantic competences from small data"},{"content":{"abstract":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training and evaluation of Named Entity Linking (NEL) tools, since different annotation styles correspond to divergent views on the entities present in the same texts. Such divergence is particularly present in texts from the media domain that contain references to creative works. In this work we present a corpus of 1000 annotated documents selected from the media domain. Each document is presented with multiple gold standard annotations representing various annotation styles. This corpus is used to evaluate a series of Named Entity Linking tools in order to understand the impact of the differences in annotation styles on the reported accuracy when processing highly ambiguous entities such as names of creative works. Relaxed annotation guidelines that include overlap styles lead to better results across all tools.","authors":["Adrian M.P. Brasoveanu","Albert Weichselbraun","Lyndon Nixon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works","tldr":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training an...","track":"CoNLL 2020"},"id":"WS-1.128","presentation_id":"38939490","rocketchat_channel":"paper-conll-128","speakers":"Adrian M.P. Brasoveanu|Albert Weichselbraun|Lyndon Nixon","title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works"},{"content":{"abstract":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to motivate two new metrics that address the issues with the standard test, and which distinguish between class-wise offset concentration (similar directions between pairs of words drawn from different broad classes, such as France-London, China-Ottawa,...) and pairing consistency (the existence of a regular transformation between correctly-matched pairs such as France:Paris::China:Beijing). We show that, while the standard analogy test is flawed, several popular word embeddings do nevertheless encode linguistic regularities.","authors":["Louis Fournier","Emmanuel Dupoux","Ewan Dunbar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analogies minus analogy test: measuring regularities in word embeddings","tldr":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to mot...","track":"CoNLL 2020"},"id":"WS-1.136","presentation_id":"38939491","rocketchat_channel":"paper-conll-136","speakers":"Louis Fournier|Emmanuel Dupoux|Ewan Dunbar","title":"Analogies minus analogy test: measuring regularities in word embeddings"},{"content":{"abstract":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.","authors":["Maria A. Rodriguez","Paola Merlo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word associations and the distance properties of context-aware word embeddings","tldr":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embeddin...","track":"CoNLL 2020"},"id":"WS-1.137","presentation_id":"38939492","rocketchat_channel":"paper-conll-137","speakers":"Maria A. Rodriguez|Paola Merlo","title":"Word associations and the distance properties of context-aware word embeddings"},{"content":{"abstract":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on \u00c6Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear \u03bb-calculus with an accuracy of as high as 70%.","authors":["Konstantinos Kogkalidis","Michael Moortgat","Richard Moot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Proof Nets","tldr":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation o...","track":"CoNLL 2020"},"id":"WS-1.14","presentation_id":"38939465","rocketchat_channel":"paper-conll-14","speakers":"Konstantinos Kogkalidis|Michael Moortgat|Richard Moot","title":"Neural Proof Nets"},{"content":{"abstract":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims\u2019 topics and their possible negative impacts are the main factors affecting their check-worthiness.","authors":["Yavuz Selim Kartal","Mucahid Kutlu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales","tldr":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, avai...","track":"CoNLL 2020"},"id":"WS-1.142","presentation_id":"38939493","rocketchat_channel":"paper-conll-142","speakers":"Yavuz Selim Kartal|Mucahid Kutlu","title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales"},{"content":{"abstract":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.","authors":["Forrest Davis","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse structure interacts with reference but not syntax in neural language models","tldr":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different ...","track":"CoNLL 2020"},"id":"WS-1.144","presentation_id":"38939494","rocketchat_channel":"paper-conll-144","speakers":"Forrest Davis|Marten van Schijndel","title":"Discourse structure interacts with reference but not syntax in neural language models"},{"content":{"abstract":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.","authors":["Robert Hawkins","Minae Kwon","Dorsa Sadigh","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Adaptation for Efficient Machine Communication","tldr":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly a...","track":"CoNLL 2020"},"id":"WS-1.147","presentation_id":"38939495","rocketchat_channel":"paper-conll-147","speakers":"Robert Hawkins|Minae Kwon|Dorsa Sadigh|Noah Goodman","title":"Continual Adaptation for Efficient Machine Communication"},{"content":{"abstract":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing explicit representations for objects and their relations by extracting scene graphs from the images. Utilizing an embedding of this scene graph enables our model to more explicitly reason over objects and their relations during story generation, compared to the global features from an object classifier used in previous work. We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features and show that our approach outperforms previous systems. Our experiments also indicate that our models obtain competitive results on reference-based metrics.","authors":["Xudong Hong","Rakshith Shetty","Asad Sayeed","Khushboo Mehra","Vera Demberg","Bernt Schiele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings","tldr":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing expl...","track":"CoNLL 2020"},"id":"WS-1.149","presentation_id":"38939496","rocketchat_channel":"paper-conll-149","speakers":"Xudong Hong|Rakshith Shetty|Asad Sayeed|Khushboo Mehra|Vera Demberg|Bernt Schiele","title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings"},{"content":{"abstract":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TaxiNLI, a new dataset, that has 10k examples from the MNLI dataset with these taxonomic labels. Through various experiments on TaxiNLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies\u2014a large jump over the previous models\u2014some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.","authors":["Pratik Joshi","Somak Aditya","Aalok Sathe","Monojit Choudhury"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TaxiNLI: Taking a Ride up the NLU Hill","tldr":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remain...","track":"CoNLL 2020"},"id":"WS-1.15","presentation_id":"38939466","rocketchat_channel":"paper-conll-15","speakers":"Pratik Joshi|Somak Aditya|Aalok Sathe|Monojit Choudhury","title":"TaxiNLI: Taking a Ride up the NLU Hill"},{"content":{"abstract":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover, historical variations can be present in aged documents, which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models, and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets, and does not degrade the results for modern datasets.","authors":["Emanuela Boros","Ahmed Hamdi","Elvys Linhares Pontes","Luis Adri\u00e1n Cabrera-Diego","Jose G. Moreno","Nicolas Sidere","Antoine Doucet"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents","tldr":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this ...","track":"CoNLL 2020"},"id":"WS-1.152","presentation_id":"38939497","rocketchat_channel":"paper-conll-152","speakers":"Emanuela Boros|Ahmed Hamdi|Elvys Linhares Pontes|Luis Adri\u00e1n Cabrera-Diego|Jose G. Moreno|Nicolas Sidere|Antoine Doucet","title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents"},{"content":{"abstract":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.","authors":["Steven Derby","Paul Miller","Barry Devereux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models","tldr":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure...","track":"CoNLL 2020"},"id":"WS-1.155","presentation_id":"38939498","rocketchat_channel":"paper-conll-155","speakers":"Steven Derby|Paul Miller|Barry Devereux","title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models"},{"content":{"abstract":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.","authors":["Satwik Bhattamishra","Arkil Patel","Navin Goyal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling","tldr":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their po...","track":"CoNLL 2020"},"id":"WS-1.156","presentation_id":"38939499","rocketchat_channel":"paper-conll-156","speakers":"Satwik Bhattamishra|Arkil Patel|Navin Goyal","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"content":{"abstract":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a partition that places images into equivalence classes based on player position. To model this task, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these models generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.","authors":["Allen Nie","Reuben Cohn-Gordon","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.173","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pragmatic Issue-Sensitive Image Captioning","tldr":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the playe...","track":"CoNLL 2020"},"id":"WS-1.1597_F","presentation_id":"38940644","rocketchat_channel":"paper-conll-1597_F","speakers":"Allen Nie|Reuben Cohn-Gordon|Christopher Potts","title":"Pragmatic Issue-Sensitive Image Captioning"},{"content":{"abstract":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames the task as an inference problem for a general statistical model consisting of observed data (potentially cognate pairs of words), latent variables (the cognacy status of pairs) and unknown global parameters (which sounds correspond between languages). We then give a specific instance of such a model along with an expectation-maximisation algorithm to infer its parameters. We evaluate our system on a corpus of 8140 cognate sets, finding the performance of our method to be comparable to the state of the art. We additionally carry out qualitative analysis demonstrating advantages it has over existing systems. We also suggest several ways our work could be extended within the general theoretical framework we propose.","authors":["Roddy MacSween","Andrew Caines"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Expectation Maximisation Algorithm for Automated Cognate Detection","tldr":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames ...","track":"CoNLL 2020"},"id":"WS-1.162","presentation_id":"38939500","rocketchat_channel":"paper-conll-162","speakers":"Roddy MacSween|Andrew Caines","title":"An Expectation Maximisation Algorithm for Automated Cognate Detection"},{"content":{"abstract":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.","authors":["Debasmita Bhattacharya","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filler-gaps that neural networks fail to generalize","tldr":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap de...","track":"CoNLL 2020"},"id":"WS-1.168","presentation_id":"38939501","rocketchat_channel":"paper-conll-168","speakers":"Debasmita Bhattacharya|Marten van Schijndel","title":"Filler-gaps that neural networks fail to generalize"},{"content":{"abstract":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.","authors":["Qile Zhu","Haidar Khan","Saleh Soltan","Stephen Rawls","Wael Hamza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding","tldr":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, ...","track":"CoNLL 2020"},"id":"WS-1.177","presentation_id":"38939502","rocketchat_channel":"paper-conll-177","speakers":"Qile Zhu|Haidar Khan|Saleh Soltan|Stephen Rawls|Wael Hamza","title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding"},{"content":{"abstract":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments. We use SuspectGuilt to train and assess predictive models which validate the usefulness of the corpus, and show that these models benefit from genre pretraining and joint supervision from the text-level ratings and span-level annotations. Such models might be used as tools for understanding the societal effects of crime reporting.","authors":["Elisa Kreiss","Zijian Wang","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives","tldr":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime ...","track":"CoNLL 2020"},"id":"WS-1.18","presentation_id":"38939467","rocketchat_channel":"paper-conll-18","speakers":"Elisa Kreiss|Zijian Wang|Christopher Potts","title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives"},{"content":{"abstract":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network (RNN) controller a Nondeterministic Stack RNN. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.","authors":["Brian DuSell","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Context-free Languages with Nondeterministic Stack RNNs","tldr":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this dat...","track":"CoNLL 2020"},"id":"WS-1.183","presentation_id":"38939503","rocketchat_channel":"paper-conll-183","speakers":"Brian DuSell|David Chiang","title":"Learning Context-free Languages with Nondeterministic Stack RNNs"},{"content":{"abstract":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representation of the scaffolding and dynamics that guide a coherent narrative. This paper introduces a new model that integrates explicit narrative structure with neural language models, formalizing narrative modeling as a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in which the latent dynamics of the system (i.e. how the state vector transforms over time) is controlled by top-level discrete switching variables. The switching variables represent narrative structure (e.g., sentiment or discourse states), while the latent state vector encodes information on the current state of the narrative. This probabilistic formulation allows us to control generation, and can be learned in a semi-supervised fashion using both labeled and unlabeled data. Additionally, we derive a Gibbs sampler for our model that can \u201cfill in\u201d arbitrary parts of the narrative, guided by the switching variables. Our filled-in (English language) narratives outperform several baselines on both automatic and human evaluations","authors":["Noah Weber","Leena Shekhar","Heeyoung Kwon","Niranjan Balasubramanian","Nathanael Chambers"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Narrative Text in a Switching Dynamical System","tldr":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representa...","track":"CoNLL 2020"},"id":"WS-1.185","presentation_id":"38939504","rocketchat_channel":"paper-conll-185","speakers":"Noah Weber|Leena Shekhar|Heeyoung Kwon|Niranjan Balasubramanian|Nathanael Chambers","title":"Generating Narrative Text in a Switching Dynamical System"},{"content":{"abstract":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. This task is inspired bycomputational and cognitive studies of eventunderstanding, which suggest that understand-ing processes of events is often directed by rec-ognizing the goals, plans or intentions of theprotagonist(s). We develop a large dataset con-taining over 60k event processes, featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10\u02c63\u223c10\u02c64)label vocabularies. We then propose a hybridlearning framework,P2GT, which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. As our experiments indi-cate,P2GTsupports identifying the intent ofprocesses, as well as the fine semantic type ofthe affected object. It also demonstrates the ca-pability of handling few-shot cases, and stronggeneralizability on out-of-domain processes.","authors":["Muhao Chen","Hongming Zhang","Haoyu Wang","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.43","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Are You Trying to Do? Semantic Typing of Event Processes","tldr":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object ...","track":"CoNLL 2020"},"id":"WS-1.189","presentation_id":"38939505","rocketchat_channel":"paper-conll-189","speakers":"Muhao Chen|Hongming Zhang|Haoyu Wang|Dan Roth","title":"What Are You Trying to Do? Semantic Typing of Event Processes"},{"content":{"abstract":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information from it. The corpus has been constructed with two main tasks in mind. The first one, to extract entities about outbreaks such as disease, host, location among others. The second one, to retrieve relations among entities, for instance, in such geographic location fifteen cases of a given disease were reported. Overall, our goal is to offer resources and tools to perform an automated analysis so as to support early detection of disease outbreaks and therefore diminish their spreading.","authors":["Antonella Dellanzo","Viviana Cotik","Jose Ochoa-Luna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.44","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America","tldr":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information f...","track":"CoNLL 2020"},"id":"WS-1.195","presentation_id":"38939506","rocketchat_channel":"paper-conll-195","speakers":"Antonella Dellanzo|Viviana Cotik|Jose Ochoa-Luna","title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America"},{"content":{"abstract":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.","authors":["Nora Kassner","Benno Krojer","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.45","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?","tldr":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present...","track":"CoNLL 2020"},"id":"WS-1.202","presentation_id":"38939507","rocketchat_channel":"paper-conll-202","speakers":"Nora Kassner|Benno Krojer|Hinrich Sch\u00fctze","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"content":{"abstract":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.","authors":["Mark Anderson","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Frailty of Universal POS Tags for Neural UD Parsers","tldr":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear ...","track":"CoNLL 2020"},"id":"WS-1.21","presentation_id":"38939468","rocketchat_channel":"paper-conll-21","speakers":"Mark Anderson|Carlos G\u00f3mez-Rodr\u00edguez","title":"On the Frailty of Universal POS Tags for Neural UD Parsers"},{"content":{"abstract":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent\u2019s language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs.","authors":["Tanmay Parekh","Emily Ahn","Yulia Tsvetkov","Alan W Black"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.46","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues","tldr":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings....","track":"CoNLL 2020"},"id":"WS-1.218","presentation_id":"38939508","rocketchat_channel":"paper-conll-218","speakers":"Tanmay Parekh|Emily Ahn|Yulia Tsvetkov|Alan W Black","title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues"},{"content":{"abstract":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-motor behaviour of typing. Most work to date has focused solely on the timing of keypresses without reference to the linguistic content. In this paper we argue that the identity of the key combinations being produced should impact how they are handled by people with PD, and provide evidence that natural language processing methods can thus be of help in identifying signs of disease. We test the performance of a bi-directional LSTM with convolutional features in distinguishing people with PD from age-matched controls typing in English and Spanish, both in clinics and online.","authors":["Neil Dhir","Mathias Edman","\u00c1lvaro Sanchez Ferro","Tom Stafford","Colin Bannard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.47","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network","tldr":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-m...","track":"CoNLL 2020"},"id":"WS-1.221","presentation_id":"38939509","rocketchat_channel":"paper-conll-221","speakers":"Neil Dhir|Mathias Edman|\u00c1lvaro Sanchez Ferro|Tom Stafford|Colin Bannard","title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network"},{"content":{"abstract":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models\u2019 generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it\u2019s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models.","authors":["Tianyu Liu","Zheng Xin","Xiaoan Ding","Baobao Chang","Zhifang Sui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference","tldr":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is i...","track":"CoNLL 2020"},"id":"WS-1.222","presentation_id":"38939510","rocketchat_channel":"paper-conll-222","speakers":"Tianyu Liu|Zheng Xin|Xiaoan Ding|Baobao Chang|Zhifang Sui","title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference"},{"content":{"abstract":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.","authors":["Tiwalayo Eisape","Noga Zaslavsky","Roger Levy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cloze Distillation Improves Psychometric Predictive Power","tldr":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human n...","track":"CoNLL 2020"},"id":"WS-1.226","presentation_id":"38939511","rocketchat_channel":"paper-conll-226","speakers":"Tiwalayo Eisape|Noga Zaslavsky|Roger Levy","title":"Cloze Distillation Improves Psychometric Predictive Power"},{"content":{"abstract":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. We find that a highly augmented model shows highest accuracy in predicting held-out forms, and investigate other properties of interest learned by our models\u2019 representations. We outline extensions to this architecture that can better capture variation in Indo-Aryan sound change.","authors":["Chundra Cathcart","Taraka Rama"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.50","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping","tldr":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. ...","track":"CoNLL 2020"},"id":"WS-1.234","presentation_id":"38939512","rocketchat_channel":"paper-conll-234","speakers":"Chundra Cathcart|Taraka Rama","title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping"},{"content":{"abstract":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign language as a manual gesture recognition problem. However, signers use other articulators: facial expressions, head and body position and movement to convey linguistic information. Given the important role of non-manual markers, this paper proposes a dataset and presents a use case to stress the importance of including non-manual features to improve the recognition accuracy of signs. To the best of our knowledge no prior publicly available dataset exists that explicitly focuses on non-manual components responsible for the grammar of sign languages. To this end, the proposed dataset contains 28250 videos of signs of high resolution and quality, with annotation of manual and non-manual components. We conducted a series of evaluations in order to investigate whether non-manual components would improve signs\u2019 recognition accuracy. We release the dataset to encourage SLR researchers and help advance current progress in this area toward real-time sign language interpretation. Our dataset will be made publicly available at https://krslproject.github.io/krsl-corpus","authors":["Alfarabi Imashev","Medet Mukushev","Vadim Kimmelman","Anara Sandygulova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.51","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL","tldr":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign...","track":"CoNLL 2020"},"id":"WS-1.247","presentation_id":"38939513","rocketchat_channel":"paper-conll-247","speakers":"Alfarabi Imashev|Medet Mukushev|Vadim Kimmelman|Anara Sandygulova","title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL"},{"content":{"abstract":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor\u2019s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance.","authors":["Tomasz Dwojak","Micha\u0142 Pietruszka","\u0141ukasz Borchmann","Jakub Ch\u0142\u0119dowski","Filip Grali\u0144ski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.52","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Dataset Recycling to Multi-Property Extraction and Beyond","tldr":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introdu...","track":"CoNLL 2020"},"id":"WS-1.258","presentation_id":"38939514","rocketchat_channel":"paper-conll-258","speakers":"Tomasz Dwojak|Micha\u0142 Pietruszka|\u0141ukasz Borchmann|Jakub Ch\u0142\u0119dowski|Filip Grali\u0144ski","title":"From Dataset Recycling to Multi-Property Extraction and Beyond"},{"content":{"abstract":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published neurolinguistic studies of the N400. We find that surprisal can predict N400 amplitude in a wide range of cases, and the cases where it cannot do so provide valuable insight into the neurocognitive processes underlying the response.","authors":["James Michaelov","Benjamin Bergen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How well does surprisal explain N400 amplitude under different experimental conditions?","tldr":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published n...","track":"CoNLL 2020"},"id":"WS-1.259","presentation_id":"38939515","rocketchat_channel":"paper-conll-259","speakers":"James Michaelov|Benjamin Bergen","title":"How well does surprisal explain N400 amplitude under different experimental conditions?"},{"content":{"abstract":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems. Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.","authors":["Leshem Choshen","Dmitry Nikolaev","Yevgeni Berzak","Omri Abend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Classifying Syntactic Errors in Learner Language","tldr":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation sch...","track":"CoNLL 2020"},"id":"WS-1.26","presentation_id":"38939469","rocketchat_channel":"paper-conll-26","speakers":"Leshem Choshen|Dmitry Nikolaev|Yevgeni Berzak|Omri Abend","title":"Classifying Syntactic Errors in Learner Language"},{"content":{"abstract":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowledge. However, designing probing tasks for lesser-resourced languages is tricky, because these often lack largescale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in English. To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study. We show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcomes. We then probe embeddings in a multilingual setup with design choices that lie in a \u2018stable region\u2019, as we identify for English, and find that results on English do not transfer to other languages. Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future.","authors":["Steffen Eger","Johannes Daxenberger","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation","tldr":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowled...","track":"CoNLL 2020"},"id":"WS-1.28","presentation_id":"38939470","rocketchat_channel":"paper-conll-28","speakers":"Steffen Eger|Johannes Daxenberger|Iryna Gurevych","title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation"},{"content":{"abstract":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of these approaches focus strictly on leveraging the co-occurrences of relationship word pairs within sentences. In this paper, we investigate the hypothesis that examples of a lexical relation in a corpus are fundamental to a neural word embedding\u2019s ability to complete analogies involving the relation. Our experiments, in which we remove all known examples of a relation from training corpora, show only marginal degradation in analogy completion performance involving the removed relation. This finding enhances our understanding of neural word embeddings, showing that co-occurrence information of a particular semantic relation is the not the main source of their structural regularity.","authors":["Hsiao-Yu Chiang","Jose Camacho-Collados","Zachary Pardos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding the Source of Semantic Regularities in Word Embeddings","tldr":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of ...","track":"CoNLL 2020"},"id":"WS-1.29","presentation_id":"38939471","rocketchat_channel":"paper-conll-29","speakers":"Hsiao-Yu Chiang|Jose Camacho-Collados|Zachary Pardos","title":"Understanding the Source of Semantic Regularities in Word Embeddings"},{"content":{"abstract":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric element one has propagated in the limited body of computational work on one-anaphora resolution, making this task harder than it is. In the present paper, we resolve this by drawing from an adequate linguistic analysis of the word one in different syntactic environments - once again highlighting the significance of linguistic theory in Natural Language Processing (NLP) tasks. We prepare an annotated corpus marking actual instances of one-anaphora with their textual antecedents, and use the annotations to experiment with state-of-the art neural models for one-anaphora resolution. Apart from presenting a strong neural baseline for this task, we contribute a gold-standard corpus, which is, to the best of our knowledge, the biggest resource on one-anaphora till date.","authors":["Payal Khullar","Arghya Bhattacharya","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Finding The Right One and Resolving it","tldr":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric...","track":"CoNLL 2020"},"id":"WS-1.38","presentation_id":"38939472","rocketchat_channel":"paper-conll-38","speakers":"Payal Khullar|Arghya Bhattacharya|Manish Shrivastava","title":"Finding The Right One and Resolving it"},{"content":{"abstract":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.","authors":["Jonathan Malmaud","Roger Levy","Yevgeni Berzak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension","tldr":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking d...","track":"CoNLL 2020"},"id":"WS-1.49","presentation_id":"38939473","rocketchat_channel":"paper-conll-49","speakers":"Jonathan Malmaud|Roger Levy|Yevgeni Berzak","title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension"},{"content":{"abstract":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","authors":["John P. Lalor","Hong Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation","tldr":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic...","track":"CoNLL 2020"},"id":"WS-1.510_F","presentation_id":"38940641","rocketchat_channel":"paper-conll-510_F","speakers":"John P. Lalor|Hong Yu","title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation"},{"content":{"abstract":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction to syntactically and semantically sound language stimuli. In this study, we asked: how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain\u2019s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain\u2019s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.","authors":["Maryam Hashemzadeh","Greta Kaufeld","Martha White","Andrea E. Martin","Alona Fyshe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.57","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?","tldr":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction t...","track":"CoNLL 2020"},"id":"WS-1.561_F","presentation_id":"38940642","rocketchat_channel":"paper-conll-561_F","speakers":"Maryam Hashemzadeh|Greta Kaufeld|Martha White|Andrea E. Martin|Alona Fyshe","title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?"},{"content":{"abstract":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the dataset and evaluate it with state-of-the-art summarisation methods.","authors":["Yifan Chen","Tamara Polajnar","Colin Batchelor","Simone Teufel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus of Very Short Scientific Summaries","tldr":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first...","track":"CoNLL 2020"},"id":"WS-1.59","presentation_id":"38939474","rocketchat_channel":"paper-conll-59","speakers":"Yifan Chen|Tamara Polajnar|Colin Batchelor|Simone Teufel","title":"A Corpus of Very Short Scientific Summaries"},{"content":{"abstract":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training an LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model\u2019s generated output (its \u2018babbling\u2019), compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.","authors":["Ludovica Pannitto","Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data","tldr":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a chil...","track":"CoNLL 2020"},"id":"WS-1.61","presentation_id":"38939475","rocketchat_channel":"paper-conll-61","speakers":"Ludovica Pannitto|Aur\u00e9lie Herbelot","title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data"},{"content":{"abstract":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively. Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost. We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions. We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.","authors":["Jacqueline van Arkel","Marieke Woensdregt","Mark Dingemanse","Mark Blokpoel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis","tldr":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators...","track":"CoNLL 2020"},"id":"WS-1.63","presentation_id":"38939476","rocketchat_channel":"paper-conll-63","speakers":"Jacqueline van Arkel|Marieke Woensdregt|Mark Dingemanse|Mark Blokpoel","title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis"},{"content":{"abstract":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.","authors":["Cory Shain","Micha Elsner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Acquiring language from speech by learning to remember and predict","tldr":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we pro...","track":"CoNLL 2020"},"id":"WS-1.69","presentation_id":"38939477","rocketchat_channel":"paper-conll-69","speakers":"Cory Shain|Micha Elsner","title":"Acquiring language from speech by learning to remember and predict"},{"content":{"abstract":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where the questioner is not given the context from which answers are drawn, but must reason pragmatically about how to acquire new information, given the shared conversation history. We identify two core challenges: (1) formally defining the informativeness of potential questions, and (2) exploring the prohibitively large space of potential questions to find the good candidates. To generate pragmatic questions, we use reinforcement learning to optimize an informativeness metric we propose, combined with a reward function designed to promote more specific questions. We demonstrate that the resulting pragmatic questioner substantially improves the informativeness and specificity of questions generated over a baseline model, as evaluated by our metrics as well as humans.","authors":["Peng Qi","Yuhao Zhang","Christopher D. Manning"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations","tldr":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where t...","track":"CoNLL 2020"},"id":"WS-1.69_F","presentation_id":"38940640","rocketchat_channel":"paper-conll-69_F","speakers":"Peng Qi|Yuhao Zhang|Christopher D. Manning","title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations"},{"content":{"abstract":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.","authors":["Hongyu Gong","Suma Bhat","Pramod Viswanath"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enriching Word Embeddings with Temporal and Spatial Information","tldr":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may requ...","track":"CoNLL 2020"},"id":"WS-1.7","presentation_id":"38939463","rocketchat_channel":"paper-conll-7","speakers":"Hongyu Gong|Suma Bhat|Pramod Viswanath","title":"Enriching Word Embeddings with Temporal and Spatial Information"},{"content":{"abstract":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus. We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning. We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data. Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-of-the-art models.","authors":["Frederick Reiss","Hong Xu","Bryan Cutler","Karthik Muthuraman","Zachary Eichenberger"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus","tldr":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth ...","track":"CoNLL 2020"},"id":"WS-1.70","presentation_id":"38939478","rocketchat_channel":"paper-conll-70","speakers":"Frederick Reiss|Hong Xu|Bryan Cutler|Karthik Muthuraman|Zachary Eichenberger","title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus"},{"content":{"abstract":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT\u2019s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.","authors":["Gabriella Chronis","Katrin Erk"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships","tldr":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These e...","track":"CoNLL 2020"},"id":"WS-1.73","presentation_id":"38939479","rocketchat_channel":"paper-conll-73","speakers":"Gabriella Chronis|Katrin Erk","title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships"},{"content":{"abstract":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - MQA-RC, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models. However, we show this relationship does not hold true for the XLNet models \u2013 despite the fact that the XLNet performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.","authors":["Ekta Sood","Simon Tannert","Diego Frassinelli","Andreas Bulling","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension","tldr":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new metho...","track":"CoNLL 2020"},"id":"WS-1.8","presentation_id":"38939464","rocketchat_channel":"paper-conll-8","speakers":"Ekta Sood|Simon Tannert|Diego Frassinelli|Andreas Bulling|Ngoc Thang Vu","title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension"},{"content":{"abstract":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty \u2014 entropy-based UID, surprisal-based UID, and pointwise mutual information \u2014 to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.","authors":["Brennan Gonering","Emily Morgan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Processing effort is a poor predictor of cross-linguistic word order frequency","tldr":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Densit...","track":"CoNLL 2020"},"id":"WS-1.83","presentation_id":"38939480","rocketchat_channel":"paper-conll-83","speakers":"Brennan Gonering|Emily Morgan","title":"Processing effort is a poor predictor of cross-linguistic word order frequency"},{"content":{"abstract":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions (about 70%) is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.","authors":["Maja Popovi\u0107"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relations between comprehensibility and adequacy errors in machine translation output","tldr":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all ma...","track":"CoNLL 2020"},"id":"WS-1.88","presentation_id":"38939481","rocketchat_channel":"paper-conll-88","speakers":"Maja Popovi\u0107","title":"Relations between comprehensibility and adequacy errors in machine translation output"},{"content":{"abstract":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier\u2019s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","authors":["Hartger Veeman","Marc Allassonni\u00e8re-Tang","Aleksandrs Berdicevskis","Ali Basirat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment","tldr":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two e...","track":"CoNLL 2020"},"id":"WS-1.96","presentation_id":"38939482","rocketchat_channel":"paper-conll-96","speakers":"Hartger Veeman|Marc Allassonni\u00e8re-Tang|Aleksandrs Berdicevskis|Ali Basirat","title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment"},{"content":{"abstract":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a graph notation. To this end, we designed a novel Plain Graph Notation (PGN) that handles various graphs universally. Then, our parser predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our parser can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the parser tied for 1st place in both cross-framework and cross-lingual tracks.","authors":["Hiroaki Ozaki","Gaku Morio","Yuta Koreeda","Terufumi Morishita","Toshinori Miyoshi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer","tldr":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text...","track":"CoNLL 2020"},"id":"WS-1.Shared1","presentation_id":"38941228","rocketchat_channel":"paper-conll-Shared1","speakers":"Hiroaki Ozaki|Gaku Morio|Yuta Koreeda|Terufumi Morishita|Toshinori Miyoshi","title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer"},{"content":{"abstract":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. Our solution consists of two sub-systems: transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) and iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.","authors":["Longxu Dou","Yunlong Feng","Yuqiu Ji","Wanxiang Che","Ting Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser","tldr":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, A...","track":"CoNLL 2020"},"id":"WS-1.Shared2","presentation_id":"38941229","rocketchat_channel":"paper-conll-Shared2","speakers":"Longxu Dou|Yunlong Feng|Yuqiu Ji|Wanxiang Che|Ting Liu","title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser"},{"content":{"abstract":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.","authors":["Ofir Arviv","Ruixiang Cui","Daniel Hershcovich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers","tldr":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respe...","track":"CoNLL 2020"},"id":"WS-1.Shared3","presentation_id":"38941230","rocketchat_channel":"paper-conll-Shared3","speakers":"Ofir Arviv|Ruixiang Cui|Daniel Hershcovich","title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers"},{"content":{"abstract":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the abstract meaning representation framework and propose a joint state model for the graph-sequence iterative inference of (Cai and Lam, 2020) for a simplified graph-sequence inference. In our joint state model, we update only a single joint state vector during the graph-sequence inference process instead of keeping the dual state vectors, and all other components are exactly the same as in (Cai and Lam, 2020).","authors":["Seung-Hoon Na","Jinwoo Min"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference","tldr":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the...","track":"CoNLL 2020"},"id":"WS-1.Shared4","presentation_id":"38941231","rocketchat_channel":"paper-conll-Shared4","speakers":"Seung-Hoon Na|Jinwoo Min","title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference"},{"content":{"abstract":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.","authors":["David Samuel","Milan Straka"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN","tldr":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the ...","track":"CoNLL 2020"},"id":"WS-1.Shared5","presentation_id":"38941232","rocketchat_channel":"paper-conll-Shared5","speakers":"David Samuel|Milan Straka","title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN"},{"content":{"abstract":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.","authors":["Daniel Zeman","Jan Hajic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FGD at MRP 2020: Prague Tectogrammatical Graphs","tldr":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG ...","track":"CoNLL 2020"},"id":"WS-1.Shared6","presentation_id":"38941233","rocketchat_channel":"paper-conll-Shared6","speakers":"Daniel Zeman|Jan Hajic","title":"FGD at MRP 2020: Prague Tectogrammatical Graphs"},{"content":{"abstract":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.","authors":["Lasha Abzianidze","Johan Bos","Stephan Oepen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs","tldr":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpreta...","track":"CoNLL 2020"},"id":"WS-1.Shared7","presentation_id":"38941234","rocketchat_channel":"paper-conll-Shared7","speakers":"Lasha Abzianidze|Johan Bos|Stephan Oepen","title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs"},{"content":{"abstract":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu","authors":["Stephan Oepen","Omri Abend","Lasha Abzianidze","Johan Bos","Jan Hajic","Daniel Hershcovich","Bin Li","Tim O\u2019Gorman","Nianwen Xue","Daniel Zeman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing","tldr":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the ...","track":"CoNLL 2020"},"id":"WS-1.Shared8","presentation_id":"38941235","rocketchat_channel":"paper-conll-Shared8","speakers":"Stephan Oepen|Omri Abend|Lasha Abzianidze|Johan Bos|Jan Hajic|Daniel Hershcovich|Bin Li|Tim O\u2019Gorman|Nianwen Xue|Daniel Zeman","title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing"},{"content":{"abstract":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese->English.","authors":["Tanfang Chen","Weiwei Wang","Wenyang Wei","Xing Shi","Xiangang Li","Jieping Ye","Kevin Knight"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.7.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiDi's Machine Translation System for WMT2020","tldr":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1","presentation_id":"38939543","rocketchat_channel":"paper-wmt-1","speakers":"Tanfang Chen|Weiwei Wang|Wenyang Wei|Xing Shi|Xiangang Li|Jieping Ye|Kevin Knight","title":"DiDi's Machine Translation System for WMT2020"},{"content":{"abstract":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7% and 5% relative improvement, over the baseline, in sacreBLEU score on the test set for Pashto and Khmer respectively.","authors":["Muhammad ElNokrashy","Amr Hendy","Mohamed Abdelghaffar","Mohamed Afify","Ahmed Tawfik","Hany Hassan Awadalla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.106.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions","tldr":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.100","presentation_id":"38939612","rocketchat_channel":"paper-wmt-100","speakers":"Muhammad ElNokrashy|Amr Hendy|Mohamed Abdelghaffar|Mohamed Afify|Ahmed Tawfik|Hany Hassan Awadalla","title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions"},{"content":{"abstract":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting. We find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the UNMT system trained on it. Furthermore, we show that document-level data selection should be preferred for training the XLM model when possible. Finally, we show that there is a trade-off between quality and quantity of the data used to train UNMT systems.","authors":["Lukas Edman","Antonio Toral","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.130.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian","tldr":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.101","presentation_id":"38939613","rocketchat_channel":"paper-wmt-101","speakers":"Lukas Edman|Antonio Toral|Gertjan van Noord","title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian"},{"content":{"abstract":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","authors":["Art\u016brs Stafanovi\u010ds","M\u0101rcis Pinnis","Toms Bergmanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.73.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations","tldr":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.102","presentation_id":"38939614","rocketchat_channel":"paper-wmt-102","speakers":"Art\u016brs Stafanovi\u010ds|M\u0101rcis Pinnis|Toms Bergmanis","title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations"},{"content":{"abstract":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model trained with the Fairseq sequence modeling toolkit. Our final system is an ensemble of two transformer models, which ranked first in WMT20 evaluation. One model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are same alphabet languages. The other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.","authors":["Kamalkumar Rathinasamy","Amanpreet Singh","Balaguru Sivasambagupta","Prajna Prasad Neerchal","Vani Sivasankaran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.52.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task","tldr":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.103","presentation_id":"38939615","rocketchat_channel":"paper-wmt-103","speakers":"Kamalkumar Rathinasamy|Amanpreet Singh|Balaguru Sivasambagupta|Prajna Prasad Neerchal|Vani Sivasankaran","title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model, Transformer. We applied various strategies in order to improve our baseline MT systems, e.g. onolin- gual sentence selection for creating synthetic training data, mining monolingual sentences for adapting our MT systems to the task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks.","authors":["Venkatesh Parthasarathy","Akshai Ramesh","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.27.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT System Description for the WMT20 News Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.104","presentation_id":"38939616","rocketchat_channel":"paper-wmt-104","speakers":"Venkatesh Parthasarathy|Akshai Ramesh|Rejwanul Haque|Andy Way","title":"The ADAPT System Description for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical terminologies, and using the state-of-the-art neural MT (NMT) model: Transformer. In order to improve our baseline NMT system, we employ a number of methods, e.g. \u201cpseudo\u201d parallel data selection, monolingual data selection for synthetic corpus creation, mining monolingual sentences for adapting our NMT systems to this task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that systematic addition of the aforementioned techniques to the baseline yields an excellent performance in the English-to-Basque translation task.","authors":["Prashant Nayak","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.91.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical ter...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.105","presentation_id":"38939617","rocketchat_channel":"paper-wmt-105","speakers":"Prashant Nayak|Rejwanul Haque|Andy Way","title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English into French, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from English into German; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions: zero-shot and few-shot domain adaptation.","authors":["Sadaf Abdul Rauf","Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez","Minh Quang Pham","Fran\u00e7ois Yvon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.86.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LIMSI @ WMT 2020","tldr":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English int...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.107","presentation_id":"38939618","rocketchat_channel":"paper-wmt-107","speakers":"Sadaf Abdul Rauf|Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez|Minh Quang Pham|Fran\u00e7ois Yvon","title":"LIMSI @ WMT 2020"},{"content":{"abstract":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions, from Russian to English and from English to Russian. We used official training data, 102 million parallel corpora and 10 million monolingual corpora. Our baseline systems are Transformer models trained with the Sockeye sequence modeling toolkit, supplemented by bi-text data filtering schemes, back-translations, reordering and other related processing methods. The BLEU value of our translation result from Russian to English is 35.7, ranking 5th, while from English to Russian is 39.8, ranking 2th.","authors":["ariel Xv"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.35.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Russian-English Bidirectional Machine Translation System","tldr":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.109","presentation_id":"38939619","rocketchat_channel":"paper-wmt-109","speakers":"ariel Xv","title":"Russian-English Bidirectional Machine Translation System"},{"content":{"abstract":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish -> Portuguese.","authors":["Lovish Madaan","Soumya Sharma","Parag Singla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.46.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task","tldr":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.113","presentation_id":"38939620","rocketchat_channel":"paper-wmt-113","speakers":"Lovish Madaan|Soumya Sharma|Parag Singla","title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task"},{"content":{"abstract":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.","authors":["Keshaw Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.136.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20","tldr":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transforme...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.114","presentation_id":"38939621","rocketchat_channel":"paper-wmt-114","speakers":"Keshaw Singh","title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20"},{"content":{"abstract":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and data augmentation, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our system significantly outperforms all other baselines and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our submission can achieve +5.56 BLEU and -4.57 TER with respect to the official MT baseline.","authors":["Jiayi Wang","Ke Wang","Kai Fan","Yuqi Zhang","Jun Lu","Xin Ge","Yangbin Shi","Yu Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.84.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT","tldr":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Ta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.115","presentation_id":"38939622","rocketchat_channel":"paper-wmt-115","speakers":"Jiayi Wang|Ke Wang|Kai Fan|Yuqi Zhang|Jun Lu|Xin Ge|Yangbin Shi|Yu Zhao","title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT"},{"content":{"abstract":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation.","authors":["Vandan Mujadia","Dipti Sharma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.48.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMT based Similar Language Translation for Hindi - Marathi","tldr":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.116","presentation_id":"38939623","rocketchat_channel":"paper-wmt-116","speakers":"Vandan Mujadia|Dipti Sharma","title":"NMT based Similar Language Translation for Hindi - Marathi"},{"content":{"abstract":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En->Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta->En and En->Ta respectively, and 27.9 and 13.0 for Iu->En and En->Iu respectively.","authors":["Peng-Jen Chen","Ann Lee","Changhan Wang","Naman Goyal","Angela Fan","Mary Williamson","Jiatao Gu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.8.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Facebook AI's WMT20 News Translation Task Submission","tldr":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.117","presentation_id":"38939624","rocketchat_channel":"paper-wmt-117","speakers":"Peng-Jen Chen|Ann Lee|Changhan Wang|Naman Goyal|Angela Fan|Mary Williamson|Jiatao Gu","title":"Facebook AI's WMT20 News Translation Task Submission"},{"content":{"abstract":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.","authors":["Rupjyoti Baruah","Rajesh Kumar Mundotiya","Amit Kumar","Anil kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.126.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLPRL System for Very Low Resource Supervised Machine Translation","tldr":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.118","presentation_id":"38939625","rocketchat_channel":"paper-wmt-118","speakers":"Rupjyoti Baruah|Rajesh Kumar Mundotiya|Amit Kumar|Anil kumar Singh","title":"NLPRL System for Very Low Resource Supervised Machine Translation"},{"content":{"abstract":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT models to learn tag placement via training data augmentation. We study the interactions of tag representation, data augmentation size, tag complexity, and language pair to show the drawbacks and benefits of each method. We construct and release new test sets containing tagged data for three language pairs of varying difficulty.","authors":["Greg Hanneman","Georgiana Dinu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.138.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Should Markup Tags Be Translated?","tldr":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT mode...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.119","presentation_id":"38939626","rocketchat_channel":"paper-wmt-119","speakers":"Greg Hanneman|Georgiana Dinu","title":"How Should Markup Tags Be Translated?"},{"content":{"abstract":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples in order to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 12,432 language pairs that provides competitive translation quality for all language pairs.","authors":["Markus Freitag","Orhan Firat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.66.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Complete Multilingual Neural Machine Translation","tldr":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.12","presentation_id":"38939550","rocketchat_channel":"paper-wmt-12","speakers":"Markus Freitag|Orhan Firat","title":"Complete Multilingual Neural Machine Translation"},{"content":{"abstract":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on news data and fine-tune them on in-domain and pseudo-in-domain web crawled data. Our baseline systems are transformer-big models that are pre-trained on the WMT'19 News Translation task and fine-tuned on pseudo-in-domain web crawled data and in-domain task data. We also experiment with (i) adaptation using speaker and domain tags and (ii) using different types and amounts of preceding context. We observe that contrarily to expectations, exploiting context degrades the results (and on analysis the data is not highly contextual). However using domain tags does improve scores according to the automatic evaluation. Our final primary systems use domain tags and are ensembles of 4 models, with noisy channel reranking of outputs. Our en-de system was ranked second in the shared task while our de-en system outperformed all the other systems.","authors":["Nikita Moghe","Christian Hardmeier","Rachel Bawden"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.58.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task","tldr":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.121","presentation_id":"38939627","rocketchat_channel":"paper-wmt-121","speakers":"Nikita Moghe|Christian Hardmeier|Rachel Bawden","title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task"},{"content":{"abstract":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking results significantly improve the results on the training sets but decrease the test set results. RTMs can achieve to become the 5th among 13 models in ru-en subtask and 5th in the multilingual track of sentence-level Task 1 based on MAE.","authors":["Ergun Bi\u00e7ici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.114.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RTM Ensemble Learning Results at Quality Estimation Task","tldr":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking re...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.122","presentation_id":"38939628","rocketchat_channel":"paper-wmt-122","speakers":"Ergun Bi\u00e7ici","title":"RTM Ensemble Learning Results at Quality Estimation Task"},{"content":{"abstract":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our submission to the WMT20 Chat Translation Task, which consists of two language directions, English \u2013> German and German \u2013> English. The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network. Our system ensembles three models, each with different hyperparameters. Despite being trained on a very small corpus, our model produces surprisingly good results.","authors":["Roweida Mohammed","Mahmoud Al-Ayyoub","Malak Abdullah"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.59.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JUST System for WMT20 Chat Translation Task","tldr":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.123","presentation_id":"38939629","rocketchat_channel":"paper-wmt-123","speakers":"Roweida Mohammed|Mahmoud Al-Ayyoub|Malak Abdullah","title":"JUST System for WMT20 Chat Translation Task"},{"content":{"abstract":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glass-box approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multi-lingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs inTask 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a light-weight alternative to the neural-based models.","authors":["Marina Fomicheva","Shuo Sun","Lisa Yankovskaya","Fr\u00e9d\u00e9ric Blain","Vishrav Chaudhary","Mark Fishel","Francisco Guzm\u00e1n","Lucia Specia"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.116.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task","tldr":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b)...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.124","presentation_id":"38939630","rocketchat_channel":"paper-wmt-124","speakers":"Marina Fomicheva|Shuo Sun|Lisa Yankovskaya|Fr\u00e9d\u00e9ric Blain|Vishrav Chaudhary|Mark Fishel|Francisco Guzm\u00e1n|Lucia Specia","title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (English token count). Our submission focuses on filtering Pashto-English, building on previously successful methods to produce two sets of scores: LASER_LM, a combination of the LASER similarity scores provided in the shared task and perplexity scores from language models, and DCCEF_DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring.","authors":["Felicia Koerner","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.109.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task","tldr":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.125","presentation_id":"38939631","rocketchat_channel":"paper-wmt-125","speakers":"Felicia Koerner|Philipp Koehn","title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task"},{"content":{"abstract":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We participated in ten translation directions for the English/Spanish, English/Portuguese, English/Russian, English/Italian, and English/French language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources.","authors":["Felipe Soares","Delton Vaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.95.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts","tldr":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We particip...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.128","presentation_id":"38939632","rocketchat_channel":"paper-wmt-128","speakers":"Felipe Soares|Delton Vaz","title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts"},{"content":{"abstract":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and build our baseline systems to be morphologically motivated sub-word unit-based Transformer base models that we train using the Marian machine translation toolkit. Additionally, we experiment with different parallel and monolingual data selection schemes, as well as sampled back-translation. Our final models are ensembles of Transformer base and Transformer big models which feature right-to-left re-ranking.","authors":["Rihards Kri\u0161lauks","M\u0101rcis Pinnis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.15.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tilde at WMT 2020: News Task Systems","tldr":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.133","presentation_id":"38939633","rocketchat_channel":"paper-wmt-133","speakers":"Rihards Kri\u0161lauks|M\u0101rcis Pinnis","title":"Tilde at WMT 2020: News Task Systems"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.","authors":["Rebecca Knowles","Samuel Larkin","Darlene Stewart","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.132.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications","tldr":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.135","presentation_id":"38939634","rocketchat_channel":"paper-wmt-135","speakers":"Rebecca Knowles|Samuel Larkin|Darlene Stewart|Patrick Littell","title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications"},{"content":{"abstract":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology.","authors":["Prajit Dhar","Arianna Bisazza","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.9.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020","tldr":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.136","presentation_id":"38939635","rocketchat_channel":"paper-wmt-136","speakers":"Prajit Dhar|Arianna Bisazza|Gertjan van Noord","title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020"},{"content":{"abstract":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.","authors":["J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.139.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT","tldr":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that coll...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.137","presentation_id":"38939636","rocketchat_channel":"paper-wmt-137","speakers":"J\u00f6rg Tiedemann","title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT"},{"content":{"abstract":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.","authors":["Minh Quang Pham","Jitao Xu","Josep Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.63.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Priming Neural Machine Translation","tldr":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.138","presentation_id":"38939637","rocketchat_channel":"paper-wmt-138","speakers":"Minh Quang Pham|Jitao Xu|Josep Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"Priming Neural Machine Translation"},{"content":{"abstract":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for Hindi\u2194Marathi each and 1 NMT systems were developed for Hindi\u2194Marathi using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.","authors":["Atul Kr. Ojha","Priya Rani","Akanksha Bansal","Bharathi Raja Chakravarthi","Ritesh Kumar","John P. McCrae"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.49.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020","tldr":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.139","presentation_id":"38939638","rocketchat_channel":"paper-wmt-139","speakers":"Atul Kr. Ojha|Priya Rani|Akanksha Bansal|Bharathi Raja Chakravarthi|Ritesh Kumar|John P. McCrae","title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020"},{"content":{"abstract":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese\u2013English and Hindi\u2013English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40% smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40% of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.","authors":["Raj Dabre","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.61.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models","tldr":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.14","presentation_id":"38939551","rocketchat_channel":"paper-wmt-14","speakers":"Raj Dabre|Atsushi Fujita","title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.","authors":["Rebecca Knowles","Darlene Stewart","Samuel Larkin","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.13.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for the 2020 Inuktitut-English News Translation Task","tldr":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetune...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.141","presentation_id":"38939639","rocketchat_channel":"paper-wmt-141","speakers":"Rebecca Knowles|Darlene Stewart|Samuel Larkin|Patrick Littell","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"content":{"abstract":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the models and used back-translation for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each model. We also investigate the role of mutual intelligibility in model performance.","authors":["Ife Adebara","El Moatez Billah Nagoudi","Muhammad Abdul Mageed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.42.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers","tldr":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.142","presentation_id":"38939640","rocketchat_channel":"paper-wmt-142","speakers":"Ife Adebara|El Moatez Billah Nagoudi|Muhammad Abdul Mageed","title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers"},{"content":{"abstract":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.","authors":["Ivana Kvapil\u00edkov\u00e1","Tom Kocmi","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.133.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20","tldr":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.143","presentation_id":"38939641","rocketchat_channel":"paper-wmt-143","speakers":"Ivana Kvapil\u00edkov\u00e1|Tom Kocmi|Ond\u0159ej Bojar","title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20"},{"content":{"abstract":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.","authors":["Rachel Bawden","Alexandra Birch","Radina Dobreva","Arturo Oncevay","Antonio Valerio Miceli Barone","Philip Williams"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.5.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task","tldr":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.144","presentation_id":"38939642","rocketchat_channel":"paper-wmt-144","speakers":"Rachel Bawden|Alexandra Birch|Radina Dobreva|Arturo Oncevay|Antonio Valerio Miceli Barone|Philip Williams","title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task"},{"content":{"abstract":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.","authors":["Jo\u00e3o Moura","miguel vera","Daan van Stigt","Fabio Kepler","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.119.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task","tldr":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitte...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.146","presentation_id":"38939643","rocketchat_channel":"paper-wmt-146","speakers":"Jo\u00e3o Moura|miguel vera|Daan van Stigt|Fabio Kepler|Andr\u00e9 F. T. Martins","title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en\u2192ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.","authors":["Karen Hambardzumyan","Hovhannes Tamoyan","Hrant Khachatrian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.88.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs","tldr":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.147","presentation_id":"38939644","rocketchat_channel":"paper-wmt-147","speakers":"Karen Hambardzumyan|Hovhannes Tamoyan|Hrant Khachatrian","title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs"},{"content":{"abstract":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available in our GitHub repository.","authors":["Alexandre Lopes","Rodrigo Nogueira","Roberto Lotufo","Helio Pedrini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.90.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation","tldr":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Port...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.149","presentation_id":"38939645","rocketchat_channel":"paper-wmt-149","speakers":"Alexandre Lopes|Rodrigo Nogueira|Roberto Lotufo|Helio Pedrini","title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation"},{"content":{"abstract":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we implemented a noising module that simulates four types of post-editing errors, and we introduced this module into a Transformer-based multi-source APE model. Our noising module implants errors into texts on the target side of parallel corpora during the training phase to make synthetic MT outputs, increasing the entire number of training samples. We also generated additional training data using the parallel corpora and NMT model that were released for the Quality Estimation task, and we used these data to train our APE model. Experimental results on the WMT20 English-German APE data set show improvements over the baseline in terms of both the TER and BLEU scores: our primary submission achieved an improvement of -3.15 TER and +4.01 BLEU, and our contrastive submission achieved an improvement of -3.34 TER and +4.30 BLEU.","authors":["WonKee Lee","Jaehun Shin","Baikjin Jung","Jihyung Lee","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.83.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Noising Scheme for Data Augmentation in Automatic Post-Editing","tldr":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we imple...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.150","presentation_id":"38939646","rocketchat_channel":"paper-wmt-150","speakers":"WonKee Lee|Jaehun Shin|Baikjin Jung|Jihyung Lee|Jong-Hyeok Lee","title":"Noising Scheme for Data Augmentation in Automatic Post-Editing"},{"content":{"abstract":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard Transformer, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 BLEU on the agent side (en\u2192de), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.","authors":["Calvin Bao","Yow-Ting Shiue","Chujun Song","Jie Li","Marine Carpuat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.56.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation","tldr":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model tr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.153","presentation_id":"38939647","rocketchat_channel":"paper-wmt-153","speakers":"Calvin Bao|Yow-Ting Shiue|Chujun Song|Jie Li|Marine Carpuat","title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation"},{"content":{"abstract":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose strategies from previous years\u2019 efforts with larger datasets and also train models with precomputed word alignments under various settings in an effort to improve translation quality.","authors":["Jeremy Gwinnup","Tim Anderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.20.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The AFRL WMT20 News\u00ad Translation Systems","tldr":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.155","presentation_id":"38939648","rocketchat_channel":"paper-wmt-155","speakers":"Jeremy Gwinnup|Tim Anderson","title":"The AFRL WMT20 News\u00ad Translation Systems"},{"content":{"abstract":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre/post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.","authors":["Ankur Kejriwal","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.108.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20","tldr":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.156","presentation_id":"38939649","rocketchat_channel":"paper-wmt-156","speakers":"Ankur Kejriwal|Philipp Koehn","title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20"},{"content":{"abstract":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95% of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our models do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and robustness.","authors":["David Wan","Chris Kedzie","Faisal Ladhak","Marine Carpuat","Kathleen McKeown"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.141.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Terminology Constraints in Automatic Post-Editing","tldr":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.157","presentation_id":"38939650","rocketchat_channel":"paper-wmt-157","speakers":"David Wan|Chris Kedzie|Faisal Ladhak|Marine Carpuat|Kathleen McKeown","title":"Incorporating Terminology Constraints in Automatic Post-Editing"},{"content":{"abstract":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system's performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.","authors":["Qu Cui","Xiang Geng","Shujian Huang","Jiajun CHEN"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.115.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NJU's submission to the WMT20 QE Shared Task","tldr":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.158","presentation_id":"38939651","rocketchat_channel":"paper-wmt-158","speakers":"Qu Cui|Xiang Geng|Shujian Huang|Jiajun CHEN","title":"NJU's submission to the WMT20 QE Shared Task"},{"content":{"abstract":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.","authors":["Brian Thompson","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.67.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity","tldr":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.16","presentation_id":"38939552","rocketchat_channel":"paper-wmt-16","speakers":"Brian Thompson|Matt Post","title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity"},{"content":{"abstract":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1\u2019s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.","authors":["Chi-kiu Lo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.99.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation","tldr":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.160","presentation_id":"38939652","rocketchat_channel":"paper-wmt-160","speakers":"Chi-kiu Lo","title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation"},{"content":{"abstract":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2\u2019s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.","authors":["Chi-kiu Lo","Samuel Larkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.100.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model","tldr":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with con...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.161","presentation_id":"38939653","rocketchat_channel":"paper-wmt-161","speakers":"Chi-kiu Lo|Samuel Larkin","title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model"},{"content":{"abstract":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation.","authors":["Vikrant Goyal","Anoop Kunchukuttan","Rahul Kejriwal","Siddharth Jain","Amit Bhagwat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.19.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20","tldr":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares conta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.162","presentation_id":"38939654","rocketchat_channel":"paper-wmt-162","speakers":"Vikrant Goyal|Anoop Kunchukuttan|Rahul Kejriwal|Siddharth Jain|Amit Bhagwat","title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20"},{"content":{"abstract":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.","authors":["Minh Quang Pham","Josep Maria Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.72.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation","tldr":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.163","presentation_id":"38939655","rocketchat_channel":"paper-wmt-163","speakers":"Minh Quang Pham|Josep Maria Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation"},{"content":{"abstract":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.","authors":["Sumbal Naz","Sadaf Abdul Rauf","Noor-e- Hira","Sami Ul Haq"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.92.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FJWU participation for the WMT20 Biomedical Translation Task","tldr":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.167","presentation_id":"38939656","rocketchat_channel":"paper-wmt-167","speakers":"Sumbal Naz|Sadaf Abdul Rauf|Noor-e- Hira|Sami Ul Haq","title":"FJWU participation for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.","authors":["Zuchao Li","Hai Zhao","Rui Wang","Kehai Chen","Masao Utiyama","Eiichiro Sumita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.22.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.168","presentation_id":"38939657","rocketchat_channel":"paper-wmt-168","speakers":"Zuchao Li|Hai Zhao|Rui Wang|Kehai Chen|Masao Utiyama|Eiichiro Sumita","title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers' LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.","authors":["Chi-kiu Lo","Eric Joanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.110.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models","tldr":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting se...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.169","presentation_id":"38939658","rocketchat_channel":"paper-wmt-169","speakers":"Chi-kiu Lo|Eric Joanis","title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models"},{"content":{"abstract":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.","authors":["Tom Kocmi","Tomasz Limisiewicz","Gabriel Stanovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.39.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Gender Coreference and Bias Evaluation at WMT 2020","tldr":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.170","presentation_id":"38939659","rocketchat_channel":"paper-wmt-170","speakers":"Tom Kocmi|Tomasz Limisiewicz|Gabriel Stanovsky","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"content":{"abstract":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN<->FR,CS,DE,FI system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.","authors":["Annette Rios","Mathias M\u00fcller","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.64.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation","tldr":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.171","presentation_id":"38939660","rocketchat_channel":"paper-wmt-171","speakers":"Annette Rios|Mathias M\u00fcller|Rico Sennrich","title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation"},{"content":{"abstract":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech \u2194 English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.","authors":["Ulrich Germann","Roman Grundkiewicz","Martin Popel","Radina Dobreva","Nikolay Bogoychev","Kenneth Heafield"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.17.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task","tldr":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower te...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.172","presentation_id":"38939661","rocketchat_channel":"paper-wmt-172","speakers":"Ulrich Germann|Roman Grundkiewicz|Martin Popel|Radina Dobreva|Nikolay Bogoychev|Kenneth Heafield","title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task"},{"content":{"abstract":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-English. All our submissions are MarianNMT-based neural systems. We use more data compared to last year and update our back-translations with better models from the previous year. We show competitive results in terms of BLEU in most directions.","authors":["Alexander Molchanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.25.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PROMT Systems for WMT 2020 Shared News Translation Task","tldr":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.175","presentation_id":"38939662","rocketchat_channel":"paper-wmt-175","speakers":"Alexander Molchanov","title":"PROMT Systems for WMT 2020 Shared News Translation Task"},{"content":{"abstract":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.","authors":["Ulrich Germann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.18.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks","tldr":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.177","presentation_id":"38939663","rocketchat_channel":"paper-wmt-177","speakers":"Ulrich Germann","title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks"},{"content":{"abstract":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensemble technique on different transformer architectures (Deep, Hybrid, Big, Large Transformers). To enlarge the in-domain bilingual corpus, we use back-translation of monolingual in-domain data in the target language as additional in-domain training data. Our systems in German->English and English->German are ranked 1st and 3rd respectively according to the official evaluation results in terms of BLEU scores.","authors":["Xing Wang","Zhaopeng Tu","Longyue Wang","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.97.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task","tldr":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.178","presentation_id":"38939664","rocketchat_channel":"paper-wmt-178","speakers":"Xing Wang|Zhaopeng Tu|Longyue Wang|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed.","authors":["Christian Roest","Lukas Edman","Gosse Minnema","Kevin Kelly","Jennifer Spenader","Antonio Toral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.29.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training","tldr":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of corr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.179","presentation_id":"38939665","rocketchat_channel":"paper-wmt-179","speakers":"Christian Roest|Lukas Edman|Gosse Minnema|Kevin Kelly|Jennifer Spenader|Antonio Toral","title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training"},{"content":{"abstract":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.","authors":["Eleftherios Avramidis","Vivien Macketanz","Ursula Strohriegel","Aljoscha Burchardt","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.38.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation","tldr":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.18","presentation_id":"38939553","rocketchat_channel":"paper-wmt-18","speakers":"Eleftherios Avramidis|Vivien Macketanz|Ursula Strohriegel|Aljoscha Burchardt|Sebastian M\u00f6ller","title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation"},{"content":{"abstract":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.","authors":["Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.14.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Submission for the Inuktitut Language in WMT News 2020","tldr":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.180","presentation_id":"38939666","rocketchat_channel":"paper-wmt-180","speakers":"Tom Kocmi","title":"CUNI Submission for the Inuktitut Language in WMT News 2020"},{"content":{"abstract":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike the simulated scenarios used in particular in much of the unsupervised MT work in the past). We were able to obtain most of the digital data available for Upper Sorbian, a minority language of Germany, which was the original motivation for the Unsupervised MT shared task. As we were defining the task, we also obtained a small amount of parallel data (about 60000 parallel sentences), allowing us to offer a Very Low Resource Supervised MT task as well. Six primary systems participated in the unsupervised shared task, two of these systems used additional data beyond the data released by the organizers. Ten primary systems participated in the very low resource supervised task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.","authors":["Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.80.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT","tldr":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.181","presentation_id":"38939667","rocketchat_channel":"paper-wmt-181","speakers":"Alexander Fraser","title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT"},{"content":{"abstract":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.","authors":["Martin Popel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.28.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training","tldr":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating mu...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.183","presentation_id":"38939668","rocketchat_channel":"paper-wmt-183","speakers":"Martin Popel","title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training"},{"content":{"abstract":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.","authors":["Lei Zhou","Liang Ding","Koichi Takeda"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.125.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns","tldr":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Spec...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.185","presentation_id":"38939669","rocketchat_channel":"paper-wmt-185","speakers":"Lei Zhou|Liang Ding|Koichi Takeda","title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns"},{"content":{"abstract":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese \u2192 English task.","authors":["Shuangzhi Wu","Xing Wang","Longyue Wang","Fangxu Liu","Jun Xie","Zhaopeng Tu","Shuming Shi","Mu Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.34.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transf...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.187","presentation_id":"38939670","rocketchat_channel":"paper-wmt-187","speakers":"Shuangzhi Wu|Xing Wang|Longyue Wang|Fangxu Liu|Jun Xie|Zhaopeng Tu|Shuming Shi|Mu Li","title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German-to-English primary system is ranked the second in terms of BLEU scores.","authors":["Longyue Wang","Zhaopeng Tu","Xing Wang","Li Ding","Liang Ding","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.60.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task","tldr":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.188","presentation_id":"38939671","rocketchat_channel":"paper-wmt-188","speakers":"Longyue Wang|Zhaopeng Tu|Xing Wang|Li Ding|Liang Ding|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task"},{"content":{"abstract":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into German and Chinese by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source/domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year\u2019s results are not directly comparable with last year\u2019s round. However, on both language directions, participants\u2019 submissions show considerable improvements over the baseline results. On English-German, the top ranked system improves over the baseline by -11.35 TER and +16.68 BLEU points, while on EnglishChinese the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year\u2019s round.","authors":["Rajen Chatterjee","Markus Freitag","Matteo Negri","Marco Turchi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.75.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing","tldr":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different senten...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.189","presentation_id":"38939672","rocketchat_channel":"paper-wmt-189","speakers":"Rajen Chatterjee|Markus Freitag|Matteo Negri|Marco Turchi","title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing"},{"content":{"abstract":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We release our preprocessed dataset to encourage evaluations that stress-test systems under multiple data conditions.","authors":["Kelly Marchisio","Kevin Duh","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.68.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When Does Unsupervised Machine Translation Work?","tldr":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar doma...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.19","presentation_id":"38939554","rocketchat_channel":"paper-wmt-19","speakers":"Kelly Marchisio|Kevin Duh|Philipp Koehn","title":"When Does Unsupervised Machine Translation Work?"},{"content":{"abstract":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how to improve BLEU by using diverse automatically paraphrased references (Bawden et al., 2020), extending experiments to the multilingual setting for the WMT2020 metrics shared task and for three base metrics. We compare their capacity to exploit up to 100 additional synthetic references. We find that gains are possible when using additional, automatically paraphrased references, although they are not systematic. However, segment-level correlations, particularly into English, are improved for all three metrics and even with higher numbers of paraphrased references.","authors":["Rachel Bawden","Biao Zhang","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.98.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task","tldr":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.190","presentation_id":"38939673","rocketchat_channel":"paper-wmt-190","speakers":"Rachel Bawden|Biao Zhang|Andre T\u00e4ttar|Matt Post","title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task"},{"content":{"abstract":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En->De for agent utterances and De->En for customer messages. We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments (DDA) to evaluate the agent translations.","authors":["M. Amin Farajian","Ant\u00f3nio V. Lopes","Andr\u00e9 F. T. Martins","Sameen Maruf","Gholamreza Haffari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.3.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Chat Translation","tldr":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German c...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.191","presentation_id":"38939674","rocketchat_channel":"paper-wmt-191","speakers":"M. Amin Farajian|Ant\u00f3nio V. Lopes|Andr\u00e9 F. T. Martins|Sameen Maruf|Gholamreza Haffari","title":"Findings of the WMT 2020 Shared Task on Chat Translation"},{"content":{"abstract":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight language pairs. Five language pairs were previously addressed in past editions of the shared task, namely, English/German, English/French, English/Spanish, English/Portuguese, and English/Chinese. Three additional languages pairs were also introduced this year: English/Russian, English/Italian, and English/Basque. The task addressed the evaluation of both scientific abstracts (all language pairs) and terminologies (English/Basque only). We received submissions from a total of 20 teams. For recurring language pairs, we observed an improvement in the translations in terms of automatic scores and qualitative evaluations, compared to previous years.","authors":["Rachel Bawden","Giorgio Maria Di Nunzio","Cristian Grozea","Inigo Jauregi Unanue","Antonio Jimeno Yepes","Nancy Mah","David Martinez","Aur\u00e9lie N\u00e9v\u00e9ol","Mariana Neves","Maite Oronoz","Olatz Perez-de-Vi\u00f1aspre","Massimo Piccardi","Roland Roller","Amy Siu","Philippe Thomas","Federica Vezzani","Maika Vicente Navarro","Dina Wiemann","Lana Yeganova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.76.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages","tldr":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight lan...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.192","presentation_id":"38939675","rocketchat_channel":"paper-wmt-192","speakers":"Rachel Bawden|Giorgio Maria Di Nunzio|Cristian Grozea|Inigo Jauregi Unanue|Antonio Jimeno Yepes|Nancy Mah|David Martinez|Aur\u00e9lie N\u00e9v\u00e9ol|Mariana Neves|Maite Oronoz|Olatz Perez-de-Vi\u00f1aspre|Massimo Piccardi|Roland Roller|Amy Siu|Philippe Thomas|Federica Vezzani|Maika Vicente Navarro|Dina Wiemann|Lana Yeganova","title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages"},{"content":{"abstract":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs \u2013 English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for \u201dcatastrophic errors\u201d. We received 59 submissions by 11 participating teams from a variety of types of institutions.","authors":["Lucia Specia","Zhenhao Li","Juan Pino","Vishrav Chaudhary","Francisco Guzm\u00e1n","Graham Neubig","Nadir Durrani","Yonatan Belinkov","Philipp Koehn","Hassan Sajjad","Paul Michel","Xian Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.4.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness","tldr":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.193","presentation_id":"38939676","rocketchat_channel":"paper-wmt-193","speakers":"Lucia Specia|Zhenhao Li|Juan Pino|Vishrav Chaudhary|Francisco Guzm\u00e1n|Graham Neubig|Nadir Durrani|Yonatan Belinkov|Philipp Koehn|Hassan Sajjad|Paul Michel|Xian Li","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"content":{"abstract":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with open domain texts, direct assessment annotations, and multiple language pairs: English-German, English-Chinese, Russian-English, Romanian-English, Estonian-English, Sinhala-English and Nepali-English data for the sentence-level subtasks, English-German and English-Chinese for the word-level subtask, and English-French data for the document-level subtask. In addition, we made neural machine translation models available to participants. 19 participating teams from 27 institutions submitted altogether 1374 systems to different task variants and language pairs.","authors":["Lucia Specia","Fr\u00e9d\u00e9ric Blain","Marina Fomicheva","Erick Fonseca","Vishrav Chaudhary","Francisco Guzm\u00e1n","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.79.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Quality Estimation","tldr":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with ope...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.194","presentation_id":"38939677","rocketchat_channel":"paper-wmt-194","speakers":"Lucia Specia|Fr\u00e9d\u00e9ric Blain|Marina Fomicheva|Erick Fonseca|Vishrav Chaudhary|Francisco Guzm\u00e1n|Andr\u00e9 F. T. Martins","title":"Findings of the WMT 2020 Shared Task on Quality Estimation"},{"content":{"abstract":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto\u2013English and Khmer\u2013English and also included the challenge of sentence alignment from document pairs.","authors":["Philipp Koehn","Vishrav Chaudhary","Ahmed El-Kishky","Naman Goyal","Peng-Jen Chen","Francisco Guzm\u00e1n"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.78.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment","tldr":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.195","presentation_id":"38939678","rocketchat_channel":"paper-wmt-195","speakers":"Philipp Koehn|Vishrav Chaudhary|Ahmed El-Kishky|Naman Goyal|Peng-Jen Chen|Francisco Guzm\u00e1n","title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment"},{"content":{"abstract":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Marta R. Costa-juss\u00e0","Fethi Bougares","Olivier Galibert"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.2.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the First Shared Task on Lifelong Learning Machine Translation","tldr":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test dat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.196","presentation_id":"38939679","rocketchat_channel":"paper-wmt-196","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Marta R. Costa-juss\u00e0|Fethi Bougares|Olivier Galibert","title":"Findings of the First Shared Task on Lifelong Learning Machine Translation"},{"content":{"abstract":"This is the main findings paper","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Ond\u0159ej Bojar","Marta R. Costa-juss\u00e0","Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Matthias Huck","Eric Joanis","Tom Kocmi","Philipp Koehn","Chi-kiu Lo","Nikola Ljube\u0161i\u0107","Christof Monz","Makoto Morishita","Masaaki Nagata","Toshiaki Nakazawa","Santanu Pal","Matt Post","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.1.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20)","tldr":"This is the main findings paper...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.197","presentation_id":"38939680","rocketchat_channel":"paper-wmt-197","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Ond\u0159ej Bojar|Marta R. Costa-juss\u00e0|Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Matthias Huck|Eric Joanis|Tom Kocmi|Philipp Koehn|Chi-kiu Lo|Nikola Ljube\u0161i\u0107|Christof Monz|Makoto Morishita|Masaaki Nagata|Toshiaki Nakazawa|Santanu Pal|Matt Post|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"content":{"abstract":"","authors":["Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1971","presentation_id":"38940635","rocketchat_channel":"paper-wmt-1971","speakers":"Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Tom Kocmi","title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task"},{"content":{"abstract":"","authors":["Magdalena Biesialska","Marta R. Costa-juss\u00e0","Nikola Ljube\u0161i\u0107","Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1972","presentation_id":"38940636","rocketchat_channel":"paper-wmt-1972","speakers":"Magdalena Biesialska|Marta R. Costa-juss\u00e0|Nikola Ljube\u0161i\u0107|Santanu Pal|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task"},{"content":{"abstract":"","authors":["Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1973","presentation_id":"38940637","rocketchat_channel":"paper-wmt-1973","speakers":"Ond\u0159ej Bojar","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites"},{"content":{"abstract":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 \"zero-shot\" language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.","authors":["Thibault Sellam","Amy Pu","Hyung Won Chung","Sebastian Gehrmann","Qijun Tan","Markus Freitag","Dipanjan Das","Ankur Parikh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.102.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task","tldr":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.198","presentation_id":"38939681","rocketchat_channel":"paper-wmt-198","speakers":"Thibault Sellam|Amy Pu|Hyung Won Chung|Sebastian Gehrmann|Qijun Tan|Markus Freitag|Dipanjan Das|Ankur Parikh","title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task"},{"content":{"abstract":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining mod- ule adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions.","authors":["Runxin Xu","Zhuo Zhi","Jun Cao","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.112.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Volctrans Parallel Corpus Filtering System for WMT 2020","tldr":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.2","presentation_id":"38939544","rocketchat_channel":"paper-wmt-2","speakers":"Runxin Xu|Zhuo Zhi|Jun Cao|Mingxuan Wang|Lei Li","title":"Volctrans Parallel Corpus Filtering System for WMT 2020"},{"content":{"abstract":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.","authors":["Pere Verg\u00e9s Boncompte","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.54.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages","tldr":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.20","presentation_id":"38939555","rocketchat_channel":"paper-wmt-20","speakers":"Pere Verg\u00e9s Boncompte|Marta R. Costa-juss\u00e0","title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages"},{"content":{"abstract":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to-German and English-to-Chinese. Our approach is based on multi-task fine-tuned cross-lingual language models (XLM), initially pre-trained and further domain-adapted through intermediate training using the translation language model (TLM) approach complemented with a novel self-supervised learning task which aim is to model errors inherent to machine translation outputs. Results obtained on both word and sentence-level QE show that the proposed intermediate training method is complementary to language model domain adaptation and outperforms the fine-tuning only approach.","authors":["Raphael Rubino"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.121.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation","tldr":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.21","presentation_id":"38939556","rocketchat_channel":"paper-wmt-21","speakers":"Raphael Rubino","title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation"},{"content":{"abstract":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.","authors":["Sheila Castilho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.137.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation","tldr":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.22","presentation_id":"38939557","rocketchat_channel":"paper-wmt-22","speakers":"Sheila Castilho","title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation"},{"content":{"abstract":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English \u2194 Czech, English \u2194 Russian, French \u2192 German and Tamil \u2192 English), third in 2 directions (English \u2192 German, English \u2192 Japanese), and fourth in 2 directions (English \u2192 Pashto and and English \u2192 Tamil).","authors":["Tingxun Shi","Shiyu Zhao","Xiaopu Li","Xiaoxue Wang","Qian Zhang","Di Ai","Dawei Dang","Xue Zhengshan","JIE HAO"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.30.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OPPO's Machine Translation Systems for WMT20","tldr":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.23","presentation_id":"38939558","rocketchat_channel":"paper-wmt-23","speakers":"Tingxun Shi|Shiyu Zhao|Xiaopu Li|Xiaoxue Wang|Qian Zhang|Di Ai|Dawei Dang|Xue Zhengshan|JIE HAO","title":"OPPO's Machine Translation Systems for WMT20"},{"content":{"abstract":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.","authors":["Aizhan Imankulova","Masahiro Kaneko","Tosho Hirasawa","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.70.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Multimodal Simultaneous Neural Machine Translation","tldr":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence transla...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.26","presentation_id":"38939559","rocketchat_channel":"paper-wmt-26","speakers":"Aizhan Imankulova|Masahiro Kaneko|Tosho Hirasawa|Mamoru Komachi","title":"Towards Multimodal Simultaneous Neural Machine Translation"},{"content":{"abstract":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of techniques such as back-translation and fine-tuning, which are already widely adopted in translation tasks. We attempted to develop new methods for both synthetic data filtering and reranking. However, the methods turned out to be ineffective, and they provided us with no significant improvement over the baseline. We analyze these negative results to provide insights for future studies.","authors":["Shun Kiyono","Takumi Ito","Ryuto Konno","Makoto Morishita","Jun Suzuki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.12.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task","tldr":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.3","presentation_id":"38939545","rocketchat_channel":"paper-wmt-3","speakers":"Shun Kiyono|Takumi Ito|Ryuto Konno|Makoto Morishita|Jun Suzuki","title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task"},{"content":{"abstract":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.","authors":["Mat\u012bss Rikters","Ryokan Ri","Tong Li","Toshiaki Nakazawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.74.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document-aligned Japanese-English Conversation Parallel Corpus","tldr":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.31","presentation_id":"38939560","rocketchat_channel":"paper-wmt-31","speakers":"Mat\u012bss Rikters|Ryokan Ri|Tong Li|Toshiaki Nakazawa","title":"Document-aligned Japanese-English Conversation Parallel Corpus"},{"content":{"abstract":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of -3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of -5.29 and a BLEU score of +7.32 for the En-Zh subtask.","authors":["Jihyung Lee","WonKee Lee","Jaehun Shin","Baikjin Jung","Young-Kil Kim","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.82.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model","tldr":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, wh...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.32","presentation_id":"38939561","rocketchat_channel":"paper-wmt-32","speakers":"Jihyung Lee|WonKee Lee|Jaehun Shin|Baikjin Jung|Young-Kil Kim|Jong-Hyeok Lee","title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model"},{"content":{"abstract":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two training techniques, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.","authors":["Inigo Jauregi Unanue","Massimo Piccardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.89.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation","tldr":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.35","presentation_id":"38939562","rocketchat_channel":"paper-wmt-35","speakers":"Inigo Jauregi Unanue|Massimo Piccardi","title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation"},{"content":{"abstract":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.","authors":["Vil\u00e9m Zouhar","Tereza Vojt\u011bchov\u00e1","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.41.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WMT20 Document-Level Markable Error Exploration","tldr":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused o...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.36","presentation_id":"38939563","rocketchat_channel":"paper-wmt-36","speakers":"Vil\u00e9m Zouhar|Tereza Vojt\u011bchov\u00e1|Ond\u0159ej Bojar","title":"WMT20 Document-Level Markable Error Exploration"},{"content":{"abstract":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.","authors":["Jind\u0159ich Libovick\u00fd","Viktor Hangya","Helmut Schmid","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.131.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task","tldr":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data wit...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.37","presentation_id":"38939564","rocketchat_channel":"paper-wmt-37","speakers":"Jind\u0159ich Libovick\u00fd|Viktor Hangya|Helmut Schmid|Alexander Fraser","title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task"},{"content":{"abstract":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.","authors":["Jin Xu","Yinuo Guo","Junfeng Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.104.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA","tldr":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that ther...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.39","presentation_id":"38939565","rocketchat_channel":"paper-wmt-39","speakers":"Jin Xu|Yinuo Guo|Junfeng Hu","title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA"},{"content":{"abstract":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuktitut and Inuktitut to English. For each, we trained a single-direction model. However, directions including English, Polish and Czech were derived from a common multilingual base, which was later fine-tuned on each particular direction. For all the translation directions, we used a similar training regime, with iterative training corpora improvement through back-translation and model ensembling. For the En \u2192 Cs direction, we additionally leveraged document-level information by re-ranking the beam output with a separate model.","authors":["Mateusz Krubi\u0144ski","Marcin Chochowski","Bart\u0142omiej Boczek","Miko\u0142aj Koszowski","Adam Dobrowolski","Marcin Szyma\u0144ski","Pawe\u0142 Przybysz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.16.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task","tldr":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuk...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.40","presentation_id":"38939566","rocketchat_channel":"paper-wmt-40","speakers":"Mateusz Krubi\u0144ski|Marcin Chochowski|Bart\u0142omiej Boczek|Miko\u0142aj Koszowski|Adam Dobrowolski|Marcin Szyma\u0144ski|Pawe\u0142 Przybysz","title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task"},{"content":{"abstract":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -> Czech, English -> German, and English -> Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year.","authors":["Yves Scherrer","Alessandro Raganato","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.40.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The MUCOW word sense disambiguation test suite at WMT 2020","tldr":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.42","presentation_id":"38939567","rocketchat_channel":"paper-wmt-42","speakers":"Yves Scherrer|Alessandro Raganato|J\u00f6rg Tiedemann","title":"The MUCOW word sense disambiguation test suite at WMT 2020"},{"content":{"abstract":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling. The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na \u0308\u0131ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.","authors":["Shruti Bhosale","Kyra Yee","Sergey Edunov","Michael Auli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.69.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling","tldr":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy ch...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.43","presentation_id":"38939568","rocketchat_channel":"paper-wmt-43","speakers":"Shruti Bhosale|Kyra Yee|Sergey Edunov|Michael Auli","title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling"},{"content":{"abstract":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Randomised Trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. To train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. In addition we re-score the output of Bicleaner using character-level language models and n-gram saturation.","authors":["Miquel Espl\u00e0-Gomis","V\u00edctor M. S\u00e1nchez-Cartagena","Jaume Zaragoza-Bernabeu","Felipe S\u00e1nchez-Mart\u00ednez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.107.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task","tldr":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Ra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.44","presentation_id":"38939569","rocketchat_channel":"paper-wmt-44","speakers":"Miquel Espl\u00e0-Gomis|V\u00edctor M. S\u00e1nchez-Cartagena|Jaume Zaragoza-Bernabeu|Felipe S\u00e1nchez-Mart\u00ednez","title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task"},{"content":{"abstract":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020 News Translation corpora, and fine-tuned on the APE corpus. Bottleneck Adapter Layers are integrated into the model to prevent over-fitting. We further collect external translations as the augmented MT candidates to improve the performance. The experiment demonstrates that pre-trained NMT models are effective when fine-tuning with the APE corpus of a limited size, and the performance can be further improved with external MT augmentation. Our system achieves competitive results on both directions in the final evaluation.","authors":["Hao Yang","Minghan Wang","Daimeng Wei","Hengchao Shang","Jiaxin Guo","Zongyao Li","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.85.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.45","presentation_id":"38939570","rocketchat_channel":"paper-wmt-45","speakers":"Hao Yang|Minghan Wang|Daimeng Wei|Hengchao Shang|Jiaxin Guo|Zongyao Li|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word/sentence-level sub-tasks for both of En-De/Zh language pairs.","authors":["Minghan Wang","Hao Yang","Hengchao Shang","Daimeng Wei","Jiaxin Guo","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen","Liangyou Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.123.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.46","presentation_id":"38939571","rocketchat_channel":"paper-wmt-46","speakers":"Minghan Wang|Hao Yang|Hengchao Shang|Daimeng Wei|Jiaxin Guo|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen|Liangyou Li","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut->English and Tamil->English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.","authors":["Yuhao Zhang","Ziyang Wang","Runzhe Cao","Binghao Wei","Weiqiao Shan","Shuhan Zhou","Abudurexiti Reheman","Tao Zhou","Xin Zeng","Laohu Wang","Yongyu Mu","Jingnan Zhang","Xiaoqian Liu","Xuanjun Zhou","Yinqiao Li","Bei Li","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.37.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans Machine Translation Systems for WMT20","tldr":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.47","presentation_id":"38939572","rocketchat_channel":"paper-wmt-47","speakers":"Yuhao Zhang|Ziyang Wang|Runzhe Cao|Binghao Wei|Weiqiao Shan|Shuhan Zhou|Abudurexiti Reheman|Tao Zhou|Xin Zeng|Laohu Wang|Yongyu Mu|Jingnan Zhang|Xiaoqian Liu|Xuanjun Zhou|Yinqiao Li|Bei Li|Tong Xiao|Jingbo Zhu","title":"The NiuTrans Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the baseline and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our models such as Back Translation, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.","authors":["Daimeng Wei","Hengchao Shang","Zhanglin Wu","Zhengzhe Yu","Liangyou Li","Jiaxin Guo","Minghan Wang","Hao Yang","Lizhi Lei","Ying Qin","Shiliang Sun"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.31.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task","tldr":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the b...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.48","presentation_id":"38939573","rocketchat_channel":"paper-wmt-48","speakers":"Daimeng Wei|Hengchao Shang|Zhanglin Wu|Zhengzhe Yu|Liangyou Li|Jiaxin Guo|Minghan Wang|Hao Yang|Lizhi Lei|Ying Qin|Shiliang Sun","title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task"},{"content":{"abstract":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared to last year, for some language pairs we dedicated a lot more resources to training, and tried to follow standard best practices to build competitive systems which can achieve good results in the rankings. By using deep and complex architectures we sacrificed direct re-usability of our systems in production environments but evaluation showed that this approach could result in better models that significantly outperform baseline architectures. We submitted two systems to the zero shot robustness task. These submissions are described briefly in this paper as well.","authors":["Csaba Oravecz","Katina Bontcheva","L\u00e1szl\u00f3 Tihanyi","David Kolovratnik","Bhavani Bhaskar","Adrien Lardilleux","Szymon Klocek","Andreas Eisele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.26.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"eTranslation's Submissions to the WMT 2020 News Translation Task","tldr":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.49","presentation_id":"38939574","rocketchat_channel":"paper-wmt-49","speakers":"Csaba Oravecz|Katina Bontcheva|L\u00e1szl\u00f3 Tihanyi|David Kolovratnik|Bhavani Bhaskar|Adrien Lardilleux|Szymon Klocek|Andreas Eisele","title":"eTranslation's Submissions to the WMT 2020 News Translation Task"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.118.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language mod...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.5","presentation_id":"38939546","rocketchat_channel":"paper-wmt-5","speakers":"Dongjun Lee","title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation"},{"content":{"abstract":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model using monolingual data from both the languages by jointly pre-training the encoder and decoder and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).","authors":["Salam Michael Singh","Thoudam Doren Singh","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.135.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020","tldr":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.51","presentation_id":"38939575","rocketchat_channel":"paper-wmt-51","speakers":"Salam Michael Singh|Thoudam Doren Singh|Sivaji Bandyopadhyay","title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020"},{"content":{"abstract":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English<->French, English->German and English->Italian.","authors":["Wei Peng","Jianfeng Liu","Minghan Wang","Liangyou Li","Xupeng Meng","Hao Yang","Qun Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.93.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Huawei's Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine tran...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.52","presentation_id":"38939576","rocketchat_channel":"paper-wmt-52","speakers":"Wei Peng|Jianfeng Liu|Minghan Wang|Liangyou Li|Xupeng Meng|Hao Yang|Qun Liu","title":"Huawei's Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire documents (or bilingual dialogues) at once. We use the same ensemble of such models as our primary submission to all three tasks and achieve competitive results. We also experiment with language model pre-training techniques and evaluate their impact on robustness to noise and out-of-domain translation. For German, Spanish, Italian, and French to English translation in the Biomedical Task, we also submit our recently released multilingual Covid19NMT model.","authors":["Alexandre Berard","Ioan Calapodescu","Vassilina Nikoulina","Jerin Philip"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.57.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020","tldr":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire docume...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.53","presentation_id":"38939577","rocketchat_channel":"paper-wmt-53","speakers":"Alexandre Berard|Ioan Calapodescu|Vassilina Nikoulina|Jerin Philip","title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020"},{"content":{"abstract":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems.","authors":["Maja Popovi\u0107","Alberto Poncelas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.51.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation between similar South-Slavic languages","tldr":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation dir...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.54","presentation_id":"38939578","rocketchat_channel":"paper-wmt-54","speakers":"Maja Popovi\u0107|Alberto Poncelas","title":"Neural Machine Translation between similar South-Slavic languages"},{"content":{"abstract":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.","authors":["Xiangpeng Wei","Ping Guo","Yunpeng Li","Xingsheng Zhang","Luxi Xing","Yue Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.32.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIE's Neural Machine Translation Systems for WMT20","tldr":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.57","presentation_id":"38939579","rocketchat_channel":"paper-wmt-57","speakers":"Xiangpeng Wei|Ping Guo|Yunpeng Li|Xingsheng Zhang|Luxi Xing|Yue Hu","title":"IIE's Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. a) Dual Bilingual GPT-2 model, b) Dual Conditional Cross-Entropy Model and c) IBM word alignment model. The scores of these models are combined by using a positive-unlabeled (PU) learning model and a brute-force search to obtain additional gains. Besides, a few simple but efficient rules are adopted to evaluate the quality and the diversity of the corpus. In the alignment-filtering task, the extraction pipeline of bilingual sentence pairs includes the following steps: bilingual lexicon mining, language identification, sentence segmentation and sentence alignment. The final result shows that, in both filtering and alignment tasks, our system significantly outperforms the LASER-based system.","authors":["Jun Lu","Xin Ge","Yangbin Shi","Yuqi Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.111.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task","tldr":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.58","presentation_id":"38939580","rocketchat_channel":"paper-wmt-58","speakers":"Jun Lu|Xin Ge|Yangbin Shi|Yuqi Zhang","title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task"},{"content":{"abstract":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE<cit.>), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.","authors":["Liwei Wu","Xiao Pan","Zehui Lin","Yaoming ZHU","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.33.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Volctrans Machine Translation System for WMT20","tldr":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.59","presentation_id":"38939581","rocketchat_channel":"paper-wmt-59","speakers":"Liwei Wu|Xiao Pan|Zehui Lin|Yaoming ZHU|Mingxuan Wang|Lei Li","title":"The Volctrans Machine Translation System for WMT20"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our system improves the NMT output by -3.95 and +4.50 in terms of TER and BLEU, respectively.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.81.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Transformers for Neural Automatic Post-Editing","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (M...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.6","presentation_id":"38939547","rocketchat_channel":"paper-wmt-6","speakers":"Dongjun Lee","title":"Cross-Lingual Transformers for Neural Automatic Post-Editing"},{"content":{"abstract":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian\u2192German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German\u2192Upper Sorbian and 35.2 on Upper Sorbian\u2192German.","authors":["Alexandra Chronopoulou","Dario Stojanovski","Viktor Hangya","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.128.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task","tldr":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.60","presentation_id":"38939582","rocketchat_channel":"paper-wmt-60","speakers":"Alexandra Chronopoulou|Dario Stojanovski|Viktor Hangya|Alexander Fraser","title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task"},{"content":{"abstract":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.","authors":["Danielle Saunders","Bill Byrne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.94.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task","tldr":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.62","presentation_id":"38939583","rocketchat_channel":"paper-wmt-62","speakers":"Danielle Saunders|Bill Byrne","title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit systems for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.","authors":["Sourav Dutta","Jesujoba Alabi","Saptarashmi Bandyopadhyay","Dana Ruiter","Josef van Genabith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.129.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian","tldr":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.65","presentation_id":"38939584","rocketchat_channel":"paper-wmt-65","speakers":"Sourav Dutta|Jesujoba Alabi|Saptarashmi Bandyopadhyay|Dana Ruiter|Josef van Genabith","title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian"},{"content":{"abstract":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.","authors":["Jingjing Huo","Christian Herold","Yingbo Gao","Leonard Dahlmann","Shahram Khadivi","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.71.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diving Deep into Context-Aware Neural Machine Translation","tldr":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectur...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.68","presentation_id":"38939585","rocketchat_channel":"paper-wmt-68","speakers":"Jingjing Huo|Christian Herold|Yingbo Gao|Leonard Dahlmann|Shahram Khadivi|Hermann Ney","title":"Diving Deep into Context-Aware Neural Machine Translation"},{"content":{"abstract":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This approach allows the flexible combination of a number of independent component models which are further augmented with back-translation, distillation, fine-tuning with in-domain data, Monte-Carlo Tree Search decoding, and improved uncertainty estimation. In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques. Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set (newstest 2019).","authors":["Lei Yu","Laurent Sartran","Po-Sen Huang","Wojciech Stokowiec","Domenic Donato","Srivatsan Srinivasan","Alek Andreev","Wang Ling","Sona Mokra","Agustin Dal Lago","Yotam Doron","Susannah Young","Phil Blunsom","Chris Dyer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.36.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020","tldr":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This app...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.69","presentation_id":"38939586","rocketchat_channel":"paper-wmt-69","speakers":"Lei Yu|Laurent Sartran|Po-Sen Huang|Wojciech Stokowiec|Domenic Donato|Srivatsan Srinivasan|Alek Andreev|Wang Ling|Sona Mokra|Agustin Dal Lago|Yotam Doron|Susannah Young|Phil Blunsom|Chris Dyer","title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020"},{"content":{"abstract":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems.","authors":["Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.50.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages","tldr":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language f...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.70","presentation_id":"38939587","rocketchat_channel":"paper-wmt-70","speakers":"Santanu Pal|Marcos Zampieri","title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages"},{"content":{"abstract":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of \u00a02x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x\u201311x across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average)","authors":["Biao Zhang","Ivan Titov","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.62.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fast Interleaved Bidirectional Sequence Generation","tldr":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decodin...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.71","presentation_id":"38939588","rocketchat_channel":"paper-wmt-71","speakers":"Biao Zhang|Ivan Titov|Rico Sennrich","title":"Fast Interleaved Bidirectional Sequence Generation"},{"content":{"abstract":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -> German and was ranked second for German -> Upper Sorbian according to BLEU scores. For English\u2013Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.","authors":["Yves Scherrer","Stig-Arne Gr\u00f6nroos","Sami Virpioja"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.134.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks","tldr":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For bot...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.72","presentation_id":"38939589","rocketchat_channel":"paper-wmt-72","speakers":"Yves Scherrer|Stig-Arne Gr\u00f6nroos|Sami Virpioja","title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks"},{"content":{"abstract":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes to train statistical models. Using optimal tokenization scheme among these we created synthetic source side text with back translation. And prune synthetic text with language model scores. This synthetic data was then used along with training data in various settings to build translation models. We also report configuration of the submitted systems and results produced by them.","authors":["Saumitra Yadav","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.55.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020","tldr":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.73","presentation_id":"38939590","rocketchat_channel":"paper-wmt-73","speakers":"Saumitra Yadav|Manish Shrivastava","title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020"},{"content":{"abstract":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.","authors":["Ander Corral","Xabier Saralegi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.87.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation","tldr":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.74","presentation_id":"38939591","rocketchat_channel":"paper-wmt-74","speakers":"Ander Corral|Xabier Saralegi","title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation"},{"content":{"abstract":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.","authors":["Farhan Dhanani","Muhammad Rafi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.43.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attention Transformer Model for Translation of Similar Languages","tldr":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.75","presentation_id":"38939592","rocketchat_channel":"paper-wmt-75","speakers":"Farhan Dhanani|Muhammad Rafi","title":"Attention Transformer Model for Translation of Similar Languages"},{"content":{"abstract":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.","authors":["Markus Freitag","George Foster","David Grangier","Colin Cherry"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.140.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Human-Paraphrased References Improve Neural Machine Translation","tldr":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.76","presentation_id":"38939593","rocketchat_channel":"paper-wmt-76","speakers":"Markus Freitag|George Foster|David Grangier|Colin Cherry","title":"Human-Paraphrased References Improve Neural Machine Translation"},{"content":{"abstract":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive transfer from high resource languages. We use iterative backtranslation to fine-tune the system and benefit from the monolingual data available. In order to measure the effectivity of such methods, we compare our results to a bilingual baseline system.","authors":["Carlos Escolano","Marta R. Costa-juss\u00e0","Jos\u00e9 A. R. Fonollosa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.10.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT","tldr":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.77","presentation_id":"38939594","rocketchat_channel":"paper-wmt-77","speakers":"Carlos Escolano|Marta R. Costa-juss\u00e0|Jos\u00e9 A. R. Fonollosa","title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT"},{"content":{"abstract":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system- and segment-level are analyzed in detail and their shortcomings are pointed out.","authors":["Peter Stanchev","Weiyue Wang","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.103.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Better Evaluation of Metrics for Machine Translation","tldr":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlati...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.8","presentation_id":"38939548","rocketchat_channel":"paper-wmt-8","speakers":"Peter Stanchev|Weiyue Wang|Hermann Ney","title":"Towards a Better Evaluation of Metrics for Machine Translation"},{"content":{"abstract":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for domain Adaptation.","authors":["Luis A. Men\u00e9ndez-Salazar","Grigori Sidorov","Marta R. Costa-Juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.47.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The IPN-CIC team system submission for the WMT 2020 similar language task","tldr":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.80","presentation_id":"38939595","rocketchat_channel":"paper-wmt-80","speakers":"Luis A. Men\u00e9ndez-Salazar|Grigori Sidorov|Marta R. Costa-Juss\u00e0","title":"The IPN-CIC team system submission for the WMT 2020 similar language task"},{"content":{"abstract":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese\u2192English system achieves 36.9 case-sensitive BLEU score, which is thehighest among all submissions.","authors":["Fandong Meng","Jianhao Yan","Yijin Liu","Yuan Gao","Xianfeng Zeng","Qinsong Zeng","Peng Li","Ming Chen","Jie Zhou","Sifan Liu","Hao Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.24.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WeChat Neural Machine Translation Systems for WMT20","tldr":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.81","presentation_id":"38939596","rocketchat_channel":"paper-wmt-81","speakers":"Fandong Meng|Jianhao Yan|Yijin Liu|Yuan Gao|Xianfeng Zeng|Qinsong Zeng|Peng Li|Ming Chen|Jie Zhou|Sifan Liu|Hao Zhou","title":"WeChat Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for \u201cattaching\u201d dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.","authors":["Xing Jie Zhong","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.65.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation","tldr":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.82","presentation_id":"38939597","rocketchat_channel":"paper-wmt-82","speakers":"Xing Jie Zhong|David Chiang","title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation"},{"content":{"abstract":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system\u2019s performance.","authors":["Tucker Berckmann","Berkan Hiziroglu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.127.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Low-Resource Translation as Language Modeling","tldr":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.83","presentation_id":"38939598","rocketchat_channel":"paper-wmt-83","speakers":"Tucker Berckmann|Berkan Hiziroglu","title":"Low-Resource Translation as Language Modeling"},{"content":{"abstract":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compare to the baseline model.","authors":["Jiwan Kim","Soyoon Park","Sangha Kim","Yoonjung Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.11.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task","tldr":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model wi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.84","presentation_id":"38939599","rocketchat_channel":"paper-wmt-84","speakers":"Jiwan Kim|Soyoon Park|Sangha Kim|Yoonjung Choi","title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task"},{"content":{"abstract":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from monolingual data, and combining many NMT systems through n-best list reranking improve translation quality. However, while we observed such improvements on the validation data, we did not observed similar improvements on the test data. Our analysis reveals that the presence of translationese texts in the validation data led us to take decisions in building NMT systems that were not optimal to obtain the best results on the test data.","authors":["Benjamin Marie","Raphael Rubino","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.23.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combination of Neural Machine Translation Systems at WMT20","tldr":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from mono...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.86","presentation_id":"38939600","rocketchat_channel":"paper-wmt-86","speakers":"Benjamin Marie|Raphael Rubino|Atsushi Fujita","title":"Combination of Neural Machine Translation Systems at WMT20"},{"content":{"abstract":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task.","authors":["Chi Hu","Hui Liu","Kai Feng","Chen Xu","Nuo Xu","Zefan Zhou","Shiqin Yan","Yingfeng Luo","Chenglong Wang","Xia Meng","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.117.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task","tldr":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. R...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.87","presentation_id":"38939601","rocketchat_channel":"paper-wmt-87","speakers":"Chi Hu|Hui Liu|Kai Feng|Chen Xu|Nuo Xu|Zefan Zhou|Shiqin Yan|Yingfeng Luo|Chenglong Wang|Xia Meng|Tong Xiao|Jingbo Zhu","title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.","authors":["Akifumi Nakamachi","Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.120.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TMUOU Submission for WMT20 Quality Estimation Shared Task","tldr":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.88","presentation_id":"38939602","rocketchat_channel":"paper-wmt-88","speakers":"Akifumi Nakamachi|Hiroki Shimanaka|Tomoyuki Kajiwara|Mamoru Komachi","title":"TMUOU Submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU scores in the directions of English to Pashto, Pashto to English and Khmer to English (13.1, 23.1 and 25.5 respectively) among all the participants. Our submitted systems are unconstrained and focus on mBART (Multilingual Bidirectional and Auto-Regressive Transformers), back-translation and forward-translation. Also, we apply rules, language model and RoBERTa model to filter monolingual, parallel sentences and synthetic sentences. Besides, we validate the difference of the vocabulary built from monolingual data and parallel data.","authors":["Chao Bei","Hao Zong","Qingmin Liu","Conghu Yuan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.6.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GTCOM Neural Machine Translation Systems for WMT20","tldr":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU sco...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.89","presentation_id":"38939603","rocketchat_channel":"paper-wmt-89","speakers":"Chao Bei|Hao Zong|Qingmin Liu|Conghu Yuan","title":"GTCOM Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems.","authors":["Xabier Soto","Olatz Perez-de-Vi\u00f1aspre","Gorka Labaka","Maite Oronoz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.96.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation","tldr":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora rece...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.9","presentation_id":"38939549","rocketchat_channel":"paper-wmt-9","speakers":"Xabier Soto|Olatz Perez-de-Vi\u00f1aspre|Gorka Labaka|Maite Oronoz","title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation"},{"content":{"abstract":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art.","authors":["Ricardo Rei","Craig Stewart","Ana C Farinha","Alon Lavie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.101.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task","tldr":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.90","presentation_id":"38939604","rocketchat_channel":"paper-wmt-90","speakers":"Ricardo Rei|Craig Stewart|Ana C Farinha|Alon Lavie","title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task"},{"content":{"abstract":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translation directions for this language pair. The trained model is evaluated on the corpus provided by shared task organizers, using BLEU, RIBES, and TER scores. There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively.","authors":["Amit Kumar","Rupjyoti Baruah","Rajesh Kumar Mundotiya","Anil Kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.44.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task","tldr":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.91","presentation_id":"38939605","rocketchat_channel":"paper-wmt-91","speakers":"Amit Kumar|Rupjyoti Baruah|Rajesh Kumar Mundotiya|Anil Kumar Singh","title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task"},{"content":{"abstract":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing neural machine translation system to perform the same task. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years\u2019 state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.","authors":["Haluk A\u00e7ar\u00e7i\u00e7ek","Talha \u00c7olako\u011flu","p\u0131nar ece aktan hatipo\u011flu","Chong Hsuan Huang","Wei Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.105.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning","tldr":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the fil...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.92","presentation_id":"38939606","rocketchat_channel":"paper-wmt-92","speakers":"Haluk A\u00e7ar\u00e7i\u00e7ek|Talha \u00c7olako\u011flu|p\u0131nar ece aktan hatipo\u011flu|Chong Hsuan Huang|Wei Peng","title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning"},{"content":{"abstract":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.","authors":["Tharindu Ranasinghe","Constantin Orasan","Ruslan Mitkov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.122.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TransQuest at WMT2020: Sentence-Level Direct Assessment","tldr":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.93","presentation_id":"38939607","rocketchat_channel":"paper-wmt-93","speakers":"Tharindu Ranasinghe|Constantin Orasan|Ruslan Mitkov","title":"TransQuest at WMT2020: Sentence-Level Direct Assessment"},{"content":{"abstract":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.","authors":["Sami Ul Haq","Sadaf Abdul Rauf","Arsalan Shaukat","Abdullah Saeed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.53.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document Level NMT of Low-Resource Languages with Backtranslation","tldr":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an ext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.94","presentation_id":"38939608","rocketchat_channel":"paper-wmt-94","speakers":"Sami Ul Haq|Sadaf Abdul Rauf|Arsalan Shaukat|Abdullah Saeed","title":"Document Level NMT of Low-Resource Languages with Backtranslation"},{"content":{"abstract":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications: using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a Pearson correlation of 0.664, ranking first (tied) on English-Chinese.","authors":["Haijiang Wu","Zixuan Wang","Qingsong Ma","Xinjie Wen","Ruichen Wang","Xiaoli Wang","Yulin Zhang","Zhipeng Yao","Siyao Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.124.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent submission for WMT20 Quality Estimation Shared Task","tldr":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator m...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.96","presentation_id":"38939609","rocketchat_channel":"paper-wmt-96","speakers":"Haijiang Wu|Zixuan Wang|Qingsong Ma|Xinjie Wen|Ruichen Wang|Xiaoli Wang|Yulin Zhang|Zhipeng Yao|Siyao Peng","title":"Tencent submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).","authors":["Yujin Baek","Zae Myung Kim","Jihyung Moon","Hyunjoong Kim","Eunjeong Park"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.113.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PATQUEST: Papago Translation Quality Estimation","tldr":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former foc...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.97","presentation_id":"38939610","rocketchat_channel":"paper-wmt-97","speakers":"Yujin Baek|Zae Myung Kim|Jihyung Moon|Hyunjoong Kim|Eunjeong Park","title":"PATQUEST: Papago Translation Quality Estimation"},{"content":{"abstract":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We have participated in WMT20 shared task of similar language translation on Hindi-Marathi pair. The main challenge of this task is by utilization of monolingual data and similarity features of similar language pair to overcome the limitation of available parallel data. In this work, we have implemented NMT based model that simultaneously learns bilingual embedding from both the source and target language pairs. Our model has achieved Hindi to Marathi bilingual evaluation understudy (BLEU) score of 11.59, rank-based intuitive bilingual evaluation score (RIBES) score of 57.76 and translation edit rate (TER) score of 79.07 and Marathi to Hindi BLEU score of 15.44, RIBES score of 61.13 and TER score of 75.96.","authors":["Sahinur Rahman Laskar","Abdullah Faiz Ur Rahman Khilji","Partha Pakray","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.45.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hindi-Marathi Cross Lingual Model","tldr":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We hav...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.99","presentation_id":"38939611","rocketchat_channel":"paper-wmt-99","speakers":"Sahinur Rahman Laskar|Abdullah Faiz Ur Rahman Khilji|Partha Pakray|Sivaji Bandyopadhyay","title":"Hindi-Marathi Cross Lingual Model"},{"content":{"abstract":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.","authors":["Fran\u00e7ois Hernandez","Vincent Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.21.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Ubiqus English-Inuktitut System for WMT20","tldr":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. T...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.21","presentation_id":"","rocketchat_channel":"paper-wmt-21","speakers":"Fran\u00e7ois Hernandez|Vincent Nguyen","title":"The Ubiqus English-Inuktitut System for WMT20"},{"content":{"abstract":"This is a placeholder for the metrics task paper as the rsults are not available yet","authors":["Nitika Mathur","Johnny Wei","Qingsong Ma","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.77.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Results of the WMT20 Metrics Shared Task","tldr":"This is a placeholder for the metrics task paper as the rsults are not available yet...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.77","presentation_id":"","rocketchat_channel":"paper-wmt-77","speakers":"Nitika Mathur|Johnny Wei|Qingsong Ma|Ond\u0159ej Bojar","title":"Results of the WMT20 Metrics Shared Task"},{"content":{"abstract":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good performance on a dataset might not indicate performance that correlates well with human\u2019s expectations from models that \u201cunderstand\u201d language. In this work we consider a top performing model on several Multiple Choice Question Answering (MCQA) datasets, and evaluate it against a set of expectations one might have from such a model, using a series of zero-information perturbations of the model\u2019s inputs. Our results show that the model clearly falls short of our expectations, and motivates a modified training approach that forces the model to better attend to the inputs. We show that the new training paradigm leads to a model that performs on par with the original model while better satisfying our expectations.","authors":["Krunal Shah","Nitish Gupta","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.317","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What do we expect from Multiple-choice QA Systems?","tldr":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good perf...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2575","presentation_id":"38940132","rocketchat_channel":"paper-insights-2575","speakers":"Krunal Shah|Nitish Gupta|Dan Roth","title":"What do we expect from Multiple-choice QA Systems?"},{"content":{"abstract":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several research challenges which are not tackled well for domain specific corpus found in industries. In this paper, we have highlighted these problems through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model. Our experiments have lead to interesting findings that showed: 1) Largest substring from the left that is found in the vocabulary (in-vocab) is always chosen at every sub-word unit that can lead to suboptimal tokenization choices, 2) Semantic meaning of a vocabulary word deteriorates when found as a substring in an Out-Of-Vocabulary (OOV) word, and 3) Minor misspellings in words are inadequately handled. We believe that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.","authors":["Anmol Nayak","Hariprasad Timmapathini","Karthikeyan Ponnalagu","Vijendran Gopalan Venkoparao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words","tldr":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several rese...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.1","presentation_id":"38940788","rocketchat_channel":"paper-insights-1","speakers":"Anmol Nayak|Hariprasad Timmapathini|Karthikeyan Ponnalagu|Vijendran Gopalan Venkoparao","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"content":{"abstract":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.","authors":["Prasanna Parthasarathi","Sharan Narang","Arvind Neelakantan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Task-Level Dialogue Composition of Generative Transformer Model","tldr":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.12","presentation_id":"38940793","rocketchat_channel":"paper-insights-12","speakers":"Prasanna Parthasarathi|Sharan Narang|Arvind Neelakantan","title":"On Task-Level Dialogue Composition of Generative Transformer Model"},{"content":{"abstract":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to determine the veracity of news. Our experiments find that the success of these detectors can be limited since they are rarely sensitive to semantic perturbations and are very sensitive to syntactic perturbations. Also, we would like to open-source our code and believe it could be a useful diagnostic tool for evaluating models aimed at fighting machine-generated fake news.","authors":["Meghana Moorthy Bhat","Srinivasan Parthasarathy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study","tldr":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to de...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.19","presentation_id":"38940794","rocketchat_channel":"paper-insights-19","speakers":"Meghana Moorthy Bhat|Srinivasan Parthasarathy","title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study"},{"content":{"abstract":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.","authors":["Ashwin Geet D\u2019Sa","Irina Illina","Dominique Fohr","Dietrich Klakow","Dana Ruiter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification","tldr":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of lab...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.20","presentation_id":"38940795","rocketchat_channel":"paper-insights-20","speakers":"Ashwin Geet D\u2019Sa|Irina Illina|Dominique Fohr|Dietrich Klakow|Dana Ruiter","title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification"},{"content":{"abstract":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p <0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.","authors":["Catherine Finegan-Dollak","Ashish Verma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layout-Aware Text Representations Harm Clustering Documents by Type","tldr":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document lay...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.22","presentation_id":"38940796","rocketchat_channel":"paper-insights-22","speakers":"Catherine Finegan-Dollak|Ashish Verma","title":"Layout-Aware Text Representations Harm Clustering Documents by Type"},{"content":{"abstract":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Networks (CapsNets) are a relatively new architecture in NLP. Due to their novelty, CapsNets are being used more and more in NLP tasks. However, their usefulness is still mostly untested.In this paper, we compare three neural network architectures\u2014LSTM, CNN, and CapsNet\u2014on a part of speech tagging task. We compare these architectures in both high- and low-resource training conditions and find that no architecture consistently performs the best. Our analysis shows that our CapsNet performs nearly as well as a more complex LSTM under certain training conditions, but not others, and that our CapsNet almost always outperforms our CNN. We also find that our CapsNet implementation shows faster prediction times than the LSTM for Scottish Gaelic but not for Spanish, highlighting the effect that the choice of languages can have on the models.","authors":["Andrew Zupon","Faiz Rafique","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios","tldr":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Network...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.23","presentation_id":"38940797","rocketchat_channel":"paper-insights-23","speakers":"Andrew Zupon|Faiz Rafique|Mihai Surdeanu","title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios"},{"content":{"abstract":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve around claims made by people about a given topic of interest. Fact-checking portals offer partially structured information that can assist such analysis. However, exploiting the network structure of such online discourse data is as of yet under-explored. We study the effectiveness of using neural-graph embedding features for claim topic prediction and their complementarity with text embeddings. We show that graph embeddings are modestly complementary with text embeddings, but the low performance of graph embedding features alone indicate that the model fails to capture topological features pertinent of the topic prediction task.","authors":["Valentina Beretta","S\u00e9bastien Harispe","Katarina Boland","Luke Lo Seen","Konstantin Todorov","Andon Tchechmedjiev"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?","tldr":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve aroun...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.24","presentation_id":"38940798","rocketchat_channel":"paper-insights-24","speakers":"Valentina Beretta|S\u00e9bastien Harispe|Katarina Boland|Luke Lo Seen|Konstantin Todorov|Andon Tchechmedjiev","title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?"},{"content":{"abstract":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB\u201905 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.","authors":["Piotr Szyma\u0144ski","Piotr \u017belasko","Mikolaj Morzy","Adrian Szymczak","Marzena \u017by\u0142a-Hoppe","Joanna Banaszczak","Lukasz Augustyniak","Jan Mizgajski","Yishay Carmiel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.295","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WER we are and WER we think we are","tldr":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Re...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2436","presentation_id":"38940634","rocketchat_channel":"paper-insights-2436","speakers":"Piotr Szyma\u0144ski|Piotr \u017belasko|Mikolaj Morzy|Adrian Szymczak|Marzena \u017by\u0142a-Hoppe|Joanna Banaszczak|Lukasz Augustyniak|Jan Mizgajski|Yishay Carmiel","title":"WER we are and WER we think we are"},{"content":{"abstract":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge.","authors":["Zhengzhong Liang","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?","tldr":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.26","presentation_id":"38940799","rocketchat_channel":"paper-insights-26","speakers":"Zhengzhong Liang|Mihai Surdeanu","title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?"},{"content":{"abstract":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data\u2014data built by minimally editing a set of seed examples to yield counterfactual labels\u2014to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.","authors":["William Huang","Haokun Liu","Samuel R. Bowman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data","tldr":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain e...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.27","presentation_id":"38940800","rocketchat_channel":"paper-insights-27","speakers":"William Huang|Haokun Liu|Samuel R. Bowman","title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data"},{"content":{"abstract":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive empirical investigation indicates otherwise. In this paper, we establish that ensemble summary for single document using NMF is no better than the best base model summary.","authors":["Alka Khurana","Vasudha Bhatnagar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMF Ensembles? Not for Text Summarization!","tldr":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive emp...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.29","presentation_id":"38940801","rocketchat_channel":"paper-insights-29","speakers":"Alka Khurana|Vasudha Bhatnagar","title":"NMF Ensembles? Not for Text Summarization!"},{"content":{"abstract":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two scorers differ in their handling of them, finding that one approach produces F1 scores approximately 0.5 points higher on the CoNLL 2003 English development and test sets. We propose best practices to increase the replicability of NER evaluations by increasing transparency regarding the handling of improper label sequences.","authors":["Constantine Lignos","Marjan Kamyab"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come","tldr":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.30","presentation_id":"38940802","rocketchat_channel":"paper-insights-30","speakers":"Constantine Lignos|Marjan Kamyab","title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come"},{"content":{"abstract":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.","authors":["Jatin Ganhotra","Robert Moore","Sachindra Joshi","Kahini Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.358","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effects of Naturalistic Variation in Goal-Oriented Dialog","tldr":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fix...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3004","presentation_id":"38940807","rocketchat_channel":"paper-insights-3004","speakers":"Jatin Ganhotra|Robert Moore|Sachindra Joshi|Kahini Wadhawan","title":"Effects of Naturalistic Variation in Goal-Oriented Dialog"},{"content":{"abstract":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.","authors":["Gaurav Arora","Chirag Jain","Manas Chaturvedi","Krupal Modi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HINT3: Raising the bar for Intent Detection in the Wild","tldr":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-wor...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.31","presentation_id":"38940803","rocketchat_channel":"paper-insights-31","speakers":"Gaurav Arora|Chirag Jain|Manas Chaturvedi|Krupal Modi","title":"HINT3: Raising the bar for Intent Detection in the Wild"},{"content":{"abstract":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier\u2019s predictions can be steered to the poison target class with success rates of >80\\% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.","authors":["Alvin Chan","Yi Tay","Yew-Soon Ong","Aston Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.373","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder","tldr":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regula...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3106","presentation_id":"38940808","rocketchat_channel":"paper-insights-3106","speakers":"Alvin Chan|Yi Tay|Yew-Soon Ong|Aston Zhang","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"content":{"abstract":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei andZou, 2019) and Back-Translation (Sennrichet al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XLNet, and RoBERTa. We observe a negative result, finding that techniques which previously reported strong improvements for non-pretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.","authors":["Shayne Longpre","Yu Wang","Chris DuBois"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.394","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?","tldr":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3296","presentation_id":"38940806","rocketchat_channel":"paper-insights-3296","speakers":"Shayne Longpre|Yu Wang|Chris DuBois","title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?"},{"content":{"abstract":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action \u2014 e.g., \u201cI started a new book I bought last week\u201d, where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods.","authors":["Yanai Elazar","Victoria Basmov","Shauli Ravfogel","Yoav Goldberg","Reut Tsarfaty"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Extraordinary Failure of Complement Coercion Crowdsourcing","tldr":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied acti...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.33","presentation_id":"38940804","rocketchat_channel":"paper-insights-33","speakers":"Yanai Elazar|Victoria Basmov|Shauli Ravfogel|Yoav Goldberg|Reut Tsarfaty","title":"The Extraordinary Failure of Complement Coercion Crowdsourcing"},{"content":{"abstract":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment with a multi-task learning approach for explicitly incorporating the structured elements of dictionary entries, such as user-assigned tags and usage examples, when learning embeddings for dictionary headwords. Our work generalizes several existing models for learning word embeddings from dictionaries. However, we find that the most effective representations overall are learned by simply training with a skip-gram objective over the concatenated text of all entries in the dictionary, giving no particular focus to the structure of the entries.","authors":["Steven Wilson","Walid Magdy","Barbara McGillivray","Gareth Tyson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Embedding Structured Dictionary Entries","tldr":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.34","presentation_id":"38940805","rocketchat_channel":"paper-insights-34","speakers":"Steven Wilson|Walid Magdy|Barbara McGillivray|Gareth Tyson","title":"Embedding Structured Dictionary Entries"},{"content":{"abstract":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as unstable model behaviour and some noise within the dataset itself. We then experiment with two approaches for integrating information from knowledge graphs: (i) concatenating knowledge graph triples to text passages and (ii) encoding knowledge with a Graph Neural Network. Neither of these approaches show a clear improvement and we hypothesize that this may be due to a combination of inaccuracies in the knowledge graph, imprecision in entity linking, and the models\u2019 inability to capture additional information from knowledge graphs.","authors":["Daria Dzendzik","Carl Vogel","Jennifer Foster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!","tldr":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as uns...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.4","presentation_id":"38940789","rocketchat_channel":"paper-insights-4","speakers":"Daria Dzendzik|Carl Vogel|Jennifer Foster","title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!"},{"content":{"abstract":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied. We propose a systematic approach aimed at evaluating data selection in scenarios of increasing complexity. Specifically, we compare the case in which source and target tasks are the same while source and target domains are different, against the more challenging scenario where both tasks and domains are different. We run a number of experiments on semantic sequence tagging tasks, which are relatively less investigated in data selection, and conclude that data selection has more benefit on the scenario when the tasks are the same, while in case of different (although related) tasks from distant domains, a combination of data selection and multi-task learning is ineffective for most cases.","authors":["Samuel Louvan","Bernardo Magnini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks","tldr":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied....","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.6","presentation_id":"38940790","rocketchat_channel":"paper-insights-6","speakers":"Samuel Louvan|Bernardo Magnini","title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks"},{"content":{"abstract":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search (ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS to NLP tasks, our results are mixed \u2013 we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.","authors":["Ansel MacLaughlin","Jwala Dhamala","Anoop Kumar","Sriram Venkatapathy","Ragav Venkatesan","Rahul Gupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks","tldr":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and c...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.7","presentation_id":"38940791","rocketchat_channel":"paper-insights-7","speakers":"Ansel MacLaughlin|Jwala Dhamala|Anoop Kumar|Sriram Venkatapathy|Ragav Venkatesan|Rahul Gupta","title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks"},{"content":{"abstract":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.","authors":["Silvia Terragni","Debora Nozza","Elisabetta Fersini","Messina Enza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models","tldr":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. Whil...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.8","presentation_id":"38940792","rocketchat_channel":"paper-insights-8","speakers":"Silvia Terragni|Debora Nozza|Elisabetta Fersini|Messina Enza","title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models"},{"content":{"abstract":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring (FIRE) for this task. In this method, a context filter and a knowledge filter are first built, which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. Besides, the entries irrelevant to the conversation are discarded by the knowledge filter. After that, iteratively referring is performed between context and response representations as well as between knowledge and response representations, in order to collect deep matching features for scoring response candidates. Experimental results show that FIRE outperforms previous methods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with original and revised personas respectively, and margins larger than 3.1% on the CMU_DoG dataset in terms of top-1 accuracy. We also show that FIRE is more interpretable by visualizing the knowledge grounding process.","authors":["Jia-Chen Gu","Zhenhua Ling","Quan Liu","Zhigang Chen","Xiaodan Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.127","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots","tldr":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method n...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1175","presentation_id":"38940705","rocketchat_channel":"paper-scai-1175","speakers":"Jia-Chen Gu|Zhenhua Ling|Quan Liu|Zhigang Chen|Xiaodan Zhu","title":"Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots"},{"content":{"abstract":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete response at a stroke. This tends to generate high-frequency function words with less semantic information rather than low-frequency content words with more semantic information. To address this issue, we propose a content-aware model with two-stage decoding process named Two-stage Dialogue Generation (TSDG). We separate the decoding process of content words and function words so that content words can be generated independently without the interference of function words. Experimental results on two datasets indicate that our model significantly outperforms several competitive generative models in terms of automatic and human evaluation.","authors":["Junsheng Kong","Zhicheng Zhong","Yi Cai","Xin Wu","Da Ren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.192","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process","tldr":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete r...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1735","presentation_id":"38940706","rocketchat_channel":"paper-scai-1735","speakers":"Junsheng Kong|Zhicheng Zhong|Yi Cai|Xin Wu|Da Ren","title":"TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process"},{"content":{"abstract":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse data sets. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications.","authors":["Matthew Henderson","I\u00f1igo Casanueva","Nikola Mrk\u0161i\u0107","Pei-Hao Su","Tsung-Hsien Wen","Ivan Vuli\u0107"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.196","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ConveRT: Efficient and Accurate Conversational Representations from Transformers","tldr":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers)...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1761-ws4","presentation_id":"38940707","rocketchat_channel":"paper-scai-1761-ws4","speakers":"Matthew Henderson|I\u00f1igo Casanueva|Nikola Mrk\u0161i\u0107|Pei-Hao Su|Tsung-Hsien Wen|Ivan Vuli\u0107","title":"ConveRT: Efficient and Accurate Conversational Representations from Transformers"},{"content":{"abstract":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator\u2019s goal is to convert the feedback into a response that answers the user\u2019s previous utterance and to fool the discriminator which distinguishes feedback from natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94%to 75.96% in ranking correct responses on the PERSONACHATdataset, a large improvement given that the original model is already trained on 131k samples.","authors":["Makesh Narsimhan Sreedhar","Kun Ni","Siva Reddy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.221","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback","tldr":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissati...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1947","presentation_id":"38940708","rocketchat_channel":"paper-scai-1947","speakers":"Makesh Narsimhan Sreedhar|Kun Ni|Siva Reddy","title":"Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback"},{"content":{"abstract":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-based representation (e.g. \u201cI have two cats.\u201d). We argue that these representations remain superficial w.r.t. the complexity of human personality. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, values, and beliefs to drive language generation. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.","authors":["Thomas Scialom","Serra Sinem Tekiro\u011flu","Jacopo Staiano","Marco Guerini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.238","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Toward Stance-based Personas for Opinionated Dialogues","tldr":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-ba...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2041","presentation_id":"38940704","rocketchat_channel":"paper-scai-2041","speakers":"Thomas Scialom|Serra Sinem Tekiro\u011flu|Jacopo Staiano|Marco Guerini","title":"Toward Stance-based Personas for Opinionated Dialogues"},{"content":{"abstract":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn\u2019s contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features. On dialogues sampled from 28 Alexa domains, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27% (0.43 -> 0.70) and 7% (0.63 -> 0.70) improvement in linear correlation performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.","authors":["Praveen Kumar Bodigutla","Aditya Tiwari","Spyros Matsoukas","Josep Valls-Vargas","Lazaros Polymenakos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.347","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations","tldr":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which redu...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2889","presentation_id":"38940709","rocketchat_channel":"paper-scai-2889","speakers":"Praveen Kumar Bodigutla|Aditya Tiwari|Spyros Matsoukas|Josep Valls-Vargas|Lazaros Polymenakos","title":"Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations"},{"content":{"abstract":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing the first-stage of passage retrieval. Given an input question, it uses a BERT-based classifier (trained with weak supervision) to de-contextualize the input by selecting relevant terms from the dialog history. Using the question and the selected terms, it issues a query to a search engine to perform the first-stage of passage retrieval. On the other hand, MVR is responsible for contextualized passage reranking. It first constructs multiple views of the information need embedded within an input question. The views are based on the dialog history and the top documents obtained in the first-stage of retrieval. It then uses each view to rerank passages using BERT (fine-tuned for passage ranking). Finally, MVR performs a fusion over the rankings produced by the individual views. Experiments show that the above combination improves first-state retrieval as well as the overall accuracy in a reranking pipeline. On the key metric of NDCG@3, the proposed combination achieves a relative performance improvement of 14.8% over the state-of-the-art baseline and is also able to surpass the Oracle.","authors":["Vaibhav Kumar","Jamie Callan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.354","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Making Information Seeking Easier: An Improved Pipeline for Conversational Search","tldr":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing ...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2957","presentation_id":"38940710","rocketchat_channel":"paper-scai-2957","speakers":"Vaibhav Kumar|Jamie Callan","title":"Making Information Seeking Easier: An Improved Pipeline for Conversational Search"},{"content":{"abstract":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves over a strong Transformer baseline as measured by human and automatic quality scores and lexical diversity. We also find SMRT is comparable to pretraining in human evaluation quality, and outperforms pretraining on automatic quality and lexical diversity, without requiring related-domain dialog data.","authors":["Huda Khayrallah","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.403","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training","tldr":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple re...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.3361","presentation_id":"38940711","rocketchat_channel":"paper-scai-3361","speakers":"Huda Khayrallah|Jo\u00e3o Sedoc","title":"SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training"},{"content":{"abstract":"","authors":["Tba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper1","presentation_id":"38940061","rocketchat_channel":"paper-scai-paper1","speakers":"Tba","title":"TBA"},{"content":{"abstract":"The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting (QR) of the conversational context allows to shed more light on this phenomenon and also use it to evaluate robustness of different answer selection approaches. We introduce a simple framework that enables an automated analysis of the conversational question answering (QA) performance using question rewrites, and present the results of this analysis on the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover sensitivity to question formulation of the popular state-of-the-art question answering approaches. Our results demonstrate that the reading comprehension model is insensitive to question formulation, while the passage ranking changes dramatically with a little variation in the input question. The benefit of QR is that it allows us to pinpoint and group such cases automatically. We show how to use this methodology to verify whether QA models are really learning the task or just finding shortcuts in the dataset, and better understand the frequent types of error they make.","authors":["Svitlana Vakulenko","Shayne Longpre","Zhucheng Tu","Raviteja Anantha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering","tldr":"The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting (QR) of the conversational context allows to shed more light on thi...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper2","presentation_id":"38940062","rocketchat_channel":"paper-scai-paper2","speakers":"Svitlana Vakulenko|Shayne Longpre|Zhucheng Tu|Raviteja Anantha","title":"A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering"},{"content":{"abstract":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders models from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the model with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over state-of-the-art models.","authors":["Eyal Ben-David","Orgad Keller","Eric Malmi","Idan Szpektor","Roi Reichart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.135","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Semantically Driven Sentence Fusion: Modeling and Evaluation","tldr":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders mod...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper3","presentation_id":"38940063","rocketchat_channel":"paper-scai-paper3","speakers":"Eyal Ben-David|Orgad Keller|Eric Malmi|Idan Szpektor|Roi Reichart","title":"Semantically Driven Sentence Fusion: Modeling and Evaluation"},{"content":{"abstract":"","authors":["Tba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper4","presentation_id":"38940064","rocketchat_channel":"paper-scai-paper4","speakers":"Tba","title":"TBA"},{"content":{"abstract":"","authors":["Marco Guerini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper5","presentation_id":"38940065","rocketchat_channel":"paper-scai-paper5","speakers":"Marco Guerini","title":"TBA"},{"content":{"abstract":"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response candidates) for which a neural ranker is ineffective and (ii) improving neural ranking for such instances. To address both challenges we resort to slice-based learning for which the goal is to improve effectiveness of neural models for slices (subsets) of data. We address challenge (i) by proposing different slicing functions (SFs) that select slices of the dataset\u2014based on prior work we heuristically capture different failures of neural rankers. Then, for challenge (ii) we adapt a neural ranking model to learn slice-aware representations, i.e. the adapted model learns to represent the question and responses differently based on the model\u2019s prediction of which slices they belong to. Our experimental results (the source code and data are available at https://github.com/Guzpenha/slice_based_learning) across three different ranking tasks and four corpora show that slice-based learning improves the effectiveness by an average of 2% over a neural ranker that is not slice-aware.","authors":["Gustavo Penha","Claudia Hauff"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Slice-Aware Neural Ranking","tldr":"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response ...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2020.scai-1.1","presentation_id":"","rocketchat_channel":"paper-scai-1","speakers":"Gustavo Penha|Claudia Hauff","title":"Slice-Aware Neural Ranking"},{"content":{"abstract":"Conversational Question Answering (ConvQA) is a Conversational Search task in a simplified setting, where an answer must be extracted from a given passage. Neural language models, such as BERT, fine-tuned on large-scale ConvQA datasets such as CoQA and QuAC have been used to address this task. Recently, Multi-Task Learning (MTL) has emerged as a particularly interesting approach for developing ConvQA models, where the objective is to enhance the performance of a primary task by sharing the learned structure across several related auxiliary tasks. However, existing ConvQA models that leverage MTL have not investigated the dynamic adjustment of the relative importance of the different tasks during learning, nor the resulting impact on the performance of the learned models. In this paper, we first study the effectiveness and efficiency of dynamic MTL methods including Evolving Weighting, Uncertainty Weighting, and Loss-Balanced Task Weighting, compared to static MTL methods such as the uniform weighting of tasks. Furthermore, we propose a novel hybrid dynamic method combining Abridged Linear for the main task with a Loss-Balanced Task Weighting (LBTW) for the auxiliary tasks, so as to automatically fine-tune task weighting during learning, ensuring that each of the task\u2019s weights is adjusted by the relative importance of the different tasks. We conduct experiments using QuAC, a large-scale ConvQA dataset. Our results demonstrate the effectiveness of our proposed method, which significantly outperforms both the single-task learning and static task weighting methods with improvements ranging from +2.72% to +3.20% in F1 scores. Finally, our findings show that the performance of using MTL in developing ConvQA model is sensitive to the correct selection of the auxiliary tasks as well as to an adequate balancing of the loss rates of these tasks during training by using LBTW.","authors":["Sarawoot Kongyoung","Craig Macdonald","Iadh Ounis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering","tldr":"Conversational Question Answering (ConvQA) is a Conversational Search task in a simplified setting, where an answer must be extracted from a given passage. Neural language models, such as BERT, fine-tuned on large-scale ConvQA datasets such as CoQA a...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2020.scai-1.3","presentation_id":"","rocketchat_channel":"paper-scai-3","speakers":"Sarawoot Kongyoung|Craig Macdonald|Iadh Ounis","title":"Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering"},{"content":{"abstract":"By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model\u2019s ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell\u2019s analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.","authors":["Yiding Hao","Simon Mendelsohn","Rachel Sterneck","Randi Martinez","Robert Frank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling","tldr":"By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a c...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.16","presentation_id":"38939682","rocketchat_channel":"paper-cmcl2020-16","speakers":"Yiding Hao|Simon Mendelsohn|Rachel Sterneck|Randi Martinez|Robert Frank","title":"Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling"},{"content":{"abstract":"Different aspects of language processing have been shown to be sensitive to priming but the findings of studies examining priming effects in adolescents with Autism Spectrum Disorder (ASD) and Developmental Language Disorder (DLD) have been inconclusive. We present a study analysing visual and implicit semantic priming in adolescents with ASD and DLD. Based on a dataset of fictional and script-like narratives, we evaluate how often and how extensively, content of two different priming sources is used by the participants. The first priming source was visual, consisting of images shown to the participants to assist them with their storytelling. The second priming source originated from commonsense knowledge, using crowdsourced data containing prototypical script elements. Our results show that individuals with ASD are less sensitive to both types of priming, but show typical usage of primed cues when they use them at all. In contrast, children with DLD show mostly average priming sensitivity, but exhibit an over-proportional use of the priming cues.","authors":["Michaela Regneri","Diane King","Fahreen Walji","Olympia Palikara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Images and Imagination: Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder","tldr":"Different aspects of language processing have been shown to be sensitive to priming but the findings of studies examining priming effects in adolescents with Autism Spectrum Disorder (ASD) and Developmental Language Disorder (DLD) have been inconclus...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.7","presentation_id":"38939683","rocketchat_channel":"paper-cmcl2020-7","speakers":"Michaela Regneri|Diane King|Fahreen Walji|Olympia Palikara","title":"Images and Imagination: Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder"},{"content":{"abstract":"Word order flexibility is one of the distinctive features of SOV languages. In this work, we investigate whether the order and relative distance of preverbal dependents in Hindi, an SOV language, is affected by factors motivated by efficiency considerations during comprehension/production. We investigate the influence of Head\u2013Dependent Mutual Information (HDMI), similarity-based interference, accessibility and case-marking. Results show that preverbal dependents remain close to the verbal head when the HDMI between the verb and its dependent is high. This demonstrates the influence of locality constraints on dependency distance and word order in an SOV language. Additionally, dependency distance were found to be longer when the dependent was animate, when it was case-marked and when it was semantically similar to other preverbal dependents. Together the results highlight the crosslinguistic generalizability of these factors and provide evidence for a functionally motivated account of word order in SOV languages such as Hindi.","authors":["Kartik Sharma","Richard Futrell","Samar Husain"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Determines the Order of Verbal Dependents in Hindi? Effects of Efficiency in Comprehension and Production","tldr":"Word order flexibility is one of the distinctive features of SOV languages. In this work, we investigate whether the order and relative distance of preverbal dependents in Hindi, an SOV language, is affected by factors motivated by efficiency conside...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.1","presentation_id":"","rocketchat_channel":"paper-cmcl2020-1","speakers":"Kartik Sharma|Richard Futrell|Samar Husain","title":"What Determines the Order of Verbal Dependents in Hindi? Effects of Efficiency in Comprehension and Production"},{"content":{"abstract":"We introduce a framework in which production-rule based computational cognitive modeling and Reinforcement Learning can systematically interact and inform each other. We focus on linguistic applications because the sophisticated rule-based cognitive models needed to capture linguistic behavioral data promise to provide a stringent test suite for RL algorithms, connecting RL algorithms to both accuracy and reaction-time experimental data. Thus, we open a path towards assembling an experimentally rigorous and cognitively realistic benchmark for RL algorithms. We extend our previous work on lexical decision tasks and tabular RL algorithms (Brasoveanu and Dotla\u010dil, 2020b) with a discussion of neural-network based approaches, and a discussion of how parsing can be formalized as an RL problem.","authors":["Adrian Brasoveanu","Jakub Dotlacil"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Production-based Cognitive Models as a Test Suite for Reinforcement Learning Algorithms","tldr":"We introduce a framework in which production-rule based computational cognitive modeling and Reinforcement Learning can systematically interact and inform each other. We focus on linguistic applications because the sophisticated rule-based cognitive ...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.3","presentation_id":"","rocketchat_channel":"paper-cmcl2020-3","speakers":"Adrian Brasoveanu|Jakub Dotlacil","title":"Production-based Cognitive Models as a Test Suite for Reinforcement Learning Algorithms"},{"content":{"abstract":"Continuous vector word representations (or word embeddings) have shown success in capturing semantic relations between words, as evidenced with evaluation against behavioral data of adult performance on semantic tasks (Pereira et al. 2016). Adult semantic knowledge is the endpoint of a language acquisition process; thus, a relevant question is whether these models can also capture emerging word representations of young language learners. However, the data of semantic knowledge of children is scarce or non-existent for some age groups. In this paper, we propose to bridge this gap by using Age of Acquisition norms to evaluate word embeddings learnt from child-directed input. We present two methods that evaluate word embeddings in terms of (a) the semantic neighbourhood density of learnt words, and (b) the convergence to adult word associations. We apply our methods to bag-of-words models, and we find that (1) children acquire words with fewer semantic neighbours earlier, and (2) young learners only attend to very local context. These findings provide converging evidence for validity of our methods in understanding the prerequisite features for a distributional model of word learning.","authors":["Raquel G. Alhama","Caroline Rowland","Evan Kidd"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Word Embeddings for Language Acquisition","tldr":"Continuous vector word representations (or word embeddings) have shown success in capturing semantic relations between words, as evidenced with evaluation against behavioral data of adult performance on semantic tasks (Pereira et al. 2016). Adult sem...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.4","presentation_id":"","rocketchat_channel":"paper-cmcl2020-4","speakers":"Raquel G. Alhama|Caroline Rowland|Evan Kidd","title":"Evaluating Word Embeddings for Language Acquisition"},{"content":{"abstract":"The age of acquisition of a word is a psycholinguistic variable concerning the age at which a word is typically learned. It correlates with other psycholinguistic variables such as familiarity, concreteness, and imageability. Existing datasets for multiple languages also include linguistic variables such as the length and the frequency of lemmas in different corpora. There are substantial sets of normative values for English, but for other languages, such as Italian, the coverage is scarce. In this paper,a set of regression experiments investigates whether it is possible to guess the age of acquisition of Italian lemmas that have not been previously rated by humans. An intrinsic evaluation is proposed, correlating estimated Italian lemmas\u2019 AoA with English lemmas\u2019 AoA. An extrinsic evaluation - using AoA values as features for the classification of literary excerpts labeled by age appropriateness - shows how es-sential is lexical coverage for this task.","authors":["Irene Russo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guessing the Age of Acquisition of Italian Lemmas through Linear Regression","tldr":"The age of acquisition of a word is a psycholinguistic variable concerning the age at which a word is typically learned. It correlates with other psycholinguistic variables such as familiarity, concreteness, and imageability. Existing datasets for mu...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.5","presentation_id":"","rocketchat_channel":"paper-cmcl2020-5","speakers":"Irene Russo","title":"Guessing the Age of Acquisition of Italian Lemmas through Linear Regression"},{"content":{"abstract":"The free association task has been very influential both in cognitive science and in computational linguistics. However, little research has been done to study how free associations develop in childhood. The current work focuses on the developmental hypothesis according to which free word associations emerge by mirroring the co-occurrence distribution of children\u2019s linguistic environment. I trained a distributional semantic model on a large corpus of child language and I tested if it could predict children\u2019s responses. The results largely supported the hypothesis: Co-occurrence-based similarity was a strong predictor of children\u2019s associative behavior even controlling for other possible predictors such as phonological similarity, word frequency, and word length. I discuss the findings in the light of theories of conceptual development.","authors":["Abdellah Fourtassi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word Co-occurrence in Child-directed Speech Predicts Children\u2019s Free Word Associations","tldr":"The free association task has been very influential both in cognitive science and in computational linguistics. However, little research has been done to study how free associations develop in childhood. The current work focuses on the developmental ...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.6","presentation_id":"","rocketchat_channel":"paper-cmcl2020-6","speakers":"Abdellah Fourtassi","title":"Word Co-occurrence in Child-directed Speech Predicts Children\u2019s Free Word Associations"},{"content":{"abstract":"Interactive alignment is a major mechanism of linguistic coordination. Here we study the way this mechanism emerges in development across the lexical, syntactic, and conceptual levels. We leverage NLP tools to analyze a large-scale corpus of child-adult conversations between 2 and 5 years old. We found that, across development, children align consistently to adults above chance and that adults align consistently more to children than vice versa (even controlling for language production abilities). Besides these consistencies, we found a diversity of developmental trajectories across linguistic levels. These corpus-based findings provide strong support for an early onset of multi-level linguistic alignment in children and invites new experimental work.","authors":["Thomas Misiek","Benoit Favre","Abdellah Fourtassi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Development of Multi-level Linguistic Alignment in Child-adult Conversations","tldr":"Interactive alignment is a major mechanism of linguistic coordination. Here we study the way this mechanism emerges in development across the lexical, syntactic, and conceptual levels. We leverage NLP tools to analyze a large-scale corpus of child-ad...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.7","presentation_id":"","rocketchat_channel":"paper-cmcl2020-7","speakers":"Thomas Misiek|Benoit Favre|Abdellah Fourtassi","title":"Development of Multi-level Linguistic Alignment in Child-adult Conversations"},{"content":{"abstract":"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.","authors":["Kate McCurdy","Adam Lopez","Sharon Goldwater"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection","tldr":"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This sugges...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.8","presentation_id":"","rocketchat_channel":"paper-cmcl2020-8","speakers":"Kate McCurdy|Adam Lopez|Sharon Goldwater","title":"Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection"},{"content":{"abstract":"Case is an abstract grammatical feature that indicates argument relationship in a sentence. In English, cases are expressed on pronouns, as nominative case (e.g. I, he), accusative case (e.g. me, him) and genitive case (e.g. my, his). Children correctly use cased pronouns at a very young age. How do they acquire abstract case in the first place, when different cases are not associated with different meanings? This paper proposes that the distributional patterns in parents\u2019 input could be used to distinguish grammatical cases in English.","authors":["Xiaomeng Ma","Martin Chodorow","Virginia Valian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition","tldr":"Case is an abstract grammatical feature that indicates argument relationship in a sentence. In English, cases are expressed on pronouns, as nominative case (e.g. I, he), accusative case (e.g. me, him) and genitive case (e.g. my, his). Children correc...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.9","presentation_id":"","rocketchat_channel":"paper-cmcl2020-9","speakers":"Xiaomeng Ma|Martin Chodorow|Virginia Valian","title":"Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition"},{"content":{"abstract":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google Assistant on edge devices with limited memory budgets. We propose to learn compositional code embeddings to greatly reduce the sizes of BERT-base and RoBERTa-base. We also apply the technique to DistilBERT, ALBERT-base, and ALBERT-large, three already compressed BERT variants which attain similar state-of-the-art performances on semantic parsing with much smaller model sizes. We observe 95.15% 98.46% embedding compression rates and 20.47% 34.22% encoder compression rates, while preserving >97.5% semantic parsing performances. We provide the recipe for training and analyze the trade-off between code embedding sizes and downstream performances.","authors":["Prafull Prakash","Saurabh Kumar Shashidhar","Wenlong Zhao","Subendhu Rongali","Haidar Khan","Michael Kayser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.423","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings","tldr":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google A...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.3476","presentation_id":"38940116","rocketchat_channel":"paper-intex-sempar2020-3476","speakers":"Prafull Prakash|Saurabh Kumar Shashidhar|Wenlong Zhao|Subendhu Rongali|Haidar Khan|Michael Kayser","title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings"},{"content":{"abstract":"Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.","authors":["Siddharth Karamcheti","Dorsa Sadigh","Percy Liang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Adaptive Language Interfaces through Decomposition","tldr":"Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions th...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.10","presentation_id":"38939456","rocketchat_channel":"paper-intex-sempar2020-10","speakers":"Siddharth Karamcheti|Dorsa Sadigh|Percy Liang","title":"Learning Adaptive Language Interfaces through Decomposition"},{"content":{"abstract":"Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL\u2019s superior performance extends to well-formed text, achieving an 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.","authors":["Karthik Radhakrishnan","Arvind Srikantan","Xi Victoria Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ColloQL: Robust Text-to-SQL Over Search Queries","tldr":"Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.11","presentation_id":"38939457","rocketchat_channel":"paper-intex-sempar2020-11","speakers":"Karthik Radhakrishnan|Arvind Srikantan|Xi Victoria Lin","title":"ColloQL: Robust Text-to-SQL Over Search Queries"},{"content":{"abstract":"Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL shared task organized in the IntEx-SemPar workshop at EMNLP 2020. We have trained a number of Neural Machine Translation (NMT) models to efficiently generate the natural language responses from SQL. Our shuffled back-translation model has led to a BLEU score of 7.47 on the unknown test dataset. In this paper, we will discuss our methodologies to approach the problem and future directions to improve the quality of the generated natural language responses.","authors":["Saptarashmi Bandyopadhyay","Tianyang Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Natural Language Response Generation from SQL with Generalization and Back-translation","tldr":"Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.12","presentation_id":"38939458","rocketchat_channel":"paper-intex-sempar2020-12","speakers":"Saptarashmi Bandyopadhyay|Tianyang Zhao","title":"Natural Language Response Generation from SQL with Generalization and Back-translation"},{"content":{"abstract":"","authors":["Tao Yu","Chien-Sheng Wu","Xi Victoria Lin","Bailin Wang","Yi Chern Tan","Xinyi Yang","Dragomir Radev","Richard Socher","Caiming Xiong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GRAPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.13","presentation_id":"38939459","rocketchat_channel":"paper-intex-sempar2020-13","speakers":"Tao Yu|Chien-Sheng Wu|Xi Victoria Lin|Bailin Wang|Yi Chern Tan|Xinyi Yang|Dragomir Radev|Richard Socher|Caiming Xiong","title":"GRAPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing"},{"content":{"abstract":"","authors":["Yu Gu","Sue Kase","Michelle Vanni","Brian Sadler","Percy Liang","Xifeng Yan","Yu Su"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Re-thinking Open-domain Semantic Parsing","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.14","presentation_id":"38939460","rocketchat_channel":"paper-intex-sempar2020-14","speakers":"Yu Gu|Sue Kase|Michelle Vanni|Brian Sadler|Percy Liang|Xifeng Yan|Yu Su","title":"Re-thinking Open-domain Semantic Parsing"},{"content":{"abstract":"","authors":["Yusen Zhang","Xiangyu Dong","Shuaichen Chang","Tao Yu","Peng Shi","Rui Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.15","presentation_id":"38939461","rocketchat_channel":"paper-intex-sempar2020-15","speakers":"Yusen Zhang|Xiangyu Dong|Shuaichen Chang|Tao Yu|Peng Shi|Rui Zhang","title":"Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL"},{"content":{"abstract":"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their \u201cblack box\u201d nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.","authors":["Saeedeh Shekarpour","Abhishek Nadgeri","Kuldeep Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph","tldr":"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide accep...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.7","presentation_id":"38939453","rocketchat_channel":"paper-intex-sempar2020-7","speakers":"Saeedeh Shekarpour|Abhishek Nadgeri|Kuldeep Singh","title":"QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph"},{"content":{"abstract":"Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit customer data handled by annotators. In this paper, we propose uncertainty and traffic-aware active learning, a novel active learning method that uses model confidence and utterance frequencies from customer traffic to select utterances for annotation. We show that our method significantly outperforms baselines on an internal customer dataset and the Facebook Task Oriented Parsing (TOP) dataset. On our internal dataset, our method achieves the same accuracy as random sampling with 2,000 fewer annotations.","authors":["Priyanka Sen","Emine Yilmaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncertainty and Traffic-Aware Active Learning for Semantic Parsing","tldr":"Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit c...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.8","presentation_id":"38939454","rocketchat_channel":"paper-intex-sempar2020-8","speakers":"Priyanka Sen|Emine Yilmaz","title":"Uncertainty and Traffic-Aware Active Learning for Semantic Parsing"},{"content":{"abstract":"Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to address TOP. In this paper, we propose a new format of meaning representation that is more compact and amenable to sequence-to-sequence (seq-to-seq) models. A simple copy-augmented seq-to-seq parser is built and evaluated over a public TOP dataset, resulting in 3.44% improvement over prior best seq-to-seq parser (exact match accuracy), which is also comparable to constituency parsers\u2019 performance.","authors":["Chaoting Xuan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog","tldr":"Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to add...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.9","presentation_id":"38939455","rocketchat_channel":"paper-intex-sempar2020-9","speakers":"Chaoting Xuan","title":"Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog"},{"content":{"abstract":"Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global information. Specifically, we include citing article\u2019s title and abstract into context representation. We evaluate our model on datasets with different citation context sizes and demonstrate improvements with globally-enhanced context representations when citation contexts are smaller.","authors":["Zoran Medi\u0107","Jan Snajder"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information","tldr":"Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global informati...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.14","presentation_id":"38940720","rocketchat_channel":"paper-sdp2020-14","speakers":"Zoran Medi\u0107|Jan Snajder","title":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information"},{"content":{"abstract":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we study translational research at the level of scientific concepts for all scientific fields. We do this through text mining and predictive modeling using three corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28 million clinical trials. We extract scientific concepts (i.e., phrases) from corpora as instantiations of \u201cresearch ideas\u201d, create concept-level features as motivated by literature, and then follow the trajectories of over 450,000 new concepts (emerged from 1995-2014) to identify factors that lead only a small proportion of these ideas to be used in inventions and drug trials. Results from our analysis suggest several mechanisms that distinguish which scientific concept will be adopted in practice, and which will not. We also demonstrate that our derived features can be used to explain and predict knowledge transfer with high accuracy. Our work provides greater understanding of knowledge transfer for researchers, practitioners, and government agencies interested in encouraging translational research.","authors":["Hancheng Cao","Mengjie Cheng","Zhepeng Cen","Daniel McFarland","Xiang Ren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.158","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora","tldr":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.1457","presentation_id":"38940721","rocketchat_channel":"paper-sdp2020-1457","speakers":"Hancheng Cao|Mengjie Cheng|Zhepeng Cen|Daniel McFarland|Xiang Ren","title":"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora"},{"content":{"abstract":"Our system participates in two shared tasks, CL-SciSumm 2020 and LongSumm 2020. In the CL-SciSumm shared task, based on our previous work, we apply more machine learning methods on position features and content features for facet classification in Task1B. And GCN is introduced in Task2 to perform extractive summarization. In the LongSumm shared task, we integrate both the extractive and abstractive summarization ways. Three methods were tested which are T5 Fine-tuning, DPPs Sampling, and GRU-GCN/GAT.","authors":["Lei Li","Yang Xie","Wei Liu","Yinan Liu","Yafei Jiang","Siya Qi","Xingyuan Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization","tldr":"Our system participates in two shared tasks, CL-SciSumm 2020 and LongSumm 2020. In the CL-SciSumm shared task, based on our previous work, we apply more machine learning methods on position features and content features for facet classification in Ta...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.15shared","presentation_id":"38940743","rocketchat_channel":"paper-sdp2020-15shared","speakers":"Lei Li|Yang Xie|Wei Liu|Yinan Liu|Yafei Jiang|Siya Qi|Xingyuan Li","title":"CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization"},{"content":{"abstract":"We introduce SciWING, an open-source soft-ware toolkit which provides access to state-of-the-art pre-trained models for scientific document processing (SDP) tasks, such as citation string parsing, logical structure recovery and citation intent classification. Compared to other toolkits, SciWING follows a full neural pipeline and provides a Python inter-face for SDP. When needed, SciWING provides fine-grained control for rapid experimentation with different models by swapping and stacking different modules. Transfer learning from general and scientific documents specific pre-trained transformers (i.e., BERT, SciBERT, etc.) can be performed. SciWING incorporates ready-to-use web and terminal-based applications and demonstrations to aid adoption and development. The toolkit is available from http://sciwing.io and the demos are available at http://rebrand.ly/sciwing-demo.","authors":["Abhinav Ramesh Kashyap","Min-Yen Kan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SciWING\u2013 A Software Toolkit for Scientific Document Processing","tldr":"We introduce SciWING, an open-source soft-ware toolkit which provides access to state-of-the-art pre-trained models for scientific document processing (SDP) tasks, such as citation string parsing, logical structure recovery and citation intent classi...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.17","presentation_id":"38940731","rocketchat_channel":"paper-sdp2020-17","speakers":"Abhinav Ramesh Kashyap|Min-Yen Kan","title":"SciWING\u2013 A Software Toolkit for Scientific Document Processing"},{"content":{"abstract":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in scientific papers focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of medical images in context. MedICaT consists of 217K images from 131K open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.","authors":["Sanjay Subramanian","Lucy Lu Wang","Ben Bogin","Sachin Mehta","Madeleine van Zuylen","Sravanthi Parasa","Sameer Singh","Matt Gardner","Hannaneh Hajishirzi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.191","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References","tldr":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describin...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.1728","presentation_id":"38940723","rocketchat_channel":"paper-sdp2020-1728","speakers":"Sanjay Subramanian|Lucy Lu Wang|Ben Bogin|Sachin Mehta|Madeleine van Zuylen|Sravanthi Parasa|Sameer Singh|Matt Gardner|Hannaneh Hajishirzi","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"content":{"abstract":"We focus on systems for TASK1 (TASK 1A and TASK 1B) of CL-SciSumm Shared Task 2020 in this paper. Task 1A is regarded as a binary classification task of sentence pairs. The strategies of domain-specific embedding and special tokens based on language models are proposed. Fusion of contextualized embedding and extra information is further explored in this article. We leverage Sembert to capture the structured semantic information. The joint of BERT-based model and classifiers without neural networks is also exploited. For the Task 1B, a language model with different weights for classes is fine-tuned to accomplish a multi-label classification task. The results show that extra information can improve the identification of cited text spans. The end-to-end trained models outperform models trained with two stages, and the averaged prediction of multi-models is more accurate than an individual one.","authors":["Ling Chai","Guizhen Fu","Yuan Ni"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP-PINGAN-TECH @ CL-SciSumm 2020","tldr":"We focus on systems for TASK1 (TASK 1A and TASK 1B) of CL-SciSumm Shared Task 2020 in this paper. Task 1A is regarded as a binary classification task of sentence pairs. The strategies of domain-specific embedding and special tokens based on language ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.18","presentation_id":"38941223","rocketchat_channel":"paper-sdp2020-18","speakers":"Ling Chai|Guizhen Fu|Yuan Ni","title":"NLP-PINGAN-TECH @ CL-SciSumm 2020"},{"content":{"abstract":"Lay summarization aims to generate lay summaries of scientific papers automatically. It is an essential task that can increase the relevance of science for all of society. In this paper, we build a lay summary generation system based on BART model. We leverage sentence labels as extra supervision signals to improve the performance of lay summarization. In the CL-LaySumm 2020 shared task, our model achieves 46.00 Rouge1-F1 score.","authors":["Tiezheng Yu","Dan Su","Wenliang Dai","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dimsum @LaySumm 20","tldr":"Lay summarization aims to generate lay summaries of scientific papers automatically. It is an essential task that can increase the relevance of science for all of society. In this paper, we build a lay summary generation system based on BART model. W...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.20shared","presentation_id":"38940741","rocketchat_channel":"paper-sdp2020-20shared","speakers":"Tiezheng Yu|Dan Su|Wenliang Dai|Pascale Fung","title":"Dimsum @LaySumm 20"},{"content":{"abstract":"Automatic prediction on the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures. We propose a multi-task shared structure encoding approach which automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naive multi-task methods.","authors":["Jiyi Li","Ayaka Sato","Kazuya Shimura","Fumiyo Fukumoto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-task Peer-Review Score Prediction","tldr":"Automatic prediction on the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.21","presentation_id":"38940727","rocketchat_channel":"paper-sdp2020-21","speakers":"Jiyi Li|Ayaka Sato|Kazuya Shimura|Fumiyo Fukumoto","title":"Multi-task Peer-Review Score Prediction"},{"content":{"abstract":"We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from the Open Source CORD-19 dataset by fully automating the procedure of information extraction using SciBERT. The best latent entity representations are then found by benchnmarking different KG embedding techniques on the task of link prediction using a Graph Convolution Network Auto Encoder (GCN-AE). We demonstrate the utility of ERLKG with respect to COVID-19 through multiple qualitative evaluations. Due to the lack of a gold standard, we propose a relatively large intrinsic evaluation dataset for COVID-19 and use it for validating the top two performing KG embedding techniques. We find TransD to be the best performing KG embedding technique with Pearson and Spearman correlation scores of 0.4348 and 0.4570 respectively. We demonstrate that a considerable number of ERLKG\u2019s top protein, chemical and disease predictions are currently in consideration for COVID-19 related research.","authors":["Sayantan Basu","Sinchani Chakraborty","Atif Hassan","Sana Siddique","Ashish Anand"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ERLKG: Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora","tldr":"We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.22","presentation_id":"38940725","rocketchat_channel":"paper-sdp2020-22","speakers":"Sayantan Basu|Sinchani Chakraborty|Atif Hassan|Sana Siddique|Ashish Anand","title":"ERLKG: Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora"},{"content":{"abstract":"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, is necessary to enable computers to understand and retrieve information from the documents. However, as we will show in this project, a mathematical notation can change its meaning even within the scope of a single paragraph. This flexibility makes it difficult to extract the exact meaning of a mathematical formula. In this project, we will propose a new task direction for grounding mathematical formulae. Particularly, we are addressing the widespread misconception of various research projects in mathematical information retrieval, which presume that mathematical notations have a fixed meaning within a single document. We manually annotated a long scientific paper to illustrate the task concept. Our high inter-annotator agreement shows that the task is well understood for humans. Our results indicate that it is worthwhile to grow the techniques for the proposed task to contribute to the further progress of mathematical language processing.","authors":["Takuto Asakura","Andr\u00e9 Greiner-Petter","Akiko Aizawa","Yusuke Miyao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Grounding of Formulae","tldr":"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.24","presentation_id":"38940733","rocketchat_channel":"paper-sdp2020-24","speakers":"Takuto Asakura|Andr\u00e9 Greiner-Petter|Akiko Aizawa|Yusuke Miyao","title":"Towards Grounding of Formulae"},{"content":{"abstract":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets (e.g., disease, mutation) that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: (a). document-query matching (b). keyword extraction and (c). facet-conditioned abstractive summarization. The outcomes of (b) and (c) are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component (a) directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST\u2019s TREC-PM track datasets (2017\u20132019) show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: https://github.com/bionlproc/text-summ-for-doc-retrieval.","authors":["Jiho Noh","Ramakanth Kavuluru"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.304","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization","tldr":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patie...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2502","presentation_id":"38940722","rocketchat_channel":"paper-sdp2020-2502","speakers":"Jiho Noh|Ramakanth Kavuluru","title":"Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization"},{"content":{"abstract":"Author name disambiguation (AND) algorithms identify a unique author entity record from all similar or same publication records in scholarly or similar databases. Typically, a clustering method is used that requires calculation of similarities between each possible record pair. However, the total number of pairs grows quadratically with the size of the author database making such clustering difficult for millions of records. One remedy is a blocking function that reduces the number of pairwise similarity calculations. Here, we introduce a new way of learning blocking schemes by using a conjunctive normal form (CNF) in contrast to the disjunctive normal form (DNF). We demonstrate on PubMed author records that CNF blocking reduces more pairs while preserving high pairs completeness compared to the previous methods that use a DNF and that the computation time is significantly reduced. In addition, we also show how to ensure that the method produces disjoint blocks so that much of the AND algorithm can be efficiently paralleled. Our CNF blocking method is tested on the entire PubMed database of 80 million author mentions and efficiently removes 82.17% of all author record pairs in 10 minutes.","authors":["Kunho Kim","Athar Sefid","C. Lee Giles"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning CNF Blocking for Large-scale Author Name Disambiguation","tldr":"Author name disambiguation (AND) algorithms identify a unique author entity record from all similar or same publication records in scholarly or similar databases. Typically, a clustering method is used that requires calculation of similarities betwee...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.26","presentation_id":"38940717","rocketchat_channel":"paper-sdp2020-26","speakers":"Kunho Kim|Athar Sefid|C. Lee Giles","title":"Learning CNF Blocking for Large-scale Author Name Disambiguation"},{"content":{"abstract":"Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information, it also has a wider use as an imperfect proxy for quality which has the advantage of being cheaply available for large volumes of scholarly documents. Previous work has dealt with number of citations prediction with relatively small training data sets, or larger datasets but with short, incomplete input text. In this work we leverage the open access ACL Anthology collection in combination with the Semantic Scholar bibliometric database to create a large corpus of scholarly documents with associated citation information and we propose a new citation prediction model called SChuBERT. In our experiments we compare SChuBERT with several state-of-the-art citation prediction models and show that it outperforms previous methods by a large margin. We also show the merit of using more training data and longer input for number of citations prediction.","authors":["Thomas van Dongen","Gideon Maillette de Buy Wenniger","Lambert Schomaker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction.","tldr":"Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information, it also has a wider use as an imperfect proxy for quality which has the advantage of bein...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.27","presentation_id":"38940730","rocketchat_channel":"paper-sdp2020-27","speakers":"Thomas van Dongen|Gideon Maillette de Buy Wenniger|Lambert Schomaker","title":"SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction."},{"content":{"abstract":"Training recurrent neural networks on long texts, in particular scholarly documents, causes problems for learning. While hierarchical attention networks (HANs) are effective in solving these problems, they still lose important information about the structure of the text. To tackle these problems, we propose the use of HANs combined with structure-tags which mark the role of sentences in the document. Adding tags to sentences, marking them as corresponding to title, abstract or main body text, yields improvements over the state-of-the-art for scholarly document quality prediction. The proposed system is applied to the task of accept/reject prediction on the PeerRead dataset and compared against a recent BiLSTM-based model and joint textual+visual model as well as against plain HANs. Compared to plain HANs, accuracy increases on all three domains.On the computation and language domain our new model works best overall, and increases accuracy 4.7% over the best literature result. We also obtain improvements when introducing the tags for prediction of the number of citations for 88k scientific publications that we compiled from the Allen AI S2ORC dataset. For our HAN-system with structure-tags we reach 28.5% explained variance, an improvement of 1.8% over our reimplementation of the BiLSTM-based model as well as 1.0% improvement over plain HANs.","authors":["Gideon Maillette de Buy Wenniger","Thomas van Dongen","Eleri Aedmaa","Herbert Teun Kruitbosch","Edwin A. Valentijn","Lambert Schomaker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structure-Tags Improve Text Classification for Scholarly Document Quality Prediction","tldr":"Training recurrent neural networks on long texts, in particular scholarly documents, causes problems for learning. While hierarchical attention networks (HANs) are effective in solving these problems, they still lose important information about the s...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.29","presentation_id":"38940732","rocketchat_channel":"paper-sdp2020-29","speakers":"Gideon Maillette de Buy Wenniger|Thomas van Dongen|Eleri Aedmaa|Herbert Teun Kruitbosch|Edwin A. Valentijn|Lambert Schomaker","title":"Structure-Tags Improve Text Classification for Scholarly Document Quality Prediction"},{"content":{"abstract":"Cydex is a platform that provides neural search infrastructure for domain-specific scholarly literature. The platform represents an abstraction of Covidex, our recently developed full-stack open-source search engine for the COVID-19 Open Research Dataset (CORD-19) from AI2. While Covidex takes advantage of the latest best practices for keyword search using the popular Lucene search library as well as state-of-the-art neural ranking models using T5, parts of the system were hard coded to only work with CORD-19. This paper describes our efforts to generalize Covidex into Cydex, which can be applied to scholarly literature in different domains. By decoupling corpus-specific configurations from the frontend implementation, we are able to demonstrate the generality of Cydex on two very different corpora: the ACL Anthology and a collection of hydrology abstracts. Our platform is entirely open source and available at cydex.ai.","authors":["Shane Ding","Edwin Zhang","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cydex: Neural Search Infrastructure for the Scholarly Literature","tldr":"Cydex is a platform that provides neural search infrastructure for domain-specific scholarly literature. The platform represents an abstraction of Covidex, our recently developed full-stack open-source search engine for the COVID-19 Open Research Dat...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.30","presentation_id":"38940734","rocketchat_channel":"paper-sdp2020-30","speakers":"Shane Ding|Edwin Zhang|Jimmy Lin","title":"Cydex: Neural Search Infrastructure for the Scholarly Literature"},{"content":{"abstract":"Automatically generating question answer (QA) pairs from the rapidly growing coronavirus-related literature is of great value to the medical community. Creating high quality QA pairs would allow researchers to build models to address scientific queries for answers which are not readily available in support of the ongoing fight against the pandemic. QA pair generation is, however, a very tedious and time consuming task requiring domain expertise for annotation and evaluation. In this paper we present our contribution in addressing some of the challenges of building a QA system without gold data. We first present a method to create QA pairs from a large semi-structured dataset through the use of transformer and rule-based models. Next, we propose a means of engaging subject matter experts (SMEs) for annotating the QA pairs through the usage of a web application. Finally, we demonstrate some experiments showcasing the effectiveness of leveraging active learning in designing a high performing model with a substantially lower annotation effort from the domain experts.","authors":["Rohan Bhambhoria","Luna Feng","Dawn Sepehr","John Chen","Conner Cowling","Sedef Kocak","Elham Dolatabadi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Smart System to Generate and Validate Question Answer Pairs for COVID-19 Literature","tldr":"Automatically generating question answer (QA) pairs from the rapidly growing coronavirus-related literature is of great value to the medical community. Creating high quality QA pairs would allow researchers to build models to address scientific queri...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.32","presentation_id":"38940713","rocketchat_channel":"paper-sdp2020-32","speakers":"Rohan Bhambhoria|Luna Feng|Dawn Sepehr|John Chen|Conner Cowling|Sedef Kocak|Elham Dolatabadi","title":"A Smart System to Generate and Validate Question Answer Pairs for COVID-19 Literature"},{"content":{"abstract":"Despite the advancements in search engine features, ranking methods, technologies, and the availability of programmable APIs, current-day open-access digital libraries still rely on crawl-based approaches for acquiring their underlying document collections. In this paper, we propose a novel search-driven framework for acquiring documents for such scientific portals. Within our framework, publicly-available research paper titles and author names are used as queries to a Web search engine. We were able to obtain ~267,000 unique research papers through our fully-automated framework using ~76,000 queries, resulting in almost 200,000 more papers than the number of queries. Moreover, through a combination of title and author name search, we were able to recover 78% of the original searched titles.","authors":["Krutarth Patel","Cornelia Caragea","Sujatha Das Gollapalli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Use of Web Search to Improve Scientific Collections","tldr":"Despite the advancements in search engine features, ranking methods, technologies, and the availability of programmable APIs, current-day open-access digital libraries still rely on crawl-based approaches for acquiring their underlying document colle...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.35","presentation_id":"38940728","rocketchat_channel":"paper-sdp2020-35","speakers":"Krutarth Patel|Cornelia Caragea|Sujatha Das Gollapalli","title":"On the Use of Web Search to Improve Scientific Collections"},{"content":{"abstract":"Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.","authors":["Seraphina Goldfarb-Tarrant","Alexander Robertson","Jasmina Lazic","Theodora Tsouloufi","Louise Donnison","Karen Smyth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scaling Systematic Literature Reviews with Machine Learning Pipelines","tldr":"Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.36","presentation_id":"38940729","rocketchat_channel":"paper-sdp2020-36","speakers":"Seraphina Goldfarb-Tarrant|Alexander Robertson|Jasmina Lazic|Theodora Tsouloufi|Louise Donnison|Karen Smyth","title":"Scaling Systematic Literature Reviews with Machine Learning Pipelines"},{"content":{"abstract":"In this paper, we tack lay summarization tasks, which aim to automatically produce lay summaries for scientific papers, to participate in the first CL-LaySumm 2020 in SDP workshop at EMNLP 2020. We present our approach of using Pre-training with Extracted Gap-sentences for Abstractive Summarization (PEGASUS; Zhang et al., 2019b) to produce the lay summary and combining those with the extractive summarization model using Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) and readability metrics that measure the readability of the sentence to further improve the quality of the summary. Our model achieves a remarkable performance on ROUGE metrics, demonstrating the produced summary is more readable while it summarizes the main points of the document.","authors":["Seungwon Kim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Pre-Trained Transformer for Better Lay Summarization","tldr":"In this paper, we tack lay summarization tasks, which aim to automatically produce lay summaries for scientific papers, to participate in the first CL-LaySumm 2020 in SDP workshop at EMNLP 2020. We present our approach of using Pre-training with Extr...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.37shared","presentation_id":"38940740","rocketchat_channel":"paper-sdp2020-37shared","speakers":"Seungwon Kim","title":"Using Pre-Trained Transformer for Better Lay Summarization"},{"content":{"abstract":"Acknowledgements are ubiquitous in scholarly papers. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing sentence structure. We develop an acknowledgement extraction system, AckExtract based on open-source text mining software and evaluate our method using manually labeled data. AckExtract uses the PDF of a scholarly paper as input and outputs acknowledgement entities. Results show an overall performance of F_1=0.92. We built a supplementary database by linking CORD-19 papers with acknowledgement entities extracted by AckExtract including persons and organizations and find that only up to 50\u201360% of named entities are actually acknowledged. We further analyze chronological trends of acknowledgement entities in CORD-19 papers. All codes and labeled data are publicly available at https://github.com/lamps-lab/ackextract.","authors":["Jian Wu","Pei Wang","Xin Wei","Sarah Rajtmajer","C. Lee Giles","Christopher Griffin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Acknowledgement Entity Recognition in CORD-19 Papers","tldr":"Acknowledgements are ubiquitous in scholarly papers. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing sentence structure....","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.39","presentation_id":"38940712","rocketchat_channel":"paper-sdp2020-39","speakers":"Jian Wu|Pei Wang|Xin Wei|Sarah Rajtmajer|C. Lee Giles|Christopher Griffin","title":"Acknowledgement Entity Recognition in CORD-19 Papers"},{"content":{"abstract":"We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data and trained CNNs to extract eight categories of both textual (titles, abstracts, headers, figure and table captions, and other texts) and non-textural content (figures and tables) from 30 years of IEEE VIS conference papers, of which a third were scanned bitmap PDFs. We curated this dataset and named it VISpaper-3K. We then showed our initial benchmark performance using VISpaper-3K over itself and CS-150 using YOLOv3 and Faster-RCNN. We open-source DeepPaperComposer of our training data generation and released the resulting annotation data VISpaper-3K to promote re-producible research.","authors":["Meng Ling","Jian Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers","tldr":"We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.40","presentation_id":"38940719","rocketchat_channel":"paper-sdp2020-40","speakers":"Meng Ling|Jian Chen","title":"DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers"},{"content":{"abstract":"The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.","authors":["Dongyeop Kang","Andrew Head","Risham Sidhu","Kyle Lo","Daniel Weld","Marti A. Hearst"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions","tldr":"The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate e...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.42","presentation_id":"38940724","rocketchat_channel":"paper-sdp2020-42","speakers":"Dongyeop Kang|Andrew Head|Risham Sidhu|Kyle Lo|Daniel Weld|Marti A. Hearst","title":"Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions"},{"content":{"abstract":"In this paper, we present the IIIT Bhagalpur and IIT Patna team\u2019s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries, respectively, for scientific articles. For the first two tasks, unsupervised systems are developed, while for the third one, we develop a supervised system.The performances of all the systems were evaluated on the associated datasets with the shared tasks in term of well-known ROUGE metric.","authors":["Saichethan Reddy","Naveen Saini","Sriparna Saha","Pushpak Bhattacharyya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIITBH-IITP@CL-SciSumm20, CL-LaySumm20, LongSumm20","tldr":"In this paper, we present the IIIT Bhagalpur and IIT Patna team\u2019s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.43shared","presentation_id":"38940739","rocketchat_channel":"paper-sdp2020-43shared","speakers":"Saichethan Reddy|Naveen Saini|Sriparna Saha|Pushpak Bhattacharyya","title":"IIITBH-IITP@CL-SciSumm20, CL-LaySumm20, LongSumm20"},{"content":{"abstract":"We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological database SABIO-RK, provide a definition of the task, and report findings from preliminary experiments. Rigorous evaluation proved challenging due to lack of gold-standard data and a difficult notion of correctness. Qualitative inspection of results, however, showed the feasibility and usefulness of the task","authors":["Mark-Christoph M\u00fcller","Sucheta Ghosh","Maja Rey","Ulrike Wittig","Wolfgang M\u00fcller","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain","tldr":"We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological d...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.44","presentation_id":"38940718","rocketchat_channel":"paper-sdp2020-44","speakers":"Mark-Christoph M\u00fcller|Sucheta Ghosh|Maja Rey|Ulrike Wittig|Wolfgang M\u00fcller|Michael Strube","title":"Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain"},{"content":{"abstract":"We present the systems we submitted for the shared tasks of the Workshop on Scholarly Document Processing at EMNLP 2020. Our approaches to the tasks are focused on exploiting large Transformer models pre-trained on huge corpora and adapting them to the different shared tasks. For tasks 1A and 1B of CL-SciSumm we are using different variants of the BERT model to tackle the tasks of \u201ccited text span\u201d and \u201cfacet\u201d identification. For the summarization tasks 2 of CL-SciSumm, LaySumm and LongSumm we make use of different variants of the PEGASUS model, with and without fine-tuning, adapted to the nuances of each one of those particular tasks.","authors":["Alexios Gidiotis","Stefanos Stefanidis","Grigorios Tsoumakas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AUTH @ CLSciSumm 20, LaySumm 20, LongSumm 20","tldr":"We present the systems we submitted for the shared tasks of the Workshop on Scholarly Document Processing at EMNLP 2020. Our approaches to the tasks are focused on exploiting large Transformer models pre-trained on huge corpora and adapting them to t...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.45","presentation_id":"38941222","rocketchat_channel":"paper-sdp2020-45","speakers":"Alexios Gidiotis|Stefanos Stefanidis|Grigorios Tsoumakas","title":"AUTH @ CLSciSumm 20, LaySumm 20, LongSumm 20"},{"content":{"abstract":"Automatic text summarization has been widely studied as an important task in natural language processing. Traditionally, various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summarization. Recently, deep learning based, specifically Transformer-based systems have been immensely popular. Summarization is a cognitively challenging task \u2013 extracting summary worthy sentences is laborious, and expressing semantics in brief when doing abstractive summarization is complicated. In this paper, we specifically look at the problem of summarizing scientific research papers from multiple domains. We differentiate between two types of summaries, namely, (a) LaySumm: A very short summary that captures the essence of the research paper in layman terms restricting overtly specific technical jargon and (b) LongSumm: A much longer detailed summary aimed at providing specific insights into various ideas touched upon in the paper. While leveraging latest Transformer-based models, our systems are simple, intuitive and based on how specific paper sections contribute to human summaries of the two types described above. Evaluations against gold standard summaries using ROUGE metrics prove the effectiveness of our approach. On blind test corpora, our system ranks first and third for the LongSumm and LaySumm tasks respectively.","authors":["Sayar Ghosh Roy","Nikhil Pinnaparaju","Risubh Jain","Manish Gupta","Vasudeva Varma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Summaformers @ LaySumm 20, LongSumm 20","tldr":"Automatic text summarization has been widely studied as an important task in natural language processing. Traditionally, various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.48_2shared","presentation_id":"38940742","rocketchat_channel":"paper-sdp2020-48_2shared","speakers":"Sayar Ghosh Roy|Nikhil Pinnaparaju|Risubh Jain|Manish Gupta|Vasudeva Varma","title":"Summaformers @ LaySumm 20, LongSumm 20"},{"content":{"abstract":"","authors":["Sayar Ghosh Roy","Nikhil Pinnaparaju","Risubh Jain","Manish Gupta\u2217","Vasudeva Varma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scientific Document Summarization for LaySumm '20 and LongSumm '2","tldr":null,"track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.48shared","presentation_id":"38940738","rocketchat_channel":"paper-sdp2020-48shared","speakers":"Sayar Ghosh Roy|Nikhil Pinnaparaju|Risubh Jain|Manish Gupta\u2217|Vasudeva Varma","title":"Scientific Document Summarization for LaySumm '20 and LongSumm '2"},{"content":{"abstract":"This work presents the entry by the team from Heidelberg University in the CL-SciSumm 2020 shared task at the Scholarly Document Processing workshop at EMNLP 2020. As in its previous iterations, the task is to highlight relevant parts in a reference paper, depending on a citance text excerpt from a citing paper. We participated in tasks 1A (citation identification) and 1B (citation context classification). Contrary to most previous works, we frame Task 1A as a search relevance problem, and introduce a 2-step re-ranking approach, which consists of a preselection based on BM25 in addition to positional document features, and a top-k re-ranking with BERT. For Task 1B, we follow previous submissions in applying methods that deal well with low resources and imbalanced classes.","authors":["Dennis Aumiller","Satya Almasian","Philip Hausner","Michael Gertz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UniHD@CL-SciSumm 2020: Citation Extraction as Search","tldr":"This work presents the entry by the team from Heidelberg University in the CL-SciSumm 2020 shared task at the Scholarly Document Processing workshop at EMNLP 2020. As in its previous iterations, the task is to highlight relevant parts in a reference ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.49","presentation_id":"38941224","rocketchat_channel":"paper-sdp2020-49","speakers":"Dennis Aumiller|Satya Almasian|Philip Hausner|Michael Gertz","title":"UniHD@CL-SciSumm 2020: Citation Extraction as Search"},{"content":{"abstract":"Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce two small ELECTRA based model named Bio-ELECTRA and Bio-ELECTRA++ that are eight times smaller than BERT Base and Bio-BERT and achieves comparable or better performance on biomedical question answering, yes/no question answer classification, question answer candidate ranking and relation extraction tasks. Bio-ELECTRA is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. Bio-ELECTRA++ is the further pre-trained version of Bio-ELECTRA trained on a corpus of open access full papers from PubMed Central. While, for biomedical named entity recognition, large BERT Base model outperforms Bio-ELECTRA++, Bio-ELECTRA and ELECTRA-Small++, with hyperparameter tuning Bio-ELECTRA++ achieves results comparable to BERT.","authors":["Ibrahim Burak Ozyurt"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining","tldr":"Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.5","presentation_id":"38940735","rocketchat_channel":"paper-sdp2020-5","speakers":"Ibrahim Burak Ozyurt","title":"On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining"},{"content":{"abstract":"In academic publications, citations are used to build context for a concept by highlighting relevant aspects from reference papers. Automatically identifying referenced snippets can help researchers swiftly isolate principal contributions of scientific works. In this paper, we exploit the underlying structure of scientific articles to predict reference paper spans and facets corresponding to a citation. We propose two methods to detect citation spans - keyphrase overlap, BERT along with structural priors. We fine-tune FastText embeddings and leverage textual, positional features to predict citation facets.","authors":["Anjana Umapathy","Karthik Radhakrishnan","Kinjal Jain","Rahul Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CiteQA@CLSciSumm 2020","tldr":"In academic publications, citations are used to build context for a concept by highlighting relevant aspects from reference papers. Automatically identifying referenced snippets can help researchers swiftly isolate principal contributions of scientif...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.54","presentation_id":"38941225","rocketchat_channel":"paper-sdp2020-54","speakers":"Anjana Umapathy|Karthik Radhakrishnan|Kinjal Jain|Rahul Singh","title":"CiteQA@CLSciSumm 2020"},{"content":{"abstract":"This paper presents our methods for the LongSumm 2020: Shared Task on Generating Long Summaries for Scientific Documents, where the task is to generatelong summaries given a set of scientific papers provided by the organizers. We explore 3 main approaches for this task: 1. An extractive approach using a BERT-based summarization model; 2. A two stage model that additionally includes an abstraction step using BART; and 3. A new multi-tasking approach on incorporating document structure into the summarizer. We found that our new multi-tasking approach outperforms the two other methods by large margins. Among 9 participants in the shared task, our best model ranks top according to Rouge-1 score (53.11%) while staying competitive in terms of Rouge-2.","authors":["Sajad Sotudeh Gharebagh","Arman Cohan","Nazli Goharian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GUIR @ LongSumm 2020: Learning to Generate Long Summaries from Scientific Documents","tldr":"This paper presents our methods for the LongSumm 2020: Shared Task on Generating Long Summaries for Scientific Documents, where the task is to generatelong summaries given a set of scientific papers provided by the organizers. We explore 3 main appro...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.56shared","presentation_id":"38940737","rocketchat_channel":"paper-sdp2020-56shared","speakers":"Sajad Sotudeh Gharebagh|Arman Cohan|Nazli Goharian","title":"GUIR @ LongSumm 2020: Learning to Generate Long Summaries from Scientific Documents"},{"content":{"abstract":"We study whether novel ideas in biomedical literature appear first in preprints or traditional journals. We develop a Bayesian method to estimate the time of appearance for a phrase in the literature, and apply it to a number of phrases, both automatically extracted and suggested by experts. We see that presently most phrases appear first in the traditional journals, but there is a number of phrases with the first appearance on preprint servers. A comparison of the general composition of texts from bioRxiv and traditional journals shows a growing trend of bioRxiv being predictive of traditional journals. We discuss the application of the method for related problems.","authors":["Swarup Satish","Zonghai Yao","Andrew Drozdov","Boris Veytsman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The impact of preprint servers in the formation of novel ideas","tldr":"We study whether novel ideas in biomedical literature appear first in preprints or traditional journals. We develop a Bayesian method to estimate the time of appearance for a phrase in the literature, and apply it to a number of phrases, both automat...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.6","presentation_id":"38940715","rocketchat_channel":"paper-sdp2020-6","speakers":"Swarup Satish|Zonghai Yao|Andrew Drozdov|Boris Veytsman","title":"The impact of preprint servers in the formation of novel ideas"},{"content":{"abstract":"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the multi-round TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the best systems. In round 3, we submitted the highest-scoring run that took advantage of previous training data and the second-highest fully automatic run. In rounds 4 and 5, we submitted the highest-scoring fully automatic runs.","authors":["Edwin Zhang","Nikhil Gupta","Raphael Tang","Xiao Han","Ronak Pradeep","Kuang Lu","Yue Zhang","Rodrigo Nogueira","Kyunghyun Cho","Hui Fang","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset","tldr":"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late Marc...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.60","presentation_id":"38940714","rocketchat_channel":"paper-sdp2020-60","speakers":"Edwin Zhang|Nikhil Gupta|Raphael Tang|Xiao Han|Ronak Pradeep|Kuang Lu|Yue Zhang|Rodrigo Nogueira|Kyunghyun Cho|Hui Fang|Jimmy Lin","title":"Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset"},{"content":{"abstract":"To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover and organize relevant literature. The system provides search at multiple levels of textual granularity, from sentences to aggregations across documents, both in natural language and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, question answering, search, analytics, expert search, and recommendations.","authors":["Marzieh Fadaee","Olga Gureenkova","Fernando Rejon Barrera","Carsten Schnober","Wouter Weerkamp","Jakub Zavrel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A New Neural Search and Insights Platform for Navigating and Organizing AI Research","tldr":"To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.61","presentation_id":"38940726","rocketchat_channel":"paper-sdp2020-61","speakers":"Marzieh Fadaee|Olga Gureenkova|Fernando Rejon Barrera|Carsten Schnober|Wouter Weerkamp|Jakub Zavrel","title":"A New Neural Search and Insights Platform for Navigating and Organizing AI Research"},{"content":{"abstract":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keywords of a given paper. We adapt the TextCNN architecture and automatically analyze its predictions using the Integrated Gradients method to highlight words and phrases that led to the recommendation of a scientific venue. We train and test our method on publications from the fields of artificial intelligence (AI) and medicine, both derived from the Semantic Scholar dataset. WTS achieves an Accuracy@5 of approximately 83% for AI papers and 95% in the field of medicine. It is open source and available for testing on https://wheretosubmit.ml.","authors":["Konstantin Kobs","Tobias Koopmann","Albin Zehe","David Fernes","Philipp Krop","Andreas Hotho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.78","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Where to Submit? Helping Researchers to Choose the Right Venue","tldr":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keyword...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.758","presentation_id":"38940736","rocketchat_channel":"paper-sdp2020-758","speakers":"Konstantin Kobs|Tobias Koopmann|Albin Zehe|David Fernes|Philipp Krop|Andreas Hotho","title":"Where to Submit? Helping Researchers to Choose the Right Venue"},{"content":{"abstract":"Expert search aims to find and rank experts based on a user\u2019s query. In academia, retrieving experts is an efficient way to navigate through a large amount of academic knowledge. Here, we study how different distributed representations of academic papers (i.e. embeddings) impact academic expert retrieval. We use the Microsoft Academic Graph dataset and experiment with different configurations of a document-centric voting model for retrieval. In particular, we explore the impact of the use of contextualized embeddings on search performance. We also present results for paper embeddings that incorporate citation information through retrofitting. Additionally, experiments are conducted using different techniques for assigning author weights based on author order. We observe that using contextual embeddings produced by a transformer model trained for sentence similarity tasks produces the most effective paper representations for document-centric expert retrieval. However, retrofitting the paper embeddings and using elaborate author contribution weighting strategies did not improve retrieval performance.","authors":["Mark Berger","Jakub Zavrel","Paul Groth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effective distributed representations for academic expert search","tldr":"Expert search aims to find and rank experts based on a user\u2019s query. In academia, retrieving experts is an efficient way to navigate through a large amount of academic knowledge. Here, we study how different distributed representations of academic pa...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.8","presentation_id":"38940716","rocketchat_channel":"paper-sdp2020-8","speakers":"Mark Berger|Jakub Zavrel|Paul Groth","title":"Effective distributed representations for academic expert search"},{"content":{"abstract":"Next to keeping up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges, computational work on enhancing search, summarization, and analysis of scholarly documents has flourished. However, the various strands of research on scholarly document processing remain fragmented. To reach to the broader NLP and AI/ML community, pool distributed efforts and enable shared access to published research, we held the 1st Workshop on Scholarly Document Processing at EMNLP 2020 as a virtual event. The SDP workshop consisted of a research track (including a poster session), two invited talks and three Shared Tasks (CL-SciSumm, Lay-Summ and LongSumm), geared towards easier access to scientific methods and results. Website: https://ornlcda.github.io/SDProc\n      ","authors":["Muthu Kumar Chandrasekaran","Guy Feigenblat","Dayne Freitag","Tirthankar Ghosal","Eduard Hovy","Philipp Mayr","Michal Shmueli-Scheuer","Anita de Waard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview of the First Workshop on Scholarly Document Processing (SDP)","tldr":"Next to keeping up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges, computational work on enhancing search, summarization, and analys...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.1","presentation_id":"","rocketchat_channel":"paper-sdp2020-1","speakers":"Muthu Kumar Chandrasekaran|Guy Feigenblat|Dayne Freitag|Tirthankar Ghosal|Eduard Hovy|Philipp Mayr|Michal Shmueli-Scheuer|Anita de Waard","title":"Overview of the First Workshop on Scholarly Document Processing (SDP)"},{"content":{"abstract":"arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greater. I will discuss the status and future of arXiv, and possibilities and plans to make more effective use of the research database to enhance ongoing research efforts.","authors":["Steinn Sigurdsson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The future of arXiv and knowledge discovery in open science","tldr":"arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greate...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.2","presentation_id":"","rocketchat_channel":"paper-sdp2020-2","speakers":"Steinn Sigurdsson","title":"The future of arXiv and knowledge discovery in open science"},{"content":{"abstract":"We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.","authors":["Muthu Kumar Chandrasekaran","Guy Feigenblat","Eduard Hovy","Abhilasha Ravichander","Michal Shmueli-Scheuer","Anita de Waard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm","tldr":"We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing t...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.24","presentation_id":"","rocketchat_channel":"paper-sdp2020-24","speakers":"Muthu Kumar Chandrasekaran|Guy Feigenblat|Eduard Hovy|Abhilasha Ravichander|Michal Shmueli-Scheuer|Anita de Waard","title":"Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm"},{"content":{"abstract":"The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and findings of the document. In the current paper, we present the participation of IITP-AI-NLP-ML team in three shared tasks, namely, CL-SciSumm 2020, LaySumm 2020, LongSumm 2020, which aims to generate medium, lay, and long summaries of the scientific articles, respectively. To solve CL-SciSumm 2020 and LongSumm 2020 tasks, three well-known clustering techniques are used, and then various sentence scoring functions, including textual entailment, are used to extract the sentences from each cluster for a summary generation. For LaySumm 2020, an encoder-decoder based deep learning model has been utilized. Performances of our developed systems are evaluated in terms of ROUGE measures on the associated datasets with the shared task.","authors":["Santosh Kumar Mishra","Harshavardhan Kundarapu","Naveen Saini","Sriparna Saha","Pushpak Bhattacharyya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IITP-AI-NLP-ML@ CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020","tldr":"The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.30","presentation_id":"","rocketchat_channel":"paper-sdp2020-30","speakers":"Santosh Kumar Mishra|Harshavardhan Kundarapu|Naveen Saini|Sriparna Saha|Pushpak Bhattacharyya","title":"IITP-AI-NLP-ML@ CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020"},{"content":{"abstract":"This document demonstrates our groups approach to the CL-SciSumm shared task 2020. There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation. In Task 1b, we use a SVM to classify the facet of a citation.","authors":["Artur Jurk","Maik Boltze","Georg Keller","Lorna Ulbrich","Anja Fischer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"1A-Team / Martin-Luther-Universit\u00e4t Halle-Wittenberg@CLSciSumm 20","tldr":"This document demonstrates our groups approach to the CL-SciSumm shared task 2020. There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.31","presentation_id":"","rocketchat_channel":"paper-sdp2020-31","speakers":"Artur Jurk|Maik Boltze|Georg Keller|Lorna Ulbrich|Anja Fischer","title":"1A-Team / Martin-Luther-Universit\u00e4t Halle-Wittenberg@CLSciSumm 20"},{"content":{"abstract":"This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity scores to identify spans of the reference text for the given citance. In Task 1b, we use a logistic regression to classifying the discourse facets.","authors":["Rong Huang","Kseniia Krylova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Team MLU@CL-SciSumm20: Methods for Computational Linguistics Scientific Citation Linkage","tldr":"This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity sco...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.32","presentation_id":"","rocketchat_channel":"paper-sdp2020-32","speakers":"Rong Huang|Kseniia Krylova","title":"Team MLU@CL-SciSumm20: Methods for Computational Linguistics Scientific Citation Linkage"},{"content":{"abstract":"This paper mainly introduces our methods for Task 1A and Task 1B of CL-SciSumm 2020. Task 1A is to identify reference text in reference paper. Traditional machine learning models and MLP model are used. We evaluate the performances of these models and submit the final results from the optimal model. Compared with previous work, we optimize the ratio of positive to negative examples after data sampling. In order to construct features for classification, we calculate similarities between reference text and candidate sentences based on sentence vectors. Accordingly, nine similarities are used, of which eight are chosen from what we used in CL-SciSumm 2019 and a new sentence similarity based on fastText is added. Task 1B is to classify the facets of reference text. Unlike the methods used in CL-SciSumm 2019, we construct inputs of models based on word vectors and add deep learning models for classification this year.","authors":["Heng Zhang","Lifan Liu","Ruping Wang","Shaohu Hu","Shutian Ma","Chengzhi Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IR&TM-NJUST@CLSciSumm 20","tldr":"This paper mainly introduces our methods for Task 1A and Task 1B of CL-SciSumm 2020. Task 1A is to identify reference text in reference paper. Traditional machine learning models and MLP model are used. We evaluate the performances of these models an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.33","presentation_id":"","rocketchat_channel":"paper-sdp2020-33","speakers":"Heng Zhang|Lifan Liu|Ruping Wang|Shaohu Hu|Shutian Ma|Chengzhi Zhang","title":"IR&TM-NJUST@CLSciSumm 20"},{"content":{"abstract":"In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in scientific articles. The task is to generate abstractive and extractive summaries of a given scientific article. In the proposed approach, we are inspired by the concept of Argumentative Zoning (AZ) that de- fines the main rhetorical structure in scientific articles. We define two aspects that should be covered in scientific paper summary, namely Claim/Method and Conclusion/Result aspects. We use Solr index to expand the sentences of the paper abstract. We formulate each abstract sentence in a given publication as query to retrieve similar sentences from the text body of the document itself. We utilize a sentence selection algorithm described in previous literature to select sentences for the final summary that covers the two aforementioned aspects.","authors":["Alaa El-Ebshihy","Annisa Maulida Ningtyas","Linda Andersson","Florina Piroi","Andreas Rauber"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ARTU / TU Wien and Artificial Researcher@ LongSumm 20","tldr":"In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in scientific a...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.36","presentation_id":"","rocketchat_channel":"paper-sdp2020-36","speakers":"Alaa El-Ebshihy|Annisa Maulida Ningtyas|Linda Andersson|Florina Piroi|Andreas Rauber","title":"ARTU / TU Wien and Artificial Researcher@ LongSumm 20"},{"content":{"abstract":"The Scholarly Document Processing (SDP) workshop is to encourage more efforts on natural language understanding of scientific task. It contains three shared tasks and we participate in the LongSumm shared task. In this paper, we describe our text summarization system, SciSummPip, inspired by SummPip (Zhao et al., 2020) that is an unsupervised text summarization system for multi-document in News domain. Our SciSummPip includes a transformer-based language model SciBERT (Beltagy et al., 2019) for contextual sentence representation, content selection with PageRank (Page et al., 1999), sentence graph construction with both deep and linguistic information, sentence graph clustering and within-graph summary generation. Our work differs from previous method in that content selection and a summary length constraint is applied to adapt to the scientific domain. The experiment results on both training dataset and blind test dataset show the effectiveness of our method, and we empirically verify the robustness of modules used in SciSummPip with BERTScore (Zhang et al., 2019a).","authors":["Jiaxin Ju","Ming Liu","Longxiang Gao","Shirui Pan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Monash-Summ@LongSumm 20 SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline","tldr":"The Scholarly Document Processing (SDP) workshop is to encourage more efforts on natural language understanding of scientific task. It contains three shared tasks and we participate in the LongSumm shared task. In this paper, we describe our text sum...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.37","presentation_id":"","rocketchat_channel":"paper-sdp2020-37","speakers":"Jiaxin Ju|Ming Liu|Longxiang Gao|Shirui Pan","title":"Monash-Summ@LongSumm 20 SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline"},{"content":{"abstract":"We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed with the domain of the research article. We propose a two step divide-and-conquer approach. First, we judiciously select segments of the documents that are not overly pedantic and are likely to be of interest to the laity, and over-extract sentences from each segment using an unsupervised network based method. Next, we perform abstractive summarization on these extractions and systematically merge the abstractions. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. Our approach leverages state-of-the-art pre-trained deep neural network based models as zero-shot learners to achieve high scores on the task.","authors":["Rochana Chaturvedi","Saachi .","Jaspreet Singh Dhani","Anurag Joshi","Ankush Khanna","Neha Tomar","Swagata Duari","Alka Khurana","Vasudha Bhatnagar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Divide and Conquer: From Complexity to Simplicity for Lay Summarization","tldr":"We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.40","presentation_id":"","rocketchat_channel":"paper-sdp2020-40","speakers":"Rochana Chaturvedi|Saachi .|Jaspreet Singh Dhani|Anurag Joshi|Ankush Khanna|Neha Tomar|Swagata Duari|Alka Khurana|Vasudha Bhatnagar","title":"Divide and Conquer: From Complexity to Simplicity for Lay Summarization"},{"content":{"abstract":"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.","authors":["Paul McCann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"fugashi, a Tool for Tokenizing Japanese in Python","tldr":"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without s...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.10","presentation_id":"38939744","rocketchat_channel":"paper-nlposs-10","speakers":"Paul McCann","title":"fugashi, a Tool for Tokenizing Japanese in Python"},{"content":{"abstract":"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.","authors":["Vincent Warmerdam","Thomas Kober","Rachael Tatman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Going Beyond T-SNE: Exposing whatlies in Text Embeddings","tldr":"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface tra...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.11","presentation_id":"38939745","rocketchat_channel":"paper-nlposs-11","speakers":"Vincent Warmerdam|Thomas Kober|Rachael Tatman","title":"Going Beyond T-SNE: Exposing whatlies in Text Embeddings"},{"content":{"abstract":"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.","authors":["Raphael Tang","Jaejun Lee","Afsaneh Razi","Julia Cambre","Ian Bicking","Jofish Kaye","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Howl: A Deployed, Open-Source Wake Word Detection System","tldr":"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on G...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.12","presentation_id":"38939746","rocketchat_channel":"paper-nlposs-12","speakers":"Raphael Tang|Jaejun Lee|Afsaneh Razi|Julia Cambre|Ian Bicking|Jofish Kaye|Jimmy Lin","title":"Howl: A Deployed, Open-Source Wake Word Detection System"},{"content":{"abstract":"We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages. By using pre-trained models from iNLTK for text classification on publicly available datasets, we significantly outperform previously reported results. On these datasets, we also show that by using pre-trained models and data augmentation from iNLTK, we can achieve more than 95% of the previous best performance by using less than 10% of the training data. iNLTK is already being widely used by the community and has 40,000+ downloads, 600+ stars and 100+ forks on GitHub. The library is available at https://github.com/goru001/inltk.","authors":["Gaurav Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"iNLTK: Natural Language Toolkit for Indic Languages","tldr":"We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages....","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.13","presentation_id":"38939747","rocketchat_channel":"paper-nlposs-13","speakers":"Gaurav Arora","title":"iNLTK: Natural Language Toolkit for Indic Languages"},{"content":{"abstract":"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each token is given a label and these labels are prefixed with special tokens such as B- or I-. After a model assigns labels to each token, these prefixes are used to group the tokens into spans. Properly parsing these annotations is critical for producing fair and comparable metrics; however, despite its importance, there is not an easy-to-use, standardized, programmatically integratable library to help work with span labeling. To remedy this, we introduce our open-source library, iobes. iobes is used for parsing, converting, and processing spans represented as token level decisions.","authors":["Brian Lester"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"iobes: Library for Span Level Processing","tldr":"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each t...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.14","presentation_id":"38939748","rocketchat_channel":"paper-nlposs-14","speakers":"Brian Lester","title":"iobes: Library for Span Level Processing"},{"content":{"abstract":"","authors":["Yada Pruksachatkun","Phil Yeres","Haokun Liu","Jason Phang","Phu Mon Htut","Alex Wang","Ian Tenney","Samuel R. Bowman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models","tldr":null,"track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.15","presentation_id":"38939749","rocketchat_channel":"paper-nlposs-15","speakers":"Yada Pruksachatkun|Phil Yeres|Haokun Liu|Jason Phang|Phu Mon Htut|Alex Wang|Ian Tenney|Samuel R. Bowman","title":"jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models"},{"content":{"abstract":"Despite the recent advances in applying language-independent approaches to various natural language processing tasks thanks to artificial intelligence, some language-specific tools are still essential to process a language in a viable manner. Kurdish language is a less-resourced language with a remarkable diversity in dialects and scripts and lacks basic language processing tools. To address this issue, we introduce a language processing toolkit to handle such a diversity in an efficient way. Our toolkit is composed of fundamental components such as text preprocessing, stemming, tokenization, lemmatization and transliteration and is able to get further extended by future developers. The project is publicly available.","authors":["Sina Ahmadi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KLPT \u2013 Kurdish Language Processing Toolkit","tldr":"Despite the recent advances in applying language-independent approaches to various natural language processing tasks thanks to artificial intelligence, some language-specific tools are still essential to process a language in a viable manner. Kurdish...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.16","presentation_id":"38939750","rocketchat_channel":"paper-nlposs-16","speakers":"Sina Ahmadi","title":"KLPT \u2013 Kurdish Language Processing Toolkit"},{"content":{"abstract":"Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Korean corpora, first describing institution-level resource development, then further iterate through a list of current open datasets for different types of tasks. We then propose a direction on how open-source dataset construction and releases should be done for less-resourced languages to promote research.","authors":["Won Ik Cho","Sangwhan Moon","Youngsook Song"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Open Korean Corpora: A Practical Report","tldr":"Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Kor...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.17","presentation_id":"38939751","rocketchat_channel":"paper-nlposs-17","speakers":"Won Ik Cho|Sangwhan Moon|Youngsook Song","title":"Open Korean Corpora: A Practical Report"},{"content":{"abstract":"This document describes shared development of finite-state description of two closely related but endangered minority languages, Erzya and Moksha. It touches upon morpholexical unity and diversity of the two languages and how this provides a motivation for shared open-source FST development. We describe how we have designed the transducers so that they can benefit from existing open-source infrastructures and are as reusable as possible.","authors":["Jack Rueter","Mika H\u00e4m\u00e4l\u00e4inen","Niko Partanen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Open-Source Morphology for Endangered Mordvinic Languages","tldr":"This document describes shared development of finite-state description of two closely related but endangered minority languages, Erzya and Moksha. It touches upon morpholexical unity and diversity of the two languages and how this provides a motivati...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.18","presentation_id":"38939752","rocketchat_channel":"paper-nlposs-18","speakers":"Jack Rueter|Mika H\u00e4m\u00e4l\u00e4inen|Niko Partanen","title":"Open-Source Morphology for Endangered Mordvinic Languages"},{"content":{"abstract":"We present Pimlico, an open source toolkit for building pipelines for processing large corpora. It is especially focused on processing linguistic corpora and provides wrappers around existing, widely used NLP tools. A particular goal is to ease distribution of reproducible and extensible experiments by making it easy to document and re-run all steps involved, including data loading, pre-processing, model training and evaluation. Once a pipeline is released, it is easy to adapt, for example, to run on a new dataset, or to re-run an experiment with different parameters. The toolkit takes care of many common challenges in writing and distributing corpus-processing code, such as managing data between the steps of a pipeline, installing required software and combining existing toolkits with new, task-specific code.","authors":["Mark Granroth-Wilding"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pimlico: A toolkit for corpus-processing pipelines and reproducible experiments","tldr":"We present Pimlico, an open source toolkit for building pipelines for processing large corpora. It is especially focused on processing linguistic corpora and provides wrappers around existing, widely used NLP tools. A particular goal is to ease distr...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.19","presentation_id":"38939753","rocketchat_channel":"paper-nlposs-19","speakers":"Mark Granroth-Wilding","title":"Pimlico: A toolkit for corpus-processing pipelines and reproducible experiments"},{"content":{"abstract":"We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to Python with additional improvements and functionality. PySBD passes 97.92% of the Golden Rule Set examplars for English, an improvement of 25% over the next best open source Python tool.","authors":["Nipun Sadvilkar","Mark Neumann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PySBD: Pragmatic Sentence Boundary Disambiguation","tldr":"We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unkno...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.20","presentation_id":"38939754","rocketchat_channel":"paper-nlposs-20","speakers":"Nipun Sadvilkar|Mark Neumann","title":"PySBD: Pragmatic Sentence Boundary Disambiguation"},{"content":{"abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","authors":["Daniel Deutsch","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","tldr":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the off...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.21","presentation_id":"38939755","rocketchat_channel":"paper-nlposs-21","speakers":"Daniel Deutsch|Dan Roth","title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics"},{"content":{"abstract":"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.","authors":["John Morris","Jin Yong Yoo","Yanjun Qi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TextAttack: Lessons learned in designing Python frameworks for NLP","tldr":"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused acro...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.22","presentation_id":"38939756","rocketchat_channel":"paper-nlposs-22","speakers":"John Morris|Jin Yong Yoo|Yanjun Qi","title":"TextAttack: Lessons learned in designing Python frameworks for NLP"},{"content":{"abstract":"From LDA to neural models, different topic modeling approaches have been proposed in the literature. However, their suitability and performance is not easy to compare, particularly when the algorithms are being used in the wild on heterogeneous datasets. In this paper, we introduce ToModAPI (TOpic MOdeling API), a wrapper library to easily train, evaluate and infer using different topic modeling algorithms through a unified interface. The library is extensible and can be used in Python environments or through a Web API.","authors":["Pasquale Lisena","Ismail Harrando","Oussama Kandakji","Raphael Troncy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TOMODAPI: A Topic Modeling API to Train, Use and Compare Topic Models","tldr":"From LDA to neural models, different topic modeling approaches have been proposed in the literature. However, their suitability and performance is not easy to compare, particularly when the algorithms are being used in the wild on heterogeneous datas...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.23","presentation_id":"38939757","rocketchat_channel":"paper-nlposs-23","speakers":"Pasquale Lisena|Ismail Harrando|Oussama Kandakji|Raphael Troncy","title":"TOMODAPI: A Topic Modeling API to Train, Use and Compare Topic Models"},{"content":{"abstract":"For the last 5 years, we have developed and maintained RSMTool \u2013 an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its cross-disciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper.","authors":["Nitin Madnani","Anastassia Loukina"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"User-centered & Robust NLP OSS: Lessons Learned from Developing & Maintaining RSMTool","tldr":"For the last 5 years, we have developed and maintained RSMTool \u2013 an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine l...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.24","presentation_id":"38939758","rocketchat_channel":"paper-nlposs-24","speakers":"Nitin Madnani|Anastassia Loukina","title":"User-centered & Robust NLP OSS: Lessons Learned from Developing & Maintaining RSMTool"},{"content":{"abstract":"The WordNet database of English (Fellbaum, 1998) is a key source of semantic information for research and development of natural language processing applications. As the sophistication of these applications increases with the use of large datasets, deep learning, and graph-based methods, so should the use of WordNet. To this end, we introduce WAFFLE: WordNet Applied to FreeForm Linguistic Exploration which makes WordNet available in an open source graph data structure. The WAFFLE graph relies on platform agnostic formats for robust interrogation and flexibility. Where existing implementations of WordNet offer dictionary-like lookup, single degree neighborhood operations, and path based similarity-scoring, the WAFFLE graph makes all nodes (semantic relation sets) and relationships queryable at scale, enabling local and global analysis of all relationships without the need for custom code. We demonstrate WAFFLE\u2019s ease of use, visualization capabilities, and scalable efficiency with common queries, operations, and interactions. WAFFLE is available at github.com/TRSS-NLP/WAFFLE.","authors":["Berk Ekmekci","Blake Howald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WAFFLE: A Graph for WordNet Applied to FreeForm Linguistic Exploration","tldr":"The WordNet database of English (Fellbaum, 1998) is a key source of semantic information for research and development of natural language processing applications. As the sophistication of these applications increases with the use of large datasets, d...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.25","presentation_id":"38939759","rocketchat_channel":"paper-nlposs-25","speakers":"Berk Ekmekci|Blake Howald","title":"WAFFLE: A Graph for WordNet Applied to FreeForm Linguistic Exploration"},{"content":{"abstract":"Conversational agents can be used to make diagnoses, classify mental states, promote health education, and provide emotional support. The benefits of adopting conversational agents include widespread access, increased treatment engagement, and improved patient relationships with the intervention. We propose here a framework to assist chat operators of mental healthcare services, instead of a fully automated conversational agent. This design eases to avoid the adverse effects of applying chatbots in mental healthcare. The proposed framework is capable of improving the quality and reducing the time of interactions via chat between a user and a chat operator. We also present a case study in the context of health promotion on reducing tobacco use. The proposed framework uses artificial intelligence, specifically natural language processing (NLP) techniques, to classify messages from chat users. A list of suggestions is offered to the chat operator, with topics to be discussed in the session. These suggestions were created based on service protocols and the classification of previous chat sessions. The operator can also edit the suggested messages. Data collected can be used in the future to improve the quality of the suggestions offered.","authors":["Thiago Madeira","Heder Bernardino","Jairo Francisco De Souza","Henrique Gomide","Nath\u00e1lia Munck Machado","Bruno Marcos Pinheiro da Silva","Alexandre Vieira Pereira Pacelli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Framework to Assist Chat Operators of Mental Healthcare Services","tldr":"Conversational agents can be used to make diagnoses, classify mental states, promote health education, and provide emotional support. The benefits of adopting conversational agents include widespread access, increased treatment engagement, and improv...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.4","presentation_id":"38939738","rocketchat_channel":"paper-nlposs-4","speakers":"Thiago Madeira|Heder Bernardino|Jairo Francisco De Souza|Henrique Gomide|Nath\u00e1lia Munck Machado|Bruno Marcos Pinheiro da Silva|Alexandre Vieira Pereira Pacelli","title":"A Framework to Assist Chat Operators of Mental Healthcare Services"},{"content":{"abstract":"Automating natural language understanding is a lifelong quest addressed for decades. With the help of advances in machine learning and particularly, deep learning, we are able to produce state of the art models that can imitate human interactions with languages. Unfortunately, these advances are controlled by the availability of language resources. Arabic advances in this field , although it has a great potential, are still limited. This is apparent in both research and development. In this paper, we showcase some NLP models we trained for Arabic. We also present our methodology and pipeline to build such models from data collection, data preprocessing, tokenization and model deployment. These tools help in the advancement of the field and provide a systematic approach for extending NLP tools to many languages.","authors":["Zaid Alyafeai","Maged Al-Shaibani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ARBML: Democritizing Arabic Natural Language Processing Tools","tldr":"Automating natural language understanding is a lifelong quest addressed for decades. With the help of advances in machine learning and particularly, deep learning, we are able to produce state of the art models that can imitate human interactions wit...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.5","presentation_id":"38939739","rocketchat_channel":"paper-nlposs-5","speakers":"Zaid Alyafeai|Maged Al-Shaibani","title":"ARBML: Democritizing Arabic Natural Language Processing Tools"},{"content":{"abstract":"The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components \u2013 parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network (GNN) libraries. Additionally, we discuss downstream usage and applications of the library, and how it can accelerate research for the NLP community.","authors":["Raeid Saqur","Ameet Deshpande"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes","tldr":"The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.6","presentation_id":"38939740","rocketchat_channel":"paper-nlposs-6","speakers":"Raeid Saqur|Ameet Deshpande","title":"CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes"},{"content":{"abstract":"The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been instrumental in the democratization of this technology. The maturity of the open-source NLP ecosystem however varies between languages. This work proposes a new open-source library aimed at bringing state-of-the-art NLP to Rust. Rust is a systems programming language for which the foundations required to build machine learning applications are available but still lacks ready-to-use, end-to-end NLP libraries. The proposed library, rust-bert, implements modern language models and ready-to-use pipelines (for example translation or summarization). This allows further development by the Rust community from both NLP experts and non-specialists. It is hoped that this library will accelerate the development of the NLP ecosystem in Rust. The library is under active development and available at https://github.com/guillaume-be/rust-bert.","authors":["Guillaume Becquin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End-to-end NLP Pipelines in Rust","tldr":"The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been i...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.7","presentation_id":"38939741","rocketchat_channel":"paper-nlposs-7","speakers":"Guillaume Becquin","title":"End-to-end NLP Pipelines in Rust"},{"content":{"abstract":"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.","authors":["Vaibhav Kumar","Tenzin Bhotia","Vaibhav Kumar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings","tldr":"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases wh...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.8","presentation_id":"38939742","rocketchat_channel":"paper-nlposs-8","speakers":"Vaibhav Kumar|Tenzin Bhotia|Vaibhav Kumar","title":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings"},{"content":{"abstract":"Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage.","authors":["Leonid Boytsov","Eric Nyberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Flexible retrieval with NMSLIB and FlexNeuART","tldr":"Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of d...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.9","presentation_id":"38939743","rocketchat_channel":"paper-nlposs-9","speakers":"Leonid Boytsov|Eric Nyberg","title":"Flexible retrieval with NMSLIB and FlexNeuART"},{"content":{"abstract":"","authors":["Sayali Kulkarni","Shailee Jain","Mohammad Javad Hosseini","Jason Baldridge","Eugene Ie","Li Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Geocoding with multi-level loss for spatial language representation","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.11","presentation_id":"38940083","rocketchat_channel":"paper-splu2020-11","speakers":"Sayali Kulkarni|Shailee Jain|Mohammad Javad Hosseini|Jason Baldridge|Eugene Ie|Li Zhang","title":"Geocoding with multi-level loss for spatial language representation"},{"content":{"abstract":"","authors":["Roshanak Mirzaee","Hossein Rajaby Faghihi","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.12","presentation_id":"38940084","rocketchat_channel":"paper-splu2020-12","speakers":"Roshanak Mirzaee|Hossein Rajaby Faghihi|Parisa Kordjamshidi","title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning"},{"content":{"abstract":"","authors":["Yue Zhang","Quan Guo","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.13","presentation_id":"38940085","rocketchat_channel":"paper-splu2020-13","speakers":"Yue Zhang|Quan Guo|Parisa Kordjamshidi","title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations"},{"content":{"abstract":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent RMM where robots need to both ask and answer questions.","authors":["Homero Roman Roman","Yonatan Bisk","Jesse Thomason","Asli Celikyilmaz","Jianfeng Gao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.157","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RMM: A Recursive Mental Model for Dialogue Navigation","tldr":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and ask...","track":"Spatial Language Understanding"},"id":"WS-10.1453","presentation_id":"38940095","rocketchat_channel":"paper-splu2020-1453","speakers":"Homero Roman Roman|Yonatan Bisk|Jesse Thomason|Asli Celikyilmaz|Jianfeng Gao","title":"RMM: A Recursive Mental Model for Dialogue Navigation"},{"content":{"abstract":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","authors":["Tsung-Yen Yang","Andrew Lan","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.172","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Robust and Interpretable Grounding of Spatial References with Relation Networks","tldr":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for...","track":"Spatial Language Understanding"},"id":"WS-10.1595","presentation_id":"38940094","rocketchat_channel":"paper-splu2020-1595","speakers":"Tsung-Yen Yang|Andrew Lan|Karthik Narasimhan","title":"Robust and Interpretable Grounding of Spatial References with Relation Networks"},{"content":{"abstract":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.","authors":["Alberto Testoni","Claudio Greco","Tobias Bianchi","Mauricio Mazuecos","Agata Marcante","Luciana Benotti","Raffaella Bernardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"They are not all alike: answering different spatial questions requires different grounding strategies","tldr":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We b...","track":"Spatial Language Understanding"},"id":"WS-10.2","presentation_id":"38940076","rocketchat_channel":"paper-splu2020-2","speakers":"Alberto Testoni|Claudio Greco|Tobias Bianchi|Mauricio Mazuecos|Agata Marcante|Luciana Benotti|Raffaella Bernardi","title":"They are not all alike: answering different spatial questions requires different grounding strategies"},{"content":{"abstract":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ARRAMON. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language (English) instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work.","authors":["Hyounghun Kim","Abhaysinh Zala","Graham Burri","Hao Tan","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.348","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments","tldr":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We ...","track":"Spatial Language Understanding"},"id":"WS-10.2904","presentation_id":"38940093","rocketchat_channel":"paper-splu2020-2904","speakers":"Hyounghun Kim|Abhaysinh Zala|Graham Burri|Hao Tan|Mohit Bansal","title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments"},{"content":{"abstract":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while other features are more salient when assessing typicality. In this paper we explore the extent to which this is the case for English spatial prepositions and discuss the implications for pragmatic strategies and semantic models. We hypothesise that object-specific features \u2014 related to object properties and affordances \u2014 are more salient in categorisation, while geometric and physical relationships between objects are more salient in typicality judgements. In order to test this hypothesis we conducted a study using virtual environments to collect both category and typicality judgements in 3D scenes. Based on the collected data we cannot verify the hypothesis and conclude that object-specific features appear to be salient in both category and typicality judgements, further evidencing the need to include these types of features in semantic models.","authors":["Adam Richard-Bollans","Anthony Cohn","Luc\u00eda G\u00f3mez \u00c1lvarez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions","tldr":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while ot...","track":"Spatial Language Understanding"},"id":"WS-10.3","presentation_id":"38940077","rocketchat_channel":"paper-splu2020-3","speakers":"Adam Richard-Bollans|Anthony Cohn|Luc\u00eda G\u00f3mez \u00c1lvarez","title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions"},{"content":{"abstract":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Currently, the best-performing models are able to complete less than 1% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information, the starting location in the virtual environment, is incorporated, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases, suggesting contextualized language models may provide strong planning modules for grounded virtual agents.","authors":["Peter Jansen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.395","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions","tldr":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Curre...","track":"Spatial Language Understanding"},"id":"WS-10.3302","presentation_id":"38940098","rocketchat_channel":"paper-splu2020-3302","speakers":"Peter Jansen","title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions"},{"content":{"abstract":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our method with a user study, validating that our generated spatial arrangements align with human expectation.","authors":["Gorjan Radevski","Guillem Collell","Marie-Francine Moens","Tinne Tuytelaars"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.408","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Decoding Language Spatial Relations to 2D Spatial Arrangements","tldr":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-...","track":"Spatial Language Understanding"},"id":"WS-10.3382","presentation_id":"38940092","rocketchat_channel":"paper-splu2020-3382","speakers":"Gorjan Radevski|Guillem Collell|Marie-Francine Moens|Tinne Tuytelaars","title":"Decoding Language Spatial Relations to 2D Spatial Arrangements"},{"content":{"abstract":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with multiple visual features with different sizes of receptive fields, though the proper size of the receptive field of visual features intuitively varies depending on expressions. In this paper, we introduce a neural network architecture that modulates visual features with varying sizes of receptive field by linguistic features. We evaluate our architecture on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our architecture. Source code is available at https://github.com/Alab-NII/lcfp .","authors":["Taichi Iki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.420","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks","tldr":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models cons...","track":"Spatial Language Understanding"},"id":"WS-10.3466","presentation_id":"38940091","rocketchat_channel":"paper-splu2020-3466","speakers":"Taichi Iki|Akiko Aizawa","title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks"},{"content":{"abstract":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).","authors":["Hyeong Jin Shin","Jeong Yeon Park","Dae Bum Yuk","Jae Sung Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-based Spatial Information Extraction","tldr":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018)...","track":"Spatial Language Understanding"},"id":"WS-10.5","presentation_id":"38940078","rocketchat_channel":"paper-splu2020-5","speakers":"Hyeong Jin Shin|Jeong Yeon Park|Dae Bum Yuk|Jae Sung Lee","title":"BERT-based Spatial Information Extraction"},{"content":{"abstract":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.","authors":["Chao Xu","Emmanuelle-Anna Dietz Saldanha","Dagmar Gromann","Beihai Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Cognitively Motivated Approach to Spatial Information Extraction","tldr":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing s...","track":"Spatial Language Understanding"},"id":"WS-10.6","presentation_id":"38940079","rocketchat_channel":"paper-splu2020-6","speakers":"Chao Xu|Emmanuelle-Anna Dietz Saldanha|Dagmar Gromann|Beihai Zhou","title":"A Cognitively Motivated Approach to Spatial Information Extraction"},{"content":{"abstract":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model\u2019s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues.","authors":["Takuma Udagawa","Takato Yamazaki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.67","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions","tldr":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize th...","track":"Spatial Language Understanding"},"id":"WS-10.676","presentation_id":"38940097","rocketchat_channel":"paper-splu2020-676","speakers":"Takuma Udagawa|Takato Yamazaki|Akiko Aizawa","title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions"},{"content":{"abstract":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with respect to some anatomical structures. As the expressions result from the mental visualization of the radiologist\u2019s interpretations, they are varied and complex. The focus of this work is to automatically identify the spatial expression terms from three different radiology sub-domains. We propose a hybrid deep learning-based NLP method that includes \u2013 1) generating a set of candidate spatial triggers by exact match with the known trigger terms from the training data, 2) applying domain-specific constraints to filter the candidate triggers, and 3) utilizing a BERT-based classifier to predict whether a candidate trigger is a true spatial trigger or not. The results are promising, with an improvement of 24 points in the average F1 measure compared to a standard BERT-based sequence labeler.","authors":["Surabhi Datta","Kirk Roberts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports","tldr":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with r...","track":"Spatial Language Understanding"},"id":"WS-10.7","presentation_id":"38940080","rocketchat_channel":"paper-splu2020-7","speakers":"Surabhi Datta|Kirk Roberts","title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports"},{"content":{"abstract":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an image retrieval engine, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed method achieves a higher F1 value (89.67%) than a baseline method, demonstrating the effectiveness of using element-wise visual information.","authors":["Takuya Komada","Takashi Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition","tldr":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NE...","track":"Spatial Language Understanding"},"id":"WS-10.8","presentation_id":"38940081","rocketchat_channel":"paper-splu2020-8","speakers":"Takuya Komada|Takashi Inui","title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition"},{"content":{"abstract":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the LiMiT dataset and share it publicly as a new resource for the research community.","authors":["Irene Manotas","Ngoc Phuoc An Vo","Vadim Sheinin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.88","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LiMiT: The Literal Motion in Text Dataset","tldr":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) datase...","track":"Spatial Language Understanding"},"id":"WS-10.857","presentation_id":"38940096","rocketchat_channel":"paper-splu2020-857","speakers":"Irene Manotas|Ngoc Phuoc An Vo|Vadim Sheinin","title":"LiMiT: The Literal Motion in Text Dataset"},{"content":{"abstract":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.","authors":["Harsh Mehta","Yoav Artzi","Jason Baldridge","Eugene Ie","Piotr Mirowski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View","tldr":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively wi...","track":"Spatial Language Understanding"},"id":"WS-10.9","presentation_id":"38940082","rocketchat_channel":"paper-splu2020-9","speakers":"Harsh Mehta|Yoav Artzi|Jason Baldridge|Eugene Ie|Piotr Mirowski","title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View"},{"content":{"abstract":"","authors":["Isabel Papadimitriou","Ethan A. Chi","Richard Futrell","Kyle Mahowald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual BERT Learns Abstract Case Representations","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.10","presentation_id":"38939802","rocketchat_channel":"paper-sigtyp-10","speakers":"Isabel Papadimitriou|Ethan A. Chi|Richard Futrell|Kyle Mahowald","title":"Multilingual BERT Learns Abstract Case Representations"},{"content":{"abstract":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models\u2019 pretraining data and target language varieties.","authors":["Ethan C. Chau","Lucy H. Lin","Noah A. Smith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.118","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank","tldr":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar t...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.1093-WS11","presentation_id":"38940630","rocketchat_channel":"paper-sigtyp-1093-WS11","speakers":"Ethan C. Chau|Lucy H. Lin|Noah A. Smith","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"content":{"abstract":"","authors":["Harald Hammarstr\u00f6m"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Keyword Spotting: A quick-and-dirty method for extracting typological features of language from grammatical descriptions","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.11","presentation_id":"38939803","rocketchat_channel":"paper-sigtyp-11","speakers":"Harald Hammarstr\u00f6m","title":"Keyword Spotting: A quick-and-dirty method for extracting typological features of language from grammatical descriptions"},{"content":{"abstract":"This paper describes a workflow to impute missing values in a typological database, a sub- set of the World Atlas of Language Structures (WALS). Using a world-wide phylogeny de- rived from lexical data, the model assumes a phylogenetic continuous time Markov chain governing the evolution of typological val- ues. Data imputation is performed via a Max- imum Likelihood estimation on the basis of this model. As back-off model for languages whose phylogenetic position is unknown, a k- nearest neighbor classification based on geo- graphic distance is performed.","authors":["Gerhard J\u00e4ger"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Imputing typological values via phylogenetic inference","tldr":"This paper describes a workflow to impute missing values in a typological database, a sub- set of the World Atlas of Language Structures (WALS). Using a world-wide phylogeny de- rived from lexical data, the model assumes a phylogenetic continuous tim...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.12","presentation_id":"38939793","rocketchat_channel":"paper-sigtyp-12","speakers":"Gerhard J\u00e4ger","title":"Imputing typological values via phylogenetic inference"},{"content":{"abstract":"","authors":["Sonia Cristofaro","Guglielmo Inglese"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DEmA: the Pavia Diachronic Emergence of Alignment database","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.13","presentation_id":"38939804","rocketchat_channel":"paper-sigtyp-13","speakers":"Sonia Cristofaro|Guglielmo Inglese","title":"DEmA: the Pavia Diachronic Emergence of Alignment database"},{"content":{"abstract":"","authors":["Barend Beekhuizen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A dataset and metric to evaluate lexical extraction from parallel corpora","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.14","presentation_id":"38939805","rocketchat_channel":"paper-sigtyp-14","speakers":"Barend Beekhuizen","title":"A dataset and metric to evaluate lexical extraction from parallel corpora"},{"content":{"abstract":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings \u2013 both static and contextualized \u2013 for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners \u2013 even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.","authors":["Masoud Jalili Sabet","Philipp Dufter","Fran\u00e7ois Yvon","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.147","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings","tldr":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. Howeve...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.1409","presentation_id":"38940631","rocketchat_channel":"paper-sigtyp-1409","speakers":"Masoud Jalili Sabet|Philipp Dufter|Fran\u00e7ois Yvon|Hinrich Sch\u00fctze","title":"SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings"},{"content":{"abstract":"","authors":["Limor Raviv","Antje Meyer","Shiri Lev-Ari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The role of community size and network structure in shaping linguistic diversity: experimental evidence","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.16","presentation_id":"38939806","rocketchat_channel":"paper-sigtyp-16","speakers":"Limor Raviv|Antje Meyer|Shiri Lev-Ari","title":"The role of community size and network structure in shaping linguistic diversity: experimental evidence"},{"content":{"abstract":"The paper describes the Multitasking Self-attention based approach to constrained sub-task within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers (CITATION) model. The model uses Multitasking to compute values of all WALS features for a given input language simultaneously.\n\nResults show that our approach performs at par with the baseline approaches, even though our proposed approach requires only phylogenetic and geographical attributes namely Longitude, Latitude, Genus-index, Family-index and Country-index and do not use any of the known WALS features of the respective input language, to compute its missing WALS features.","authors":["Chinmay Choudhary"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NUIG: Multitasking Self-attention based approach to SigTyp 2020 Shared Task","tldr":"The paper describes the Multitasking Self-attention based approach to constrained sub-task within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers (CITATION) model. The model uses Multitasking to...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.17","presentation_id":"38939794","rocketchat_channel":"paper-sigtyp-17","speakers":"Chinmay Choudhary","title":"NUIG: Multitasking Self-attention based approach to SigTyp 2020 Shared Task"},{"content":{"abstract":"This paper enumerates SigTyP 2020 Shared Task on the prediction of typological features as performed by the KMI-Panlingua-IITKGP team. The task entailed the prediction of missing values in a particular language, provided, the name of the language family, its genus, location (in terms of latitude and longitude coordinates and name of the country where it is spoken) and a set of feature-value pair are available. As part of fulfillment of the aforementioned task, the team submitted 3 kinds of system - 2 rule-based and one hybrid system. Of these 3, one rule-based system generated the best performance on the test set. All the systems were \u2018constrained\u2019 in the sense that no additional dataset or information, other than those provided by the organisers, was used for developing the systems.","authors":["Ritesh Kumar","Deepak Alok","Akanksha Bansal","Bornini Lahiri","Atul Kr. Ojha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KMI-Panlingua-IITKGP @SIGTYP2020: Exploring rules and hybrid systems for automatic prediction of typological features","tldr":"This paper enumerates SigTyP 2020 Shared Task on the prediction of typological features as performed by the KMI-Panlingua-IITKGP team. The task entailed the prediction of missing values in a particular language, provided, the name of the language fam...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.18","presentation_id":"38939795","rocketchat_channel":"paper-sigtyp-18","speakers":"Ritesh Kumar|Deepak Alok|Akanksha Bansal|Bornini Lahiri|Atul Kr. Ojha","title":"KMI-Panlingua-IITKGP @SIGTYP2020: Exploring rules and hybrid systems for automatic prediction of typological features"},{"content":{"abstract":"","authors":["Michael Richter","Tariq Yousef"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information from Topic Contexts: The Prediction of Aspectual Coding of Verbs in Russian","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.2","presentation_id":"38939796","rocketchat_channel":"paper-sigtyp-2","speakers":"Michael Richter|Tariq Yousef","title":"Information from Topic Contexts: The Prediction of Aspectual Coding of Verbs in Russian"},{"content":{"abstract":"","authors":["Chiara Alzetta","Felice Dell'Orletta","Simonetta Montemagni","Giulia Venturi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncovering Typological Context-Sensitive Features","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.3","presentation_id":"38939797","rocketchat_channel":"paper-sigtyp-3","speakers":"Chiara Alzetta|Felice Dell'Orletta|Simonetta Montemagni|Giulia Venturi","title":"Uncovering Typological Context-Sensitive Features"},{"content":{"abstract":"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We employ frequentist inference to represent correlations between typological features and use this representation to train simple multi-class estimators that predict individual features. We describe two submitted ridge regression-based configurations which ranked second and third overall in the constrained task. Our best configuration achieved the microaveraged accuracy score of 0.66 on 149 test languages.","authors":["Alexander Gutkin","Richard Sproat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task","tldr":"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We ...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.4","presentation_id":"38939791","rocketchat_channel":"paper-sigtyp-4","speakers":"Alexander Gutkin|Richard Sproat","title":"NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task"},{"content":{"abstract":"","authors":["Alexander Gutkin","Martin Jansche","Lucy Skidmore"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Induction of Structured Phoneme Inventories","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.5","presentation_id":"38939798","rocketchat_channel":"paper-sigtyp-5","speakers":"Alexander Gutkin|Martin Jansche|Lucy Skidmore","title":"Towards Induction of Structured Phoneme Inventories"},{"content":{"abstract":"","authors":["Ahmet \u00dcst\u00fcn","Arianna Bisazza","Gosse Bouma","Gertjan Van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is Typology-Based Adaptation Effective for Multilingual Sequence Labelling?","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.6","presentation_id":"38939799","rocketchat_channel":"paper-sigtyp-6","speakers":"Ahmet \u00dcst\u00fcn|Arianna Bisazza|Gosse Bouma|Gertjan Van Noord","title":"Is Typology-Based Adaptation Effective for Multilingual Sequence Labelling?"},{"content":{"abstract":"We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing conditional probabilities and mutual information. The second approach is to train a neural predictor operating on precomputed language embeddings based on WALS features. Our submitted system combines the two approaches based on their self-estimated confidence scores. We reach the accuracy of 70.7% on the test data and rank first in the shared task.","authors":["Martin Vastl","Daniel Zeman","Rudolf Rosa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities: \u00daFAL Submission to the SIGTYP 2020 Shared Task","tldr":"We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.7","presentation_id":"38939792","rocketchat_channel":"paper-sigtyp-7","speakers":"Martin Vastl|Daniel Zeman|Rudolf Rosa","title":"Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities: \u00daFAL Submission to the SIGTYP 2020 Shared Task"},{"content":{"abstract":"","authors":["Aryaman Arora","Nathan Schneider"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SNACS Annotation of Case Markers and Adpositions in Hindi","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.8","presentation_id":"38939800","rocketchat_channel":"paper-sigtyp-8","speakers":"Aryaman Arora|Nathan Schneider","title":"SNACS Annotation of Case Markers and Adpositions in Hindi"},{"content":{"abstract":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.","authors":["Saurabh Kulshreshtha","Jose Luis Redondo Garcia","Ching-Yun Chang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.83","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study","tldr":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved b...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.816","presentation_id":"38940629","rocketchat_channel":"paper-sigtyp-816","speakers":"Saurabh Kulshreshtha|Jose Luis Redondo Garcia|Ching-Yun Chang","title":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study"},{"content":{"abstract":"","authors":["Yushi Hu","Shane Settle","Karen Livescu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual Jointly Trained Acoustic and Written Word Embeddings","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.9","presentation_id":"38939801","rocketchat_channel":"paper-sigtyp-9","speakers":"Yushi Hu|Shane Settle|Karen Livescu","title":"Multilingual Jointly Trained Acoustic and Written Word Embeddings"},{"content":{"abstract":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world\u2019s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.","authors":["Johannes Bjerva","Elizabeth Salesky","Sabrina J. Mielke","Aditi Chaudhary","Celano Giuseppe","Edoardo Maria Ponti","Ekaterina Vylomova","Ryan Cotterell","Isabelle Augenstein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SIGTYP 2020 Shared Task: Prediction of Typological Features","tldr":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world\u2019s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer lear...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.2020.sigtyp-1.1","presentation_id":"","rocketchat_channel":"paper-sigtyp-1","speakers":"Johannes Bjerva|Elizabeth Salesky|Sabrina J. Mielke|Aditi Chaudhary|Celano Giuseppe|Edoardo Maria Ponti|Ekaterina Vylomova|Ryan Cotterell|Isabelle Augenstein","title":"SIGTYP 2020 Shared Task: Prediction of Typological Features"},{"content":{"abstract":"Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods. However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network (DCAN), integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art.","authors":["Shaoxiong Ji","Erik Cambria","Pekka Marttinen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text","tldr":"Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignme...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1","presentation_id":"38939807","rocketchat_channel":"paper-clinicalnlp-1","speakers":"Shaoxiong Ji|Erik Cambria|Pekka Marttinen","title":"Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text"},{"content":{"abstract":"Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) tasks. In this work, we present a novel extension to the Transformer architecture, by incorporating signature transform with the self-attention model. This architecture is added between embedding and prediction layers. Experiments on a new Swedish prescription data show the proposed architecture to be superior in two of the three information extraction tasks, comparing to baseline models. Finally, we evaluate two different embedding approaches between applying Multilingual BERT and translating the Swedish text to English then encode with a BERT model pretrained on clinical notes.","authors":["John Pougu\u00e9 Biyong","Bo Wang","Terry Lyons","Alejo Nevado-Holgado"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder","tldr":"Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.10","presentation_id":"38939813","rocketchat_channel":"paper-clinicalnlp-10","speakers":"John Pougu\u00e9 Biyong|Bo Wang|Terry Lyons|Alejo Nevado-Holgado","title":"Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder"},{"content":{"abstract":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinically incorrect radiology reports. In this work, we develop a radiology report generation model utilizing the transformer architecture that produces superior reports as measured by both standard language generation and clinical coherence metrics compared to competitive baselines. We then develop a method to differentiably extract clinical information from generated reports and utilize this differentiability to fine-tune our model to produce more clinically coherent reports.","authors":["Justin Lovelace","Bobak Mortazavi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.110","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to Generate Clinically Coherent Chest X-Ray Reports","tldr":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinica...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1041","presentation_id":"38940177","rocketchat_channel":"paper-clinicalnlp-1041","speakers":"Justin Lovelace|Bobak Mortazavi","title":"Learning to Generate Clinically Coherent Chest X-Ray Reports"},{"content":{"abstract":"Reading comprehension style question-answering (QA) based on patient-specific documents represents a growing area in clinical NLP with plentiful applications. Bidirectional Encoder Representations from Transformers (BERT) and its derivatives lead the state-of-the-art accuracy on the task, but most evaluation has treated the data as a pre-mixture without systematically looking into the potential effect of imperfect train/test questions. The current study seeks to address this gap by experimenting with full versus partial train/test data consisting of paraphrastic questions. Our key findings include 1) training with all pooled question variants yielded best accuracy, 2) the accuracy varied widely, from 0.74 to 0.80, when trained with each single question variant, and 3) questions of similar lexical/syntactic structure tended to induce identical answers. The results suggest that how you ask questions matters in BERT-based QA, especially at the training stage.","authors":["Sungrim (Riea) Moon","Jungwei Fan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How You Ask Matters: The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset","tldr":"Reading comprehension style question-answering (QA) based on patient-specific documents represents a growing area in clinical NLP with plentiful applications. Bidirectional Encoder Representations from Transformers (BERT) and its derivatives lead the...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.11","presentation_id":"38939814","rocketchat_channel":"paper-clinicalnlp-11","speakers":"Sungrim (Riea) Moon|Jungwei Fan","title":"How You Ask Matters: The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset"},{"content":{"abstract":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall \u2018EDSS\u2019 score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS-BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS-BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state-of-the-art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule-based approaches.","authors":["Alister D\u2019Costa","Stefan Denkovski","Michal Malyska","Sae Young Moon","Brandon Rufino","Zhen Yang","Taylor Killian","Marzyeh Ghassemi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multiple Sclerosis Severity Classification From Clinical Text","tldr":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.12","presentation_id":"38939815","rocketchat_channel":"paper-clinicalnlp-12","speakers":"Alister D\u2019Costa|Stefan Denkovski|Michal Malyska|Sae Young Moon|Brandon Rufino|Zhen Yang|Taylor Killian|Marzyeh Ghassemi","title":"Multiple Sclerosis Severity Classification From Clinical Text"},{"content":{"abstract":"Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists\u2019 reporting style varies from one to another as sentences are written in a telegraphic format and do not follow general English grammar rules. In this work, we present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1% accuracy.","authors":["Morteza Pourreza Shahri","Amir Tahmasebi","Bingyang Ye","Henghui Zhu","Javed Aslam","Timothy Ferris"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Ensemble Approach to Automatic Structuring of Radiology Reports","tldr":"Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, speci...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.13","presentation_id":"38939816","rocketchat_channel":"paper-clinicalnlp-13","speakers":"Morteza Pourreza Shahri|Amir Tahmasebi|Bingyang Ye|Henghui Zhu|Javed Aslam|Timothy Ferris","title":"An Ensemble Approach to Automatic Structuring of Radiology Reports"},{"content":{"abstract":"Stroke is one of the leading causes of death and disability worldwide. Stroke is treatable, but it is prone to disability after treatment and must be prevented. To grasp the degree of disability caused by stroke, we use magnetic resonance imaging text records to predict stroke and measure the performance according to the document-level and sentence-level representation. As a result of the experiment, the document-level representation shows better performance.","authors":["Tak-Sung Heo","Chulho Kim","Jeong-Myeong Choi","Yeong-Seok Jeong","Yu-Seop Kim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records","tldr":"Stroke is one of the leading causes of death and disability worldwide. Stroke is treatable, but it is prone to disability after treatment and must be prevented. To grasp the degree of disability caused by stroke, we use magnetic resonance imaging tex...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.15","presentation_id":"38939817","rocketchat_channel":"paper-clinicalnlp-15","speakers":"Tak-Sung Heo|Chulho Kim|Jeong-Myeong Choi|Yeong-Seok Jeong|Yu-Seop Kim","title":"Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records"},{"content":{"abstract":"Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches that aim to accurately anchor temporal information on a timeline. Relative and incomplete time expressions (RI-Timexes) are expressions that require additional information for their temporal anchor to be resolved, but few studies have addressed this challenge specifically. In this study, we aimed to reproduce and verify a classification approach for identifying anchor dates and relations in clinical text, and propose a novel relation classification approach for this task.","authors":["Louise Dupuis","Nicol Bergou","Hegler Tissot","Sumithra Velupillai"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relative and Incomplete Time Expression Anchoring for Clinical Text","tldr":"Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches tha...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.16","presentation_id":"38939818","rocketchat_channel":"paper-clinicalnlp-16","speakers":"Louise Dupuis|Nicol Bergou|Hegler Tissot|Sumithra Velupillai","title":"Relative and Incomplete Time Expression Anchoring for Clinical Text"},{"content":{"abstract":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.","authors":["Jianmo Ni","Chun-Nan Hsu","Amilcare Gentili","Julian McAuley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.176","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays","tldr":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such model...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1640","presentation_id":"38940178","rocketchat_channel":"paper-clinicalnlp-1640","speakers":"Jianmo Ni|Chun-Nan Hsu|Amilcare Gentili|Julian McAuley","title":"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays"},{"content":{"abstract":"One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks.","authors":["Zhi Wen","Xing Han Lu","Siva Reddy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining","tldr":"One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designe...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.17","presentation_id":"38939819","rocketchat_channel":"paper-clinicalnlp-17","speakers":"Zhi Wen|Xing Han Lu|Siva Reddy","title":"MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining"},{"content":{"abstract":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical notes only provide additional predictive power over structured information in readmission prediction. We further propose a probing framework to select parts of notes that enable more accurate predictions than using all notes, despite that the selected information leads to a distribution shift from the training data (\u201call notes\u201d). Finally, we demonstrate that models trained on the selected valuable information achieve even better predictive performance, with only 6.8%of all the tokens for readmission prediction.","authors":["Chao-Chun Hsu","Shantanu Karnwal","Sendhil Mullainathan","Ziad Obermeyer","Chenhao Tan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.187","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Characterizing the Value of Information in Medical Notes","tldr":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmis...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1713","presentation_id":"38940179","rocketchat_channel":"paper-clinicalnlp-1713","speakers":"Chao-Chun Hsu|Shantanu Karnwal|Sendhil Mullainathan|Ziad Obermeyer|Chenhao Tan","title":"Characterizing the Value of Information in Medical Notes"},{"content":{"abstract":"We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extraction (Track 2) n2c2 data-set. We identify best practices for transfer learning, such as language-model fine-tuning and scalar mix. Our transfer learning models achieve strong performance in the overall task (F1=92.91%) as well as in ADE identification (F1=53.08%). Flair-based embeddings out-perform in the identification of context-dependent entities such as ADE. BERT-based embeddings out-perform in recognizing clinical terminology such as Drug and Form entities. ELMo-based embeddings deliver competitive performance in all entities. We develop a sentence-augmentation method for enhanced ADE identification benefiting BERT-based and ELMo-based models by up to 3.13% in F1 gains. Finally, we show that a simple ensemble of these models out-paces most current methods in ADE extraction (F1=55.77%).","authors":["Sankaran Narayanan","Kaivalya Mannam","Sreeranga P Rajan","P Venkat Rangan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Transfer Learning for Adverse Drug Event (ADE) and Medication Entity Extraction","tldr":"We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extracti...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.18","presentation_id":"38939820","rocketchat_channel":"paper-clinicalnlp-18","speakers":"Sankaran Narayanan|Kaivalya Mannam|Sreeranga P Rajan|P Venkat Rangan","title":"Evaluation of Transfer Learning for Adverse Drug Event (ADE) and Medication Entity Extraction"},{"content":{"abstract":"In this work, we propose a novel goal-oriented dialog task, automatic symptom detection. We build a system that can interact with patients through dialog to detect and collect clinical symptoms automatically, which can save a doctor\u2019s time interviewing the patient. Given a set of explicit symptoms provided by the patient to initiate a dialog for diagnosing, the system is trained to collect implicit symptoms by asking questions, in order to collect more information for making an accurate diagnosis. After getting the reply from the patient for each question, the system also decides whether current information is enough for a human doctor to make a diagnosis. To achieve this goal, we propose two neural models and a training pipeline for the multi-step reasoning task. We also build a knowledge graph as additional inputs to further improve model performance. Experiments show that our model significantly outperforms the baseline by 4%, discovering 67% of implicit symptoms on average with a limited number of questions.","authors":["Hongyin Luo","Shang-Wen Li","James Glass"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowledge Grounded Conversational Symptom Detection with Graph Memory Networks","tldr":"In this work, we propose a novel goal-oriented dialog task, automatic symptom detection. We build a system that can interact with patients through dialog to detect and collect clinical symptoms automatically, which can save a doctor\u2019s time interviewi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.19","presentation_id":"38939821","rocketchat_channel":"paper-clinicalnlp-19","speakers":"Hongyin Luo|Shang-Wen Li|James Glass","title":"Knowledge Grounded Conversational Symptom Detection with Graph Memory Networks"},{"content":{"abstract":"A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications.","authors":["Patrick Lewis","Myle Ott","Jingfei Du","Veselin Stoyanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art","tldr":"A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial t...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.20","presentation_id":"38939822","rocketchat_channel":"paper-clinicalnlp-20","speakers":"Patrick Lewis|Myle Ott|Jingfei Du|Veselin Stoyanov","title":"Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"},{"content":{"abstract":"Reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of risk factors that have been hypothesized to be important for understanding re-admission risk, including such factors as problems with substance abuse, ability to maintain work, relations with family. In this work, we develop Roberta-based models to predict the sentiment of sentences describing readmission risk factors in discharge summaries of patients with psychosis. We improve substantially on previous results by a scheme that shares information across risk factors while also allowing the model to learn risk factor-specific information.","authors":["Xiyu Ding","Mei-Hua Hall","Timothy Miller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries","tldr":"Reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of risk factors that have been hypothesized to be important for understanding re-admission risk, ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.21","presentation_id":"38939823","rocketchat_channel":"paper-clinicalnlp-21","speakers":"Xiyu Ding|Mei-Hua Hall|Timothy Miller","title":"Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries"},{"content":{"abstract":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce additional errors that can lead to potentially severe health outcomes. We propose a novel machine translation-based approach, PharmMT, to automatically and reliably simplify prescription directions into patient-friendly language, thereby significantly reducing pharmacist workload. We evaluate the proposed approach over a dataset consisting of over 530K prescriptions obtained from a large mail-order pharmacy. The end-to-end system achieves a BLEU score of 60.27 against the reference directions generated by pharmacists, a 39.6% relative improvement over the rule-based normalization. Pharmacists judged 94.3% of the simplified directions as usable as-is or with minimal changes. This work demonstrates the feasibility of a machine translation-based tool for simplifying prescription directions in real-life.","authors":["Jiazhao Li","Corey Lester","Xinyan Zhao","Yuting Ding","Yun Jiang","V.G.Vinod Vydiswaran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.251","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions","tldr":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2127","presentation_id":"38940180","rocketchat_channel":"paper-clinicalnlp-2127","speakers":"Jiazhao Li|Corey Lester|Xinyan Zhao|Yuting Ding|Yun Jiang|V.G.Vinod Vydiswaran","title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions"},{"content":{"abstract":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT\u2019s F1 score was lower by 4 points on average than medical BERT variants.","authors":["Macarious Abadeer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts","tldr":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.23","presentation_id":"38939824","rocketchat_channel":"paper-clinicalnlp-23","speakers":"Macarious Abadeer","title":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts"},{"content":{"abstract":"","authors":["Zixu Wang","Julia Ive","Sinead Moylett","Christoph Mueller","Rudolf Cardinal","Sumithra Velupillai","John O'Brien","Robert Stewart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer's Disease (AD) using Mental Health Records: a Classification Approach","tldr":null,"track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.25","presentation_id":"38939825","rocketchat_channel":"paper-clinicalnlp-25","speakers":"Zixu Wang|Julia Ive|Sinead Moylett|Christoph Mueller|Rudolf Cardinal|Sumithra Velupillai|John O'Brien|Robert Stewart","title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer's Disease (AD) using Mental Health Records: a Classification Approach"},{"content":{"abstract":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited efforts leveraging inter-dependencies among the two granularities. We instead propose a multi-grained joint deep network to concurrently learn the ADE entity recognition and ADE sentence classification tasks. Our joint approach takes advantage of their symbiotic relationship, with a transfer of knowledge between the two levels of granularity. Our dual-attention mechanism constructs multiple distinct representations of a sentence that capture both task-specific and semantic information in the sentence, providing stronger emphasis on the key elements essential for sentence classification. Our model improves state-of- art F1-score for both tasks: (i) entity recognition of ADE words (12.5% increase) and (ii) ADE sentence classification (13.6% increase) on MADE 1.0 benchmark of EHR notes.","authors":["Susmitha Wunnava","Xiao Qin","Tabassum Kakar","Xiangnan Kong","Elke Rundensteiner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.306","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events","tldr":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2509","presentation_id":"38940181","rocketchat_channel":"paper-clinicalnlp-2509","speakers":"Susmitha Wunnava|Xiao Qin|Tabassum Kakar|Xiangnan Kong|Elke Rundensteiner","title":"A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events"},{"content":{"abstract":"Automated Medication Regimen (MR) extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spans for frequency, route and change, corresponding to medications discussed in the conversation. We first describe a unique dataset of annotated doctor-patient conversations and then present a weakly supervised model architecture that can perform span extraction using noisy classification data. The model utilizes an attention bottleneck inside a classification model to perform the extraction. We experiment with several variants of attention scoring and projection functions and propose a novel transformer-based attention scoring function (TAScore). The proposed combination of TAScore and Fusedmax projection achieves a 10 point increase in Longest Common Substring F1 compared to the baseline of additive scoring plus softmax projection.","authors":["Dhruvesh Patel","Sandeep Konam","Sai Prabhakar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Weakly Supervised Medication Regimen Extraction from Medical Conversations","tldr":"Automated Medication Regimen (MR) extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spa...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.26","presentation_id":"38939826","rocketchat_channel":"paper-clinicalnlp-26","speakers":"Dhruvesh Patel|Sandeep Konam|Sai Prabhakar","title":"Weakly Supervised Medication Regimen Extraction from Medical Conversations"},{"content":{"abstract":"We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardized fashions using jargon not found in other medical specialties, complicating the already difficult task of manual data extraction. We examine the performance of several state-of-the-art neural methods for relation extraction of radiotherapy treatment details, with a goal of automating detailed information extraction. The neural systems perform at 0.82-0.88 macro-average F1, which approximates or in some cases exceeds the inter-annotator agreement. To the best of our knowledge, this is the first effort to develop models for radiotherapy relation extraction and one of the few efforts for relation extraction to describe cancer treatment in general.","authors":["Danielle Bitterman","Timothy Miller","David Harris","Chen Lin","Sean Finan","Jeremy Warner","Raymond Mak","Guergana Savova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extracting Relations between Radiotherapy Treatment Details","tldr":"We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardiz...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.27","presentation_id":"38939827","rocketchat_channel":"paper-clinicalnlp-27","speakers":"Danielle Bitterman|Timothy Miller|David Harris|Chen Lin|Sean Finan|Jeremy Warner|Raymond Mak|Guergana Savova","title":"Extracting Relations between Radiotherapy Treatment Details"},{"content":{"abstract":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient\u2019s medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","authors":["Anirudh Joshi","Namit Katariya","Xavier Amatriain","Anitha Kannan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.335","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures.","tldr":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2801","presentation_id":"38940182","rocketchat_channel":"paper-clinicalnlp-2801","speakers":"Anirudh Joshi|Namit Katariya|Xavier Amatriain|Anitha Kannan","title":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures."},{"content":{"abstract":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either medical domain-specific knowledge, or patients\u2019 prior diagnoses and clinical encounters. In this paper, we propose a novel model for automated clinical assessment generation (MCAG). MCAG is built on an innovative graph neural network, where rich clinical knowledge is incorporated into an end-to-end corpus-learning system. Our evaluation results against physician generated gold standard show that MCAG significantly improves the BLEU and rouge score compared with competitive baseline models. Further, physicians\u2019 evaluation showed that MCAG could generate high-quality assessments.","authors":["Zhichao Yang","Hong Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.336","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Accurate Electronic Health Assessment from Medical Graph","tldr":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic syste...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2804","presentation_id":"38940183","rocketchat_channel":"paper-clinicalnlp-2804","speakers":"Zhichao Yang|Hong Yu","title":"Generating Accurate Electronic Health Assessment from Medical Graph"},{"content":{"abstract":"In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 attributes, and 284 pairs of relations with clinical relevance. A trained medical doctor annotated these referrals, and then together with other three researchers, consolidated each of the annotations. The annotated corpus has nested entities, with 32.2% of entities embedded in other entities. We use this annotated corpus to obtain preliminary results for Named Entity Recognition (NER). The best results were achieved by using a biLSTM-CRF architecture using word embeddings trained over Spanish Wikipedia together with clinical embeddings computed by the group. NER models applied to this corpus can leverage statistics of diseases and pending procedures within this waiting list. This work constitutes the first annotated corpus using clinical narratives from Chile, and one of the few for the Spanish language. The annotated corpus, the clinical word embeddings, and the annotation guidelines are freely released to the research community.","authors":["Pablo B\u00e1ez","Fabi\u00e1n Villena","Mat\u00edas Rojas","Manuel Dur\u00e1n","Jocelyn Dunstan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Chilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in Spanish","tldr":"In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 at...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.29","presentation_id":"38939828","rocketchat_channel":"paper-clinicalnlp-29","speakers":"Pablo B\u00e1ez|Fabi\u00e1n Villena|Mat\u00edas Rojas|Manuel Dur\u00e1n|Jocelyn Dunstan","title":"The Chilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in Spanish"},{"content":{"abstract":"Loss of consciousness, so-called syncope, is a commonly occurring symptom associated with worse prognosis for a number of heart-related diseases. We present a comparison of methods for a diagnosis classification task in Norwegian clinical notes, targeting syncope, i.e. fainting cases. We find that an often neglected baseline with keyword matching constitutes a rather strong basis, but more advanced methods do offer some improvement in classification performance, especially a convolutional neural network model. The developed pipeline is planned to be used for quantifying unregistered syncope cases in Norway.","authors":["Ildiko Pilan","P\u00e5l H. Brekke","Fredrik A. Dahl","Tore Gundersen","Haldor Husby","\u00d8ystein Nytr\u00f8","Lilja \u00d8vrelid"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Classification of Syncope Cases in Norwegian Medical Records","tldr":"Loss of consciousness, so-called syncope, is a commonly occurring symptom associated with worse prognosis for a number of heart-related diseases. We present a comparison of methods for a diagnosis classification task in Norwegian clinical notes, targ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.3","presentation_id":"38939808","rocketchat_channel":"paper-clinicalnlp-3","speakers":"Ildiko Pilan|P\u00e5l H. Brekke|Fredrik A. Dahl|Tore Gundersen|Haldor Husby|\u00d8ystein Nytr\u00f8|Lilja \u00d8vrelid","title":"Classification of Syncope Cases in Norwegian Medical Records"},{"content":{"abstract":"With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for Portuguese, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in Brazilian Portuguese. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the baseline model in F1-score by 2.72%, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new model.","authors":["Elisa Terumi Rubel Schneider","Jo\u00e3o Vitor Andrioli de Souza","Julien Knafou","Lucas Emanuel Silva e Oliveira","Jenny Copara","Yohan Bonescki Gumiel","Lucas Ferro Antunes de Oliveira","Emerson Cabrera Paraiso","Douglas Teodoro","Cl\u00e1udia Maria Cabral Moro Barra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition","tldr":"With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity reco...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.30","presentation_id":"38939829","rocketchat_channel":"paper-clinicalnlp-30","speakers":"Elisa Terumi Rubel Schneider|Jo\u00e3o Vitor Andrioli de Souza|Julien Knafou|Lucas Emanuel Silva e Oliveira|Jenny Copara|Yohan Bonescki Gumiel|Lucas Ferro Antunes de Oliveira|Emerson Cabrera Paraiso|Douglas Teodoro|Cl\u00e1udia Maria Cabral Moro Barra","title":"BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition"},{"content":{"abstract":"A cancer registry is a critical and massive database for which various types of domain knowledge are needed and whose maintenance requires labor-intensive data curation. In order to facilitate the curation process for building a high-quality and integrated cancer registry database, we compiled a cross-hospital corpus and applied neural network methods to develop a natural language processing system for extracting cancer registry variables buried in unstructured pathology reports. The performance of the developed networks was compared with various baselines using standard micro-precision, recall and F-measure. Furthermore, we conducted experiments to study the feasibility of applying transfer learning to rapidly develop a well-performing system for processing reports from different sources that might be presented in different writing styles and formats. The results demonstrate that the transfer learning method enables us to develop a satisfactory system for a new hospital with only a few annotations and suggest more opportunities to reduce the burden of cancer registry curation.","authors":["Yan-Jie Lin","Hong-Jie Dai","You-Chen Zhang","Chung-Yang Wu","Yu-Cheng Chang","Pin-Jou Lu","Chih-Jen Huang","Yu-Tsang Wang","Hui-Min Hsieh","Kun-San Chao","Tsang-Wu Liu","I-Shou Chang","Yi-Hsin Connie Yang","Ti-Hao Wang","Ko-Jiunn Liu","Li-Tzong Chen","Sheau-Fang Yang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cancer Registry Information Extraction via Transfer Learning","tldr":"A cancer registry is a critical and massive database for which various types of domain knowledge are needed and whose maintenance requires labor-intensive data curation. In order to facilitate the curation process for building a high-quality and inte...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.31","presentation_id":"38939830","rocketchat_channel":"paper-clinicalnlp-31","speakers":"Yan-Jie Lin|Hong-Jie Dai|You-Chen Zhang|Chung-Yang Wu|Yu-Cheng Chang|Pin-Jou Lu|Chih-Jen Huang|Yu-Tsang Wang|Hui-Min Hsieh|Kun-San Chao|Tsang-Wu Liu|I-Shou Chang|Yi-Hsin Connie Yang|Ti-Hao Wang|Ko-Jiunn Liu|Li-Tzong Chen|Sheau-Fang Yang","title":"Cancer Registry Information Extraction via Transfer Learning"},{"content":{"abstract":"Recent studies have shown that adversarial examples can be generated by applying small perturbations to the inputs such that the well- trained deep learning models will misclassify. With the increasing number of safety and security-sensitive applications of deep learn- ing models, the robustness of deep learning models has become a crucial topic. The robustness of deep learning models for health- care applications is especially critical because the unique characteristics and the high financial interests of the medical domain make it more sensitive to adversarial attacks. Among the modalities of medical data, the clinical summaries have higher risks to be attacked because they are generated by third-party companies. As few works studied adversarial threats on clinical summaries, in this work we first apply adversarial attack to clinical summaries of electronic health records (EHR) to show the text-based deep learning systems are vulnerable to adversarial examples. Secondly, benefiting from the multi-modality of the EHR dataset, we propose a novel defense method, MATCH (Multimodal feATure Consistency cHeck), which leverages the consistency between multiple modalities in the data to defend against adversarial examples on a single modality. Our experiments demonstrate the effectiveness of MATCH on a hospital readmission prediction task comparing with baseline methods.","authors":["Wenjie Wang","Youngja Park","Taesung Lee","Ian Molloy","Pengfei Tang","Li Xiong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Utilizing Multimodal Feature Consistency to Detect Adversarial Examples on Clinical Summaries","tldr":"Recent studies have shown that adversarial examples can be generated by applying small perturbations to the inputs such that the well- trained deep learning models will misclassify. With the increasing number of safety and security-sensitive applicat...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.33","presentation_id":"38939831","rocketchat_channel":"paper-clinicalnlp-33","speakers":"Wenjie Wang|Youngja Park|Taesung Lee|Ian Molloy|Pengfei Tang|Li Xiong","title":"Utilizing Multimodal Feature Consistency to Detect Adversarial Examples on Clinical Summaries"},{"content":{"abstract":"De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 de-identification challenge datasets show that PHICON can help three selected de-identification models boost F1-score (by at most 8.6%) on cross-dataset test setting. We also discuss how much augmentation to use and how each augmentation method influences the performance.","authors":["Xiang Yue","Shuang Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation","tldr":"De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICO...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.37","presentation_id":"38939832","rocketchat_channel":"paper-clinicalnlp-37","speakers":"Xiang Yue|Shuang Zhou","title":"PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation"},{"content":{"abstract":"In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via \u201cexpert-review\u201d, where clinicians can have a dialogue with a domain expert (reviewers) and ask them questions about data entry rules. Automatically identifying \u201creal questions\u201d in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting. In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect an answer (information or help) about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence (e.g., can you clarify this?), which we will refer as \u201cc-questions\u201d. We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.","authors":["George Michalopoulos","Helen Chen","Alexander Wong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Where\u2019s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data","tldr":"In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via \u201cexpert-review...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.38","presentation_id":"38939833","rocketchat_channel":"paper-clinicalnlp-38","speakers":"George Michalopoulos|Helen Chen|Alexander Wong","title":"Where\u2019s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data"},{"content":{"abstract":"We address the problem of model generalization for sequence to sequence (seq2seq) architectures. We propose going beyond data augmentation via paraphrase-optimized multi-task learning and observe that it is useful in correctly handling unseen sentential paraphrases as inputs. Our models greatly outperform SOTA seq2seq models for semantic parsing on diverse domains (Overnight - up to 3.2% and emrQA - 7%) and Nematus, the winning solution for WMT 2017, for Czech to English translation (CzENG 1.6 - 1.5 BLEU).","authors":["So Yeon Min","Preethi Raghavan","Peter Szolovits"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Advancing Seq2seq with Joint Paraphrase Learning","tldr":"We address the problem of model generalization for sequence to sequence (seq2seq) architectures. We propose going beyond data augmentation via paraphrase-optimized multi-task learning and observe that it is useful in correctly handling unseen sentent...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.39","presentation_id":"38939834","rocketchat_channel":"paper-clinicalnlp-39","speakers":"So Yeon Min|Preethi Raghavan|Peter Szolovits","title":"Advancing Seq2seq with Joint Paraphrase Learning"},{"content":{"abstract":"In this paper, we evaluate several machine learning methods for multi-label classification of text questions. Every nursing student in the United States must pass the National Council Licensure Examination (NCLEX) to begin professional practice. NCLEX defines a number of competencies on which students are evaluated. By labeling test questions with NCLEX competencies, we can score students according to their performance in each competency. This information helps instructors measure how prepared students are for the NCLEX, as well as which competencies they may need help with. A key challenge is that questions may be related to more than one competency. Labeling questions with NCLEX competencies, therefore, equates to a multi-label, text classification problem where each competency is a label. Here we present an evaluation of several methods to support this use case along with a proposed approach. While our work is grounded in the nursing education domain, the methods described here can be used for any multi-label, text classification use case.","authors":["John Langton","Krishna Srihasam","Junlin Jiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions","tldr":"In this paper, we evaluate several machine learning methods for multi-label classification of text questions. Every nursing student in the United States must pass the National Council Licensure Examination (NCLEX) to begin professional practice. NCLE...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.4","presentation_id":"38939809","rocketchat_channel":"paper-clinicalnlp-4","speakers":"John Langton|Krishna Srihasam|Junlin Jiang","title":"Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions"},{"content":{"abstract":"Domain pretraining followed by task fine-tuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise domain unlabelled data by assigning pseudo labels from a general model. We evaluate the approach on two clinical STS datasets, and achieve r= 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r= 0.90, a new SOTA.","authors":["Yuxia Wang","Karin Verspoor","Timothy Baldwin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity","tldr":"Domain pretraining followed by task fine-tuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise domain unlabelled data by assigning pseudo labels from ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.40","presentation_id":"38939835","rocketchat_channel":"paper-clinicalnlp-40","speakers":"Yuxia Wang|Karin Verspoor|Timothy Baldwin","title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity"},{"content":{"abstract":"ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient\u2019s visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in free text medical notes.In this paper, we propose a machine learningmodel, BERT-XML, for large scale automatedICD coding of EHR notes, utilizing recentlydeveloped unsupervised pretraining that haveachieved state of the art performance on a va-riety of NLP tasks. We train a BERT modelfrom scratch on EHR notes, learning with vo-cabulary better suited for EHR tasks and thusoutperform off-the-shelf models. We furtheradapt the BERT architecture for ICD codingwith multi-label attention. We demonstratethe effectiveness of BERT-based models on thelarge scale ICD code classification task usingmillions of EHR notes to predict thousands ofunique codes.","authors":["Zachariah Zhang","Jingshu Liu","Narges Razavian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining","tldr":"ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient\u2019s visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.43","presentation_id":"38939836","rocketchat_channel":"paper-clinicalnlp-43","speakers":"Zachariah Zhang|Jingshu Liu|Narges Razavian","title":"BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining"},{"content":{"abstract":"In drug development, protocols define how clinical trials are conducted, and are therefore of paramount importance. They contain key patient-, investigator-, medication-, and study-related information, often elaborated in different sections in the protocol texts. Granular-level parsing on large quantity of existing protocols can accelerate clinical trial design and provide actionable insights into trial optimization. Here, we report our progresses in using deep learning NLP algorithms to enable automated protocol analytics. In particular, we combined a pre-trained BERT transformer model with joint-learning strategies to simultaneously identify clinically relevant entities (i.e. Named Entity Recognition) and extract the syntactic relations between these entities (i.e. Relation Extraction) from the eligibility criteria section in protocol texts. When comparing to standalone NER and RE models, our joint-learning strategy can effectively improve the performance of RE task while retaining similarly high NER performance, likely due to the synergy of optimizing toward both tasks\u2019 objectives via shared parameters. The derived NLP model provides an end-to-end solution to convert unstructured protocol texts into structured data source, which will be embedded into a comprehensive clinical analytics workflow for downstream trial design missions such like patient population extraction, patient enrollment rate estimation, and protocol amendment prediction.","authors":["Miao Chen","Ganhui Lan","Fang Du","Victor Lobanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics","tldr":"In drug development, protocols define how clinical trials are conducted, and are therefore of paramount importance. They contain key patient-, investigator-, medication-, and study-related information, often elaborated in different sections in the pr...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.44","presentation_id":"38939837","rocketchat_channel":"paper-clinicalnlp-44","speakers":"Miao Chen|Ganhui Lan|Fang Du|Victor Lobanov","title":"Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics"},{"content":{"abstract":"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.","authors":["John Chen","Ian Berlot-Attwell","Xindi Wang","Safwan Hossain","Frank Rudzicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Text Specific vs Blackbox Fairness Algorithms in Multimodal Clinical NLP","tldr":"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for t...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.48","presentation_id":"38939838","rocketchat_channel":"paper-clinicalnlp-48","speakers":"John Chen|Ian Berlot-Attwell|Xindi Wang|Safwan Hossain|Frank Rudzicz","title":"Analyzing Text Specific vs Blackbox Fairness Algorithms in Multimodal Clinical NLP"},{"content":{"abstract":"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consuming, automatic structuring of the eligibility criteria into various semantic categories or aspects is the need of the hour. Existing methods use hand-crafted rules and feature-based statistical machine learning methods to dynamically induce semantic aspects. However, in order to deal with paucity of aspect-annotated clinical trials data, we propose a novel weakly-supervised co-training based method which can exploit a large pool of unlabeled criteria sentences to augment the limited supervised training data, and consequently enhance the performance. Experiments with 0.2M criteria sentences show that the proposed approach outperforms the competitive supervised baselines by 12% in terms of micro-averaged F1 score for all the aspects. Probing deeper into analysis, we observe domain-specific information boosts up the performance by a significant margin.","authors":["Tirthankar Dasgupta","Ishani Mondal","Abir Naskar","Lipika Dey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria","tldr":"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consumi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.49","presentation_id":"38939839","rocketchat_channel":"paper-clinicalnlp-49","speakers":"Tirthankar Dasgupta|Ishani Mondal|Abir Naskar|Lipika Dey","title":"Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria"},{"content":{"abstract":"Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the sequence of the notes. We evaluated our models on prolonged mechanical ventilation prediction problem and our experiments demonstrated that Clinical XLNet outperforms the best baselines consistently. The models and scripts are made publicly available.","authors":["Kexin Huang","Abhishek Singh","Sitong Chen","Edward Moseley","Chih-Ying Deng","Naomi George","Charolotta Lindvall"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation","tldr":"Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the s...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.6","presentation_id":"38939810","rocketchat_channel":"paper-clinicalnlp-6","speakers":"Kexin Huang|Abhishek Singh|Sitong Chen|Edward Moseley|Chih-Ying Deng|Naomi George|Charolotta Lindvall","title":"Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation"},{"content":{"abstract":"Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdominal lymph nodes with a hierarchical relationship. We then introduce an end-to-end approach based on the combination of rules and transformer-based methods to detect these abdominal lymph node mentions and classify their types from the MRI radiology reports. We demonstrate the superior performance of a model fine-tuned on MRI reports using BlueBERT, called MriBERT. We find that MriBERT outperforms the rule-based labeler (0.957 vs 0.644 in micro weighted F1-score) as well as other BERT-based variations (0.913 - 0.928). We make the code and MriBERT publicly available at https://github.com/ncbi-nlp/bluebert, with the hope that this method can facilitate the development of medical report annotators to produce labels from scratch at scale.","authors":["Yifan Peng","Sungwon Lee","Daniel C. Elton","Thomas Shen","Yu-xing Tang","Qingyu Chen","Shuai Wang","Yingying Zhu","Ronald Summers","Zhiyong Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatic recognition of abdominal lymph nodes from clinical text","tldr":"Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdomi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.7","presentation_id":"38939811","rocketchat_channel":"paper-clinicalnlp-7","speakers":"Yifan Peng|Sungwon Lee|Daniel C. Elton|Thomas Shen|Yu-xing Tang|Qingyu Chen|Shuai Wang|Yingying Zhu|Ronald Summers|Zhiyong Lu","title":"Automatic recognition of abdominal lymph nodes from clinical text"},{"content":{"abstract":"Ample evidence suggests that better machine learning models may be steadily obtained by training on increasingly larger datasets on natural language processing (NLP) problems from non-medical domains. Whether the same holds true for medical NLP has by far not been thoroughly investigated. This work shows that this is indeed not always the case. We reveal the somehow counter-intuitive observation that performant medical NLP models may be obtained with small amount of labeled data, quite the opposite to the common belief, most likely due to the domain specificity of the problem. We show quantitatively the effect of training data size on a fixed test set composed of two of the largest public chest x-ray radiology report datasets on the task of abnormality classification. The trained models not only make use of the training data efficiently, but also outperform the current state-of-the-art rule-based systems by a significant margin.","authors":["Jean-Baptiste Lamare","Oloruntobiloba Olatunji","Li Yao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the diminishing return of labeling clinical reports","tldr":"Ample evidence suggests that better machine learning models may be steadily obtained by training on increasingly larger datasets on natural language processing (NLP) problems from non-medical domains. Whether the same holds true for medical NLP has b...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.8","presentation_id":"38939812","rocketchat_channel":"paper-clinicalnlp-8","speakers":"Jean-Baptiste Lamare|Oloruntobiloba Olatunji|Li Yao","title":"On the diminishing return of labeling clinical reports"},{"content":{"abstract":"While Dementia with Lewy Bodies (DLB) is the second most common type of neurodegenerative dementia following Alzheimer\u2019s Disease (AD), it is difficult to distinguish from AD. We propose a method for DLB detection by using mental health record (MHR) documents from a (3-month) period before a patient has been diagnosed with DLB or AD. Our objective is to develop a model that could be clinically useful to differentiate between DLB and AD across datasets from different healthcare institutions. We cast this as a classification task using Convolutional Neural Network (CNN), an efficient neural model for text classification. We experiment with different representation models, and explore the features that contribute to model performances. In addition, we apply temperature scaling, a simple but efficient model calibration method, to produce more reliable predictions. We believe the proposed method has important potential for clinical applications using routine healthcare records, and for generalising to other relevant clinical record datasets. To the best of our knowledge, this is the first attempt to distinguish DLB from AD using mental health records, and to improve the reliability of DLB predictions.","authors":["Zixu Wang","Julia Ive","Sinead Moylett","Christoph Mueller","Rudolf Cardinal","Sumithra Velupillai","John O\u2019Brien","Robert Stewart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer\u2019s Disease (AD) using Mental Health Records: a Classification Approach","tldr":"While Dementia with Lewy Bodies (DLB) is the second most common type of neurodegenerative dementia following Alzheimer\u2019s Disease (AD), it is difficult to distinguish from AD. We propose a method for DLB detection by using mental health record (MHR) d...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2020.clinicalnlp-1.19","presentation_id":"","rocketchat_channel":"paper-clinicalnlp-19","speakers":"Zixu Wang|Julia Ive|Sinead Moylett|Christoph Mueller|Rudolf Cardinal|Sumithra Velupillai|John O\u2019Brien|Robert Stewart","title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer\u2019s Disease (AD) using Mental Health Records: a Classification Approach"},{"content":{"abstract":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common misuse of Chinese idioms leads to error in corpus and causes error in the learned semantic representation of Chinese idioms. In this paper, we introduce the definition written by Chinese experts to correct the misuse. We propose a model for the Chinese idiom cloze test integrating various information effectively. We propose an attention mechanism called Attribute Attention to balance the weight of different attributes among different descriptions of the Chinese idiom. Besides the given candidates of every blank, we also try to choose the answer from all Chinese idioms that appear in the dataset as the extra loss due to the uniqueness and specificity of Chinese idioms. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Hongsheng Zhao","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test","tldr":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common mi...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1","presentation_id":"38939724","rocketchat_channel":"paper-deelio-1","speakers":"Xinyu Wang|Hongsheng Zhao|Tan Yang|Hongbo Wang","title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test"},{"content":{"abstract":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.","authors":["Guanglin Niu","Bo Li","Yongfei Zhang","Shiliang Pu","Jingyang Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.105","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding","tldr":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1008","presentation_id":"38940167","rocketchat_channel":"paper-deelio-1008","speakers":"Guanglin Niu|Bo Li|Yongfei Zhang|Shiliang Pu|Jingyang Li","title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding"},{"content":{"abstract":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available.","authors":["Hoang Nguyen","Chenwei Zhang","Congying Xia","Philip Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.108","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection","tldr":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1039","presentation_id":"38940168","rocketchat_channel":"paper-deelio-1039","speakers":"Hoang Nguyen|Chenwei Zhang|Congying Xia|Philip Yu","title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection"},{"content":{"abstract":"","authors":["Kung-Hsiang Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs","tldr":null,"track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1059","presentation_id":"38940169","rocketchat_channel":"paper-deelio-1059","speakers":"Kung-Hsiang Huang","title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs"},{"content":{"abstract":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attributes, even for common entities. To improve the precision of model-based entity-attribute extraction, we propose attribute-aware embeddings, which embeds entities and attributes in the same space by the similarity of their attributes. Our model, EANET, learns these embeddings by representing entities as a weighted sum of their attributes and concatenates these embeddings to mention level features. EANET achieves up to 91% classification accuracy, outperforming strong baselines and achieves 83% precision on manually labeled high confidence extractions, outperforming Biperpedia (Gupta et al., 2014), a previous state-of-the-art for large scale entity-attribute extraction.","authors":["Dan Iter","Xiao Yu","Fangtao Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings","tldr":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attrib...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.12","presentation_id":"38939729","rocketchat_channel":"paper-deelio-12","speakers":"Dan Iter|Xiao Yu|Fangtao Li","title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings"},{"content":{"abstract":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task\u2019s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.","authors":["Xin Guo","Yu Tian","Qinghan Xue","Panos Lampropoulos","Steven Eliuk","Kenneth Barner","Xiaolong Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.164","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Learning Long Short Term Memory","tldr":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell i...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1524","presentation_id":"38940170","rocketchat_channel":"paper-deelio-1524","speakers":"Xin Guo|Yu Tian|Qinghan Xue|Panos Lampropoulos|Steven Eliuk|Kenneth Barner|Xiaolong Wang","title":"Continual Learning Long Short Term Memory"},{"content":{"abstract":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we present OSCR (Ontology-based Semantic Composition Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pre-training. We evaluated the performance of BERT pre-trained on Wikipedia with and without OSCR by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33.3%, 18.6%, and 4% improved accuracy compared to pre-training BERT without OSCR.","authors":["Travis Goodwin","Dina Demner-Fushman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization","tldr":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we prese...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.16","presentation_id":"38939730","rocketchat_channel":"paper-deelio-16","speakers":"Travis Goodwin|Dina Demner-Fushman","title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization"},{"content":{"abstract":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) is learning target concept vector representations from scratch which requires more number of training instances. Our model is based on RoBERTa and target concept embeddings. In our model, we integrate a) target concept information in the form of target concept vectors generated by encoding target concept descriptions using SRoBERTa, state-of-the-art RoBERTa based sentence embedding model and b) domain lexicon knowledge by enriching target concept vectors with synonym relationship knowledge using retrofitting algorithm. It is the first attempt in MCN to exploit both target concept information as well as domain lexicon knowledge in the form of retrofitted target concept vectors. Our model outperforms all the existing models with an accuracy improvement up to 1.36% on three standard datasets. Further, our model when trained only on mapping lexicon synonyms achieves up to 4.87% improvement in accuracy.","authors":["Katikapalli Subramanyam Kalyan","Sivanesan Sangeetha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts","tldr":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.17","presentation_id":"38939731","rocketchat_channel":"paper-deelio-17","speakers":"Katikapalli Subramanyam Kalyan|Sivanesan Sangeetha","title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts"},{"content":{"abstract":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes.","authors":["Ting-Yun Chang","Yang Liu","Karthik Gopalakrishnan","Behnam Hedayatnia","Pei Zhou","Dilek Hakkani-Tur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks","tldr":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to pe...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.18","presentation_id":"38939732","rocketchat_channel":"paper-deelio-18","speakers":"Ting-Yun Chang|Yang Liu|Karthik Gopalakrishnan|Behnam Hedayatnia|Pei Zhou|Dilek Hakkani-Tur","title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks"},{"content":{"abstract":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for our work is the BERT assumption according to which a large number of NLP tasks can be solved with pre-trained Transformers with no substantial task-specific changes of the architecture. However, our experiments show that the encoding strategy can have a great impact on the quality of the fine-tuning. The combination between cross-encoding and multi-input models worked better than one cross-encoder and allowed us to achieve comparable results with the state-of-the-art without the use of any external data.","authors":["Sonia Cibu","Anca Marginean"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Commonsense Statements Identification and Explanation with Transformer based Encoders","tldr":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for ou...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.20","presentation_id":"38939733","rocketchat_channel":"paper-deelio-20","speakers":"Sonia Cibu|Anca Marginean","title":"Commonsense Statements Identification and Explanation with Transformer based Encoders"},{"content":{"abstract":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.","authors":["Marjan Albooyeh","Rishab Goel","Seyed Mehran Kazemi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.241","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Out-of-Sample Representation Learning for Knowledge Graphs","tldr":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2047","presentation_id":"38940171","rocketchat_channel":"paper-deelio-2047","speakers":"Marjan Albooyeh|Rishab Goel|Seyed Mehran Kazemi","title":"Out-of-Sample Representation Learning for Knowledge Graphs"},{"content":{"abstract":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH assumes that a context surrounding a hyponym is more informative than that of a hypernym, it has never been tested on visual objects. Since our perception is tightly associated with language, it is meaningful to explore whether the DIH holds on visual objects. To this end, we consider visual objects as the context of a word and represent a word as a bag of visual objects found in images associated with the word. This allows us to test the feasibility of the visual DIH. To better distinguish word pairs in a hypernym relation from other relations such as co-hypernyms, we also propose a new measurable function that takes into account both the difference in the generality of meaning and similarity of meaning between words. Our experimental results show that the DIH holds on visual objects and that the proposed method combined with the proposed function outperforms existing unsupervised representation methods.","authors":["Masayasu Muraoka","Tetsuya Nasukawa","Bishwaranjan Bhattacharjee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.246","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment","tldr":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH as...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2085","presentation_id":"38940172","rocketchat_channel":"paper-deelio-2085","speakers":"Masayasu Muraoka|Tetsuya Nasukawa|Bishwaranjan Bhattacharjee","title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment"},{"content":{"abstract":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge graph embeddings and fine-grain entity type representations. Our work also shows that jointly modeling both structured knowledge tuples and language improves both.","authors":["Rajat Patel","Francis Ferraro"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling","tldr":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge gr...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.22","presentation_id":"38939734","rocketchat_channel":"paper-deelio-22","speakers":"Rajat Patel|Francis Ferraro","title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling"},{"content":{"abstract":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional information retrieval step (IR) and a kNN search over a large datastore of an embedded text collection. Our contributions are as follows: i) BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training. ii) We show that BERT often identifies the correct response category (e.g., US city), but only kNN recovers the factually correct answer (e.g.,\u201cMiami\u201d). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN can easily handle facts not covered by BERT\u2019s training set, e.g., recent events.","authors":["Nora Kassner","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.307","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA","tldr":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we comb...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2513","presentation_id":"38940173","rocketchat_channel":"paper-deelio-2513","speakers":"Nora Kassner|Hinrich Sch\u00fctze","title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA"},{"content":{"abstract":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train classifiers without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates \u201cweak\u201d supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages: by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier: leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of word translations.","authors":["Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.323","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher","tldr":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2666","presentation_id":"38940174","rocketchat_channel":"paper-deelio-2666","speakers":"Giannis Karamanolakis|Daniel Hsu|Luis Gravano","title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher"},{"content":{"abstract":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner.","authors":["Xiaoyu Chen","Rohan Badlani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relation Extraction with Contextualized Relation Embedding","tldr":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.4","presentation_id":"38939725","rocketchat_channel":"paper-deelio-4","speakers":"Xiaoyu Chen|Rohan Badlani","title":"Relation Extraction with Contextualized Relation Embedding"},{"content":{"abstract":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In attacks of this variety, the adversary substitutes words with synonyms. Since synonym substitution perturbations aim to satisfy all lexical, grammatical, and semantic constraints, they are difficult to detect with automatic syntax check as well as by humans. In this paper, we propose a structure-free defensive method that is capable of improving the performance of DNN-based models with both clean and adversarial data. Our findings show that replacing the embeddings of the important words in the input samples with the average of their synonyms\u2019 embeddings can significantly improve the generalization of DNN-based classifiers. By doing so, we reduce model sensitivity to particular words in the input samples. Our results indicate that the proposed defense is not only capable of defending against adversarial attacks, but is also capable of improving the performance of DNN-based models when tested on benign data. On average, the proposed defense improved the classification accuracy of the CNN and Bi-LSTM models by 41.30% and 55.66%, respectively, when tested under adversarial attacks. Extended investigation shows that our defensive method can improve the robustness of nonneural models, achieving an average of 17.62% and 22.93% classification accuracy increase on the SVM and XGBoost models, respectively. The proposed defensive method has also shown an average of 26.60% classification accuracy improvement when tested with the infamous BERT model. Our algorithm is generic enough to be applied in any NLP domain and to any model trained on any natural language.","authors":["Basemah Alshemali","Jugal Kalita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generalization to Mitigate Synonym Substitution Attacks","tldr":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In at...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.6","presentation_id":"38939726","rocketchat_channel":"paper-deelio-6","speakers":"Basemah Alshemali|Jugal Kalita","title":"Generalization to Mitigate Synonym Substitution Attacks"},{"content":{"abstract":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors. The resulting entity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no expensive further pre-training of the BERT encoder. We evaluate E-BERT on unsupervised question answering (QA), supervised relation classification (RC) and entity linking (EL). On all three tasks, E-BERT outperforms BERT and other baselines. We also show quantitatively that the original BERT model is overly reliant on the surface form of entity names (e.g., guessing that someone with an Italian-sounding name speaks Italian), and that E-BERT mitigates this problem.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.71","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT","tldr":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entit...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.696","presentation_id":"38940166","rocketchat_channel":"paper-deelio-696","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"content":{"abstract":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.","authors":["Steven Y. Feng","Varun Gangal","Dongyeop Kang","Teruko Mitamura","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GenAug: Data Augmentation for Finetuning Text Generators","tldr":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose a...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.7","presentation_id":"38939727","rocketchat_channel":"paper-deelio-7","speakers":"Steven Y. Feng|Varun Gangal|Dongyeop Kang|Teruko Mitamura|Eduard Hovy","title":"GenAug: Data Augmentation for Finetuning Text Generators"},{"content":{"abstract":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, joint pre-training (i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective) may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/wluper/retrograph.","authors":["Anne Lauscher","Olga Majewska","Leonardo F. R. Ribeiro","Iryna Gurevych","Nikolai Rozanov","Goran Glava\u0161"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers","tldr":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.9","presentation_id":"38939728","rocketchat_channel":"paper-deelio-9","speakers":"Anne Lauscher|Olga Majewska|Leonardo F. R. Ribeiro|Iryna Gurevych|Nikolai Rozanov|Goran Glava\u0161","title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"},{"content":{"abstract":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.","authors":["Adyasha Maharana","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.333","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension","tldr":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2797","presentation_id":"38940111","rocketchat_channel":"paper-deelio-2797","speakers":"Adyasha Maharana|Mohit Bansal","title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension"},{"content":{"abstract":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at https://github.com/weakrules/Denoise-multi-weak-sources.","authors":["Wendi Ren","Yinghao Li","Hanting Su","David Kartchner","Cassie Mitchell","Chao Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.334","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Denoising Multi-Source Weak Supervision for Neural Text Classification","tldr":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2800","presentation_id":"38940139","rocketchat_channel":"paper-deelio-2800","speakers":"Wendi Ren|Yinghao Li|Hanting Su|David Kartchner|Cassie Mitchell|Chao Zhang","title":"Denoising Multi-Source Weak Supervision for Neural Text Classification"},{"content":{"abstract":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG\u02c6C, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG\u02c6C consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG\u02c6C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.","authors":["Yiben Yang","Chaitanya Malaviya","Jared Fernandez","Swabha Swayamdipta","Ronan Le Bras","Ji-Ping Wang","Chandra Bhagavatula","Yejin Choi","Doug Downey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.90","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generative Data Augmentation for Commonsense Reasoning","tldr":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models c...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.884","presentation_id":"38940138","rocketchat_channel":"paper-deelio-884","speakers":"Yiben Yang|Chaitanya Malaviya|Jared Fernandez|Swabha Swayamdipta|Ronan Le Bras|Ji-Ping Wang|Chandra Bhagavatula|Yejin Choi|Doug Downey","title":"Generative Data Augmentation for Commonsense Reasoning"},{"content":{"abstract":"We investigate using Named Entity Recognition on a new type of user-generated text: a call center conversation. These conversations combine problems from spontaneous speech with problems novel to conversational Automated Speech Recognition, including incorrect recognition, alongside other common problems from noisy user-generated text. Using our own corpus with new annotations, training custom contextual string embeddings, and applying a BiLSTM-CRF, we match state-of- the-art results on our novel task.","authors":["Micaela Kaplan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"May I Ask Who\u2019s Calling? Named Entity Recognition on Call Center Transcripts for Privacy Law Compliance","tldr":"We investigate using Named Entity Recognition on a new type of user-generated text: a call center conversation. These conversations combine problems from spontaneous speech with problems novel to conversational Automated Speech Recognition, including...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.1","presentation_id":"","rocketchat_channel":"paper-wnut2020-1","speakers":"Micaela Kaplan","title":"May I Ask Who\u2019s Calling? Named Entity Recognition on Call Center Transcripts for Privacy Law Compliance"},{"content":{"abstract":"With the increased use of social media platforms by people across the world, many new interesting NLP problems have come into existence. One such being the detection of sarcasm in the social media texts. We present a corpus of tweets for training custom word embeddings and a Hinglish dataset labelled for sarcasm detection. We propose a deep learning based approach to address the issue of sarcasm detection in Hindi-English code mixed tweets using bilingual word embeddings derived from FastText and Word2Vec approaches. We experimented with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention). We were able to outperform all state-of-the-art performances with our deep learning models, with attention based Bi-directional LSTMs giving the best performance exhibiting an accuracy of 78.49%.","authors":["Akshita Aggarwal","Anshul Wadhawan","Anshima Chaudhary","Kavita Maurya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cDid you really mean what you said?\u201d : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings","tldr":"With the increased use of social media platforms by people across the world, many new interesting NLP problems have come into existence. One such being the detection of sarcasm in the social media texts. We present a corpus of tweets for training cus...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.2","presentation_id":"","rocketchat_channel":"paper-wnut2020-2","speakers":"Akshita Aggarwal|Anshul Wadhawan|Anshima Chaudhary|Kavita Maurya","title":"\u201cDid you really mean what you said?\u201d : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings"},{"content":{"abstract":"Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by practitioners to build industrial NLP applications, it is hard to guarantee absence of any noise in the data. While BERT has performed exceedingly well for transferring the learnings from one use case to another, it remains unclear how BERT performs when fine-tuned on noisy text. In this work, we explore the sensitivity of BERT to noise in the data. We work with most commonly occurring noise (spelling mistakes, typos) and show that this results in significant degradation in the performance of BERT. We present experimental results to show that BERT\u2019s performance on fundamental NLP tasks like sentiment analysis and textual similarity drops significantly in the presence of (simulated) noise on benchmark datasets viz. IMDB Movie Review, STS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline that are responsible for this drop in performance. Our findings suggest that practitioners need to be vary of presence of noise in their datasets while fine-tuning BERT to solve industry use cases.","authors":["Ankit Kumar","Piyush Makhija","Anuj Gupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Noisy Text Data: Achilles\u2019 Heel of BERT","tldr":"Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.3","presentation_id":"","rocketchat_channel":"paper-wnut2020-3","speakers":"Ankit Kumar|Piyush Makhija|Anuj Gupta","title":"Noisy Text Data: Achilles\u2019 Heel of BERT"},{"content":{"abstract":"Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer (QA) plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response. We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers (Question Plausibility AUROC=0.75, Response Plausibility AUROC=0.78, Answer Extraction F1=0.665).","authors":["Rachel Gardner","Maya Varma","Clare Zhu","Ranjay Krishna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning","tldr":"Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.4","presentation_id":"","rocketchat_channel":"paper-wnut2020-4","speakers":"Rachel Gardner|Maya Varma|Clare Zhu|Ranjay Krishna","title":"Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning"},{"content":{"abstract":"Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the social media genre, despite the fact that social media often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the Arabic language, which is successful in categorizing social media posts in Arabic dialects, despite only having been trained on Modern Standard Arabic. Our hypothesis in this paper is that the performance of LMs for social media can nonetheless be improved by incorporating static word vectors that have been specifically trained on social media. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).","authors":["Israa Alghanmi","Luis Espinosa Anke","Steven Schockaert"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combining BERT with Static Word Embeddings for Categorizing Social Media","tldr":"Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the social media genre, despite the fact that social media often has ver...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.5","presentation_id":"","rocketchat_channel":"paper-wnut2020-5","speakers":"Israa Alghanmi|Luis Espinosa Anke|Steven Schockaert","title":"Combining BERT with Static Word Embeddings for Categorizing Social Media"},{"content":{"abstract":"Cross-sentence attention has been widely applied in text matching, in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. However, commonly the intermediate representations are generated solely based on the preceding layers and the models may suffer from error propagation and unstable matching, especially when multiple attention layers are used. In this paper, we pro-pose an enhanced sentence alignment network with simple gated feature augmentation, where the model is able to flexibly integrate both original word and contextual features to improve the cross-sentence attention. Moreover, our model is less complex with fewer parameters compared to many state-of-the-art structures.Experiments on three benchmark datasets validate our model capacity for text matching.","authors":["Zhe Hu","Zuohui Fu","Cheng Peng","Weiwei Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhanced Sentence Alignment Network for Efficient Short Text Matching","tldr":"Cross-sentence attention has been widely applied in text matching, in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. However, commonly the intermediate representati...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.6","presentation_id":"","rocketchat_channel":"paper-wnut2020-6","speakers":"Zhe Hu|Zuohui Fu|Cheng Peng|Weiwei Wang","title":"Enhanced Sentence Alignment Network for Efficient Short Text Matching"},{"content":{"abstract":"Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding natural language to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as Google Translate fail at times to translate code-mixed text effectively. To address this challenge, we present a parallel corpus of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in English. In addition, we also propose a translation pipeline build on top of Google Translate. The evaluation of the proposed pipeline on PHINC demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.","authors":["Vivek Srivastava","Mayank Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation","tldr":"Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text messa...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.7","presentation_id":"","rocketchat_channel":"paper-wnut2020-7","speakers":"Vivek Srivastava|Mayank Singh","title":"PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation"},{"content":{"abstract":"Sentiment analysis research in low-resource languages such as Bengali is still unexplored due to the scarcity of annotated data and the lack of text processing tools. Therefore, in this work, we focus on generating resources and showing the applicability of the cross-lingual sentiment analysis approach in Bengali. For benchmarking, we created and annotated a comprehensive corpus of around 12000 Bengali reviews. To address the lack of standard text-processing tools in Bengali, we leverage resources from English utilizing machine translation. We determine the performance of supervised machine learning (ML) classifiers in machine-translated English corpus and compare it with the original Bengali corpus. Besides, we examine sentiment preservation in the machine-translated corpus utilizing Cohen\u2019s Kappa and Gwet\u2019s AC1. To circumvent the laborious data labeling process, we explore lexicon-based methods and study the applicability of utilizing cross-domain labeled data from the resource-rich language. We find that supervised ML classifiers show comparable performances in Bengali and machine-translated English corpus. By utilizing labeled data, they achieve 15%-20% higher F1 scores compared to both lexicon-based and transfer learning-based methods. Besides, we observe that machine translation does not alter the sentiment polarity of the review for most of the cases. Our experimental results demonstrate that the machine translation based cross-lingual approach can be an effective way for sentiment classification in Bengali.","authors":["Salim Sazzed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual sentiment classification in low-resource Bengali language","tldr":"Sentiment analysis research in low-resource languages such as Bengali is still unexplored due to the scarcity of annotated data and the lack of text processing tools. Therefore, in this work, we focus on generating resources and showing the applicabi...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.8","presentation_id":"","rocketchat_channel":"paper-wnut2020-8","speakers":"Salim Sazzed","title":"Cross-lingual sentiment classification in low-resource Bengali language"},{"content":{"abstract":"As the largest institutionalized second language variety of English, Indian English has received a sustained focus from linguists for decades. However, to the best of our knowledge, no prior study has contrasted web-expressions of Indian English in noisy social media with English generated by a social media user base that are predominantly native speakers. In this paper, we address this gap in the literature through conducting a comprehensive analysis considering multiple structural and semantic aspects. In addition, we propose a novel application of language models to perform automatic linguistic quality assessment.","authors":["Rupak Sarkar","Sayantan Mahinder","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Non-native Speaker Aspect: Indian English in Social Media","tldr":"As the largest institutionalized second language variety of English, Indian English has received a sustained focus from linguists for decades. However, to the best of our knowledge, no prior study has contrasted web-expressions of Indian English in n...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.9","presentation_id":"","rocketchat_channel":"paper-wnut2020-9","speakers":"Rupak Sarkar|Sayantan Mahinder|Ashiqur KhudaBukhsh","title":"The Non-native Speaker Aspect: Indian English in Social Media"},{"content":{"abstract":"For NLP, sentence boundary detection (SBD) is an essential task to decompose a text into sentences. Most of the previous studies have used a simple rule that uses only typical characters as sentence boundaries. However, some characters may or may not be sentence boundaries depending on the context. We focused on line breaks in them. We newly constructed annotated corpora, implemented sentence boundary detectors, and analyzed performance of SBD in several settings.","authors":["Yuta Hayashibe","Kensuke Mitsuzawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Sentence Boundary Detection on Line Breaks in Japanese","tldr":"For NLP, sentence boundary detection (SBD) is an essential task to decompose a text into sentences. Most of the previous studies have used a simple rule that uses only typical characters as sentence boundaries. However, some characters may or may not...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.10","presentation_id":"","rocketchat_channel":"paper-wnut2020-10","speakers":"Yuta Hayashibe|Kensuke Mitsuzawa","title":"Sentence Boundary Detection on Line Breaks in Japanese"},{"content":{"abstract":"Recently, the number of user-generated recipes on the Internet has increased. In such recipes, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a user-generated recipe are not actually edible ingredients. For example, headings, comments, and kitchenware sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use recipes for a variety of tasks, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed method achieved a 93.3 F1 score.","authors":["Yasuhiro Yamaguchi","Shintaro Inuzuka","Makoto Hiramatsu","Jun Harashima"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach","tldr":"Recently, the number of user-generated recipes on the Internet has increased. In such recipes, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a user-generate...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.11","presentation_id":"","rocketchat_channel":"paper-wnut2020-11","speakers":"Yasuhiro Yamaguchi|Shintaro Inuzuka|Makoto Hiramatsu|Jun Harashima","title":"Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach"},{"content":{"abstract":"We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenting a diversified set of sentences from the documents that explain its decision on the correctness of the textual claim. Prior approaches to address the problem of fact checking and evidence extraction have relied on simple concatenation of claim and document word embeddings as an input to claim driven attention weight computation. This is done so as to extract salient words and sentences from the documents that help establish the correctness of the claim. However this design of claim-driven attention fails to capture the contextual information in documents properly. We improve on the prior art by using improved claim and title guided hierarchical attention to model effective contextual cues. We show the efficacy of our approach on political, healthcare, and environmental datasets.","authors":["Rahul Mishra","Dhruv Gupta","Markus Leippold"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Fact Checking Summaries for Web Claims","tldr":"We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenti...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.12","presentation_id":"","rocketchat_channel":"paper-wnut2020-12","speakers":"Rahul Mishra|Dhruv Gupta|Markus Leippold","title":"Generating Fact Checking Summaries for Web Claims"},{"content":{"abstract":"This paper explores how Dutch diary fragments, written by family coaches in the social sector, can be analysed automatically using machine learning techniques to quantitatively measure the impact of social coaching. The focus lays on two tasks: determining which sentiment a fragment contains (sentiment analysis) and investigating which fundamental social rights (education, employment, legal aid, etc.) are addressed in the fragment. To train and test the new algorithms, a dataset consisting of 1715 Dutch diary fragments is used. These fragments are manually labelled on sentiment and on the applicable fundamental social rights. The sentiment analysis models were trained to classify the fragments into three classes: negative, neutral or positive. Fine-tuning the Dutch pre-trained Bidirectional Encoder Representations from Transformers (BERTje) (de Vries et al., 2019) language model surpassed the more classic algorithms by correctly classifying 79.6% of the fragments on the sentiment analysis, which is considered as a good result. This technique also achieved the best results in the identification of the fundamental rights, where for every fragment the three most likely fundamental rights were given as output. In this way, 93% of the present fundamental rights were correctly recognised. To our knowledge, we are the first to try to extract social rights from written text with the help of Natural Language Processing techniques.","authors":["Koen Kicken","Tessa De Maesschalck","Bart Vanrumste","Tom De Keyser","Hee Reen Shim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Intelligent Analyses on Storytelling for Impact Measurement","tldr":"This paper explores how Dutch diary fragments, written by family coaches in the social sector, can be analysed automatically using machine learning techniques to quantitatively measure the impact of social coaching. The focus lays on two tasks: deter...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.13","presentation_id":"","rocketchat_channel":"paper-wnut2020-13","speakers":"Koen Kicken|Tessa De Maesschalck|Bart Vanrumste|Tom De Keyser|Hee Reen Shim","title":"Intelligent Analyses on Storytelling for Impact Measurement"},{"content":{"abstract":"Automated agents (\u201cbots\u201d) have emerged as an ubiquitous and influential presence on social media. Bots engage on social media platforms by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single bot on Reddit. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on Reddit. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on social media with relatively simple bots is important for preparing for more advanced bots in the future.","authors":["Ming-Cheng Ma","John P. Lalor"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Analysis of Human-Bot Interaction on Reddit","tldr":"Automated agents (\u201cbots\u201d) have emerged as an ubiquitous and influential presence on social media. Bots engage on social media platforms by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.14","presentation_id":"","rocketchat_channel":"paper-wnut2020-14","speakers":"Ming-Cheng Ma|John P. Lalor","title":"An Empirical Analysis of Human-Bot Interaction on Reddit"},{"content":{"abstract":"We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, orthographic variation, acronyms, and slang. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a topic model for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on information retrieval.","authors":["Jack Hughes","Seth Aycock","Andrew Caines","Paula Buttery","Alice Hutchings"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Trending Terms in Cybersecurity Forum Discussions","tldr":"We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.15","presentation_id":"","rocketchat_channel":"paper-wnut2020-15","speakers":"Jack Hughes|Seth Aycock|Andrew Caines|Paula Buttery|Alice Hutchings","title":"Detecting Trending Terms in Cybersecurity Forum Discussions"},{"content":{"abstract":"Crowdsourcing is the go-to solution for data collection and annotation in the context of NLP tasks. Nevertheless, crowdsourced data is noisy by nature; the source is often unknown and additional validation work is performed to guarantee the dataset\u2019s quality. In this article, we compare two crowdsourcing sources on a dialogue paraphrasing task revolving around a chatbot service. We observe that workers hired on crowdsourcing platforms produce lexically poorer and less diverse rewrites than service users engaged voluntarily. Notably enough, on dialogue clarity and optimality, the two paraphrase sources\u2019 human-perceived quality does not differ significantly. Furthermore, for the chatbot service, the combined crowdsourced data is enough to train a transformer-based Natural Language Generation (NLG) system. To enable similar services, we also release tools for collecting data and training the dialogue-act-based transformer-based NLG module.","authors":["Luca Molteni","Mittul Singh","Juho Leinonen","Katri Leino","Mikko Kurimo","Emanuele Della Valle"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Service registration chatbot: collecting and comparing dialogues from AMT workers and service\u2019s users","tldr":"Crowdsourcing is the go-to solution for data collection and annotation in the context of NLP tasks. Nevertheless, crowdsourced data is noisy by nature; the source is often unknown and additional validation work is performed to guarantee the dataset\u2019s...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.16","presentation_id":"","rocketchat_channel":"paper-wnut2020-16","speakers":"Luca Molteni|Mittul Singh|Juho Leinonen|Katri Leino|Mikko Kurimo|Emanuele Della Valle","title":"Service registration chatbot: collecting and comparing dialogues from AMT workers and service\u2019s users"},{"content":{"abstract":"The requirement of performing assessments continually on a larger scale necessitates the implementation of automated systems for evaluation of the learners\u2019 responses to free-text questions. We target children of age group 8-14 years and use an ASR integrated assessment app to crowdsource learners\u2019 responses to free text questions in Hindi. The app helped collect 39641 user answers to 35 different questions of Science topics. Since the users are young children from rural India and may not be well-equipped with technology, it brings in various noise types in the answers. We describe these noise types and propose a preprocessing pipeline to denoise user\u2019s answers. We showcase the performance of different similarity metrics on the noisy and denoised versions of user and model answers. Our findings have large-scale applications for automated answer assessment for school children in India in low resource settings.","authors":["Dolly Agarwal","Somya Gupta","Nishant Baghel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automated Assessment of Noisy Crowdsourced Free-text Answers for Hindi in Low Resource Setting","tldr":"The requirement of performing assessments continually on a larger scale necessitates the implementation of automated systems for evaluation of the learners\u2019 responses to free-text questions. We target children of age group 8-14 years and use an ASR i...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.17","presentation_id":"","rocketchat_channel":"paper-wnut2020-17","speakers":"Dolly Agarwal|Somya Gupta|Nishant Baghel","title":"Automated Assessment of Noisy Crowdsourced Free-text Answers for Hindi in Low Resource Setting"},{"content":{"abstract":"Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.","authors":["Tanvirul Alam","Akib Khan","Firoj Alam"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Punctuation Restoration using Transformer Models for Resource-Rich and -Poor Languages","tldr":"Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.18","presentation_id":"","rocketchat_channel":"paper-wnut2020-18","speakers":"Tanvirul Alam|Akib Khan|Firoj Alam","title":"Punctuation Restoration using Transformer Models for Resource-Rich and -Poor Languages"},{"content":{"abstract":"True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user queries to a voice-controlled dia- log system. In particular, we compare a rule- based, an n-gram language model (LM) and a recurrent neural network (RNN) approaches, evaluating the results on a German Q&A cor- pus and reporting accuracy for different case categories. We show that while RNNs reach higher accuracy especially on large datasets, character n-gram models with interpolation are still competitive, in particular on mixed- case words where their fall-back mechanisms come into play.","authors":["Yulia Grishina","Thomas Gueudre","Ralf Winkler"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Truecasing German user-generated conversational text","tldr":"True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user que...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.19","presentation_id":"","rocketchat_channel":"paper-wnut2020-19","speakers":"Yulia Grishina|Thomas Gueudre|Ralf Winkler","title":"Truecasing German user-generated conversational text"},{"content":{"abstract":"The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that fine-tuning using naturally occurring noise along with pseudo-references (i.e. \u201ccorrected\u201d non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from English to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here: https://github.com/mahfuzibnalam/finetuning_for_robustness .","authors":["Md Mahfuz Ibn Alam","Antonios Anastasopoulos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations","tldr":"The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate th...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.20","presentation_id":"","rocketchat_channel":"paper-wnut2020-20","speakers":"Md Mahfuz Ibn Alam|Antonios Anastasopoulos","title":"Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations"},{"content":{"abstract":"Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.","authors":["Aparna Balagopalan","Ksenia Shkaruta","Jekaterina Novikova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Impact of ASR on Alzheimer\u2019s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others","tldr":"Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.21","presentation_id":"","rocketchat_channel":"paper-wnut2020-21","speakers":"Aparna Balagopalan|Ksenia Shkaruta|Jekaterina Novikova","title":"Impact of ASR on Alzheimer\u2019s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others"},{"content":{"abstract":"The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among polyglots. Given the rising importance of dialogue agents, it is imperative that they understand code-mixing, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The dataset by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of language modeling, data augmentation, translation, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this dataset. We obtain an 8.09% increase in test set accuracy over the current state of the art.","authors":["Sharanya Chakravarthy","Anjana Umapathy","Alan W Black"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Entailment in Code-Mixed Hindi-English Conversations","tldr":"The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages,...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.22","presentation_id":"","rocketchat_channel":"paper-wnut2020-22","speakers":"Sharanya Chakravarthy|Anjana Umapathy|Alan W Black","title":"Detecting Entailment in Code-Mixed Hindi-English Conversations"},{"content":{"abstract":"Student reviews often make reference to professors\u2019 physical appearances. Until recently RateMyProfessors.com, the website of this study\u2019s focus, used a design feature to encourage a \u201chot or not\u201d rating of college professors. In the wake of recent #MeToo and #TimesUp movements, social awareness of the inappropriateness of these reviews has grown; however, objectifying comments remain and continue to be posted in this online context. We describe two supervised text classifiers for detecting objectifying commentary in professor reviews. We then ensemble these classifiers and use the resulting model to track objectifying commentary at scale. We measure correlations between objectifying commentary, changes to the review website interface, and teacher gender across a ten-year period.","authors":["Angie Waller","Kyle Gorman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Objectifying Language in Online Professor Reviews","tldr":"Student reviews often make reference to professors\u2019 physical appearances. Until recently RateMyProfessors.com, the website of this study\u2019s focus, used a design feature to encourage a \u201chot or not\u201d rating of college professors. In the wake of recent #M...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.23","presentation_id":"","rocketchat_channel":"paper-wnut2020-23","speakers":"Angie Waller|Kyle Gorman","title":"Detecting Objectifying Language in Online Professor Reviews"},{"content":{"abstract":"India is home to several languages with more than 30m speakers. These languages exhibit significant presence on social media platforms. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these languages is also typically authored in the Roman script as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our models to facilitate downstream analyses in these low-resource languages. Experiments across multiple social media corpora demonstrate the model\u2019s robustness and provide several interesting insights on Indian language usage patterns on social media. We release an annotated data set of 1,000 comments in ten Romanized languages as a social media evaluation benchmark.","authors":["Shriphani Palakodety","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Annotation Efficient Language Identification from Weak Labels","tldr":"India is home to several languages with more than 30m speakers. These languages exhibit significant presence on social media platforms. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) m...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.24","presentation_id":"","rocketchat_channel":"paper-wnut2020-24","speakers":"Shriphani Palakodety|Ashiqur KhudaBukhsh","title":"Annotation Efficient Language Identification from Weak Labels"},{"content":{"abstract":"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an example of how this method can be used to assist classification in fields where interpretability is important, such as health care.","authors":["Ben Eyre","Aparna Balagopalan","Jekaterina Novikova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach","tldr":"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manuall...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.25","presentation_id":"","rocketchat_channel":"paper-wnut2020-25","speakers":"Ben Eyre|Aparna Balagopalan|Jekaterina Novikova","title":"Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach"},{"content":{"abstract":"Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to use may significantly impact the success of the training. In this work, we propose a metric for evaluating augmentation heuristics; specifically, we quantify the extent to which an example is \u201chard to distinguish\u201d by considering the difference between the distribution of the augmented samples of different classes. Experimenting with multiple heuristics in two prediction tasks (positive/negative sentiment and verbosity/conciseness) validates our claims by revealing the connection between the distribution difference of different classes and the classification accuracy.","authors":["Omid Kashefi","Rebecca Hwa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation","tldr":"Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.26","presentation_id":"","rocketchat_channel":"paper-wnut2020-26","speakers":"Omid Kashefi|Rebecca Hwa","title":"Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation"},{"content":{"abstract":"The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, however, unclear whether these improvements translate to noisy user-generated text, such as tweets. In this paper, we present an experimental survey of a wide range of well-known text representation techniques for the task of text clustering on noisy Twitter data. Our results indicate that the more advanced models do not necessarily work best on tweets and that more exploration in this area is needed.","authors":["Lili Wang","Chongyang Gao","Jason Wei","Weicheng Ma","Ruibo Liu","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data","tldr":"The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, how...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.27","presentation_id":"","rocketchat_channel":"paper-wnut2020-27","speakers":"Lili Wang|Chongyang Gao|Jason Wei|Weicheng Ma|Ruibo Liu|Soroush Vosoughi","title":"An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data"},{"content":{"abstract":"We present CUT, a dataset for studying Civil Unrest on Twitter. Our dataset includes 4,381 tweets related to civil unrest, hand-annotated with information related to the study of civil unrest discussion and events. Our dataset is drawn from 42 countries from 2014 to 2019. We present baseline systems trained on this data for the identification of tweets related to civil unrest. We include a discussion of ethical issues related to research on this topic.","authors":["Justin Sech","Alexandra DeLucia","Anna L. Buczak","Mark Dredze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest","tldr":"We present CUT, a dataset for studying Civil Unrest on Twitter. Our dataset includes 4,381 tweets related to civil unrest, hand-annotated with information related to the study of civil unrest discussion and events. Our dataset is drawn from 42 countr...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.28","presentation_id":"","rocketchat_channel":"paper-wnut2020-28","speakers":"Justin Sech|Alexandra DeLucia|Anna L. Buczak|Mark Dredze","title":"Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest"},{"content":{"abstract":"To identify what entities are being talked about in tweets, we need to automatically link named entities that appear in tweets to structured KBs like WikiData. Existing approaches often struggle with such short, noisy texts, or their complex design and reliance on supervision make them brittle, difficult to use and maintain, and lose significance over time. Further, there is a lack of a large, linked corpus of tweets to aid researchers, along with lack of gold dataset to evaluate the accuracy of entity linking. In this paper, we introduce (1) Tweeki, an unsupervised, modular entity linking system for Twitter, (2) TweekiData, a large, automatically-annotated corpus of Tweets linked to entities in WikiData, and (3) TweekiGold, a gold dataset for entity linking evaluation. Through comprehensive analysis, we show that Tweeki is comparable to the performance of recent state-of-the-art entity linkers models, the dataset is of high quality, and a use case of how the dataset can be used to improve downstream tasks in social media analysis (geolocation prediction).","authors":["Bahareh Harandizadeh","Sameer Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tweeki: Linking Named Entities on Twitter to a Knowledge Graph","tldr":"To identify what entities are being talked about in tweets, we need to automatically link named entities that appear in tweets to structured KBs like WikiData. Existing approaches often struggle with such short, noisy texts, or their complex design a...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.29","presentation_id":"","rocketchat_channel":"paper-wnut2020-29","speakers":"Bahareh Harandizadeh|Sameer Singh","title":"Tweeki: Linking Named Entities on Twitter to a Knowledge Graph"},{"content":{"abstract":"In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document styles and to enhance model performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of writing style. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but it relies only on the metadata of the articles which are available in huge amounts. We evaluate the model on multiple datasets, on both the authorship clustering and the authorship attribution tasks.","authors":["Julien Hay","Bich-Lien Doan","Fabrice Popineau","Ouassim Ait Elhara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Representation learning of writing style","tldr":"In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document sty...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.30","presentation_id":"","rocketchat_channel":"paper-wnut2020-30","speakers":"Julien Hay|Bich-Lien Doan|Fabrice Popineau|Ouassim Ait Elhara","title":"Representation learning of writing style"},{"content":{"abstract":"The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a social media post. Unlike previous works, we impose inductive biases that capture platform specific user behavior. These biases, coupled with social media fine-tuning of BERT allow for better language understanding, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.","authors":["Karthik Radhakrishnan","Tushar Kanakagiri","Sharanya Chakravarthy","Vidhisha Balachandran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cA Little Birdie Told Me ... \" - Social Media Rumor Detection","tldr":"The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.31","presentation_id":"","rocketchat_channel":"paper-wnut2020-31","speakers":"Karthik Radhakrishnan|Tushar Kanakagiri|Sharanya Chakravarthy|Vidhisha Balachandran","title":"\u201cA Little Birdie Told Me ... \" - Social Media Rumor Detection"},{"content":{"abstract":"Paraphrase generation is an important problem in Natural Language Processing that has been addressed with neural network-based approaches recently. This paper presents an adversarial framework to address the paraphrase generation problem in English. Unlike previous methods, we employ the discriminator output as penalization instead of using policy gradients, and we propose a global discriminator to avoid the Monte-Carlo search. In addition, this work use and compare different settings of input representation. We compare our methods to some baselines in the Quora question pairs dataset. The results show that our framework is competitive against the previous benchmarks.","authors":["Gerson Vizcarra","Jose Ochoa-Luna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrase Generation via Adversarial Penalizations","tldr":"Paraphrase generation is an important problem in Natural Language Processing that has been addressed with neural network-based approaches recently. This paper presents an adversarial framework to address the paraphrase generation problem in English. ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.32","presentation_id":"","rocketchat_channel":"paper-wnut2020-32","speakers":"Gerson Vizcarra|Jose Ochoa-Luna","title":"Paraphrase Generation via Adversarial Penalizations"},{"content":{"abstract":"This paper presents the results of the wet labinformation extraction task at WNUT 2020.This task consisted of two sub tasks- (1) anamed entity recognition task with 13 partic-ipants; and (2) a relation extraction task with2 participants. We outline the task, data an-notation process, corpus statistics, and providea high-level overview of the participating sys-tems for each sub task.","authors":["Jeniya Tabassum","Wei Xu","Alan Ritter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols","tldr":"This paper presents the results of the wet labinformation extraction task at WNUT 2020.This task consisted of two sub tasks- (1) anamed entity recognition task with 13 partic-ipants; and (2) a relation extraction task with2 participants. We outline t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.33","presentation_id":"","rocketchat_channel":"paper-wnut2020-33","speakers":"Jeniya Tabassum|Wei Xu|Alan Ritter","title":"WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols"},{"content":{"abstract":"Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks.For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the medical field contains a lot of different tokens used only in the medical industry such as the name of different diseases, devices, organisms,medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the System for Named Entity Tagging based on Bio-Bert. Experimental results show that our model gives substantial improvements over the baseline and stood the fourth runner up in terms of F1 score, and first runner up in terms of Recall with just 2.21 F1 score behind the best one.","authors":["Tejas Vaidhya","Ayush Kaushal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IITKGP at W-NUT 2020 Shared Task-1: Domain specific BERT representation for Named Entity Recognition of lab protocol","tldr":"Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks.For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.34","presentation_id":"","rocketchat_channel":"paper-wnut2020-34","speakers":"Tejas Vaidhya|Ayush Kaushal","title":"IITKGP at W-NUT 2020 Shared Task-1: Domain specific BERT representation for Named Entity Recognition of lab protocol"},{"content":{"abstract":"In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings (like Flair, BERT-based) and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling (SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.","authors":["Janvijay Singh","Anshul Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings","tldr":"In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with vari...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.35","presentation_id":"","rocketchat_channel":"paper-wnut2020-35","speakers":"Janvijay Singh|Anshul Wadhawan","title":"PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings"},{"content":{"abstract":"Relation and event extraction is an important task in natural language processing. We introduce a system which uses contextualized knowledge graph completion to classify relations and events between known entities in a noisy text environment. We report results which show that our system is able to effectively extract relations and events from a dataset of wet lab protocols.","authors":["Chris Miller","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Big Green at WNUT 2020 Shared Task-1: Relation Extraction as Contextualized Sequence Classification","tldr":"Relation and event extraction is an important task in natural language processing. We introduce a system which uses contextualized knowledge graph completion to classify relations and events between known entities in a noisy text environment. We repo...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.36","presentation_id":"","rocketchat_channel":"paper-wnut2020-36","speakers":"Chris Miller|Soroush Vosoughi","title":"Big Green at WNUT 2020 Shared Task-1: Relation Extraction as Contextualized Sequence Classification"},{"content":{"abstract":"The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols.","authors":["Kaushik Acharya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT 2020 Shared Task-1: Conditional Random Field(CRF) based Named Entity Recognition(NER) for Wet Lab Protocols","tldr":"The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols....","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.37","presentation_id":"","rocketchat_channel":"paper-wnut2020-37","speakers":"Kaushik Acharya","title":"WNUT 2020 Shared Task-1: Conditional Random Field(CRF) based Named Entity Recognition(NER) for Wet Lab Protocols"},{"content":{"abstract":"We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60% in terms of F-score as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46% in terms of F-score as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.","authors":["Mohammad Golam Sohrab","Anh-Khoa Duong Nguyen","Makoto Miwa","Hiroya Takamura"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"mgsohrab at WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols","tldr":"We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.38","presentation_id":"","rocketchat_channel":"paper-wnut2020-38","speakers":"Mohammad Golam Sohrab|Anh-Khoa Duong Nguyen|Makoto Miwa|Hiroya Takamura","title":"mgsohrab at WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols"},{"content":{"abstract":"Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more interests with the development of deep learning. This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity extract, that we conducted studies in several models, including a BiLSTM CRF model and a Bert case model which can be used to complete wet lab entity extraction. And we mainly discussed the performance differences of Bert case under different situations such as transformers versions, case sensitivity that may don\u2019t get enough attention before.","authors":["Qingcheng Zeng","Xiaoyang Fang","Zhexin Liang","Haoding Meng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fancy Man Launches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction","tldr":"Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more i...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.39","presentation_id":"","rocketchat_channel":"paper-wnut2020-39","speakers":"Qingcheng Zeng|Xiaoyang Fang|Zhexin Liang|Haoding Meng","title":"Fancy Man Launches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction"},{"content":{"abstract":"Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper, we present a 3-step method based on deep neural language models that reported the best overall exact match F1-score (77.99%) of the competition. By fine-tuning 10 times, 10 different pretrained language models, this work shows the advantage of having more models in an ensemble based on a majority of votes strategy. On top of that, having 100 different models allowed us to analyse the combinations of ensemble that demonstrated the impact of having multiple pretrained models versus fine-tuning a pretrained model multiple times.","authors":["Julien Knafou","Nona Naderi","Jenny Copara","Douglas Teodoro","Patrick Ruch"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models","tldr":"Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper,...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.40","presentation_id":"","rocketchat_channel":"paper-wnut2020-40","speakers":"Julien Knafou|Nona Naderi|Jenny Copara|Douglas Teodoro|Patrick Ruch","title":"BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models"},{"content":{"abstract":"In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.","authors":["Dat Quoc Nguyen","Thanh Vu","Afshin Rahimi","Mai Hoang Dao","Linh The Nguyen","Long Doan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets","tldr":"In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.41","presentation_id":"","rocketchat_channel":"paper-wnut2020-41","speakers":"Dat Quoc Nguyen|Thanh Vu|Afshin Rahimi|Mai Hoang Dao|Linh The Nguyen|Long Doan","title":"WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.","authors":["Anh Tuan Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TATL at WNUT-2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets","tldr":"As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the m...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.42","presentation_id":"","rocketchat_channel":"paper-wnut2020-42","speakers":"Anh Tuan Nguyen","title":"TATL at WNUT-2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"The outbreak of COVID-19 has greatly impacted our daily lives. In these circumstances, it is important to grasp the latest information to avoid causing too much fear and panic. To help grasp new information, extracting information from social networking sites is one of the effective ways. In this paper, we describe a method to identify whether a tweet related to COVID-19 is informative or not, which can help to grasp new information. The key features of our method are its use of graph attention networks to encode syntactic dependencies and word positions in the sentence, and a loss function based on connectionist temporal classification (CTC) that can learn a label for each token without reference data for each token. Experimental results show that the proposed method achieved an F1 score of 0.9175, out- performing baseline methods.","authors":["Yuki Yasuda","Taichi Ishiwatari","Taro Miyazaki","Jun Goto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.43","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NHK_STRL at WNUT-2020 Task 2: GATs with Syntactic Dependencies as Edges and CTC-based Loss for Text Classification","tldr":"The outbreak of COVID-19 has greatly impacted our daily lives. In these circumstances, it is important to grasp the latest information to avoid causing too much fear and panic. To help grasp new information, extracting information from social network...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.43","presentation_id":"","rocketchat_channel":"paper-wnut2020-43","speakers":"Yuki Yasuda|Taichi Ishiwatari|Taro Miyazaki|Jun Goto","title":"NHK_STRL at WNUT-2020 Task 2: GATs with Syntactic Dependencies as Edges and CTC-based Loss for Text Classification"},{"content":{"abstract":"With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.","authors":["Anders Giovanni M\u00f8ller","Rob van der Goot","Barbara Plank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.44","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets","tldr":"With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Iden...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.44","presentation_id":"","rocketchat_channel":"paper-wnut2020-44","speakers":"Anders Giovanni M\u00f8ller|Rob van der Goot|Barbara Plank","title":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets"},{"content":{"abstract":"Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as \u201cinfodemic.\u201d The ill-effects of such misinformation are multifarious. Thus, identifying and eliminating the sources of misinformation becomes very crucial, especially when mass panic can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on social media. WNUT-2020 Task 2 aims at building systems for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3% in the final evaluation.","authors":["Siva Sai"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.45","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Siva at WNUT-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets","tldr":"Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as \u201cinfodemic.\u201d The ill-effects of such misinformation are multifarious. Thus...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.45","presentation_id":"","rocketchat_channel":"paper-wnut2020-45","speakers":"Siva Sai","title":"Siva at WNUT-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets"},{"content":{"abstract":"In this paper, we present IIITBH team\u2019s effort to solve the second shared task of the 6th Workshop on Noisy User-generated Text (W-NUT)i.e Identification of informative COVID-19 English Tweets. The central theme of the task is to develop a system that automatically identify whether an English Tweet related to the novel coronavirus (COVID-19) is Informative or not. Our approach is based on exploiting semantic information from both max pooling and average pooling, to this end we propose two models.","authors":["Saichethan Reddy","Pradeep Biswal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.46","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIITBH at WNUT-2020 Task 2: Exploiting the best of both worlds","tldr":"In this paper, we present IIITBH team\u2019s effort to solve the second shared task of the 6th Workshop on Noisy User-generated Text (W-NUT)i.e Identification of informative COVID-19 English Tweets. The central theme of the task is to develop a system tha...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.46","presentation_id":"","rocketchat_channel":"paper-wnut2020-46","speakers":"Saichethan Reddy|Pradeep Biswal","title":"IIITBH at WNUT-2020 Task 2: Exploiting the best of both worlds"},{"content":{"abstract":"This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to the novel coronavirus (COVID-19) is informative or not. We solve the task in three stages. The first stage involves pre-processing the dataset by filtering only relevant information. This is followed by experimenting with multiple deep learning models like CNNs, RNNs and Transformer based models. In the last stage, we propose an ensemble of the best model trained on different subsets of the provided dataset. Our final approach achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score as the evaluation criteria.","authors":["Anshul Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.47","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting","tldr":"This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.47","presentation_id":"","rocketchat_channel":"paper-wnut2020-47","speakers":"Anshul Wadhawan","title":"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting"},{"content":{"abstract":"This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned features in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional features can improve classification results and achieve a score within 2 points of the top performing team.","authors":["Calum Perrio","Harish Tayyar Madabushi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets - RoBERTa Ensembles and The Continued Relevance of Handcrafted Features","tldr":"This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation th...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.48","presentation_id":"","rocketchat_channel":"paper-wnut2020-48","speakers":"Calum Perrio|Harish Tayyar Madabushi","title":"CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets - RoBERTa Ensembles and The Continued Relevance of Handcrafted Features"},{"content":{"abstract":"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10th place in the final rankings scoring 0.9004 F1 score for the test set.","authors":["Hansi Hettiarachchi","Tharindu Ranasinghe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction","tldr":"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.49","presentation_id":"","rocketchat_channel":"paper-wnut2020-49","speakers":"Hansi Hettiarachchi|Tharindu Ranasinghe","title":"InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction"},{"content":{"abstract":"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81% on the test set.","authors":["Tin Huynh","Luan Thanh Luan","Son T. Luu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.50","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models","tldr":"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction s...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.50","presentation_id":"","rocketchat_channel":"paper-wnut2020-50","speakers":"Tin Huynh|Luan Thanh Luan|Son T. Luu","title":"BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models"},{"content":{"abstract":"This document describes the system description developed by team datamafia at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. This paper contains a thorough study of pre-trained language models on downstream binary classification task over noisy user generated Twitter data. The solution submitted to final test leaderboard is a fine tuned RoBERTa model which achieves F1 score of 90.8% and 89.4% on the dev and test data respectively. In the later part, we explore several techniques for injecting regularization explicitly into language models to generalize predictions over noisy data. Our experiments show that adding regularizations to RoBERTa pre-trained model can be very robust to data and annotation noises and can improve overall performance by more than 1.2%.","authors":["Ayan Sengupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.51","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DATAMAFIA at WNUT-2020 Task 2: A Study of Pre-trained Language Models along with Regularization Techniques for Downstream Tasks","tldr":"This document describes the system description developed by team datamafia at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. This paper contains a thorough study of pre-trained language models on downstream binary classifica...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.51","presentation_id":"","rocketchat_channel":"paper-wnut2020-51","speakers":"Ayan Sengupta","title":"DATAMAFIA at WNUT-2020 Task 2: A Study of Pre-trained Language Models along with Regularization Techniques for Downstream Tasks"},{"content":{"abstract":"Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic of infectious diseases that are informative in nature also span various topics such as news, politics and humor which makes the data mining challenging. We present a system to identify tweets about the COVID19 disease outbreak that are deemed to be informative on Twitter for use in downstream applications. The system scored a F1-score of 0.8941, Precision of 0.9028, Recall of 0.8856 and Accuracy of 0.9010. In the shared task organized as part of the 6th Workshop of Noisy User-generated Text (WNUT), the system was ranked 18th by F1-score and 13th by Accuracy.","authors":["Arjun Magge","Varad Pimpalkhute","Divya Rallapalli","David Siguenza","Graciela Gonzalez-Hernandez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.52","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UPennHLP at WNUT-2020 Task 2 : Transformer models for classification of COVID19 posts on Twitter","tldr":"Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.52","presentation_id":"","rocketchat_channel":"paper-wnut2020-52","speakers":"Arjun Magge|Varad Pimpalkhute|Divya Rallapalli|David Siguenza|Graciela Gonzalez-Hernandez","title":"UPennHLP at WNUT-2020 Task 2 : Transformer models for classification of COVID19 posts on Twitter"},{"content":{"abstract":"Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94% with the third place on the leaderboard of this task which attracted 56 submitted teams in total.","authors":["Khiem Tran","Hao Phan","Kiet Nguyen","Ngan Luu Thuy Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19 Information on the Twitter Social Network","tldr":"Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.53","presentation_id":"","rocketchat_channel":"paper-wnut2020-53","speakers":"Khiem Tran|Hao Phan|Kiet Nguyen|Ngan Luu Thuy Nguyen","title":"UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19 Information on the Twitter Social Network"},{"content":{"abstract":"This paper describes the system developed by the Emory team for the WNUT-2020 Task 2: \u201cIdentifi- cation of Informative COVID-19 English Tweet\u201d. Our system explores three recent Transformer- based deep learning models pretrained on large- scale data to encode documents. Moreover, we developed two feature enrichment methods to en- hance document embeddings by integrating emoji embeddings and syntactic features into deep learn- ing models. Our system achieved F1-score of 0.897 and accuracy of 90.1% on the test set, and ranked in the top-third of all 55 teams.","authors":["Yuting Guo","Mohammed Ali Al-Garadi","Abeed Sarker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.54","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emory at WNUT-2020 Task 2: Combining Pretrained Deep Learning Models and Feature Enrichment for Informative Tweet Identification","tldr":"This paper describes the system developed by the Emory team for the WNUT-2020 Task 2: \u201cIdentifi- cation of Informative COVID-19 English Tweet\u201d. Our system explores three recent Transformer- based deep learning models pretrained on large- scale data t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.54","presentation_id":"","rocketchat_channel":"paper-wnut2020-54","speakers":"Yuting Guo|Mohammed Ali Al-Garadi|Abeed Sarker","title":"Emory at WNUT-2020 Task 2: Combining Pretrained Deep Learning Models and Feature Enrichment for Informative Tweet Identification"},{"content":{"abstract":"COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of twitter makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this task. We propose a neural model that adopts the strength of transfer learning and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7% (approx.).","authors":["Fareen Tasneem","Jannatun Naim","Radiathun Tasnia","Tashin Hossain","Abu Nowshed Chy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.55","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CSECU-DSG at WNUT-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets","tldr":"COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.55","presentation_id":"","rocketchat_channel":"paper-wnut2020-55","speakers":"Fareen Tasneem|Jannatun Naim|Radiathun Tasnia|Tashin Hossain|Abu Nowshed Chy","title":"CSECU-DSG at WNUT-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks: DistilBERT and FastText. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.","authors":["Supriya Chanda","Eshita Nandy","Sukomal Pal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.56","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IRLab@IITBHU at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets using BERT","tldr":"This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.56","presentation_id":"","rocketchat_channel":"paper-wnut2020-56","speakers":"Supriya Chanda|Eshita Nandy|Sukomal Pal","title":"IRLab@IITBHU at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets using BERT"},{"content":{"abstract":"We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task 2 and ranks 1st on the leaderboard. The ensemble of the models trained using adversarial training also produces similar result.","authors":["Priyanshu Kumar","Aadarsh Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.57","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training","tldr":"We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.57","presentation_id":"","rocketchat_channel":"paper-wnut2020-57","speakers":"Priyanshu Kumar|Aadarsh Singh","title":"NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training"},{"content":{"abstract":"Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our model on a public dataset with an F1-score of 0.89 on the validation dataset and 0.87 on the leaderboard.","authors":["Sirigireddy Dhana Laxmi","Rohit Agarwal","Aman Sinha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.58","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa","tldr":"Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we presen...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.58","presentation_id":"","rocketchat_channel":"paper-wnut2020-58","speakers":"Sirigireddy Dhana Laxmi|Rohit Agarwal|Aman Sinha","title":"DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa"},{"content":{"abstract":"Since the outbreak of COVID-19, there has been a surge of digital content on social media. The content ranges from news articles, academic reports, tweets, videos, and even memes. Among such an overabundance of data, it is crucial to distinguish which information is actually informative or merely sensational, redundant or false. This work focuses on developing such a language system that can differentiate between Informative or Uninformative tweets associated with COVID-19 for WNUT-2020 Shared Task 2. For this purpose, we employ deep transfer learning models such as BERT along other techniques such as Noisy Data Augmentation and Progress Training. The approach achieves a competitive F1-score of 0.8715 on the final testing dataset.","authors":["Vasudev Awatramani","Anupam Kumar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.59","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguist Geeks on WNUT-2020 Task 2: COVID-19 Informative Tweet Identification using Progressive Trained Language Models and Data Augmentation","tldr":"Since the outbreak of COVID-19, there has been a surge of digital content on social media. The content ranges from news articles, academic reports, tweets, videos, and even memes. Among such an overabundance of data, it is crucial to distinguish whic...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.59","presentation_id":"","rocketchat_channel":"paper-wnut2020-59","speakers":"Vasudev Awatramani|Anupam Kumar","title":"Linguist Geeks on WNUT-2020 Task 2: COVID-19 Informative Tweet Identification using Progressive Trained Language Models and Data Augmentation"},{"content":{"abstract":"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85% and 78.54% as F1-score on the provided test dataset. The experimental code is available online.","authors":["Rajesh Kumar Mundotiya","Rupjyoti Baruah","Bhavana Srivastava","Anil Kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.60","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLPRL at WNUT-2020 Task 2: ELMo-based System for Identification of COVID-19 Tweets","tldr":"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.60","presentation_id":"","rocketchat_channel":"paper-wnut2020-60","speakers":"Rajesh Kumar Mundotiya|Rupjyoti Baruah|Bhavana Srivastava|Anil Kumar Singh","title":"NLPRL at WNUT-2020 Task 2: ELMo-based System for Identification of COVID-19 Tweets"},{"content":{"abstract":"In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classification performance of classification models such as BERT and CNN. We show that ensembling can reduce the variance in performance, specifically for BERT base models.","authors":["Kenan Fayoumi","Reyyan Yeniterzi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.61","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SU-NLP at WNUT-2020 Task 2: The Ensemble Models","tldr":"In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classif...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.61","presentation_id":"","rocketchat_channel":"paper-wnut2020-61","speakers":"Kenan Fayoumi|Reyyan Yeniterzi","title":"SU-NLP at WNUT-2020 Task 2: The Ensemble Models"},{"content":{"abstract":"We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.","authors":["Sora Ohashi","Tomoyuki Kajiwara","Chenhui Chu","Noriko Takemura","Yuta Nakashima","Hajime Nagahara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.62","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IDSOU at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets","tldr":"We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score....","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.62","presentation_id":"","rocketchat_channel":"paper-wnut2020-62","speakers":"Sora Ohashi|Tomoyuki Kajiwara|Chenhui Chu|Noriko Takemura|Yuta Nakashima|Hajime Nagahara","title":"IDSOU at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as graph classification, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.","authors":["Kellin Pelrine","Jacob Danovitch","Albert Orozco Camacho","Reihaneh Rabbany"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.63","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ComplexDataLab at W-NUT 2020 Task 2: Detecting Informative COVID-19 Tweets by Attending over Linked Documents","tldr":"Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as graph cl...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.63","presentation_id":"","rocketchat_channel":"paper-wnut2020-63","speakers":"Kellin Pelrine|Jacob Danovitch|Albert Orozco Camacho|Reihaneh Rabbany","title":"ComplexDataLab at W-NUT 2020 Task 2: Detecting Informative COVID-19 Tweets by Attending over Linked Documents"},{"content":{"abstract":"Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in finding relevant information. In this paper, we present a BERT classifier system for W-NUT2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Further, we show that BERT exploits some easy signals to identify informative tweets, and adding simple patterns to uninformative tweets drastically degrades BERT performance. In particular, simply adding \u201c10 deaths\u201d to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also propose a simple data augmentation technique that helps in improving the robustness and generalization ability of the BERT classifier.","authors":["Kumud Chauhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.64","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative","tldr":"Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in find...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.64","presentation_id":"","rocketchat_channel":"paper-wnut2020-64","speakers":"Kumud Chauhan","title":"NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative"},{"content":{"abstract":"In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.","authors":["Abhilasha Sancheti","Kushal Chawla","Gaurav Verma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.65","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets","tldr":"In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.65","presentation_id":"","rocketchat_channel":"paper-wnut2020-65","speakers":"Abhilasha Sancheti|Kushal Chawla|Gaurav Verma","title":"LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (novel coronavirus) or not. These informative tweets provide information about recovered, confirmed, suspected, and death cases as well as location or travel history of the cases. The proposed approach includes pre-processing techniques and pre-trained RoBERTa with suitable hyperparameters for English coronavirus tweet classification. The performance achieved by the proposed model for shared task WNUT 2020 Task2 is 89.14% in the F1-score metric.","authors":["Jagadeesh M S","Alphonse P J A"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.66","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets","tldr":"This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (n...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.66","presentation_id":"","rocketchat_channel":"paper-wnut2020-66","speakers":"Jagadeesh M S|Alphonse P J A","title":"NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets"},{"content":{"abstract":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.","authors":["Nickil Maveli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.67","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets","tldr":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically mo...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.67","presentation_id":"","rocketchat_channel":"paper-wnut2020-67","speakers":"Nickil Maveli","title":"EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets"},{"content":{"abstract":"In this system paper, we present a transformer-based approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recovery, suspected and confirmed cases and COVID-19 related deaths, from uninformative tweets. We present two transformer-based approaches as well as a Naive Bayes classifier and a support vector machine as baseline systems. The transformer models outperform the baselines by more than 0.1 in F1-score, with F1-scores of 0.9091 and 0.9036. Our models were submitted to the shared task Identification of informative COVID-19 English tweets WNUT-2020 Task 2.","authors":["Hanna Varachkina","Stefan Ziehe","Tillmann D\u00f6nicke","Franziska Pannach"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.68","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"#GCDH at WNUT-2020 Task 2: BERT-Based Models for the Detection of Informativeness in English COVID-19 Related Tweets","tldr":"In this system paper, we present a transformer-based approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recover...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.68","presentation_id":"","rocketchat_channel":"paper-wnut2020-68","speakers":"Hanna Varachkina|Stefan Ziehe|Tillmann D\u00f6nicke|Franziska Pannach","title":"#GCDH at WNUT-2020 Task 2: BERT-Based Models for the Detection of Informativeness in English COVID-19 Related Tweets"},{"content":{"abstract":"As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people\u2019s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overshadows the more informative ones. In response to such, we proposed a model that, given an English tweet, automatically identifies whether that tweet bears informative content regarding COVID-19 or not. By ensembling different BERTweet model configurations, we have achieved competitive results that are only shy of those by top performing teams by roughly 1% in terms of F1 score on the informative class. In the post-competition period, we have also experimented with various other approaches that potentially boost generalization to a new dataset.","authors":["Thai Hoang","Phuong Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.69","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Not-NUTs at WNUT-2020 Task 2: A BERT-based System in Identifying Informative COVID-19 English Tweets","tldr":"As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people\u2019s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overs...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.69","presentation_id":"","rocketchat_channel":"paper-wnut2020-69","speakers":"Thai Hoang|Phuong Vu","title":"Not-NUTs at WNUT-2020 Task 2: A BERT-based System in Identifying Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper presents our models for WNUT2020 shared task2. The shared task2 involves identification of COVID-19 related informative tweets. We treat this as binary text clas-sification problem and experiment with pre-trained language models. Our first model which is based on CT-BERT achieves F1-scoreof 88.7% and second model which is an ensemble of CT-BERT, RoBERTa and SVM achieves F1-score of 88.52%.","authors":["Yandrapati Prakash Babu","Rajagopal Eswari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.70","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using Pre-trained Language Models","tldr":"This paper presents our models for WNUT2020 shared task2. The shared task2 involves identification of COVID-19 related informative tweets. We treat this as binary text clas-sification problem and experiment with pre-trained language models. Our first...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.70","presentation_id":"","rocketchat_channel":"paper-wnut2020-70","speakers":"Yandrapati Prakash Babu|Rajagopal Eswari","title":"CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using Pre-trained Language Models"},{"content":{"abstract":"This paper reports our approach and the results of our experiments for W-NUT task 2: Identification of Informative COVID-19 English Tweets. In this paper, we test out the effectiveness of transfer learning method with state of the art language models as RoBERTa on this text classification task. Moreover, we examine the benefit of applying additional fine-tuning and training techniques including fine-tuning discrimination, gradual unfreezing as well as our custom head for the classifier. Our best model results in a high F1-score of 89.89 on the task\u2019s private test dataset and that of 90.96 on public test set without ensembling multiple models and additional data.","authors":["Huy Dao Quang","Tam Nguyen Minh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.71","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UET at WNUT-2020 Task 2: A Study of Combining Transfer Learning Methods for Text Classification with RoBERTa","tldr":"This paper reports our approach and the results of our experiments for W-NUT task 2: Identification of Informative COVID-19 English Tweets. In this paper, we test out the effectiveness of transfer learning method with state of the art language models...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.71","presentation_id":"","rocketchat_channel":"paper-wnut2020-71","speakers":"Huy Dao Quang|Tam Nguyen Minh","title":"UET at WNUT-2020 Task 2: A Study of Combining Transfer Learning Methods for Text Classification with RoBERTa"},{"content":{"abstract":"We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT\u2019s performance in this classification task by fine-tuning BERT and concatenating its embeddings with Tweet-specific features and training a Support Vector Machine (SVM) for classification (henceforth called BERT+). We compared its performance to a suite of machine learning models. We used a Twitter specific data cleaning pipeline and word-level TF-IDF to extract features for the non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.","authors":["Dylan Whang","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.72","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dartmouth CS at WNUT-2020 Task 2: Fine tuning BERT for Tweet classification","tldr":"We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT\u2019s performance in this classification ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.72","presentation_id":"","rocketchat_channel":"paper-wnut2020-72","speakers":"Dylan Whang|Soroush Vosoughi","title":"Dartmouth CS at WNUT-2020 Task 2: Fine tuning BERT for Tweet classification"},{"content":{"abstract":"This paper proposes an improved custom model for WNUT task 2: Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model RoBERTa. We make a preliminary instantiation of this formal model for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 F1-score on public validation set and the ensemble version settles at top 9 F1-score (0.9005) and top 2 Recall (0.9301) on private test set.","authors":["Linh Doan Bao","Viet Anh Nguyen","Quang Pham Huu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.73","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SunBear at WNUT-2020 Task 2: Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain","tldr":"This paper proposes an improved custom model for WNUT task 2: Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model RoBERTa. We make a preli...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.73","presentation_id":"","rocketchat_channel":"paper-wnut2020-73","speakers":"Linh Doan Bao|Viet Anh Nguyen|Quang Pham Huu","title":"SunBear at WNUT-2020 Task 2: Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain"},{"content":{"abstract":"This paper presents Iswara\u2019s participation in the WNUT-2020 Task 2 \u201cIdentification of Informative COVID-19 English Tweets using BERT and FastText Embeddings\u201d,which tries to classify whether a certain tweet is considered informative or not. We proposed a method that utilizes word embeddings and using word occurrence related to the topic for this task. We compare several models to get the best performance. Results show that pairing BERT with word occurrences outperforms fastText with F1-Score, precision, recall, and accuracy on test data of 76%, 81%, 72%, and 79%, respectively","authors":["Wava Carissa Putri","Rani Aulia Hidayat","Isnaini Nurul Khasanah","Rahmad Mahendra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.74","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ISWARA at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings","tldr":"This paper presents Iswara\u2019s participation in the WNUT-2020 Task 2 \u201cIdentification of Informative COVID-19 English Tweets using BERT and FastText Embeddings\u201d,which tries to classify whether a certain tweet is considered informative or not. We propose...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.74","presentation_id":"","rocketchat_channel":"paper-wnut2020-74","speakers":"Wava Carissa Putri|Rani Aulia Hidayat|Isnaini Nurul Khasanah|Rahmad Mahendra","title":"ISWARA at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings"},{"content":{"abstract":"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.","authors":["Ali H\u00fcrriyeto\u011flu","Ali Safaya","Osman Mutlu","Nelleke Oostdijk","Erdem Y\u00f6r\u00fck"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.75","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules","tldr":"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically infor...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.75","presentation_id":"","rocketchat_channel":"paper-wnut2020-75","speakers":"Ali H\u00fcrriyeto\u011flu|Ali Safaya|Osman Mutlu|Nelleke Oostdijk|Erdem Y\u00f6r\u00fck","title":"COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules"},{"content":{"abstract":"The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he/she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro F1.","authors":["Chacha Chen","Chieh-Yang Huang","Yaqi Hou","Yang Shi","Enyan Dai","Jiaqi Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.76","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TEST_POSITIVE at W-NUT 2020 Shared Task-3: Cross-task modeling","tldr":"The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important ques...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.76","presentation_id":"","rocketchat_channel":"paper-wnut2020-76","speakers":"Chacha Chen|Chieh-Yang Huang|Yaqi Hou|Yang Shi|Enyan Dai|Jiaqi Wang","title":"TEST_POSITIVE at W-NUT 2020 Shared Task-3: Cross-task modeling"},{"content":{"abstract":"In this paper, we present our system designed to address the W-NUT 2020 shared task for COVID-19 Event Extraction from Twitter. To mitigate the noisy nature of the Twitter stream, our system makes use of the COVID-Twitter-BERT (CT-BERT), which is a language model pre-trained on a large corpus of COVID-19 related Twitter messages. Our system is trained on the COVID-19 Twitter Event Corpus and is able to identify relevant text spans that answer pre-defined questions (i.e., slot types) for five COVID-19 related events (i.e., TESTED POSITIVE, TESTED NEGATIVE, CAN-NOT-TEST, DEATH and CURE & PREVENTION). We have experimented with different architectures; our best performing model relies on a multilabel classifier on top of the CT-BERT model that jointly trains all the slot types for a single event. Our experimental results indicate that our Multilabel-CT-BERT system outperforms the baseline methods by 7 percentage points in terms of micro average F1 score. Our model ranked as 4th in the shared task leaderboard.","authors":["Xiangyu Yang","Giannis Bekoulis","Nikos Deligiannis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.77","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"imec-ETRO-VUB at W-NUT 2020 Shared Task-3: A multilabel BERT-based system for predicting COVID-19 events","tldr":"In this paper, we present our system designed to address the W-NUT 2020 shared task for COVID-19 Event Extraction from Twitter. To mitigate the noisy nature of the Twitter stream, our system makes use of the COVID-Twitter-BERT (CT-BERT), which is a l...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.77","presentation_id":"","rocketchat_channel":"paper-wnut2020-77","speakers":"Xiangyu Yang|Giannis Bekoulis|Nikos Deligiannis","title":"imec-ETRO-VUB at W-NUT 2020 Shared Task-3: A multilabel BERT-based system for predicting COVID-19 events"},{"content":{"abstract":"In this paper, we describe our approach in the shared task: COVID-19 event extraction from Twitter. The objective of this task is to extract answers from COVID-related tweets to a set of predefined slot-filling questions. Our approach treats the event extraction task as a question answering task by leveraging the transformer-based T5 text-to-text model. According to the official evaluation scores returned, namely F1, our submitted run achieves competitive performance compared to other participating runs (Top 3). However, we argue that this evaluation may underestimate the actual performance of runs based on text-generation. Although some such runs may answer the slot questions well, they may not be an exact string match for the gold standard answers. To measure the extent of this underestimation, we adopt a simple exact-answer transformation method aiming at converting the well-answered predictions to exactly-matched predictions. The results show that after this transformation our run overall reaches the same level of performance as the best participating run and state-of-the-art F1 scores in three of five COVID-related events. Our code is publicly available to aid reproducibility","authors":["Congcong Wang","David Lillis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.78","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19 Event Extraction on Social Media","tldr":"In this paper, we describe our approach in the shared task: COVID-19 event extraction from Twitter. The objective of this task is to extract answers from COVID-related tweets to a set of predefined slot-filling questions. Our approach treats the even...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.78","presentation_id":"","rocketchat_channel":"paper-wnut2020-78","speakers":"Congcong Wang|David Lillis","title":"UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19 Event Extraction on Social Media"},{"content":{"abstract":"Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks, while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-BERT with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leaderboard with F1 of 0.6598, without using any ensembles or additional datasets.","authors":["Ayush Kaushal","Tejas Vaidhya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.79","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Winners at W-NUT 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting COVID Entities from Tweets","tldr":"Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction o...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.79","presentation_id":"","rocketchat_channel":"paper-wnut2020-79","speakers":"Ayush Kaushal|Tejas Vaidhya","title":"Winners at W-NUT 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting COVID Entities from Tweets"},{"content":{"abstract":"Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from Twitter has the potential to inform surveillance systems that play a critical role in public health. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This architecture uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of Hopfield Networks, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while it remains competitive when training on smaller datasets.","authors":["Maxwell Weinzierl","Sanda Harabagiu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.80","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HLTRI at W-NUT 2020 Shared Task-3: COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling","tldr":"Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from Twitter has the potential to inform surveillance systems that play a critical role in public health. The event extraction challenge presented by the ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.80","presentation_id":"","rocketchat_channel":"paper-wnut2020-80","speakers":"Maxwell Weinzierl|Sanda Harabagiu","title":"HLTRI at W-NUT 2020 Shared Task-3: COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling"},{"content":{"abstract":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.","authors":["Ali Akbar Septiandri","Yosef Ardhito Winatmoko","Ilham Firdausi Putra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?","tldr":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent develo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1","presentation_id":"38939419","rocketchat_channel":"paper-sustainlp2020-1","speakers":"Ali Akbar Septiandri|Yosef Ardhito Winatmoko|Ilham Firdausi Putra","title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?"},{"content":{"abstract":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but they are seldom available and expensive to hire. This has lead to the thriving of crowdsourcing platforms such as Amazon Mechanical Turk (AMT). However, the annotations provided by one worker cannot be used directly to train the model due to the lack of expertise. Existing literature in annotation aggregation focuses on binary and multi-choice problems. In contrast, little work has been done on complex tasks such as sequence labeling with imbalanced classes, a ubiquitous task in Natural Language Processing (NLP), and Bio-Informatics. We propose OptSLA, an Optimization-based Sequential Label Aggregation method, that jointly considers the characteristics of sequential labeling tasks, workers reliabilities, and advanced deep learning techniques to conquer the challenge. We evaluate our model on crowdsourced data for named entity recognition task. Our results show that the proposed OptSLA outperforms the state-of-the-art aggregation methods, and the results are easier to interpret.","authors":["Nasim Sabetpour","Adithya Kulkarni","Qi Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.119","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation","tldr":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but th...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1098","presentation_id":"38940107","rocketchat_channel":"paper-sustainlp2020-1098","speakers":"Nasim Sabetpour|Adithya Kulkarni|Qi Li","title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation"},{"content":{"abstract":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.","authors":["Moshe Wasserblat","Oren Pereg","Peter Izsak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring the Boundaries of Low-Resource BERT Distillation","tldr":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.12","presentation_id":"38939426","rocketchat_channel":"paper-sustainlp2020-12","speakers":"Moshe Wasserblat|Oren Pereg|Peter Izsak","title":"Exploring the Boundaries of Low-Resource BERT Distillation"},{"content":{"abstract":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT - BERT F1 delta, at 5% of BioBERT\u2019s CO 2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.134","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","tldr":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1286","presentation_id":"38940121","rocketchat_channel":"paper-sustainlp2020-1286","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"content":{"abstract":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.","authors":["Sosuke Kobayashi","Sho Yokoi","Jun Suzuki","Kentaro Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Estimation of Influence of a Training Instance","tldr":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.13","presentation_id":"38939427","rocketchat_channel":"paper-sustainlp2020-13","speakers":"Sosuke Kobayashi|Sho Yokoi|Jun Suzuki|Kentaro Inui","title":"Efficient Estimation of Influence of a Training Instance"},{"content":{"abstract":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.","authors":["Yi-Te Hsu","Sarthak Garg","Yi-Hsiu Liao","Ilya Chatsviorkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Inference For Neural Machine Translation","tldr":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.14","presentation_id":"38939429","rocketchat_channel":"paper-sustainlp2020-14","speakers":"Yi-Te Hsu|Sarthak Garg|Yi-Hsiu Liao|Ilya Chatsviorkin","title":"Efficient Inference For Neural Machine Translation"},{"content":{"abstract":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations \u2013 a main performance bottleneck. Consequently, our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets.","authors":["Yatin Chaudhary","Pankaj Gupta","Khushbu Saxena","Vivek Kulkarni","Thomas Runkler","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.152","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TopicBERT for Energy Efficient Document Classification","tldr":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1418","presentation_id":"38940122","rocketchat_channel":"paper-sustainlp2020-1418","speakers":"Yatin Chaudhary|Pankaj Gupta|Khushbu Saxena|Vivek Kulkarni|Thomas Runkler|Hinrich Sch\u00fctze","title":"TopicBERT for Energy Efficient Document Classification"},{"content":{"abstract":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in F1 that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both PyTorch and TensorFlow.","authors":["Brian Lester","Daniel Pressel","Amy Hemmeter","Sagnik Ray Choudhury","Srinivas Bangalore"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.166","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers","tldr":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. C...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1537","presentation_id":"38940105","rocketchat_channel":"paper-sustainlp2020-1537","speakers":"Brian Lester|Daniel Pressel|Amy Hemmeter|Sagnik Ray Choudhury|Srinivas Bangalore","title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers"},{"content":{"abstract":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.","authors":["Alicia Tsai","Laurent El Ghaoui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm","tldr":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrai...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.17","presentation_id":"38939430","rocketchat_channel":"paper-sustainlp2020-17","speakers":"Alicia Tsai|Laurent El Ghaoui","title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm"},{"content":{"abstract":"","authors":["Kunal Chawla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1887","presentation_id":"38940140","rocketchat_channel":"paper-sustainlp2020-1887","speakers":"Kunal Chawla","title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization"},{"content":{"abstract":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization (HMF) that achieves this dual objective. HMF improves low-rank matrix factorization (LMF) techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-timethan pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks (Translation, Intent Detection,Language Modeling) and show that for similar accuracy values and compression factors, HMF can achieve more than 2.32x faster inference run-time than pruning and 16.77% better accuracy than LMF.","authors":["Urmish Thakker","Jesse Beu","Dibakar Gope","Ganesh Dasika","Matthew Mattina"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Rank and run-time aware compression of NLP Applications","tldr":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2","presentation_id":"38939420","rocketchat_channel":"paper-sustainlp2020-2","speakers":"Urmish Thakker|Jesse Beu|Dibakar Gope|Ganesh Dasika|Matthew Mattina","title":"Rank and run-time aware compression of NLP Applications"},{"content":{"abstract":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.","authors":["Jiezhong Qiu","Hao Ma","Omer Levy","Wen-tau Yih","Sinong Wang","Jie Tang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.232","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Blockwise Self-Attention for Long Document Understanding","tldr":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/infere...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2015","presentation_id":"38940119","rocketchat_channel":"paper-sustainlp2020-2015","speakers":"Jiezhong Qiu|Hao Ma|Omer Levy|Wen-tau Yih|Sinong Wang|Jie Tang","title":"Blockwise Self-Attention for Long Document Understanding"},{"content":{"abstract":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of severe information loss. In this paper, we present a simple but effective unsupervised neural generative semantic hashing method with a focus on few-bits hashing. Our model is built upon variational autoencoder and represents each hash bit as a Bernoulli variable, which allows the model to be end-to-end trainable. To address the issue of information loss, we introduce a set of auxiliary implicit topic vectors. With the aid of these topic vectors, the generated hash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves significant improvements over state-of-the-art semantic hashing methods in few-bits hashing.","authors":["Fanghua Ye","Jarana Manotumruksa","Emine Yilmaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.233","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling","tldr":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guarante...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2017","presentation_id":"38940106","rocketchat_channel":"paper-sustainlp2020-2017","speakers":"Fanghua Ye|Jarana Manotumruksa|Emine Yilmaz","title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling"},{"content":{"abstract":"","authors":["Jiecao Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2182","presentation_id":"38940104","rocketchat_channel":"paper-sustainlp2020-2182","speakers":"Jiecao Chen","title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling"},{"content":{"abstract":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","authors":["Yuxiang Wu","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering","tldr":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have show...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.22","presentation_id":"38939431","rocketchat_channel":"paper-sustainlp2020-22","speakers":"Yuxiang Wu|Pasquale Minervini|Pontus Stenetorp|Sebastian Riedel","title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering"},{"content":{"abstract":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational cost during inference can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an encoder and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this encoder, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher accuracy and lower computational cost once the system is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the computational cost when multiple tasks are performed on the same text.","authors":["Jingfei Du","Myle Ott","Haoran Li","Xing Zhou","Veselin Stoyanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.271","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference","tldr":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a sin...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2230","presentation_id":"38940109","rocketchat_channel":"paper-sustainlp2020-2230","speakers":"Jingfei Du|Myle Ott|Haoran Li|Xing Zhou|Veselin Stoyanov","title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference"},{"content":{"abstract":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.","authors":["Giorgos Vernikos","Katerina Margatina","Alexandra Chronopoulou","Ion Androutsopoulos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.278","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain Adversarial Fine-Tuning as an Effective Regularizer","tldr":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2288","presentation_id":"38940129","rocketchat_channel":"paper-sustainlp2020-2288","speakers":"Giorgos Vernikos|Katerina Margatina|Alexandra Chronopoulou|Ion Androutsopoulos","title":"Domain Adversarial Fine-Tuning as an Effective Regularizer"},{"content":{"abstract":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.","authors":["Zhiheng Huang","Davis Liang","Peng Xu","Bing Xiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.298","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improve Transformer Models with Better Relative Position Embeddings","tldr":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2453","presentation_id":"38940108","rocketchat_channel":"paper-sustainlp2020-2453","speakers":"Zhiheng Huang|Davis Liang|Peng Xu|Bing Xiang","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"content":{"abstract":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from \u201cgenuine\u201d ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.","authors":["Zhao Wang","Aron Culotta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.308","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Spurious Correlations for Robust Text Classification","tldr":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we pr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2516","presentation_id":"38940117","rocketchat_channel":"paper-sustainlp2020-2516","speakers":"Zhao Wang|Aron Culotta","title":"Identifying Spurious Correlations for Robust Text Classification"},{"content":{"abstract":"","authors":["Urmish Thakker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Doped Structured Matrices for Extreme Compression of LSTM Models","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.27","presentation_id":"38940744","rocketchat_channel":"paper-sustainlp2020-27","speakers":"Urmish Thakker","title":"Doped Structured Matrices for Extreme Compression of LSTM Models"},{"content":{"abstract":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.","authors":["Cennet Oguz","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings","tldr":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.28","presentation_id":"38939432","rocketchat_channel":"paper-sustainlp2020-28","speakers":"Cennet Oguz|Ngoc Thang Vu","title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings"},{"content":{"abstract":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.","authors":["Ji Xin","Rodrigo Nogueira","Yaoliang Yu","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Early Exiting BERT for Efficient Document Ranking","tldr":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for d...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.29","presentation_id":"38939433","rocketchat_channel":"paper-sustainlp2020-29","speakers":"Ji Xin|Rodrigo Nogueira|Yaoliang Yu|Jimmy Lin","title":"Early Exiting BERT for Efficient Document Ranking"},{"content":{"abstract":"","authors":["Patrick Xia","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incremental Neural Coreference Resolution in Constant Memory","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3","presentation_id":"38939421","rocketchat_channel":"paper-sustainlp2020-3","speakers":"Patrick Xia|Jo\u00e3o Sedoc|Benjamin Van Durme","title":"Incremental Neural Coreference Resolution in Constant Memory"},{"content":{"abstract":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of these approaches still need to be trained on very large datasets. In this paper, we introduce BeGanKP, a new conditional GAN model to address the problem of Keyphrase Generation in a low-resource scenario. Our main contribution relies in the Discriminator\u2019s architecture: a new BERT-based module which is able to distinguish between the generated and humancurated KPs reliably. Its characteristics allow us to use it in a low-resource scenario, where only a small amount of training data are available, obtaining an efficient Generator. The resulting architecture achieves, on five public datasets, competitive results with respect to the state-of-the-art approaches, using less than 1% of the training data.","authors":["Giuseppe Lancioni","Saida S.Mohamed","Beatrice Portelli","Giuseppe Serra","Carlo Tasso"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Keyphrase Generation with GANs in Low-Resources Scenarios","tldr":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.30","presentation_id":"38939434","rocketchat_channel":"paper-sustainlp2020-30","speakers":"Giuseppe Lancioni|Saida S.Mohamed|Beatrice Portelli|Giuseppe Serra|Carlo Tasso","title":"Keyphrase Generation with GANs in Low-Resources Scenarios"},{"content":{"abstract":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.","authors":["Umanga Bista","Alexander Mathews","Aditya Menon","Lexing Xie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.367","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy","tldr":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information pres...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3078","presentation_id":"38940131","rocketchat_channel":"paper-sustainlp2020-3078","speakers":"Umanga Bista|Alexander Mathews|Aditya Menon|Lexing Xie","title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy"},{"content":{"abstract":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we can get benefits similar to an ensemble of classifiers with a fraction of the resources required.We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique that encourages the model to develop an internal representation of the problem at hand which is beneficial to multiple output units of the classifier at the same time. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.","authors":["Norbert Kis-Szab\u00f3","G\u00e1bor Berend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles","tldr":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we c...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.32","presentation_id":"38939435","rocketchat_channel":"paper-sustainlp2020-32","speakers":"Norbert Kis-Szab\u00f3|G\u00e1bor Berend","title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles"},{"content":{"abstract":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that \u201csome\u201d labeled in-domain data can be worse than none at all.","authors":["Xinyu Zhang","Andrew Yates","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data","tldr":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.34","presentation_id":"38939436","rocketchat_channel":"paper-sustainlp2020-34","speakers":"Xinyu Zhang|Andrew Yates|Jimmy Lin","title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data"},{"content":{"abstract":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed model is able to generate fluent questions with high completeness and outperforms the strongest baseline by 20.8% in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU","authors":["Dan Su","Yan Xu","Wenliang Dai","Ziwei Ji","Tiezheng Yu","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.416","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-hop Question Generation with Graph Convolutional Network","tldr":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3444","presentation_id":"38940120","rocketchat_channel":"paper-sustainlp2020-3444","speakers":"Dan Su|Yan Xu|Wenliang Dai|Ziwei Ji|Tiezheng Yu|Pascale Fung","title":"Multi-hop Question Generation with Graph Convolutional Network"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3459","presentation_id":"38940124","rocketchat_channel":"paper-sustainlp2020-3459","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"","authors":["Rajarshi Das"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3526","presentation_id":"38940133","rocketchat_channel":"paper-sustainlp2020-3526","speakers":"Rajarshi Das","title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion"},{"content":{"abstract":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8\u00d7 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3\u00d7 reduction in run-time memory footprints and 3.5\u00d7 speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.","authors":["Insoo Chung","Byeongwook Kim","Yoonjung Choi","Se Jung Kwon","Yongkweon Jeon","Baeseong Park","Sangha Kim","Dongsoo Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.433","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation","tldr":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quan...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3562","presentation_id":"38940118","rocketchat_channel":"paper-sustainlp2020-3562","speakers":"Insoo Chung|Byeongwook Kim|Yoonjung Choi|Se Jung Kwon|Yongkweon Jeon|Baeseong Park|Sangha Kim|Dongsoo Lee","title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"},{"content":{"abstract":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that determines which existing source model would minimize error for transfer learning to a given target. This technique does not require learning for prediction, and avoids computational costs of trail-and-error. We have evaluated this technique on nine datasets across diverse domains, including newswire, user forums, air flight booking, cybersecurity news, etc. We show that it per-forms better than existing techniques such as fine-tuning over vanilla BERT, or curriculum learning over the largest dataset on top of BERT, resulting in average F1 score gains in excess of 3%. Moreover, our technique consistently selects the best model using fewer tries.","authors":["Parul Awasthy","Bishwaranjan Bhattacharjee","John Kender","Radu Florian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks","tldr":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that dete...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.36","presentation_id":"38939437","rocketchat_channel":"paper-sustainlp2020-36","speakers":"Parul Awasthy|Bishwaranjan Bhattacharjee|John Kender|Radu Florian","title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks"},{"content":{"abstract":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.","authors":["Julian Eisenschlos","Syrine Krichene","Thomas M\u00fcller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding tables with intermediate pre-training","tldr":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on t...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.361","presentation_id":"38940134","rocketchat_channel":"paper-sustainlp2020-361","speakers":"Julian Eisenschlos|Syrine Krichene|Thomas M\u00fcller","title":"Understanding tables with intermediate pre-training"},{"content":{"abstract":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.","authors":["Amine Abdaoui","Camille Pradel","Gr\u00e9goire Sigel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Load What You Need: Smaller Versions of Mutlilingual BERT","tldr":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.37","presentation_id":"38939438","rocketchat_channel":"paper-sustainlp2020-37","speakers":"Amine Abdaoui|Camille Pradel|Gr\u00e9goire Sigel","title":"Load What You Need: Smaller Versions of Mutlilingual BERT"},{"content":{"abstract":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert","authors":["Forrest Iandola","Albert Shaw","Ravi Krishna","Kurt Keutzer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?","tldr":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.38","presentation_id":"38939439","rocketchat_channel":"paper-sustainlp2020-38","speakers":"Forrest Iandola|Albert Shaw|Ravi Krishna|Kurt Keutzer","title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"},{"content":{"abstract":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification. However, it lacks sufficient contextual information enhancement and thus is less useful for sequence labelling tasks such as chunking and named entity recognition (NER). To address this limitation, we propose masked adversarial training (MAT) to improve robustness from contextual information in sequence labelling. MAT masks or replaces some words in the sentence when computing adversarial loss from perturbed inputs and consequently enhances model robustness using more context-level information. In our experiments, our method shows significant improvements on accuracy and robustness of sequence labelling. By further incorporating with ELMo embeddings, our model achieves better or comparable results to state-of-the-art on CoNLL 2000 and 2003 benchmarks using much less parameters.","authors":["Luoxin Chen","Xinyue Liu","Weitong Ruan","Jianhua Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training","tldr":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input n...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.381","presentation_id":"38940127","rocketchat_channel":"paper-sustainlp2020-381","speakers":"Luoxin Chen|Xinyue Liu|Weitong Ruan|Jianhua Lu","title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training"},{"content":{"abstract":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource constraint devices. These models try to minimize resource requirements like RAM and storage without hurting the accuracy much. We utilized these models on multiple benchmark natural language processing tasks, which were sentimental analysis, spam message detection, emotion analysis and fake news classification. The experiment results shows that the tree-based algorithm, Bonsai, surpassed the rest of the machine learning algorithms by achieve higher accuracy scores while having significantly lower memory usage.","authors":["Raj Pranesh","Ambesh Shekhar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing","tldr":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource cons...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.39","presentation_id":"38939440","rocketchat_channel":"paper-sustainlp2020-39","speakers":"Raj Pranesh|Ambesh Shekhar","title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing"},{"content":{"abstract":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.","authors":["Qingqing Cao","Aruna Balasubramanian","Niranjan Balasubramanian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Accurate and Reliable Energy Measurement of NLP Models","tldr":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate beca...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.42","presentation_id":"38939441","rocketchat_channel":"paper-sustainlp2020-42","speakers":"Qingqing Cao|Aruna Balasubramanian|Niranjan Balasubramanian","title":"Towards Accurate and Reliable Energy Measurement of NLP Models"},{"content":{"abstract":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.","authors":["Young Jin Kim","Hany Hassan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding","tldr":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficien...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.43","presentation_id":"38939442","rocketchat_channel":"paper-sustainlp2020-43","speakers":"Young Jin Kim|Hany Hassan","title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"},{"content":{"abstract":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.","authors":["Ariadna Quattoni","Xavier Carreras"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A comparison between CNNs and WFAs for Sequence Classification","tldr":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.45","presentation_id":"38939443","rocketchat_channel":"paper-sustainlp2020-45","speakers":"Ariadna Quattoni|Xavier Carreras","title":"A comparison between CNNs and WFAs for Sequence Classification"},{"content":{"abstract":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of pseudo-positive labels. We validate that the effectiveness of augmented labels are comparable to positives, such that ours outperform state-of-the-arts without augmentation.","authors":["Seungtaek Choi","Myeongho Jeong","Jinyoung Yeo","Seung-won Hwang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactual Augmentation for Training Next Response Selection","tldr":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.46","presentation_id":"38939444","rocketchat_channel":"paper-sustainlp2020-46","speakers":"Seungtaek Choi|Myeongho Jeong|Jinyoung Yeo|Seung-won Hwang","title":"Counterfactual Augmentation for Training Next Response Selection"},{"content":{"abstract":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as \u2018Entailment\u2019, \u2018Neutral\u2019, and \u2018Contradiction\u2019 for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2% data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.","authors":["Swaroop Mishra","Bhavdeep Singh Sachdeva"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Need to Create Big Datasets to Learn a Task?","tldr":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.47","presentation_id":"38939445","rocketchat_channel":"paper-sustainlp2020-47","speakers":"Swaroop Mishra|Bhavdeep Singh Sachdeva","title":"Do We Need to Create Big Datasets to Learn a Task?"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.49","presentation_id":"38939446","rocketchat_channel":"paper-sustainlp2020-49","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.","authors":["Harshil Shah","Julien Fauqueur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models","tldr":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.5","presentation_id":"38939422","rocketchat_channel":"paper-sustainlp2020-5","speakers":"Harshil Shah|Julien Fauqueur","title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models"},{"content":{"abstract":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed \u2013 non-learnable \u2013 attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.","authors":["Alessandro Raganato","Yves Scherrer","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation","tldr":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.512","presentation_id":"38940110","rocketchat_channel":"paper-sustainlp2020-512","speakers":"Alessandro Raganato|Yves Scherrer|J\u00f6rg Tiedemann","title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"},{"content":{"abstract":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.","authors":["Zhao Jinman","Shawn Zhong","Xiaomin Zhang","Yingyu Liang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding","tldr":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.547","presentation_id":"38940115","rocketchat_channel":"paper-sustainlp2020-547","speakers":"Zhao Jinman|Shawn Zhong|Xiaomin Zhang|Yingyu Liang","title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding"},{"content":{"abstract":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.","authors":["Kumar Shridhar","Harshil Jain","Akshat Agarwal","Denis Kleyko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End to End Binarized Neural Networks for Text Classification","tldr":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.6","presentation_id":"38939423","rocketchat_channel":"paper-sustainlp2020-6","speakers":"Kumar Shridhar|Harshil Jain|Akshat Agarwal|Denis Kleyko","title":"End to End Binarized Neural Networks for Text Classification"},{"content":{"abstract":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.","authors":["Zi Lin","Jeremiah Liu","Zi Yang","Nan Hua","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.64","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior","tldr":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which p...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.651","presentation_id":"38940112","rocketchat_channel":"paper-sustainlp2020-651","speakers":"Zi Lin|Jeremiah Liu|Zi Yang|Nan Hua|Dan Roth","title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"},{"content":{"abstract":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.","authors":["Mingzhu Wu","Nafise Sadat Moosavi","Andreas R\u00fcckl\u00e9","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.74","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases","tldr":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge abo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.724","presentation_id":"38940113","rocketchat_channel":"paper-sustainlp2020-724","speakers":"Mingzhu Wu|Nafise Sadat Moosavi|Andreas R\u00fcckl\u00e9|Iryna Gurevych","title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases"},{"content":{"abstract":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of pretrained language models (PLMs), we investigate how to incorporate large PKM into PLMs that can be finetuned for a wide variety of downstream NLP tasks. We define a new memory usage metric, and careful observation using this metric reveals that most memory slots remain outdated during the training of PKM-augmented models. To train better PLMs by tackling this issue, we propose simple but effective solutions: (1) initialization from the model weights pretrained without memory and (2) augmenting PKM by addition rather than replacing a feed-forward network. We verify that both of them are crucial for the pretraining of PKM-augmented PLMs, enhancing memory utilization and downstream performance. Code and pretrained weights are available at https://github.com/clovaai/pkm-transformers.","authors":["Gyuwan Kim","Tae Hwan Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.362","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Product Key Memory for Pretrained Language Models","tldr":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal langua...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.8","presentation_id":"38939424","rocketchat_channel":"paper-sustainlp2020-8","speakers":"Gyuwan Kim|Tae Hwan Jung","title":"Large Product Key Memory for Pretrained Language Models"},{"content":{"abstract":"","authors":["Vivek Gupta","Ankit Saw","Pegah Nokhiz","Praneeth Netrapalli","Piyush Rai","Partha Talukdar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"P-SIF: Document Embeddings using Partition Averaging","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.9","presentation_id":"38939425","rocketchat_channel":"paper-sustainlp2020-9","speakers":"Vivek Gupta|Ankit Saw|Pegah Nokhiz|Praneeth Netrapalli|Piyush Rai|Partha Talukdar","title":"P-SIF: Document Embeddings using Partition Averaging"},{"content":{"abstract":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on domain-specific, labeled data. In this paper, we propose ESTeR , an unsupervised model for identifying emotions using a novel similarity function based on random walks on graphs. Our model combines large-scale word co-occurrence information with word-associations from lexicons avoiding not only the dependence on labeled datasets, but also an explicit mapping of words to latent spaces used in emotion-enriched word embeddings. Our similarity function can also be computed efficiently. We study a range of datasets including recent tweets related to COVID-19 to illustrate the superior performance of our model and report insights on public emotions during the on-going pandemic.","authors":["Sujatha Das Gollapalli","Polina Rozenshtein","See-Kiong Ng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.93","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection","tldr":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using comp...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.929","presentation_id":"38940135","rocketchat_channel":"paper-sustainlp2020-929","speakers":"Sujatha Das Gollapalli|Polina Rozenshtein|See-Kiong Ng","title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection"},{"content":{"abstract":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20\u00d7 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.","authors":["Alex Wang","Thomas Wolf"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview of the SustaiNLP 2020 Shared Task","tldr":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We des...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2020.sustainlp-1.24","presentation_id":"","rocketchat_channel":"paper-sustainlp2020-24","speakers":"Alex Wang|Thomas Wolf","title":"Overview of the SustaiNLP 2020 Shared Task"},{"content":{"abstract":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an action, and propose a composed variational natural language generator (CLANG), a transformer-based conditional variational autoencoder. CLANG utilizes two latent variables to represent the utterances corresponding to two different independent parts (domain and action) in the intent, and the latent variables are composed together to generate natural examples. Additionally, to improve the generator learning, we adopt the contrastive regularization loss that contrasts the in-class with the out-of-class utterance generation given the intent. To evaluate the quality of the generated utterances, experiments are conducted on the generalized few-shot intent detection task. Empirical results show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets.","authors":["Congying Xia","Caiming Xiong","Philip Yu","Richard Socher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.303","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Composed Variational Natural Language Generation for Few-shot Intents","tldr":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2487","presentation_id":"38940699","rocketchat_channel":"paper-codi2020-2487","speakers":"Congying Xia|Caiming Xiong|Philip Yu|Richard Socher","title":"Composed Variational Natural Language Generation for Few-shot Intents"},{"content":{"abstract":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model. Our results show that the span representation is able to encode a significant amount of coreference information. In addition, we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. Last, our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.","authors":["Patrick Kahardipraja","Olena Vyshnevska","Sharid Lo\u00e1iciga"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Span Representations in Neural Coreference Resolution","tldr":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a prob...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.10","presentation_id":"38939689","rocketchat_channel":"paper-codi2020-10","speakers":"Patrick Kahardipraja|Olena Vyshnevska|Sharid Lo\u00e1iciga","title":"Exploring Span Representations in Neural Coreference Resolution"},{"content":{"abstract":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience\u2019s response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a) text classification labels, indicating which actor\u2019s lines made the studio audience laugh; b) information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles.","authors":["Maolin Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts","tldr":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.11","presentation_id":"38939690","rocketchat_channel":"paper-codi2020-11","speakers":"Maolin Li","title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts"},{"content":{"abstract":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to abstract entities and events. These features are inspired through several sources \u2013 cognitive parameters, pragmatic factors and typological status. We pay attention to the distributions of these features in a dataset containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and feature selection to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for contrastive linguistics, translation studies and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.","authors":["Ekaterina Lapshinova-Koltunski","Kerstin Kunz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Coreference Features in Heterogeneous Data with Text Classification","tldr":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communica...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.13","presentation_id":"38939691","rocketchat_channel":"paper-codi2020-13","speakers":"Ekaterina Lapshinova-Koltunski|Kerstin Kunz","title":"Exploring Coreference Features in Heterogeneous Data with Text Classification"},{"content":{"abstract":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a discourse relation or not. We study the influence of those context-specific embeddings. Further, we show the benefit of training the tasks of connective disambiguation and sense classification together at the same time. The success of our approach is supported by state-of-the-art results.","authors":["Ren\u00e9 Knaebel","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing","tldr":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a disc...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.14","presentation_id":"38939692","rocketchat_channel":"paper-codi2020-14","speakers":"Ren\u00e9 Knaebel|Manfred Stede","title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing"},{"content":{"abstract":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in DSNDM is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.","authors":["Alexander Chernyavskiy","Dmitry Ilvovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking","tldr":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political stateme...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.15","presentation_id":"38939693","rocketchat_channel":"paper-codi2020-15","speakers":"Alexander Chernyavskiy|Dmitry Ilvovsky","title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking"},{"content":{"abstract":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.","authors":["Laurine Huber","Chaker Memmadi","Mathilde Dargnat","Yannick Toussaint"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?","tldr":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.17","presentation_id":"38939694","rocketchat_channel":"paper-codi2020-17","speakers":"Laurine Huber|Chaker Memmadi|Mathilde Dargnat|Yannick Toussaint","title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.18","presentation_id":"38939695","rocketchat_channel":"paper-codi2020-18","speakers":"Patrick Huber|Giuseppe Carenini","title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Discourse Treebanks from Scalable Distant Supervision","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.19","presentation_id":"38939696","rocketchat_channel":"paper-codi2020-19","speakers":"Patrick Huber|Giuseppe Carenini","title":"Large Discourse Treebanks from Scalable Distant Supervision"},{"content":{"abstract":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining different portions of OntoNotes with Twitter data show that selecting text genres for the training data can beat the mere maximization of training data amount. In addition, we inspect several phenomena such as the role of deictic pronouns in conversational data, and present additional results for variant settings. Our best configuration improves the performance of the\u201dout of the box\u201d system by 21.6%.","authors":["Berfin Akta\u015f","Veronika Solopova","Annalena Kohnert","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.222","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adapting Coreference Resolution to Twitter Conversations","tldr":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter con...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.1951","presentation_id":"38940697","rocketchat_channel":"paper-codi2020-1951","speakers":"Berfin Akta\u015f|Veronika Solopova|Annalena Kohnert|Manfred Stede","title":"Adapting Coreference Resolution to Twitter Conversations"},{"content":{"abstract":"","authors":["Diane Litman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse for Argument Mining, and Argument Mining as Discourse","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.20","presentation_id":"38939697","rocketchat_channel":"paper-codi2020-20","speakers":"Diane Litman","title":"Discourse for Argument Mining, and Argument Mining as Discourse"},{"content":{"abstract":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI love you.\u201d We designed a system to allow virtual assistants to take a voice message from one user, convert the point of view of the message, and then deliver the result to its target user. We developed a rule-based model, which integrates a linear text classification model, part-of-speech tagging, and constituency parsing with rule-based transformation methods. We also investigated Neural Machine Translation (NMT) approaches, including LSTMs, CopyNet, and T5. We explored 5 metrics to gauge both naturalness and faithfulness automatically, and we chose to use BLEU plus METEOR for faithfulness and relative perplexity using a separately trained language model (GPT) for naturalness. Transformer-Copynet and T5 performed similarly on faithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a METEOR score of 83.0. CopyNet was the most natural, with a relative perplexity of 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly released our dataset, which is composed of 46,565 crowd-sourced samples.","authors":["Gunhee Lee","Vera Zu","Sai Srujana Buddi","Dennis Liang","Purva Kulkarni","Jack FitzGerald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Converting the Point of View of Messages Spoken to Virtual Assistants","tldr":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI lov...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.208","presentation_id":"38940694","rocketchat_channel":"paper-codi2020-208","speakers":"Gunhee Lee|Vera Zu|Sai Srujana Buddi|Dennis Liang|Purva Kulkarni|Jack FitzGerald","title":"Converting the Point of View of Messages Spoken to Virtual Assistants"},{"content":{"abstract":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.","authors":["Yunmo Chen","Tongfei Chen","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Modeling of Arguments for Event Understanding","tldr":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach all...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.21","presentation_id":"38939698","rocketchat_channel":"paper-codi2020-21","speakers":"Yunmo Chen|Tongfei Chen|Benjamin Van Durme","title":"Joint Modeling of Arguments for Event Understanding"},{"content":{"abstract":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes semi-structured commonsense inference rules and learns to incorporate them in a transformer-based reasoning cell.We assess the model\u2019s performance on two tasks that require different reasoning skills: Abductive Natural Language Inference and Counterfactual Invariance Prediction as a new task. We show that our proposed model improves performance over strong state-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we are, to the best of our knowledge, the first to demonstrate that a model that learns to perform counterfactual reasoning helps predicting the best explanation in an abductive reasoning task. We validate the robustness of the model\u2019s reasoning capabilities by perturbing the knowledge and provide qualitative analysis on the model\u2019s knowledge incorporation capabilities.","authors":["Debjit Paul","Anette Frank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.267","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention","tldr":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes se...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2195","presentation_id":"38940698","rocketchat_channel":"paper-codi2020-2195","speakers":"Debjit Paul|Anette Frank","title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention"},{"content":{"abstract":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.","authors":["Youmna Farag","Josef Valvoda","Helen Yannakoudakis","Ted Briscoe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Neural Discourse Coherence Models","tldr":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensit...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.22","presentation_id":"38939699","rocketchat_channel":"paper-codi2020-22","speakers":"Youmna Farag|Josef Valvoda|Helen Yannakoudakis|Ted Briscoe","title":"Analyzing Neural Discourse Coherence Models"},{"content":{"abstract":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on a Multi-Layer Perceptron study and a Sequential Forward Search experiment, followed by Bayes Factor analysis of the outcomes. The results suggest that recency metrics counting paragraphs and sentences contribute to referential choice prediction more than other recency-related metrics. Based on the results of our analysis, we argue that, sensitivity to discourse structure is important for recency metrics used in determining referring expression forms.","authors":["Fahime Same","Kees van Deemter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse","tldr":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.23","presentation_id":"38939700","rocketchat_channel":"paper-codi2020-23","speakers":"Fahime Same|Kees van Deemter","title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse"},{"content":{"abstract":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \u201cSynthesizer\u201d framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.","authors":["Wen Xiao","Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !","tldr":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechani...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.24","presentation_id":"38939701","rocketchat_channel":"paper-codi2020-24","speakers":"Wen Xiao|Patrick Huber|Giuseppe Carenini","title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !"},{"content":{"abstract":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.","authors":["Li Liang","Zheng Zhao","Bonnie Webber"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extending Implicit Discourse Relation Recognition to the PDTB-3","tldr":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse r...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.26","presentation_id":"38939702","rocketchat_channel":"paper-codi2020-26","speakers":"Li Liang|Zheng Zhao|Bonnie Webber","title":"Extending Implicit Discourse Relation Recognition to the PDTB-3"},{"content":{"abstract":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.","authors":["Chenguang Zhu","Ruochen Xu","Michael Zeng","Xuedong Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining","tldr":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization in...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.263","presentation_id":"38940695","rocketchat_channel":"paper-codi2020-263","speakers":"Chenguang Zhu|Ruochen Xu|Michael Zeng|Xuedong Huang","title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining"},{"content":{"abstract":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En lexicon includes 95 entries, whereas the Tr-En lexicon contains 133 entries. The lexicons constitute the first step of a larger project of developing a multilingual discourse connective lexicon.","authors":["Murathan Kurfal\u0131","Sibel Ozer","Deniz Zeyrek","Am\u00e1lia Mendes"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex","tldr":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.27","presentation_id":"38939703","rocketchat_channel":"paper-codi2020-27","speakers":"Murathan Kurfal\u0131|Sibel Ozer|Deniz Zeyrek|Am\u00e1lia Mendes","title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex"},{"content":{"abstract":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects\u2014lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements.","authors":["Haixia Chai","Wei Zhao","Steffen Eger","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks","tldr":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.28","presentation_id":"38939704","rocketchat_channel":"paper-codi2020-28","speakers":"Haixia Chai|Wei Zhao|Steffen Eger|Michael Strube","title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks"},{"content":{"abstract":"","authors":["Belen Saldias","Deb Roy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.29","presentation_id":"38939705","rocketchat_channel":"paper-codi2020-29","speakers":"Belen Saldias|Deb Roy","title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types"},{"content":{"abstract":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, starting with a strong baseline neural parser unaware of any coreference information, we compare a parser which utilizes only the output of a neural coreference resolver, with a more sophisticated model, where discourse parsing and coreference resolution are jointly learned in a neural multitask fashion. Results indicate that these initial attempts to incorporate coreference information do not boost the performance of discourse parsing in a statistically significant way.","authors":["Grigorii Guz","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Coreference for Discourse Parsing: A Neural Approach","tldr":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, startin...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.31","presentation_id":"38939706","rocketchat_channel":"paper-codi2020-31","speakers":"Grigorii Guz|Giuseppe Carenini","title":"Coreference for Discourse Parsing: A Neural Approach"},{"content":{"abstract":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \\delta-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.","authors":["Rachel Rudinger","Vered Shwartz","Jena D. Hwang","Chandra Bhagavatula","Maxwell Forbes","Ronan Le Bras","Noah A. Smith","Yejin Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.418","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language","tldr":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference ha...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3452","presentation_id":"38940700","rocketchat_channel":"paper-codi2020-3452","speakers":"Rachel Rudinger|Vered Shwartz|Jena D. Hwang|Chandra Bhagavatula|Maxwell Forbes|Ronan Le Bras|Noah A. Smith|Yejin Choi","title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language"},{"content":{"abstract":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model\u2019s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other.","authors":["Yehudit Meged","Avi Caciularu","Vered Shwartz","Ido Dagan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.440","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin","tldr":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as dista...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3598","presentation_id":"38940701","rocketchat_channel":"paper-codi2020-3598","speakers":"Yehudit Meged|Avi Caciularu|Vered Shwartz|Ido Dagan","title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin"},{"content":{"abstract":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A comparative study for the language pair can discover the discourse differences between Spanish and Chinese, and can benefit the Spanish-Chinese translation. In this work, based on a Spanish-Chinese parallel corpus annotated with discourse information, we compare the annotation results between the language pair and analyze how discourse affects Spanish-Chinese translation. The research results in our study can help human translators who work with the language pair.","authors":["Shuyuan Cao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus","tldr":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A compa...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.4","presentation_id":"38939686","rocketchat_channel":"paper-codi2020-4","speakers":"Shuyuan Cao","title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus"},{"content":{"abstract":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.","authors":["Yifan Gao","Piji Li","Wei Bi","Xiaojiang Liu","Michael Lyu","Irwin King"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning","tldr":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.475","presentation_id":"38940696","rocketchat_channel":"paper-codi2020-475","speakers":"Yifan Gao|Piji Li|Wei Bi|Xiaojiang Liu|Michael Lyu|Irwin King","title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning"},{"content":{"abstract":"","authors":["Juntao Yu","Nafise Sadat Moosavi","Silviu Paun","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.6","presentation_id":"38940702","rocketchat_channel":"paper-codi2020-6","speakers":"Juntao Yu|Nafise Sadat Moosavi|Silviu Paun|Massimo Poesio","title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution"},{"content":{"abstract":"","authors":["Juntao Yu","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multitask Learning-Based Neural Bridging Reference Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.7","presentation_id":"38940703","rocketchat_channel":"paper-codi2020-7","speakers":"Juntao Yu|Massimo Poesio","title":"Multitask Learning-Based Neural Bridging Reference Resolution"},{"content":{"abstract":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled. These features were transformed using tf-idf and word embedding; feature selection was done using Principal Component Analysis (PCA). We ran experiments on six combinations of features to predict sequences with Hierarchical Agglomerative Clustering. An analysis of the clustering results indicate that using word embeddings and syntactic features, significantly improved the results.","authors":["Maitreyee Maitreyee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues","tldr":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled....","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.8","presentation_id":"38939687","rocketchat_channel":"paper-codi2020-8","speakers":"Maitreyee Maitreyee","title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues"},{"content":{"abstract":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.","authors":["Sopan Khosla","Carolyn Rose"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Type Information to Improve Entity Coreference Resolution","tldr":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic know...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.9","presentation_id":"38939688","rocketchat_channel":"paper-codi2020-9","speakers":"Sopan Khosla|Carolyn Rose","title":"Using Type Information to Improve Entity Coreference Resolution"},{"content":{"abstract":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classification has not been fully explored. We present the first systematic study on how data augmentation techniques impact performance across toxic language classifiers, ranging from shallow logistic regression architectures to BERT \u2013 a state-of-the-art pretrained Transformer network. We compare the performance of eight techniques on very scarce seed datasets. We show that while BERT performed the best, shallow classifiers performed comparably when trained on data augmented with a combination of three techniques, including GPT-2-generated sentences. We discuss the interplay of performance and computational overhead, which can inform the choice of techniques under different constraints.","authors":["Mika Juuti","Tommi Gr\u00f6ndahl","Adrian Flanagan","N. Asokan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.269","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A little goes a long way: Improving toxic language classification despite data scarcity","tldr":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classifi...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2217","presentation_id":"38940137","rocketchat_channel":"paper-woah4-2217","speakers":"Mika Juuti|Tommi Gr\u00f6ndahl|Adrian Flanagan|N. Asokan","title":"A little goes a long way: Improving toxic language classification despite data scarcity"},{"content":{"abstract":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators \u2013 not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists\u2019 experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.","authors":["Ishaan Arora","Julia Guo","Sarah Ita Levitan","Susan McGregor","Julia Hirschberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter","tldr":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annot...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.10","presentation_id":"38939517","rocketchat_channel":"paper-woah4-10","speakers":"Ishaan Arora|Julia Guo|Sarah Ita Levitan|Susan McGregor|Julia Hirschberg","title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter"},{"content":{"abstract":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. However, its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. Here we use a unique situation in Germany where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our pipeline achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97\u2014accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.","authors":["Joshua Garland","Keyan Ghazi-Zahedi","Jean-Gabriel Young","Laurent H\u00e9bert-Dufresne","Mirta Galesic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Countering hate on social media: Large scale classification of hate and counter speech","tldr":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engag...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.13","presentation_id":"38939518","rocketchat_channel":"paper-woah4-13","speakers":"Joshua Garland|Keyan Ghazi-Zahedi|Jean-Gabriel Young|Laurent H\u00e9bert-Dufresne|Mirta Galesic","title":"Countering hate on social media: Large scale classification of hate and counter speech"},{"content":{"abstract":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politically-biased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.","authors":["Maximilian Wich","Jan Bauer","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Impact of politically biased data on hate speech classification","tldr":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their v...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.15","presentation_id":"38939519","rocketchat_channel":"paper-woah4-15","speakers":"Maximilian Wich|Jan Bauer|Georg Groh","title":"Impact of politically biased data on hate speech classification"},{"content":{"abstract":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.","authors":["Michele Banko","Brendon MacKeen","Laurie Ray"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Unified Taxonomy of Harmful Content","tldr":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.16","presentation_id":"38939520","rocketchat_channel":"paper-woah4-16","speakers":"Michele Banko|Brendon MacKeen|Laurie Ray","title":"A Unified Taxonomy of Harmful Content"},{"content":{"abstract":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.","authors":["Kosisochukwu Madukwe","Xiaoying Gao","Bing Xue"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets","tldr":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining ...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.19","presentation_id":"38939521","rocketchat_channel":"paper-woah4-19","speakers":"Kosisochukwu Madukwe|Xiaoying Gao|Bing Xue","title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets"},{"content":{"abstract":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.","authors":["Claire Pershan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach","tldr":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and inter...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2","presentation_id":"38939516","rocketchat_channel":"paper-woah4-2","speakers":"Claire Pershan","title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach"},{"content":{"abstract":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.","authors":["Kadir Bulut Ozler","Kate Kenski","Steve Rains","Yotam Shmargad","Kevin Coe","Steven Bethard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection","tldr":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.24","presentation_id":"38939522","rocketchat_channel":"paper-woah4-24","speakers":"Kadir Bulut Ozler|Kate Kenski|Steve Rains|Yotam Shmargad|Kevin Coe|Steven Bethard","title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection"},{"content":{"abstract":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of \u2018unhealthy online conversation\u2019. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.","authors":["Ilan Price","Jordan Gifford-Moore","Jory Flemming","Saul Musker","Maayan Roichman","Guillaume Sylvain","Nithum Thain","Lucas Dixon","Jeffrey Sorensen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Six Attributes of Unhealthy Conversations","tldr":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.25","presentation_id":"38939523","rocketchat_channel":"paper-woah4-25","speakers":"Ilan Price|Jordan Gifford-Moore|Jory Flemming|Saul Musker|Maayan Roichman|Guillaume Sylvain|Nithum Thain|Lucas Dixon|Jeffrey Sorensen","title":"Six Attributes of Unhealthy Conversations"},{"content":{"abstract":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now. A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or \u0436\u0435\u043d\u0449\u0438\u043d\u0430, \u0447\u0435\u0440\u043d\u044b\u0439, \u0435\u0432\u0440\u0435\u0439) that are not toxic, but serve as triggers for the classifier due to model caveats. In this paper, we describe our efforts towards classifying hate speech in Russian, and propose simple techniques of reducing unintended bias, such as generating training data with language models using terms and words related to protected identities as context and applying word dropout to such words.","authors":["Nadezhda Zueva","Madina Kabirova","Pavel Kalaidin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection","tldr":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both res...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.31","presentation_id":"38939524","rocketchat_channel":"paper-woah4-31","speakers":"Nadezhda Zueva|Madina Kabirova|Pavel Kalaidin","title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection"},{"content":{"abstract":"","authors":["Rosalie Gillett","Nicolas Suzor","Jean Burgess","Bridget Harris","Molly Dragiewicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating takedowns of abuse on Twitter","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.32","presentation_id":"38939525","rocketchat_channel":"paper-woah4-32","speakers":"Rosalie Gillett|Nicolas Suzor|Jean Burgess|Bridget Harris|Molly Dragiewicz","title":"Investigating takedowns of abuse on Twitter"},{"content":{"abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","authors":["Bertie Vidgen","Scott Hale","Ella Guest","Helen Margetts","David Broniatowski","Zeerak Waseem","Austin Botelho","Matthew Hall","Rebekah Tromble"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting East Asian Prejudice on Social Media","tldr":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier t...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.37","presentation_id":"38939526","rocketchat_channel":"paper-woah4-37","speakers":"Bertie Vidgen|Scott Hale|Ella Guest|Helen Margetts|David Broniatowski|Zeerak Waseem|Austin Botelho|Matthew Hall|Rebekah Tromble","title":"Detecting East Asian Prejudice on Social Media"},{"content":{"abstract":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.","authors":["Dante Razo","Sandra K\u00fcbler"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Sampling Bias in Abusive Language Detection","tldr":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.39","presentation_id":"38939527","rocketchat_channel":"paper-woah4-39","speakers":"Dante Razo|Sandra K\u00fcbler","title":"Investigating Sampling Bias in Abusive Language Detection"},{"content":{"abstract":"","authors":["Ian Kivlichan","Olivia Redfield","Rachel Rosen","Raquel Saxe","Nitesh Goyal","Lucy Vasserman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.42","presentation_id":"38939528","rocketchat_channel":"paper-woah4-42","speakers":"Ian Kivlichan|Olivia Redfield|Rachel Rosen|Raquel Saxe|Nitesh Goyal|Lucy Vasserman","title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity"},{"content":{"abstract":"","authors":["Viktorya Vilk","Elodie Vialle","Matt Bailey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.43","presentation_id":"38939529","rocketchat_channel":"paper-woah4-43","speakers":"Viktorya Vilk|Elodie Vialle|Matt Bailey","title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse"},{"content":{"abstract":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.","authors":["Anna Koufakou","Endang Wahyu Pamungkas","Valerio Basile","Viviana Patti"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language","tldr":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features deriv...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.44","presentation_id":"38939530","rocketchat_channel":"paper-woah4-44","speakers":"Anna Koufakou|Endang Wahyu Pamungkas|Valerio Basile|Viviana Patti","title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language"},{"content":{"abstract":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in human-based incivility detection, it is urgent to develop validated and versatile automatic approaches to identifying uncivil posts and comments. This project advances both a neural, BERT-based classifier as well as a logistic regression classifier to identify uncivil comments. The classifier is trained on a dataset of Reddit posts, which are annotated for incivility, and further expanded using a combination of labeled data from Reddit and Twitter. Our best performing model achieves an F1 of 0.802 on our Reddit test set. The final model is not only applicable across social media platforms and their distinct data structures, but also computationally versatile, and - as such - ready to be used on vast volumes of online data. All trained models and annotated data are made available to the research community.","authors":["Sam Davidson","Qiusi Sun","Magdalena Wojcieszak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Developing a New Classifier for Automated Identification of Incivility in Social Media","tldr":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.47","presentation_id":"38939531","rocketchat_channel":"paper-woah4-47","speakers":"Sam Davidson|Qiusi Sun|Magdalena Wojcieszak","title":"Developing a New Classifier for Automated Identification of Incivility in Social Media"},{"content":{"abstract":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that classifiers based on syntactic structure of the text, dependency graphical convolutional networks (DepGCNs) can achieve state-of-the-art performance on abusive language datasets. The overall performance is at par with of strong baselines such as fine-tuned BERT. Further, our GCN-based approach is much more efficient than BERT at inference time making it suitable for real-time detection.","authors":["Kanika Narang","Chris Brew"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Abusive Language Detection using Syntactic Dependency Graphs","tldr":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that cla...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.48","presentation_id":"38939532","rocketchat_channel":"paper-woah4-48","speakers":"Kanika Narang|Chris Brew","title":"Abusive Language Detection using Syntactic Dependency Graphs"},{"content":{"abstract":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comments written by victims of abuse are frequently labelled as hateful, even if they discuss or reclaim slurs. Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. We make two main contributions to this end. First, we provide an annotation guide that outlines 4 main categories of online slur usage, which we further divide into a total of 12 sub-categories. Second, we present a publicly available corpus based on our taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, our taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.","authors":["Jana Kurrek","Haji Mohammad Saleem","Derek Ruths"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage","tldr":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comme...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.49","presentation_id":"38939533","rocketchat_channel":"paper-woah4-49","speakers":"Jana Kurrek|Haji Mohammad Saleem|Derek Ruths","title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage"},{"content":{"abstract":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop computational models to incorporate emotions into textual cues to improve aggression identification. We evaluate our proposed methods on a set of corpora related to the task and show promising results with respect to abusive language detection.","authors":["Niloofar Safi Samghabadi","Afsheen Hatami","Mahsa Shafaei","Sudipta Kar","Thamar Solorio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attending the Emotions to Detect Online Abusive Language","tldr":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.50","presentation_id":"38939534","rocketchat_channel":"paper-woah4-50","speakers":"Niloofar Safi Samghabadi|Afsheen Hatami|Mahsa Shafaei|Sudipta Kar|Thamar Solorio","title":"Attending the Emotions to Detect Online Abusive Language"},{"content":{"abstract":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widely researched problem, with current research having a strong focus on a binary classification of bullying versus non-bullying. This paper proposes a novel approach to enhancing cyberbullying detection through role modeling. We utilise a dataset from ASKfm to perform multi-class classification to detect participant roles (e.g. victim, harasser). Our preliminary results demonstrate promising performance including 0.83 and 0.76 of F1-score for cyberbullying and role classification respectively, outperforming baselines.","authors":["Gathika Rathnayake","Thushari Atapattu","Mahen Herath","Georgia Zhang","Katrina Falkner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing the Identification of Cyberbullying through Participant Roles","tldr":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widel...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.51","presentation_id":"38939535","rocketchat_channel":"paper-woah4-51","speakers":"Gathika Rathnayake|Thushari Atapattu|Mahen Herath|Georgia Zhang|Katrina Falkner","title":"Enhancing the Identification of Cyberbullying through Participant Roles"},{"content":{"abstract":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either \u2018Hateful\u2019, \u2018Normal\u2019 or \u2018Offensive\u2019. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.","authors":["Vebj\u00f8rn Isaksen","Bj\u00f6rn Gamb\u00e4ck"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online","tldr":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.52","presentation_id":"38939536","rocketchat_channel":"paper-woah4-52","speakers":"Vebj\u00f8rn Isaksen|Bj\u00f6rn Gamb\u00e4ck","title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online"},{"content":{"abstract":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics\u2019 keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.","authors":["Isar Nejadgholi","Svetlana Kiritchenko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","tldr":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applie...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.56","presentation_id":"38939537","rocketchat_channel":"paper-woah4-56","speakers":"Isar Nejadgholi|Svetlana Kiritchenko","title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse"},{"content":{"abstract":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.","authors":["Hala Al Kuwatly","Maximilian Wich","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics","tldr":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. O...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.57","presentation_id":"38939538","rocketchat_channel":"paper-woah4-57","speakers":"Hala Al Kuwatly|Maximilian Wich|Georg Groh","title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics"},{"content":{"abstract":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately, machine learning is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in classification performance or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias \u2014 a form of bias that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the annotation behavior from annotators. To do so, we build a graph based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a data set. The proposed method and collected insights can contribute to developing fairer and more reliable hate speech classification models.","authors":["Maximilian Wich","Hala Al Kuwatly","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Annotator Bias with a Graph-Based Approach","tldr":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.58","presentation_id":"38939539","rocketchat_channel":"paper-woah4-58","speakers":"Maximilian Wich|Hala Al Kuwatly|Georg Groh","title":"Investigating Annotator Bias with a Graph-Based Approach"},{"content":{"abstract":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research communities in tackling online abuse. We report on the discussions that took place, and present an analysis of four key issues which emerged: Problems in tackling online abuse, Solutions, Meta concerns and the Ecosystem of content moderation and research. We argue there is a pressing need for NLP research communities to engage with human rights perspectives, and identify four key ways in which NLP research into online abuse could immediately be enhanced to create better and more ethical solutions.","authors":["Vinodkumar Prabhakaran","Zeerak Waseem","Seyi Akiwowo","Bertie Vidgen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020","tldr":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research c...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2020.alw-1.1","presentation_id":"","rocketchat_channel":"paper-woah4-1","speakers":"Vinodkumar Prabhakaran|Zeerak Waseem|Seyi Akiwowo|Bertie Vidgen","title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020"},{"content":{"abstract":"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.","authors":["Jonathan Dunn","Tom Coupe","Benjamin Adams"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Measuring Linguistic Diversity During COVID-19","tldr":"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulti...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.13","presentation_id":"38940618","rocketchat_channel":"paper-nlpcss-13","speakers":"Jonathan Dunn|Tom Coupe|Benjamin Adams","title":"Measuring Linguistic Diversity During COVID-19"},{"content":{"abstract":"","authors":["David DeFranza","Arul Mishra","Himanshu Mishra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Language Influences Attitudes Toward Brands","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.15","presentation_id":"38940628","rocketchat_channel":"paper-nlpcss-15","speakers":"David DeFranza|Arul Mishra|Himanshu Mishra","title":"How Language Influences Attitudes Toward Brands"},{"content":{"abstract":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning-based text classification models for automatic coding in the area of psycho-social online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning-based classifiers against human coders.","authors":["Philipp Grandeit","Carolyn Haberkern","Maximiliane Lang","Jens Albrecht","Robert Lehmann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling","tldr":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need t...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.17","presentation_id":"38940609","rocketchat_channel":"paper-nlpcss-17","speakers":"Philipp Grandeit|Carolyn Haberkern|Maximiliane Lang|Jens Albrecht|Robert Lehmann","title":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling"},{"content":{"abstract":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.","authors":["Nico Blokker","Erenay Dayanik","Gabriella Lapesa","Sebastian Pad\u00f3"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Swimming with the Tide? Positional Claim Detection across Political Text Types","tldr":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party posit...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.19","presentation_id":"38940616","rocketchat_channel":"paper-nlpcss-19","speakers":"Nico Blokker|Erenay Dayanik|Gabriella Lapesa|Sebastian Pad\u00f3","title":"Swimming with the Tide? Positional Claim Detection across Political Text Types"},{"content":{"abstract":"Individuals recovering from substance use often seek social support (emotional and informational) on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums is that certain posts (some of which may be support seeking) receive no comments. In this work, we use data from two Reddit substance recovery forums: /r/Leaves and /r/OpiatesRecovery, to determine the relationship between the social supports expressed in the titles of posts and the number of comments they receive. We show that the types of social support expressed in post titles that elicit comments vary from one substance use recovery forum to the other.","authors":["Anietie Andy","Sharath Chandra Guntuku"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Does Social Support (Expressed in Post Titles) Elicit Comments in Online Substance Use Recovery Forums?","tldr":"Individuals recovering from substance use often seek social support (emotional and informational) on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.20","presentation_id":"38940623","rocketchat_channel":"paper-nlpcss-20","speakers":"Anietie Andy|Sharath Chandra Guntuku","title":"Does Social Support (Expressed in Post Titles) Elicit Comments in Online Substance Use Recovery Forums?"},{"content":{"abstract":"With the world on a lockdown due to the COVID-19 pandemic, this paper studies emotions expressed on Twitter. Using a combined strategy of time series analysis of emotions augmented by tweet topics, this study provides an insight into emotion transitions during the pandemic. After tweets are annotated with dominant emotions and topics, a time-series emotion analysis is used to identify disgust and anger as the most commonly identified emotions. Through longitudinal analysis of each user, we construct emotion transition graphs, observing key transitions between disgust and anger, and self-transitions within anger and disgust emotional states. Observing user patterns through clustering of user longitudinal analyses reveals emotional transitions fall into four main clusters: (1) erratic motion over short period of time, (2) disgust -> anger, (3) optimism -> joy. (4) erratic motion over a prolonged period. Finally, we propose a method for predicting users subsequent topic, and by consequence their emotions, through constructing an Emotion Topic Hidden Markov Model, augmenting emotion transition states with topic information. Results suggests that the predictions fare better than baselines, spurring directions of predicting emotional states based on Twitter posts.","authors":["Hui Xian Lynnette Ng","Roy Ka-Wei Lee","Md Rabiul Awal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"I miss you babe: Analyzing Emotion Dynamics During COVID-19 Pandemic","tldr":"With the world on a lockdown due to the COVID-19 pandemic, this paper studies emotions expressed on Twitter. Using a combined strategy of time series analysis of emotions augmented by tweet topics, this study provides an insight into emotion transiti...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.21","presentation_id":"38940603","rocketchat_channel":"paper-nlpcss-21","speakers":"Hui Xian Lynnette Ng|Roy Ka-Wei Lee|Md Rabiul Awal","title":"I miss you babe: Analyzing Emotion Dynamics During COVID-19 Pandemic"},{"content":{"abstract":"Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easily population-level mental health data can be integrated into health and policy decision-making. Here, we demonstrate that natural language processing applied to publicly-available social media data can provide real-time estimates of psychological distress in the population (specifically, English-speaking Twitter users in the US). We examine population-level changes in linguistic correlates of mental health symptoms in response to the COVID-19 pandemic and to the killing of George Floyd. As a case study, we focus on social media data from healthcare providers, compared to a control sample. Our results provide a concrete demonstration of how the tools of computational social science can be applied to provide real-time or near-real-time insight into the impact of public events on mental health.","authors":["Alex Fine","Patrick Crutchley","Jenny Blase","Joshua Carroll","Glen Coppersmith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using NLP applied to social media data","tldr":"Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easi...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.22","presentation_id":"38940624","rocketchat_channel":"paper-nlpcss-22","speakers":"Alex Fine|Patrick Crutchley|Jenny Blase|Joshua Carroll|Glen Coppersmith","title":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using NLP applied to social media data"},{"content":{"abstract":"","authors":["Michael Yeomans","Alison Wood Brooks"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Topic preference detection: A novel approach to understand perspective taking in conversation","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.23","presentation_id":"38940626","rocketchat_channel":"paper-nlpcss-23","speakers":"Michael Yeomans|Alison Wood Brooks","title":"Topic preference detection: A novel approach to understand perspective taking in conversation"},{"content":{"abstract":"Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already being used to create fake news. They can also be exploited to generate biased news, which can then be used to attack news aggregators to change their reader\u2019s behavior and influence their bias. In this paper, we use a threat model to demonstrate that the publicly available language models can reliably generate biased news content based on an input original news. We also show that a large number of high-quality biased news articles can be generated using controllable text generation. A subjective evaluation with 80 participants demonstrated that the generated biased news is generally fluent, and a bias evaluation with 24 participants demonstrated that the bias (left or right) is usually evident in the generated articles and can be easily identified.","authors":["Saurabh Gupta","Hong Huy Nguyen","Junichi Yamagishi","Isao Echizen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Viable Threat on News Reading: Generating Biased News Using Natural Language Models","tldr":"Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.26","presentation_id":"38940610","rocketchat_channel":"paper-nlpcss-26","speakers":"Saurabh Gupta|Hong Huy Nguyen|Junichi Yamagishi|Isao Echizen","title":"Viable Threat on News Reading: Generating Biased News Using Natural Language Models"},{"content":{"abstract":"","authors":["Sandeep Soni","Lauren Klein","Jacob Eisenstein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Lexical Semantic Leadership Network of Nineteenth CenturyAbolitionist Newspapers","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.28","presentation_id":"38940625","rocketchat_channel":"paper-nlpcss-28","speakers":"Sandeep Soni|Lauren Klein|Jacob Eisenstein","title":"A Lexical Semantic Leadership Network of Nineteenth CenturyAbolitionist Newspapers"},{"content":{"abstract":"Each year, thousands of roughly 150-page parole hearing transcripts in California go unread because legal experts lack the time to review them. Yet, reviewing transcripts is the only means of public oversight in the parole process. To assist reviewers, we present a simple unsupervised technique for using language models (LMs) to identify procedural anomalies in long-form legal text. Our technique highlights unusual passages that suggest further review could be necessary. We utilize a contrastive perplexity score to identify passages, defined as the scaled difference between its perplexities from two LMs, one fine-tuned on the target (parole) domain, and another pre-trained on out-of-domain text to normalize for grammatical or syntactic anomalies. We present quantitative analysis of the results and note that our method has identified some important cases for review. We are also excited about potential applications in unsupervised anomaly detection, and present a brief analysis of results for detecting fake TripAdvisor reviews.","authors":["Graham Todd","Catalin Voss","Jenny Hong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Anomaly Detection in Parole Hearings using Language Models","tldr":"Each year, thousands of roughly 150-page parole hearing transcripts in California go unread because legal experts lack the time to review them. Yet, reviewing transcripts is the only means of public oversight in the parole process. To assist reviewer...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.29","presentation_id":"38940611","rocketchat_channel":"paper-nlpcss-29","speakers":"Graham Todd|Catalin Voss|Jenny Hong","title":"Unsupervised Anomaly Detection in Parole Hearings using Language Models"},{"content":{"abstract":"Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of machine learning models to predict worry on Twitter. While this task has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate task given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry \u2013 normative and pathological \u2013 as stated in psychology literature. In addition, we show that existing emotion detection techniques underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.","authors":["Reyha Verma","Christian von der Weth","Jithin Vachery","Mohan Kankanhalli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Worry in Twitter: Beyond Emotion Analysis","tldr":"Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e....","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.32","presentation_id":"38940602","rocketchat_channel":"paper-nlpcss-32","speakers":"Reyha Verma|Christian von der Weth|Jithin Vachery|Mohan Kankanhalli","title":"Identifying Worry in Twitter: Beyond Emotion Analysis"},{"content":{"abstract":"We present experiments to structure job ads into text zones and classify them into pro- fessions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings on the benefits of contextualized embeddings and the potential of multi-task models for this purpose. With contextualized in-domain embeddings in BiLSTM-CRF models, we reach an accuracy of 91% for token-level text zoning and outperform previous approaches. A multi-tasking BERT model performs well for our classification tasks. We further compare transfer approaches for our multilingual data.","authors":["Ann-Sophie Gnehm","Simon Clematide"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Text Zoning and Classification for Job Advertisements in German, French and English","tldr":"We present experiments to structure job ads into text zones and classify them into pro- fessions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings o...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.33","presentation_id":"38940604","rocketchat_channel":"paper-nlpcss-33","speakers":"Ann-Sophie Gnehm|Simon Clematide","title":"Text Zoning and Classification for Job Advertisements in German, French and English"},{"content":{"abstract":"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.","authors":["Katja Geertruida Schmahl","Tom Julian Viering","Stavros Makrodimitris","Arman Naseri Jahfari","David Tax","Marco Loog"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings","tldr":"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.34","presentation_id":"38940605","rocketchat_channel":"paper-nlpcss-34","speakers":"Katja Geertruida Schmahl|Tom Julian Viering|Stavros Makrodimitris|Arman Naseri Jahfari|David Tax|Marco Loog","title":"Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings"},{"content":{"abstract":"It has been shown that anonymity affects various aspects of online communications such as message credibility, the trust among communicators, and the participants\u2019 accountability and reputation. Anonymity influences social interactions in online communities in these many ways, which can lead to influences on opinion change and the persuasiveness of a message. Prior studies also suggest that the effect of anonymity can vary in different online communication contexts and online communities. In this study, we focus on Wikipedia Articles for Deletion (AfD) discussions as an example of online collaborative communities to study the relationship between anonymity and persuasiveness in this context. We find that in Wikipedia AfD discussions, more identifiable users tend to be more persuasive. The higher persuasiveness can be related to multiple aspects, including linguistic features of the comments, the user\u2019s motivation to participate, persuasive skills the user learns over time, and the user\u2019s identity and credibility established in the community through participation.","authors":["Yimin Xiao","Lu Xiao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effects of Anonymity on Comment Persuasiveness in Wikipedia Articles for Deletion Discussions","tldr":"It has been shown that anonymity affects various aspects of online communications such as message credibility, the trust among communicators, and the participants\u2019 accountability and reputation. Anonymity influences social interactions in online comm...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.36","presentation_id":"38940619","rocketchat_channel":"paper-nlpcss-36","speakers":"Yimin Xiao|Lu Xiao","title":"Effects of Anonymity on Comment Persuasiveness in Wikipedia Articles for Deletion Discussions"},{"content":{"abstract":"Methods and applications are inextricably linked in science, and in particular in the domain of text-as-data. In this paper, we examine one such text-as-data application, an established economic index that measures economic policy uncertainty from keyword occurrences in news. This index, which is shown to correlate with firm investment, employment, and excess market returns, has had substantive impact in both the private sector and academia. Yet, as we revisit and extend the original authors\u2019 annotations and text measurements we find interesting text-as-data methodological research questions: (1) Are annotator disagreements a reflection of ambiguity in language? (2) Do alternative text measurements correlate with one another and with measures of external predictive validity? We find for this application (1) some annotator disagreements of economic policy uncertainty can be attributed to ambiguity in language, and (2) switching measurements from keyword-matching to supervised machine learning classifiers results in low correlation, a concerning implication for the validity of the index.","authors":["Katherine Keith","Christoph Teichmann","Brendan O\u2019Connor","Edgar Meij"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncertainty over Uncertainty: Investigating the Assumptions, Annotations, and Text Measurements of Economic Policy Uncertainty","tldr":"Methods and applications are inextricably linked in science, and in particular in the domain of text-as-data. In this paper, we examine one such text-as-data application, an established economic index that measures economic policy uncertainty from ke...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.37","presentation_id":"38940620","rocketchat_channel":"paper-nlpcss-37","speakers":"Katherine Keith|Christoph Teichmann|Brendan O\u2019Connor|Edgar Meij","title":"Uncertainty over Uncertainty: Investigating the Assumptions, Annotations, and Text Measurements of Economic Policy Uncertainty"},{"content":{"abstract":"We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the \u2018raw\u2019 scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of online abuse. We examine two widely used classifiers (created by Perspective and Davidson et al.) on a dataset of tweets directed against candidates in the UK\u2019s 2017 general election. A Bayesian approach is presented to recalibrate the raw scores from the classifiers, using probabilistic programming and newly annotated data. We argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.","authors":["Bertie Vidgen","Scott Hale","Sam Staton","Tom Melham","Helen Margetts","Ohad Kammar","Marcin Szymczak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recalibrating classifiers for interpretable abusive content detection","tldr":"We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the \u2018raw\u2019 scores are used) align poorly with human evaluations. This limits their use for under...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.38","presentation_id":"38940621","rocketchat_channel":"paper-nlpcss-38","speakers":"Bertie Vidgen|Scott Hale|Sam Staton|Tom Melham|Helen Margetts|Ohad Kammar|Marcin Szymczak","title":"Recalibrating classifiers for interpretable abusive content detection"},{"content":{"abstract":"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients\u2019 life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss\u2019 Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures (macro) of 95% averaged across topics, showing that this task can be automatically solved.","authors":["Angelika Maier","Philipp Cimiano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predicting independent living outcomes from written reports of social workers","tldr":"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients\u2019 life. To support this tas...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.43","presentation_id":"38940617","rocketchat_channel":"paper-nlpcss-43","speakers":"Angelika Maier|Philipp Cimiano","title":"Predicting independent living outcomes from written reports of social workers"},{"content":{"abstract":"Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such a form of unfair news coverage can be exposed. This paper addresses the automatic detection of bias, but it goes one step further in that it explores how political bias and unfairness are manifested linguistically. We utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com to develop a neural model for bias assessment. Analyzing the model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.","authors":["Wei-Fan Chen","Khalid Al Khatib","Henning Wachsmuth","Benno Stein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity","tldr":"Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.44","presentation_id":"38940612","rocketchat_channel":"paper-nlpcss-44","speakers":"Wei-Fan Chen|Khalid Al Khatib|Henning Wachsmuth|Benno Stein","title":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity"},{"content":{"abstract":"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.","authors":["Sarang Gupta","Kumari Nishu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model","tldr":"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-pol...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.45","presentation_id":"38940613","rocketchat_channel":"paper-nlpcss-45","speakers":"Sarang Gupta|Kumari Nishu","title":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model"},{"content":{"abstract":"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from adult learners to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages (English, French, Italian, Spanish), extracting data from an internet forum where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker (but with different addressees), which makes it more reliable.","authors":["Aleksandrs Berdicevskis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Foreigner-directed speech is simpler than native-directed: Evidence from social media","tldr":"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addre...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.47","presentation_id":"38940614","rocketchat_channel":"paper-nlpcss-47","speakers":"Aleksandrs Berdicevskis","title":"Foreigner-directed speech is simpler than native-directed: Evidence from social media"},{"content":{"abstract":"Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity (resulting in one embedding representing several distinct and unrelated people) and unlinked references (leading to several distinct embeddings which represent the same person). In this paper, we show that using named entity recognition and heuristic name linking steps before training a diachronic embedding model leads to more accurate representations of references to people, as compared to the token-only baseline. In large news corpus of articles from The Guardian, we provide examples of several types of analysis that can be performed using these new embeddings. Further, we show that real world events and context changes can be detected using our proposed model.","authors":["Felix Hennig","Steven Wilson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diachronic Embeddings for People in the News","tldr":"Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity (resulting in one embedding representing several ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.49","presentation_id":"38940606","rocketchat_channel":"paper-nlpcss-49","speakers":"Felix Hennig|Steven Wilson","title":"Diachronic Embeddings for People in the News"},{"content":{"abstract":"In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. We apply natural language processing (NLP) techniques to more than 3 million Tweets collected from 20,000 Twitter users. We find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans\u2019 issues significantly more frequently than their male counterparts. By contrast, male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. Our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population, and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.","authors":["Kacie Kelly","Alex Fine","Glen Coppersmith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Social media data as a lens onto care-seeking behavior among women veterans of the US armed forces","tldr":"In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.50","presentation_id":"38940615","rocketchat_channel":"paper-nlpcss-50","speakers":"Kacie Kelly|Alex Fine|Glen Coppersmith","title":"Social media data as a lens onto care-seeking behavior among women veterans of the US armed forces"},{"content":{"abstract":"The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on social media.We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including social mobility and unemployment rate.","authors":["Mohammadzaman Zamani","H. Andrew Schwartz","Johannes Eichstaedt","Sharath Chandra Guntuku","Adithya Virinchipuram Ganesan","Sean Clouston","Salvatore Giorgi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling","tldr":"The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.51","presentation_id":"38940607","rocketchat_channel":"paper-nlpcss-51","speakers":"Mohammadzaman Zamani|H. Andrew Schwartz|Johannes Eichstaedt|Sharath Chandra Guntuku|Adithya Virinchipuram Ganesan|Sean Clouston|Salvatore Giorgi","title":"Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling"},{"content":{"abstract":"Emoji are widely used to express emotions and concepts on social media, and prior work has shown that users\u2019 choice of emoji reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts made by users, and this view has provided important insights into phenomena such as emotional expression and self-representation. In addition to making posts, however, social media platforms like Twitter allow for users to provide a short bio, which is an opportunity to briefly describe their account as a whole. In this work, we focus on the use of emoji in these bio statements. We explore the ways in which users include emoji in these self-descriptions, finding different patterns than those observed around emoji usage in tweets. We examine the relationships between emoji used in bios and the content of users\u2019 tweets, showing that the topics and even the average sentiment of tweets varies for users with different emoji in their bios. Lastly, we confirm that homophily effects exist with respect to the types of emoji that are included in bios of users and their followers.","authors":["Jinhang Li","Giorgos Longinos","Steven Wilson","Walid Magdy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emoji and Self-Identity in Twitter Bios","tldr":"Emoji are widely used to express emotions and concepts on social media, and prior work has shown that users\u2019 choice of emoji reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.52","presentation_id":"38940622","rocketchat_channel":"paper-nlpcss-52","speakers":"Jinhang Li|Giorgos Longinos|Steven Wilson|Walid Magdy","title":"Emoji and Self-Identity in Twitter Bios"},{"content":{"abstract":"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the \u201cgenderedness\u201d of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work\u2019s creator correlates with the types of tropes that they use.","authors":["Dhruvil Gala","Mohammad Omar Khursheed","Hannah Lerner","Brendan O\u2019Connor","Mohit Iyyer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Gender Bias within Narrative Tropes","tldr":"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias wit...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.53","presentation_id":"38940608","rocketchat_channel":"paper-nlpcss-53","speakers":"Dhruvil Gala|Mohammad Omar Khursheed|Hannah Lerner|Brendan O\u2019Connor|Mohit Iyyer","title":"Analyzing Gender Bias within Narrative Tropes"},{"content":{"abstract":"","authors":["Kunal Khadilkar","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Unfair Affinity Toward Fairness: Characterizing 70 Years of Social Biases in B^Hollywood","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.57","presentation_id":"38940627","rocketchat_channel":"paper-nlpcss-57","speakers":"Kunal Khadilkar|Ashiqur KhudaBukhsh","title":"An Unfair Affinity Toward Fairness: Characterizing 70 Years of Social Biases in B^Hollywood"},{"content":{"abstract":"Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services\u2019 interoperability within healthcare national information networks. Health and medical science are constantly evolving causing requirements to advance the terminologies editions. In this paper, we present our evaluation work of the latest machine translation techniques addressing medical terminologies. Experiments have been conducted leveraging selected statistical and neural machine translation methods. The devised procedure is tested on a validated sample of ICD-11 and ICF terminologies from English to French with promising results.","authors":["Konstantinos Skianis","Yann Briand","Florent Desgrippes"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Machine Translation Methods applied to Medical Terminologies","tldr":"Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services\u2019 interoperability within healthcare national information networks. Health and medical science are constantly evolving causi...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.12","presentation_id":"38940042","rocketchat_channel":"paper-louhi2020-12","speakers":"Konstantinos Skianis|Yann Briand|Florent Desgrippes","title":"Evaluation of Machine Translation Methods applied to Medical Terminologies"},{"content":{"abstract":"We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIE-R, a bespoke rule-based system, and two neural network models, EdIE-BiLSTM and EdIE-BERT, both multi-task learning models with a BiLSTM and BERT encoder respectively. We compare our models both on an in-sample and an out-of-sample dataset containing mentions of stroke findings and draw on our error analysis to suggest improvements for effective annotation when building clinical NLP models for a new domain. Our analysis finds that our rule-based system outperforms the neural models on both datasets and seems to generalise to the out-of-sample dataset. On the other hand, the neural models do not generalise negation to the out-of-sample dataset, despite metrics on the in-sample dataset suggesting otherwise.","authors":["Andreas Grivas","Beatrice Alex","Claire Grover","Richard Tobin","William Whiteley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports","tldr":"We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIE-R, a bespoke rule-based system, and two neural network models, EdIE-B...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.13","presentation_id":"38940043","rocketchat_channel":"paper-louhi2020-13","speakers":"Andreas Grivas|Beatrice Alex|Claire Grover|Richard Tobin|William Whiteley","title":"Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports"},{"content":{"abstract":"Biomedical event extraction from natural text is a challenging task as it searches for complex and often nested structures describing specific relationships between multiple molecular entities, such as genes, proteins, or cellular components. It usually is implemented by a complex pipeline of individual tools to solve the different relation extraction subtasks. We present an alternative approach where the detection of relationships between entities is described uniformly as questions, which are iteratively answered by a question answering (QA) system based on the domain-specific language model SciBERT. This model outperforms two strong baselines in two biomedical event extraction corpora in a Knowledge Base Population setting, and also achieves competitive performance in BioNLP challenge evaluation settings.","authors":["Xing David Wang","Leon Weber","Ulf Leser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Biomedical Event Extraction as Multi-turn Question Answering","tldr":"Biomedical event extraction from natural text is a challenging task as it searches for complex and often nested structures describing specific relationships between multiple molecular entities, such as genes, proteins, or cellular components. It usua...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.14","presentation_id":"38940044","rocketchat_channel":"paper-louhi2020-14","speakers":"Xing David Wang|Leon Weber|Ulf Leser","title":"Biomedical Event Extraction as Multi-turn Question Answering"},{"content":{"abstract":"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely dis tributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.","authors":["Florian Borchert","Christina Lohr","Luise Modersohn","Thomas Langer","Markus Follmann","Jan Philipp Sachs","Udo Hahn","Matthieu-P. Schapranow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines","tldr":"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (Ger...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.15","presentation_id":"38940045","rocketchat_channel":"paper-louhi2020-15","speakers":"Florian Borchert|Christina Lohr|Luise Modersohn|Thomas Langer|Markus Follmann|Jan Philipp Sachs|Udo Hahn|Matthieu-P. Schapranow","title":"GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines"},{"content":{"abstract":"Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a clinical knowledge base. It is much beyond simple string matching and requires a deep semantic understanding of concept mentions. Recent research approach concept normalization as either text classification or text similarity. The main drawback in existing a) text classification approach is ignoring valuable target concepts information in learning input concept mention representation b) text similarity approach is the need to separately generate target concept embeddings which is time and resource consuming. Our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. First, we learn input concept mention representation using RoBERTa. Second, we find cosine similarity between embeddings of input concept mention and all the target concepts. Here, embeddings of target concepts are randomly initialized and then updated during training. Finally, the target concept with maximum cosine similarity is assigned to the input concept mention. Our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31%.","authors":["Katikapalli Subramanyam Kalyan","Sivanesan Sangeetha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings","tldr":"Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a clinical knowledge base. It is much beyond simple string matching and requires a deep semantic underst...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.17","presentation_id":"38940046","rocketchat_channel":"paper-louhi2020-17","speakers":"Katikapalli Subramanyam Kalyan|Sivanesan Sangeetha","title":"Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings"},{"content":{"abstract":"The impact of de-identification on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-identification has a limited impact on models for downstream tasks, it remains unclear what the impact is with various levels and forms of de-identification, in particular concerning the trade-off between precision and recall. In this paper, the impact of de-identification is studied on downstream named entity recognition in Swedish clinical text. The results indicate that de-identification models with moderate to high precision lead to similar downstream performance, while low precision has a substantial negative impact. Furthermore, different strategies for concealing sensitive information affect performance to different degrees, ranging from pseudonymisation having a low impact to the removal of entire sentences with sensitive information having a high impact. This study indicates that it is possible to increase the recall of models for identifying sensitive information without negatively affecting the use of de-identified text data for training models for clinical named entity recognition; however, there is ultimately a trade-off between the level of de-identification and the subsequent utility of the data.","authors":["Hanna Berg","Aron Henriksson","Hercules Dalianis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text","tldr":"The impact of de-identification on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-ident...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.2","presentation_id":"38940038","rocketchat_channel":"paper-louhi2020-2","speakers":"Hanna Berg|Aron Henriksson|Hercules Dalianis","title":"The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text"},{"content":{"abstract":"We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relations. The main contributions are in re-defining CONTAINS and OVERLAP relations into CONTAINS, CONTAINS-SUBEVENT, OVERLAP and NOTED-ON. We demonstrate that these refinements lead to substantial gains in learnability for state-of-the-art transformer models as compared to previously reported results on the original THYME corpus. We thus establish a baseline for the automatic extraction of these refined temporal relations. Although our study is done on clinical narrative, we believe it addresses far-reaching challenges that are corpus- and domain- agnostic.","authors":["Kristin Wright-Bettner","Chen Lin","Timothy Miller","Steven Bethard","Dmitriy Dligach","Martha Palmer","James H. Martin","Guergana Savova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Defining and Learning Refined Temporal Relations in the Clinical Narrative","tldr":"We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relati...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.24","presentation_id":"38940047","rocketchat_channel":"paper-louhi2020-24","speakers":"Kristin Wright-Bettner|Chen Lin|Timothy Miller|Steven Bethard|Dmitriy Dligach|Martha Palmer|James H. Martin|Guergana Savova","title":"Defining and Learning Refined Temporal Relations in the Clinical Narrative"},{"content":{"abstract":"Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of entities and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The system performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR.","authors":["Maciej Wiatrak","Juha Iso-Sipila"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text","tldr":"Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.26","presentation_id":"38940048","rocketchat_channel":"paper-louhi2020-26","speakers":"Maciej Wiatrak|Juha Iso-Sipila","title":"Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text"},{"content":{"abstract":"We address the problem of automatic detection of psychiatric disorders from the linguistic content of social media posts. We build a large scale dataset of Reddit posts from users with eight disorders and a control user group. We extract and analyze linguistic characteristics of posts and identify differences between diagnostic groups. We build strong classification models based on deep contextualized word representations and show that they outperform previously applied statistical models with simple linguistic features by large margins. We compare user-level and post-level classification performance, as well as an ensembled multiclass model.","authors":["Zhengping Jiang","Sarah Ita Levitan","Jonathan Zomick","Julia Hirschberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detection of Mental Health from Reddit via Deep Contextualized Representations","tldr":"We address the problem of automatic detection of psychiatric disorders from the linguistic content of social media posts. We build a large scale dataset of Reddit posts from users with eight disorders and a control user group. We extract and analyze ...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.27","presentation_id":"38940049","rocketchat_channel":"paper-louhi2020-27","speakers":"Zhengping Jiang|Sarah Ita Levitan|Jonathan Zomick|Julia Hirschberg","title":"Detection of Mental Health from Reddit via Deep Contextualized Representations"},{"content":{"abstract":"","authors":["Sarah Valentin","Renaud Lancelot","Mathieu Roche"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information retrieval for animal disease surveillance: a pattern-based approach","tldr":null,"track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.28","presentation_id":"38940050","rocketchat_channel":"paper-louhi2020-28","speakers":"Sarah Valentin|Renaud Lancelot|Mathieu Roche","title":"Information retrieval for animal disease surveillance: a pattern-based approach"},{"content":{"abstract":"Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how the human body reacts to different medications. Twitter, a popular social media service, is being considered as an important alternative data source for collecting personal experience information with medications. Identifying personal experience tweets is a challenging classification task in natural language processing. In this study, we utilized three methods based on Facebook\u2019s Robustly Optimized BERT Pretraining Approach (RoBERTa) to predict personal experience tweets related to medication use: the first one combines the pre-trained RoBERTa model with a classifier, the second combines the updated pre-trained RoBERTa model using a corpus of unlabeled tweets with a classifier, and the third combines the RoBERTa model that was trained with our unlabeled tweets from scratch with the classifier too. Our results show that all of these approaches outperform the published methods (Word Embedding + LSTM) in classification performance (p < 0.05), and updating the pre-trained language model with tweets related to medications could even improve the performance further.","authors":["Minghao Zhu","Youzhe Song","Ge Jin","Keyuan Jiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating","tldr":"Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how t...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.32","presentation_id":"38940051","rocketchat_channel":"paper-louhi2020-32","speakers":"Minghao Zhu|Youzhe Song|Ge Jin|Keyuan Jiang","title":"Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating"},{"content":{"abstract":"Health departments have been deploying text classification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in English and, as a result, a promising direction is to increase coverage and recall by considering documents in additional languages, such as Spanish or Chinese. Training previous systems for more languages, however, would be expensive, as it would require the manual annotation of many documents for each new target language. To address this challenge, we consider cross-lingual learning and train multilingual classifiers using only the annotations for English-language reviews. Recent zero-shot approaches based on pre-trained multi-lingual BERT (mBERT) have been shown to effectively align languages for aspects such as sentiment. Interestingly, we show that those approaches are less effective for capturing the nuances of foodborne illness, our public health application of interest. To improve performance without extra annotations, we create artificial training documents in the target language through machine translation and train mBERT jointly for the source (English) and target language. Furthermore, we show that translating labeled documents to multiple languages leads to additional performance improvements for some target languages. We demonstrate the benefits of our approach through extensive experiments with Yelp restaurant reviews in seven languages. Our classifiers identify foodborne illness complaints in multilingual reviews from the Yelp Challenge dataset, which highlights the potential of our general approach for deployment in health departments.","authors":["Ziyi Liu","Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only","tldr":"Health departments have been deploying text classification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in Engl...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.35","presentation_id":"38940052","rocketchat_channel":"paper-louhi2020-35","speakers":"Ziyi Liu|Giannis Karamanolakis|Daniel Hsu|Luis Gravano","title":"Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only"},{"content":{"abstract":"The automatic mapping of Adverse Drug Reaction (ADR) reports from user-generated content to concepts in a controlled medical vocabulary provides valuable insights for monitoring public health. While state-of-the-art deep learning-based sequence classification techniques achieve impressive performance for medical concepts with large amounts of training data, they show their limit with long-tail concepts that have a low number of training samples. The above hinders their adaptability to the changes of layman\u2019s terminology and the constant emergence of new informal medical terms. Our objective in this paper is to tackle the problem of normalizing long-tail ADR mentions in user-generated content. In this paper, we exploit the implicit semantics of rare ADRs for which we have few training samples, in order to detect the most similar class for the given ADR. The evaluation results demonstrate that our proposed approach addresses the limitations of the existing techniques when the amount of training data is limited.","authors":["Emmanouil Manousogiannis","Sepideh Mesbah","Alessandro Bozzon","Robert-Jan Sips","Zoltan Szlanik","Selene Baez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Normalization of Long-tail Adverse Drug Reactions in Social Media","tldr":"The automatic mapping of Adverse Drug Reaction (ADR) reports from user-generated content to concepts in a controlled medical vocabulary provides valuable insights for monitoring public health. While state-of-the-art deep learning-based sequence class...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.4","presentation_id":"38940039","rocketchat_channel":"paper-louhi2020-4","speakers":"Emmanouil Manousogiannis|Sepideh Mesbah|Alessandro Bozzon|Robert-Jan Sips|Zoltan Szlanik|Selene Baez","title":"Normalization of Long-tail Adverse Drug Reactions in Social Media"},{"content":{"abstract":"Healthcare systems have increased patients\u2019 exposure to their own health materials to enhance patients\u2019 health levels, but this has been impeded by patients\u2019 lack of understanding of their health material. We address potential barriers to their comprehension by developing a context-aware text simplification system for health material. Given the scarcity of annotated parallel corpora in healthcare domains, we design our system to be independent of a parallel corpus, complementing the availability of data-driven neural methods when such corpora are available. Our system compensates for the lack of direct supervision using a biomedical lexical database: Unified Medical Language System (UMLS). Compared to a competitive prior approach that uses a tool for identifying biomedical concepts and a consumer-directed vocabulary list, we empirically show the enhanced accuracy of our system due to improved handling of ambiguous terms. We also show the enhanced accuracy of our system over directly-supervised neural methods in this low-resource setting. Finally, we show the direct impact of our system on laypeople\u2019s comprehension of health material via a human subjects\u2019 study (n=160).","authors":["Tarek Sakakini","Jong Yoon Lee","Aditya Duri","Renato F.L. Azevedo","Victor Sadauskas","Kuangxiao Gu","Suma Bhat","Dan Morrow","James Graumlich","Saqib Walayat","Mark Hasegawa-Johnson","Thomas Huang","Ann Willemsen-Dunlap","Donald Halpin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains","tldr":"Healthcare systems have increased patients\u2019 exposure to their own health materials to enhance patients\u2019 health levels, but this has been impeded by patients\u2019 lack of understanding of their health material. We address potential barriers to their compr...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.44","presentation_id":"38940053","rocketchat_channel":"paper-louhi2020-44","speakers":"Tarek Sakakini|Jong Yoon Lee|Aditya Duri|Renato F.L. Azevedo|Victor Sadauskas|Kuangxiao Gu|Suma Bhat|Dan Morrow|James Graumlich|Saqib Walayat|Mark Hasegawa-Johnson|Thomas Huang|Ann Willemsen-Dunlap|Donald Halpin","title":"Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains"},{"content":{"abstract":"Detecting negation and speculation in language has been a task of considerable interest to the biomedical community, as it is a key component of Information Extraction systems from Biomedical documents. Prior work has individually addressed Negation Detection and Speculation Detection, and both have been addressed in the same way, using 2 stage pipelined approach: Cue Detection followed by Scope Resolution. In this paper, we propose Multitask learning approaches over 2 sets of tasks: Negation Cue Detection & Speculation Cue Detection, and Negation Scope Resolution & Speculation Scope Resolution. We utilise transformer-based architectures like BERT, XLNet and RoBERTa as our core model architecture, and finetune these using the Multitask learning approaches. We show that this Multitask Learning approach outperforms the single task learning approach, and report new state-of-the-art results on Negation and Speculation Scope Resolution on the BioScope Corpus and the SFU Review Corpus.","authors":["Aditya Khandelwal","Benita Kathleen Britto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multitask Learning of Negation and Speculation using Transformers","tldr":"Detecting negation and speculation in language has been a task of considerable interest to the biomedical community, as it is a key component of Information Extraction systems from Biomedical documents. Prior work has individually addressed Negation ...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.5","presentation_id":"38940040","rocketchat_channel":"paper-louhi2020-5","speakers":"Aditya Khandelwal|Benita Kathleen Britto","title":"Multitask Learning of Negation and Speculation using Transformers"},{"content":{"abstract":"In this work we addressed the problem of capturing sequential information contained in longitudinal electronic health records (EHRs). Clinical notes, which is a particular type of EHR data, are a rich source of information and practitioners often develop clever solutions how to maximise the sequential information contained in free-texts. We proposed a systematic methodology for learning from chronological events available in clinical notes. The proposed methodological path signature framework creates a non-parametric hierarchical representation of sequential events of any type and can be used as features for downstream statistical learning tasks. The methodology was developed and externally validated using the largest in the UK secondary care mental health EHR data on a specific task of predicting survival risk of patients diagnosed with Alzheimer\u2019s disease. The signature-based model was compared to a common survival random forest model. Our results showed a 15.4% increase of risk prediction AUC at the time point of 20 months after the first admission to a specialist memory clinic and the signature method outperformed the baseline mixed-effects model by 13.2 %.","authors":["Andrey Kormilitzin","Nemanja Vaci","Qiang Liu","Hao Ni","Goran Nenadic","Alejo Nevado-Holgado"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An efficient representation of chronological events in medical texts","tldr":"In this work we addressed the problem of capturing sequential information contained in longitudinal electronic health records (EHRs). Clinical notes, which is a particular type of EHR data, are a rich source of information and practitioners often dev...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.8","presentation_id":"38940041","rocketchat_channel":"paper-louhi2020-8","speakers":"Andrey Kormilitzin|Nemanja Vaci|Qiang Liu|Hao Ni|Goran Nenadic|Alejo Nevado-Holgado","title":"An efficient representation of chronological events in medical texts"},{"content":{"abstract":"Animal diseases-related news articles are richin information useful for risk assessment. In this paper, we explore a method to automatically retrieve sentence-level epidemiological information. Our method is an incremental approach to create and expand patterns at both lexical and syntactic levels. Expert knowledge input are used at different steps of the approach. Distributed vector representations (word embedding) were used to expand the patterns at the lexical level, thus alleviating manual curation. We showed that expert validation was crucial to improve the precision of automatically generated patterns.","authors":["Sarah Valentin","Mathieu Roche","Renaud Lancelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information retrieval for animal disease surveillance: a pattern-based approach.","tldr":"Animal diseases-related news articles are richin information useful for risk assessment. In this paper, we explore a method to automatically retrieve sentence-level epidemiological information. Our method is an incremental approach to create and expa...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.2020.louhi-1.8","presentation_id":"","rocketchat_channel":"paper-louhi2020-8","speakers":"Sarah Valentin|Mathieu Roche|Renaud Lancelot","title":"Information retrieval for animal disease surveillance: a pattern-based approach."},{"content":{"abstract":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete picture of the model\u2019s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed metrics address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model\u2019s confidence score assignments have an impact on the system\u2019s behavior. We describe four key benefits of our proposed metrics as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics\u2019 definitions; the remaining benefits, generalization, and functional consistency are demonstrated empirically.","authors":["Reda Yacouby","Dustin Axman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models","tldr":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete p...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.13","presentation_id":"38939710","rocketchat_channel":"paper-eval4nlp-13","speakers":"Reda Yacouby|Dustin Axman","title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models"},{"content":{"abstract":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need for better summary evaluation methods that go beyond conventional overlap-based metrics. Our typology is structured into two dimensions. First, the Mapping Dimension describes surface-level errors and provides insight into word-sequence transformation issues. Second, the Meaning Dimension describes issues related to interpretation and provides insight into breakdowns in truth, i.e., factual faithfulness to the original text. Comparative analysis revealed that two neural summarization systems leveraging pre-trained models have an advantage in decreasing grammaticality errors, but not necessarily factual errors. We also discuss the importance of ensuring that summary length and abstractiveness do not interfere with evaluating summary quality.","authors":["Klaus-Michael Lux","Maya Sappelli","Martha Larson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries","tldr":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need fo...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.15","presentation_id":"38939711","rocketchat_channel":"paper-eval4nlp-15","speakers":"Klaus-Michael Lux|Maya Sappelli|Martha Larson","title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries"},{"content":{"abstract":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons: (1) it requires that word embeddings be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for model selection in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.","authors":["Nathan Stringham","Mike Izbicki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Word Embeddings on Low-Resource Languages","tldr":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reaso...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.16","presentation_id":"38939712","rocketchat_channel":"paper-eval4nlp-16","speakers":"Nathan Stringham|Mike Izbicki","title":"Evaluating Word Embeddings on Low-Resource Languages"},{"content":{"abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","authors":["Hila Gonen","Kellie Webster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.180","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","tldr":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Whil...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.1663","presentation_id":"38940033","rocketchat_channel":"paper-eval4nlp-1663","speakers":"Hila Gonen|Kellie Webster","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations"},{"content":{"abstract":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with human quality ratings. As a solution, we propose crowdsourcing as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of summarization by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, BLEU, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.","authors":["Neslihan Iskender","Tim Polzehl","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation","tldr":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.18","presentation_id":"38939713","rocketchat_channel":"paper-eval4nlp-18","speakers":"Neslihan Iskender|Tim Polzehl|Sebastian M\u00f6ller","title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation"},{"content":{"abstract":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.","authors":["Wanzheng Zhu","Suma Bhat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GRUEN for Evaluating Linguistic Quality of Generated Text","tldr":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge th...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.183","presentation_id":"38940645","rocketchat_channel":"paper-eval4nlp-183","speakers":"Wanzheng Zhu|Suma Bhat","title":"GRUEN for Evaluating Linguistic Quality of Generated Text"},{"content":{"abstract":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document\u2019s text. We present evidence that BLANC scores have as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.","authors":["Oleg Vasilyev","Vedant Dharnidharka","John Bohannon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fill in the BLANC: Human-free quality estimation of document summaries","tldr":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measur...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.21","presentation_id":"38939714","rocketchat_channel":"paper-eval4nlp-21","speakers":"Oleg Vasilyev|Vedant Dharnidharka|John Bohannon","title":"Fill in the BLANC: Human-free quality estimation of document summaries"},{"content":{"abstract":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model\u2019s behavior, and ignores linguistic properties of words that may allow some mis-predicted tokens to be useful in practice. Furthermore, statistics directly tied to prediction accuracy (including perplexity) may be confounded by the Zipfian nature of written language, as the majority of the prediction attempts will occur with frequently-occurring types. A model\u2019s performance may vary greatly between high- and low-frequency words, which in practice could lead to failure modes such as repetitive and dull generated text being produced by a downstream consumer of a language model. To address this, we propose two new intrinsic evaluation measures within the framework of a simple word prediction task that are designed to give a more holistic picture of a language model\u2019s performance. We evaluate several commonly-used large English language models using our proposed metrics, and demonstrate that our approach reveals functional differences in performance between the models that are obscured by more traditional metrics.","authors":["Shiran Dudy","Steven Bedrick"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Some Words Worth More than Others?","tldr":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.22","presentation_id":"38939715","rocketchat_channel":"paper-eval4nlp-22","speakers":"Shiran Dudy|Steven Bedrick","title":"Are Some Words Worth More than Others?"},{"content":{"abstract":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.","authors":["Adam Poliak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A survey on Recognizing Textual Entailment as an NLP Evaluation","tldr":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.23","presentation_id":"38939716","rocketchat_channel":"paper-eval4nlp-23","speakers":"Adam Poliak","title":"A survey on Recognizing Textual Entailment as an NLP Evaluation"},{"content":{"abstract":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedical data, and a range of evaluation measures suited to the task. The approach is applied to two recent DWSI systems, thus demonstrating its relevance and providing an in-depth analysis of the models.","authors":["Ashjan Alsulaimani","Erwan Moreau","Carl Vogel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.284","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Evaluation Method for Diachronic Word Sense Induction","tldr":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2311","presentation_id":"38940034","rocketchat_channel":"paper-eval4nlp-2311","speakers":"Ashjan Alsulaimani|Erwan Moreau|Carl Vogel","title":"An Evaluation Method for Diachronic Word Sense Induction"},{"content":{"abstract":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.","authors":["Zorik Gekhman","Roee Aharoni","Genady Beryozkin","Markus Freitag","Wolfgang Macherey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.287","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KoBE: Knowledge-Based Machine Translation Evaluation","tldr":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a la...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2378","presentation_id":"38940035","rocketchat_channel":"paper-eval4nlp-2378","speakers":"Zorik Gekhman|Roee Aharoni|Genady Beryozkin|Markus Freitag|Wolfgang Macherey","title":"KoBE: Knowledge-Based Machine Translation Evaluation"},{"content":{"abstract":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that still achieve good performance. We present several reproduction studies of intrinsic evaluation tasks that evaluate non-contextual word representations in multiple languages. Furthermore, we present 50-8-8, a new data set for the outlier identification task, which avoids limitations of the original data set, such as ambiguous words, infrequent words, and multi-word tokens, while increasing the number of test cases. The data set is expanded to contain semantic and syntactic tests and is multilingual (English, German, and Italian). We provide an in-depth analysis of word embedding models with a range of hyper-parameters. Our analysis shows the suitability of different models and hyper-parameters for different tasks and the greater difficulty of representing German and Italian languages.","authors":["Jesper Brink Andersen","Mikkel Bak Bertelsen","Mikkel H\u00f8rby Schou","Manuel R. Ciosici","Ira Assent"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations","tldr":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.25","presentation_id":"38939717","rocketchat_channel":"paper-eval4nlp-25","speakers":"Jesper Brink Andersen|Mikkel Bak Bertelsen|Mikkel H\u00f8rby Schou|Manuel R. Ciosici|Ira Assent","title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations"},{"content":{"abstract":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways (i.e. abstractive and extractive) on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in https://github.com/zide05/CDEvalSumm.","authors":["Yiran Chen","Pengfei Liu","Ming Zhong","Zi-Yi Dou","Danqing Wang","Xipeng Qiu","Xuanjing Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.329","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems","tldr":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2740","presentation_id":"38940036","rocketchat_channel":"paper-eval4nlp-2740","speakers":"Yiran Chen|Pengfei Liu|Ming Zhong|Zi-Yi Dou|Danqing Wang|Xipeng Qiu|Xuanjing Huang","title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems"},{"content":{"abstract":"","authors":["Guy Tevet","Jonathan Berant"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Evaluation of Diversity in Natural Language Generation","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.28","presentation_id":"38940785","rocketchat_channel":"paper-eval4nlp-28","speakers":"Guy Tevet|Jonathan Berant","title":"Evaluating the Evaluation of Diversity in Natural Language Generation"},{"content":{"abstract":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since it allows the assessment of both models and the prompts used to evaluate them. We use IRT to efficiently assess chatbots, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.","authors":["Jo\u00e3o Sedoc","Lyle Ungar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Item Response Theory for Efficient Human Evaluation of Chatbots","tldr":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, usin...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.29","presentation_id":"38939718","rocketchat_channel":"paper-eval4nlp-29","speakers":"Jo\u00e3o Sedoc|Lyle Ungar","title":"Item Response Theory for Efficient Human Evaluation of Chatbots"},{"content":{"abstract":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges don\u2019t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.","authors":["Rahul Jha","Keping Bi","Yang Li","Mahdi Pakdaman","Asli Celikyilmaz","Ivan Zhiboedov","Kieran McDonald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization","tldr":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarizatio...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3","presentation_id":"38939707","rocketchat_channel":"paper-eval4nlp-3","speakers":"Rahul Jha|Keping Bi|Yang Li|Mahdi Pakdaman|Asli Celikyilmaz|Ivan Zhiboedov|Kieran McDonald","title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization"},{"content":{"abstract":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the similarity score. Experimental results on three benchmark datasets show that our method correlates significantly better with human judgments than all existing metrics.","authors":["Hwanhee Lee","Seunghyun Yoon","Franck Dernoncourt","Doo Soon Kim","Trung Bui","Kyomin Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT","tldr":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic represent...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.30","presentation_id":"38939719","rocketchat_channel":"paper-eval4nlp-30","speakers":"Hwanhee Lee|Seunghyun Yoon|Franck Dernoncourt|Doo Soon Kim|Trung Bui|Kyomin Jung","title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT"},{"content":{"abstract":"","authors":["Asiye Tuba K\u00f6ksal","\u00d6zge Bozal","Emre Y\u00fcrekli","Gizem Gezici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3117","presentation_id":"38940037","rocketchat_channel":"paper-eval4nlp-3117","speakers":"Asiye Tuba K\u00f6ksal|\u00d6zge Bozal|Emre Y\u00fcrekli|Gizem Gezici","title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction"},{"content":{"abstract":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.","authors":["Harris Chan","Jamie Kiros","William Chan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.376","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels","tldr":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work,...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3148","presentation_id":"38940114","rocketchat_channel":"paper-eval4nlp-3148","speakers":"Harris Chan|Jamie Kiros|William Chan","title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels"},{"content":{"abstract":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and DBpedia facts having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in DBpedia. We in- vestigate whether\u2014and, if so, how\u2014a given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.","authors":["Kiril Gashteovski","Rainer Gemulla","Bhushan Kotnis","Sven Hertling","Christian Meilicke"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study","tldr":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and auto...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.34","presentation_id":"38939720","rocketchat_channel":"paper-eval4nlp-34","speakers":"Kiril Gashteovski|Rainer Gemulla|Bhushan Kotnis|Sven Hertling|Christian Meilicke","title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study"},{"content":{"abstract":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time, it has been argued that contextualized word representations exhibit sub-optimal statistical properties for encoding the true similarity between words or sentences. In this paper, we present two techniques for improving encoding representations for similarity metrics: a batch-mean centering strategy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERT-backbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks.","authors":["Xi Chen","Nan Ding","Tomer Levinboim","Radu Soricut"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance","tldr":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.35","presentation_id":"38939721","rocketchat_channel":"paper-eval4nlp-35","speakers":"Xi Chen|Nan Ding|Tomer Levinboim|Radu Soricut","title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance"},{"content":{"abstract":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper establishes a framework for addressing n-best evaluation by outlining three different questions one could consider when determining how one would define a \u2018good\u2019 n-best list and proposing evaluation measures for each question. The first and principal contribution is an evaluation measure that characterizes the translation quality of an entire n-best list by asking whether many of the valid translations are placed near the top of the list. The second is a measure that uses gold translations with preference annotations to ask to what degree systems can produce ranked lists in preference order. The third is a measure that rewards partial matches, evaluating the closeness of the many items in an n-best list to a set of many valid references. These three perspectives make clear that having access to many references can be useful when n-best evaluation is the goal.","authors":["Jacob Bremerman","Huda Khayrallah","Douglas Oard","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Evaluation of Machine Translation n-best Lists","tldr":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper es...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.36","presentation_id":"38939722","rocketchat_channel":"paper-eval4nlp-36","speakers":"Jacob Bremerman|Huda Khayrallah|Douglas Oard|Matt Post","title":"On the Evaluation of Machine Translation n-best Lists"},{"content":{"abstract":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as \u201cpsycholinguistic subjects\u201d and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA\u2019s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018a), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC.","authors":["Jingcheng Niu","Gerald Penn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Grammaticality and Language Modelling","tldr":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regard...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.37","presentation_id":"38939723","rocketchat_channel":"paper-eval4nlp-37","speakers":"Jingcheng Niu|Gerald Penn","title":"Grammaticality and Language Modelling"},{"content":{"abstract":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a Python-based tool for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a sentiment analysis and a patent classification task.","authors":["Hanna Wecker","Annemarie Friedrich","Heike Adel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation","tldr":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clu...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.5","presentation_id":"38939708","rocketchat_channel":"paper-eval4nlp-5","speakers":"Hanna Wecker|Annemarie Friedrich|Heike Adel","title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation"},{"content":{"abstract":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversity can be estimated using statistical measures such as perplexity, measuring language quality requires human evaluation. However, because human evaluation at scale is slow and expensive, it is used sparingly; it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for machine translation. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a kernel function. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that \u2013 on average \u2013 the quality estimation from a BLEU Neighbors model has a lower mean squared error and higher Spearman correlation with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art models on automatically grading essays, including models that have access to a gold-standard reference essay.","authors":["Kawin Ethayarajh","Dorsa Sadigh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation","tldr":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.7","presentation_id":"38939709","rocketchat_channel":"paper-eval4nlp-7","speakers":"Kawin Ethayarajh|Dorsa Sadigh","title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation"},{"content":{"abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","authors":["Daniel Deutsch","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","tldr":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the off...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.8","presentation_id":"38940784","rocketchat_channel":"paper-eval4nlp-8","speakers":"Daniel Deutsch|Dan Roth","title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics"},{"content":{"abstract":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better coverage of the space of valid translations and thereby improve its correlation with human judgments. Our experiments on the into-English language directions of the WMT19 metrics task (at both the system and sentence level) show that using paraphrased references does generally improve BLEU, and when it does, the more diverse the better. However, we also show that better results could be achieved if those paraphrases were to specifically target the parts of the space most relevant to the MT outputs being evaluated. Moreover, the gains remain slight even when human paraphrases are used, suggesting inherent limitations to BLEU\u2019s capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentence-level BLEU.","authors":["Rachel Bawden","Biao Zhang","Lisa Yankovskaya","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.82","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing","tldr":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better cove...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.815","presentation_id":"38940032","rocketchat_channel":"paper-eval4nlp-815","speakers":"Rachel Bawden|Biao Zhang|Lisa Yankovskaya|Andre T\u00e4ttar|Matt Post","title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing"},{"content":{"abstract":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.","authors":["Federico L\u00f3pez","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification","tldr":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.490","presentation_id":"38940646","rocketchat_channel":"paper-spnlp20-490","speakers":"Federico L\u00f3pez|Michael Strube","title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification"},{"content":{"abstract":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structured prediction problem, consisting in inferring the most probable instance of the semantic model given the text. In this work, we focus on the challenging sub-problem of cardinality prediction that consists in predicting the number of distinct individuals of each class in the semantic model. We show that cardinality prediction can successfully be approached by modeling the overall task as a joint inference problem, predicting the number of individuals of certain classes while at the same time extracting their properties. We approach this task with probabilistic graphical models computing the maximum-a-posteriori instance of the semantic model. Our main contribution lies on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury.","authors":["Hendrik ter Horst","Philipp Cimiano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension","tldr":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structure...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.10","presentation_id":"38940161","rocketchat_channel":"paper-spnlp20-10","speakers":"Hendrik ter Horst|Philipp Cimiano","title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension"},{"content":{"abstract":"","authors":["Yuntian Deng","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cascaded Text Generation with Markov Transformers","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.11","presentation_id":"38940151","rocketchat_channel":"paper-spnlp20-11","speakers":"Yuntian Deng|Alexander Rush","title":"Cascaded Text Generation with Markov Transformers"},{"content":{"abstract":"","authors":["Justin Chiu","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scaling Hidden Markov Language Models","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.12","presentation_id":"38940160","rocketchat_channel":"paper-spnlp20-12","speakers":"Justin Chiu|Alexander Rush","title":"Scaling Hidden Markov Language Models"},{"content":{"abstract":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.","authors":["Anh Duong Trinh","Robert J. Ross","John D. Kelleher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking","tldr":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, su...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.14","presentation_id":"38940154","rocketchat_channel":"paper-spnlp20-14","speakers":"Anh Duong Trinh|Robert J. Ross|John D. Kelleher","title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking"},{"content":{"abstract":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.","authors":["Ning Shi","Ziheng Zeng","Haotian Zhang","Yichen Gong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.159","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent Inference in Text Editing","tldr":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying dec...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1463","presentation_id":"38940648","rocketchat_channel":"paper-spnlp20-1463","speakers":"Ning Shi|Ziheng Zeng|Haotian Zhang|Yichen Gong","title":"Recurrent Inference in Text Editing"},{"content":{"abstract":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper, we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. The ability of our end-to-end method to retrieve structured information is assessed on a large set of business documents. We show that it performs competitively with a standard word classifier without requiring costly word level supervision.","authors":["Cl\u00e9ment Sage","Alex Aussem","V\u00e9ronique Eglin","Haytham Elghazel","J\u00e9r\u00e9my Espinas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks","tldr":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.16","presentation_id":"38940153","rocketchat_channel":"paper-spnlp20-16","speakers":"Cl\u00e9ment Sage|Alex Aussem|V\u00e9ronique Eglin|Haytham Elghazel|J\u00e9r\u00e9my Espinas","title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks"},{"content":{"abstract":"","authors":["Zihao Deng","Sijia Wang","Brendan Juba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntactically restricted self-attention for Semantic Role Labeling","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.17","presentation_id":"38940162","rocketchat_channel":"paper-spnlp20-17","speakers":"Zihao Deng|Sijia Wang|Brendan Juba","title":"Syntactically restricted self-attention for Semantic Role Labeling"},{"content":{"abstract":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an end-to-end deep learning framework of the quality estimation and automatic post-editing of the machine translation output. Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model. To imitate the behavior of human translators, we design three efficient delegation modules \u2013 quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them. We examine this approach with the English\u2013German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance. We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation.","authors":["Ke Wang","Jiayi Wang","Niyu Ge","Yangbin Shi","Yu Zhao","Kai Fan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.197","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing","tldr":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by p...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1774","presentation_id":"38940649","rocketchat_channel":"paper-spnlp20-1774","speakers":"Ke Wang|Jiayi Wang|Niyu Ge|Yangbin Shi|Yu Zhao|Kai Fan","title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing"},{"content":{"abstract":"","authors":["Manuel Widmoser","Maria Pacheco","Jean Honorio","Dan Goldwasser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Randomized Deep Structured Prediction for Argumentation Mining","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.19","presentation_id":"38940158","rocketchat_channel":"paper-spnlp20-19","speakers":"Manuel Widmoser|Maria Pacheco|Jean Honorio|Dan Goldwasser","title":"Randomized Deep Structured Prediction for Argumentation Mining"},{"content":{"abstract":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.","authors":["Noe Casas","Jos\u00e9 A. R. Fonollosa","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation","tldr":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradig...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2","presentation_id":"38940163","rocketchat_channel":"paper-spnlp20-2","speakers":"Noe Casas|Jos\u00e9 A. R. Fonollosa|Marta R. Costa-juss\u00e0","title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation"},{"content":{"abstract":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.","authors":["Nikolaos Manginas","Ilias Chalkidis","Prodromos Malakasiotis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations","tldr":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.20","presentation_id":"38940156","rocketchat_channel":"paper-spnlp20-20","speakers":"Nikolaos Manginas|Ilias Chalkidis|Prodromos Malakasiotis","title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations"},{"content":{"abstract":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both cost-augmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.","authors":["Lifu Tu","Richard Yuanzhe Pang","Kevin Gimpel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks","tldr":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structu...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.21","presentation_id":"38940143","rocketchat_channel":"paper-spnlp20-21","speakers":"Lifu Tu|Richard Yuanzhe Pang|Kevin Gimpel","title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks"},{"content":{"abstract":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.","authors":["Shucheng Li","Lingfei Wu","Shiwei Feng","Fangli Xu","Fengyuan Xu","Sheng Zhong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.255","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem","tldr":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as se...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2146","presentation_id":"38940650","rocketchat_channel":"paper-spnlp20-2146","speakers":"Shucheng Li|Lingfei Wu|Shiwei Feng|Fangli Xu|Fengyuan Xu|Sheng Zhong","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"content":{"abstract":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, \u201cSome person was born in some location at some time.\u201d We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.","authors":["Yunmo Chen","Tongfei Chen","Seth Ebner","Aaron Steven White","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reading the Manual: Event Extraction as Definition Comprehension","tldr":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.22","presentation_id":"38940159","rocketchat_channel":"paper-spnlp20-22","speakers":"Yunmo Chen|Tongfei Chen|Seth Ebner|Aaron Steven White|Benjamin Van Durme","title":"Reading the Manual: Event Extraction as Definition Comprehension"},{"content":{"abstract":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.","authors":["Daivik Swarup","Ahsaas Bajaj","Sheshera Mysore","Tim O\u2019Gorman","Rajarshi Das","Andrew McCallum"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.270","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text","tldr":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different way...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2220","presentation_id":"38940651","rocketchat_channel":"paper-spnlp20-2220","speakers":"Daivik Swarup|Ahsaas Bajaj|Sheshera Mysore|Tim O\u2019Gorman|Rajarshi Das|Andrew McCallum","title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text"},{"content":{"abstract":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.","authors":["Vikas Raunak","Siddharth Dalmia","Vivek Gupta","Florian Metze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.276","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Long-Tailed Phenomena in Neural Machine Translation","tldr":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2284","presentation_id":"38940652","rocketchat_channel":"paper-spnlp20-2284","speakers":"Vikas Raunak|Siddharth Dalmia|Vivek Gupta|Florian Metze","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"content":{"abstract":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y\u02c6 given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y\u02c6, y* | x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets. Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.","authors":["Jason Lee","Dustin Tran","Orhan Firat","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Discrepancy between Density Estimation and Sequence Generation","tldr":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.23","presentation_id":"38940144","rocketchat_channel":"paper-spnlp20-23","speakers":"Jason Lee|Dustin Tran|Orhan Firat|Kyunghyun Cho","title":"On the Discrepancy between Density Estimation and Sequence Generation"},{"content":{"abstract":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a target-to-source translation model and a language model. By applying Bayes\u2019 rule strategically, we reformulate this approach as a log-linear combination of translation, sentence-level and document-level language model probabilities. In addition to using static coefficients for each term, this formulation alternatively allows for the learning of dynamic per-token weights to more finely control the impact of the language models. Using both static or dynamic coefficients leads to improvements over a context-agnostic baseline and a context-aware concatenation model.","authors":["S\u00e9bastien Jean","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation","tldr":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a ta...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.24","presentation_id":"38940157","rocketchat_channel":"paper-spnlp20-24","speakers":"S\u00e9bastien Jean|Kyunghyun Cho","title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation"},{"content":{"abstract":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.","authors":["Alireza Mohammadshahi","James Henderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.294","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing","tldr":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dep...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2417","presentation_id":"38940653","rocketchat_channel":"paper-spnlp20-2417","speakers":"Alireza Mohammadshahi|James Henderson","title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing"},{"content":{"abstract":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for Natural Language Understanding without relying on a parser. Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our method is interpretable due to the bias introduced by the KR approach.","authors":["Arindam Mitra","Sanjay Narayana","Chitta Baral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective","tldr":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launc...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.26","presentation_id":"38940152","rocketchat_channel":"paper-spnlp20-26","speakers":"Arindam Mitra|Sanjay Narayana|Chitta Baral","title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective"},{"content":{"abstract":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2018) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness.","authors":["Renato Negrinho","Matthew R. Gormley","Geoff Gordon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.406","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Investigation of Beam-Aware Training in Supertagging","tldr":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.3373","presentation_id":"38940654","rocketchat_channel":"paper-spnlp20-3373","speakers":"Renato Negrinho|Matthew R. Gormley|Geoff Gordon","title":"An Empirical Investigation of Beam-Aware Training in Supertagging"},{"content":{"abstract":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.","authors":["Abhinav Singh","Patrick Xia","Guanghui Qin","Mahsa Yarmohammadi","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models","tldr":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where eac...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.4","presentation_id":"38940142","rocketchat_channel":"paper-spnlp20-4","speakers":"Abhinav Singh|Patrick Xia|Guanghui Qin|Mahsa Yarmohammadi|Benjamin Van Durme","title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models"},{"content":{"abstract":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset for navigation domain.","authors":["Ke Tran","Ming Tan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations","tldr":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabiliti...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.7","presentation_id":"38940155","rocketchat_channel":"paper-spnlp20-7","speakers":"Ke Tran|Ming Tan","title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations"},{"content":{"abstract":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.","authors":["Youngbin Ro","Yukyung Lee","Pilsung Kang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.99","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT","tldr":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.957","presentation_id":"38940647","rocketchat_channel":"paper-spnlp20-957","speakers":"Youngbin Ro|Yukyung Lee|Pilsung Kang","title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT"},{"content":{"abstract":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.","authors":["Elman Mansimov","Mitchell Stern","Mia Chen","Orhan Firat","Jakob Uszkoreit","Puneet Jain"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards End-to-End In-Image Neural Machine Translation","tldr":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model...","track":"NLP Beyond Text"},"id":"WS-23.106","presentation_id":"38939782","rocketchat_channel":"paper-nlpbt2020-106","speakers":"Elman Mansimov|Mitchell Stern|Mia Chen|Orhan Firat|Jakob Uszkoreit|Puneet Jain","title":"Towards End-to-End In-Image Neural Machine Translation"},{"content":{"abstract":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic inputs from a wide range of datasets to challenge, and sometimes surpass, the state-of-the-art in the field. To demonstrate the efficiency of our models, we carefully evaluate their performances on the IEMOCAP, MOSI, MOSEI and MELD dataset. The experiments can be directly replicated and the code is fully open for future researches.","authors":["Jean-Benoit Delbrouck","No\u00e9 Tits","St\u00e9phane Dupont"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition","tldr":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic ...","track":"NLP Beyond Text"},"id":"WS-23.110","presentation_id":"38939779","rocketchat_channel":"paper-nlpbt2020-110","speakers":"Jean-Benoit Delbrouck|No\u00e9 Tits|St\u00e9phane Dupont","title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition"},{"content":{"abstract":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.","authors":["Tejas Srinivasan","Ramon Sanabria","Florian Metze","Desmond Elliott"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multimodal Speech Recognition with Unstructured Audio Masking","tldr":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fix...","track":"NLP Beyond Text"},"id":"WS-23.114","presentation_id":"38939780","rocketchat_channel":"paper-nlpbt2020-114","speakers":"Tejas Srinivasan|Ramon Sanabria|Florian Metze|Desmond Elliott","title":"Multimodal Speech Recognition with Unstructured Audio Masking"},{"content":{"abstract":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.","authors":["Aman Khullar","Udit Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention","tldr":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only util...","track":"NLP Beyond Text"},"id":"WS-23.119","presentation_id":"38939781","rocketchat_channel":"paper-nlpbt2020-119","speakers":"Aman Khullar|Udit Arora","title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention"},{"content":{"abstract":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model\u2019s performance particularly improved on questions that required coreference resolution.","authors":["Muhammad Shah","Shikib Mehri","Tejas Srinivasan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reasoning Over History: Context Aware Visual Dialog","tldr":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existin...","track":"NLP Beyond Text"},"id":"WS-23.122","presentation_id":"38939783","rocketchat_channel":"paper-nlpbt2020-122","speakers":"Muhammad Shah|Shikib Mehri|Tejas Srinivasan","title":"Reasoning Over History: Context Aware Visual Dialog"},{"content":{"abstract":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle","authors":["Chaitanya Ahuja","Dong Won Lee","Ryo Ishii","Louis-Philippe Morency"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.170","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures","tldr":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made a...","track":"NLP Beyond Text"},"id":"WS-23.1589","presentation_id":"38940175","rocketchat_channel":"paper-nlpbt2020-1589","speakers":"Chaitanya Ahuja|Dong Won Lee|Ryo Ishii|Louis-Philippe Morency","title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures"},{"content":{"abstract":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process.","authors":["Wanqing Cui","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.392","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Language: Learning Commonsense from Images for Reasoning","tldr":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thous...","track":"NLP Beyond Text"},"id":"WS-23.3273","presentation_id":"38940176","rocketchat_channel":"paper-nlpbt2020-3273","speakers":"Wanqing Cui|Yanyan Lan|Liang Pang|Jiafeng Guo|Xueqi Cheng","title":"Beyond Language: Learning Commonsense from Images for Reasoning"},{"content":{"abstract":"","authors":["Loic Barrault"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Vision on (Simultaneous) Multimodal Machine Translation","tldr":null,"track":"NLP Beyond Text"},"id":"WS-23.Loic","presentation_id":"38939784","rocketchat_channel":"paper-nlpbt2020-Loic","speakers":"Loic Barrault","title":"A Vision on (Simultaneous) Multimodal Machine Translation"},{"content":{"abstract":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get the summaries, and fuses the summaries. Recently, some multi-modal models based on the architecture of BERT are proposed such as ViLBERT. However, they can only be pretrained on the image-text data. In this paper, we propose an image-text model for sarcasm detection using the pretrained BERT and ResNet without any further pretraining. BERT and ResNet have been pretrained on much larger text or image data than image-text data. We connect the vector spaces of BERT and ResNet to utilize more data. We use the pretrained Multi-Head Attention of BERT to model the text and image. Besides, we propose a 2D-Intra-Attention to extract the relationships between words and images. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Xiaowen Sun","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data","tldr":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get t...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.3","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-3","speakers":"Xinyu Wang|Xiaowen Sun|Tan Yang|Hongbo Wang","title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data"},{"content":{"abstract":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.","authors":["Frank F. Xu","Lei Ji","Botian Shi","Junyi Du","Graham Neubig","Yonatan Bisk","Nan Duan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos","tldr":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quant...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.4","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-4","speakers":"Frank F. Xu|Lei Ji|Botian Shi|Junyi Du|Graham Neubig|Yonatan Bisk|Nan Duan","title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos"},{"content":{"abstract":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped English LibriVox audiobooks and their corresponding English Gutenberg Project e-books to Italian e-books with a set of three complementary methods. Then we aligned the English and the Italian texts using both traditional Gale-Church based alignment methods and a recently proposed tool to perform bilingual sentences alignment computing the cosine similarity of multilingual sentence embeddings. Finally, we forced the alignment between the English audiobooks and the English side of our textual parallel corpus with a text-to-speech and dynamic time warping based forced alignment tool. For each step, we provide the reader with a critical discussion based on detailed evaluation and comparison of the results of the different methods.","authors":["Giuseppe Della Corte","Sara Stymne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation","tldr":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped En...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.5","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-5","speakers":"Giuseppe Della Corte|Sara Stymne","title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation"},{"content":{"abstract":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situations, where the answers are more likely to be sentences rather than single words. To bridge the gap between this natural VQA and existing VQA approaches, a novel unsupervised keyword extraction method is proposed. The method is based on the principle that the full-sentence answers can be decomposed into two parts: one that contains new information answering the question (i.e. keywords), and one that contains information already included in the question. Discriminative decoders were designed to achieve such decomposition, and the method was experimentally implemented on VQA datasets containing full-sentence answers. The results show that the proposed model can accurately extract the keywords without being given explicit annotations describing them.","authors":["Kohei Uehara","Tatsuya Harada"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Keyword Extraction for Full-Sentence VQA","tldr":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situation...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.6","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-6","speakers":"Kohei Uehara|Tatsuya Harada","title":"Unsupervised Keyword Extraction for Full-Sentence VQA"},{"content":{"abstract":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution defined by the ground truth labels). In this paper, we show that a malicious modeler, upon obtaining access to the Log-Loss scores on its predictions, can exploit this information to infer all the ground truth labels of arbitrary test datasets with full accuracy. We provide an efficient algorithm to perform this inference. A particularly interesting application where this attack can be exploited is to breach privacy in the setting of Membership Inference Attacks. These attacks exploit the vulnerabilities of exposing models trained on customer data to queries made by an adversary. Privacy auditing tools for measuring leakage from sensitive datasets assess the total privacy leakage based on the adversary\u2019s predictions for datapoint membership. An instance of the proposed attack can hence, cause complete membership privacy breach, obviating any attack model training or access to side knowledge with the adversary. Moreover, our algorithm is agnostic to the model under attack and hence, enables perfect membership inference even for models that do not memorize or overfit. In particular, our observations provide insight into the extent of information leakage from statistical aggregates and how they can be exploited.","authors":["Abhinav Aggarwal","Zekun Xu","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Log-Loss Scores and (No) Privacy","tldr":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.1","presentation_id":"38939769","rocketchat_channel":"paper-privatenlp2020-1","speakers":"Abhinav Aggarwal|Zekun Xu|Oluwaseyi Feyisetan|Nathanael Teissier","title":"On Log-Loss Scores and (No) Privacy"},{"content":{"abstract":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-centric encoders and leverage them to create an interface that allows users to search for privacy settings using free-form queries. In order to achieve this goal, we create a custom Semantic Similarity dataset, which consists of real user queries covering various privacy settings. We then use this dataset to fine-tune a state of the art encoder. Using this fine-tuned encoder, we perform semantic matching between the user queries and the privacy settings to retrieve the most relevant setting. Finally, we also use the encoder to generate embeddings of privacy settings from the top 100 websites and perform unsupervised clustering to learn about the online privacy settings types. We find that the most common type of privacy settings are \u2018Personalization\u2019 and \u2018Notifications\u2019, with coverage of 35.8% and 34.4%, respectively, in our dataset.","authors":["Rishabh Khandelwal","Asmit Nayak","Yao Yao","Kassem Fawaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Surfacing Privacy Settings Using Semantic Matching","tldr":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-cent...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.11","presentation_id":"38939773","rocketchat_channel":"paper-privatenlp2020-11","speakers":"Rishabh Khandelwal|Asmit Nayak|Yao Yao|Kassem Fawaz","title":"Surfacing Privacy Settings Using Semantic Matching"},{"content":{"abstract":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.","authors":["Gavin Kerrigan","Dylan Slack","Jens Tuyls"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Differentially Private Language Models Benefit from Public Pre-training","tldr":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorit...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.12","presentation_id":"38939774","rocketchat_channel":"paper-privatenlp2020-12","speakers":"Gavin Kerrigan|Dylan Slack|Jens Tuyls","title":"Differentially Private Language Models Benefit from Public Pre-training"},{"content":{"abstract":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is first mapped into a continuous embedding space, perturbed by sampling a spherical noise from an appropriate distribution, and then projected back to the discrete vocabulary space. While this allows the perturbation to admit the required metric differential privacy, often the utility of downstream tasks modeled on this perturbed data is low because the spherical noise does not account for the variability in the density around different words in the embedding space. In particular, words in a sparse region are likely unchanged even when the noise scale is large. In this paper, we propose a text perturbation mechanism based on a carefully designed regularized variant of the Mahalanobis metric to overcome this problem. For any given noise scale, this metric adds an elliptical noise to account for the covariance structure in the embedding space. This heterogeneity in the noise scale along different directions helps ensure that the words in the sparse region have sufficient likelihood of replacement without sacrificing the overall utility. We provide a text-perturbation algorithm based on this metric and formally prove its privacy guarantees. Additionally, we empirically show that our mechanism improves the privacy statistics to achieve the same level of utility as compared to the state-of-the-art Laplace mechanism.","authors":["Zekun Xu","Abhinav Aggarwal","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric","tldr":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is firs...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2","presentation_id":"38939770","rocketchat_channel":"paper-privatenlp2020-2","speakers":"Zekun Xu|Abhinav Aggarwal|Oluwaseyi Feyisetan|Nathanael Teissier","title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric"},{"content":{"abstract":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and social networks, as well as the power of modern AI to synthesize and gain insights from this data. This paper presents an approach to detect emotional and informational self-disclosure in natural language. We hypothesize that identifying frame semantics can meaningfully support this task. Specifically, we use Semantic Role Labeling to identify the lexical units and their semantic roles that signal self-disclosure. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.","authors":["Chandan Akiti","Anna Squicciarini","Sarah Rajtmajer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.312","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content","tldr":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal infor...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2534","presentation_id":"38940639","rocketchat_channel":"paper-privatenlp2020-2534","speakers":"Chandan Akiti|Anna Squicciarini|Sarah Rajtmajer","title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content"},{"content":{"abstract":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.","authors":["Yangsibo Huang","Zhao Song","Danqi Chen","Kai Li","Sanjeev Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.123","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TextHide: Tackling Data Privacy in Language Understanding Tasks","tldr":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language unders...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.3","presentation_id":"38939771","rocketchat_channel":"paper-privatenlp2020-3","speakers":"Yangsibo Huang|Zhao Song|Danqi Chen|Kai Li|Sanjeev Arora","title":"TextHide: Tackling Data Privacy in Language Understanding Tasks"},{"content":{"abstract":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations.","authors":["Mitra Bokaie Hosseini","Pragyan K C","Irwin Reyes","Serge Egelman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies","tldr":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulation...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.9","presentation_id":"38939772","rocketchat_channel":"paper-privatenlp2020-9","speakers":"Mitra Bokaie Hosseini|Pragyan K C|Irwin Reyes|Serge Egelman","title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies"},{"content":{"abstract":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving millions of clients. They cannot afford traditional fine tuning for individual clients. Many clients cannot even afford significant fine tuning, and own little or no labeled data. Recognizing that word usage and salience diversity across clients leads to reduced accuracy, we initiate a study of practical and lightweight adaptation of centralized NLP services to clients. Each client uses an unsupervised, corpus-based sketch to register to the service. The server modifies its network mildly to accommodate client sketches, and occasionally trains the augmented network over existing clients. When a new client registers with its sketch, it gets immediate accuracy benefits. We demonstrate the proposed architecture using sentiment labeling, NER, and predictive language modeling.","authors":["Sahil Shah","Vihari Piratla","Soumen Chakrabarti","Sunita Sarawagi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.357","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP Service APIs and Models for Efficient Registration of New Clients","tldr":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2976","presentation_id":"38940136","rocketchat_channel":"paper-blackboxnlp2020-2976","speakers":"Sahil Shah|Vihari Piratla|Soumen Chakrabarti|Sunita Sarawagi","title":"NLP Service APIs and Models for Efficient Registration of New Clients"},{"content":{"abstract":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages \u2013 developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance \u2013 a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.","authors":["Yilin Yang","Longyue Wang","Shuming Shi","Prasad Tadepalli","Stefan Lee","Zhaopeng Tu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.432","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Sub-layer Functionalities of Transformer Decoder","tldr":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.3561","presentation_id":"38940141","rocketchat_channel":"paper-blackboxnlp2020-3561","speakers":"Yilin Yang|Longyue Wang|Shuming Shi|Prasad Tadepalli|Stefan Lee|Zhaopeng Tu","title":"On the Sub-layer Functionalities of Transformer Decoder"},{"content":{"abstract":"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.","authors":["Jasmijn Bastings","Katja Filippova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?","tldr":"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.33","presentation_id":"38939764","rocketchat_channel":"paper-blackboxnlp2020-33","speakers":"Jasmijn Bastings|Katja Filippova","title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?"},{"content":{"abstract":"Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model\u2019s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.","authors":["Rajiv Movva","Jason Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation","tldr":"Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model\u2019s learned representations. By probing Transformers with more and mo...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.43","presentation_id":"38939765","rocketchat_channel":"paper-blackboxnlp2020-43","speakers":"Rajiv Movva|Jason Zhao","title":"Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation"},{"content":{"abstract":"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that \u201cthe doctor visited the lawyer\u201d does not entail \u201cthe lawyer visited the doctor\u201d), accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.","authors":["R. Thomas McCoy","Junghyun Min","Tal Linzen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance","tldr":"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Infere...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.45","presentation_id":"38939766","rocketchat_channel":"paper-blackboxnlp2020-45","speakers":"R. Thomas McCoy|Junghyun Min|Tal Linzen","title":"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance"},{"content":{"abstract":"Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.","authors":["Benjamin Newman","John Hewitt","Percy Liang","Christopher D. Manning"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The EOS Decision and Length Extrapolation","tldr":"Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process t...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.54","presentation_id":"38939767","rocketchat_channel":"paper-blackboxnlp2020-54","speakers":"Benjamin Newman|John Hewitt|Percy Liang|Christopher D. Manning","title":"The EOS Decision and Length Extrapolation"},{"content":{"abstract":"Interpretability methods for neural networks are difficult to evaluate because we do not understand the black-box models typically used to test them. This paper proposes a framework in which interpretability methods are evaluated using manually constructed networks, which we call white-box networks, whose behavior is understood a priori. We evaluate five methods for producing attribution heatmaps by applying them to white-box LSTM classifiers for tasks based on formal languages. Although our white-box classifiers solve their tasks perfectly and transparently, we find that all five attribution methods fail to produce the expected model explanations.","authors":["Yiding Hao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Attribution Methods using White-Box LSTMs","tldr":"Interpretability methods for neural networks are difficult to evaluate because we do not understand the black-box models typically used to test them. This paper proposes a framework in which interpretability methods are evaluated using manually const...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.59","presentation_id":"38939768","rocketchat_channel":"paper-blackboxnlp2020-59","speakers":"Yiding Hao","title":"Evaluating Attribution Methods using White-Box LSTMs"},{"content":{"abstract":"In this paper we introduce diagNNose, an open source library for analysing the activations of deep neural networks. diagNNose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of diagNNose with a case study on subject-verb agreement within language models. diagNNose is available at https://github.com/i-machine-think/diagnnose.","authors":["Jaap Jumelet"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"diagNNose: A Library for Neural Activation Analysis","tldr":"In this paper we introduce diagNNose, an open source library for analysing the activations of deep neural networks. diagNNose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural net...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.70","presentation_id":"38940638","rocketchat_channel":"paper-blackboxnlp2020-70","speakers":"Jaap Jumelet","title":"diagNNose: A Library for Neural Activation Analysis"},{"content":{"abstract":"While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques\u2014supervised probing, unsupervised similarity analysis, and layer-based ablations\u2014we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.","authors":["Amil Merchant","Elahe Rahimtoroghi","Ellie Pavlick","Ian Tenney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Happens To BERT Embeddings During Fine-tuning?","tldr":"While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis tech...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.8","presentation_id":"38939763","rocketchat_channel":"paper-blackboxnlp2020-8","speakers":"Amil Merchant|Elahe Rahimtoroghi|Ellie Pavlick|Ian Tenney","title":"What Happens To BERT Embeddings During Fine-tuning?"},{"content":{"abstract":"Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT\u2019s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, varying from a low of 17.77% for Place to a high of 51.61% for Artifact. Next, we find that linear combinations of these heads, estimated with approx. 11% of available total event argument detection supervision, can push performance well higher for some roles \u2014 highest two being Victim (68.29% Accuracy) and Artifact (58.82% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate \u201cbest heads\u201d for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in another sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated \u201cnonce\u201d words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.","authors":["Varun Gangal","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset","tldr":"Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT\u2019s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.1","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-1","speakers":"Varun Gangal|Eduard Hovy","title":"BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset"},{"content":{"abstract":"Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them.","authors":["Eugene Kharitonov","Marco Baroni"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emergent Language Generalization and Acquisition Speed are not tied to Compositionality","tldr":"Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the ag...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.2","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-2","speakers":"Eugene Kharitonov|Marco Baroni","title":"Emergent Language Generalization and Acquisition Speed are not tied to Compositionality"},{"content":{"abstract":"Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sentential, rhetorical knowledge. In this paper, we propose a method that quantitatively evaluates the rhetorical capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method shows an avenue towards quantifying the rhetorical capacities of neural LMs.","authors":["Zining Zhu","Chuer Pan","Mohamed Abdalla","Frank Rudzicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Examining the rhetorical capacities of neural language models","tldr":"Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sententia...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.3","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-3","speakers":"Zining Zhu|Chuer Pan|Mohamed Abdalla|Frank Rudzicz","title":"Examining the rhetorical capacities of neural language models"},{"content":{"abstract":"Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations.","authors":["Hila Gonen","Shauli Ravfogel","Yanai Elazar","Yoav Goldberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"It\u2019s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT","tldr":"Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that e...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.5","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-5","speakers":"Hila Gonen|Shauli Ravfogel|Yanai Elazar|Yoav Goldberg","title":"It\u2019s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT"},{"content":{"abstract":"We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY\u2014a white box attack\u2014performed on the approximate model by 25% F1, and the ADDSENT attack\u2014a black box attack\u2014by 11% F1.","authors":["Naveen Jafer Nizar","Ari Kobren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Leveraging Extracted Model Adversaries for Improved Black Box Attacks","tldr":"We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we u...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.6","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-6","speakers":"Naveen Jafer Nizar|Ari Kobren","title":"Leveraging Extracted Model Adversaries for Improved Black Box Attacks"},{"content":{"abstract":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.","authors":["Marius Mosbach","Anna Khokhlova","Michael A. Hedderich","Dietrich Klakow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers","tldr":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, u...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.7","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-7","speakers":"Marius Mosbach|Anna Khokhlova|Michael A. Hedderich|Dietrich Klakow","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"content":{"abstract":"It is challenging to automatically evaluate the answer of a QA model at inference time. Although many models provide confidence scores, and simple heuristics can go a long way towards indicating answer correctness, such measures are heavily dataset-dependent and are unlikely to generalise. In this work, we begin by investigating the hidden representations of questions, answers, and contexts in transformer-based QA architectures. We observe a consistent pattern in the answer representations, which we show can be used to automatically evaluate whether or not a predicted answer span is correct. Our method does not require any labelled data and outperforms strong heuristic baselines, across 2 datasets and 7 domains. We are able to predict whether or not a model\u2019s answer is correct with 91.37% accuracy on SQuAD, and 80.7% accuracy on SubjQA. We expect that this method will have broad applications, e.g., in semi-automatic development of QA datasets.","authors":["Lukas Muttenthaler","Isabelle Augenstein","Johannes Bjerva"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Evaluation for Question Answering with Transformers","tldr":"It is challenging to automatically evaluate the answer of a QA model at inference time. Although many models provide confidence scores, and simple heuristics can go a long way towards indicating answer correctness, such measures are heavily dataset-d...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.8","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-8","speakers":"Lukas Muttenthaler|Isabelle Augenstein|Johannes Bjerva","title":"Unsupervised Evaluation for Question Answering with Transformers"},{"content":{"abstract":"Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.","authors":["Shauli Ravfogel","Yanai Elazar","Jacob Goldberger","Yoav Goldberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Distillation of Syntactic Information from Contextualized Word Representations","tldr":"Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language represe...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.9","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-9","speakers":"Shauli Ravfogel|Yanai Elazar|Jacob Goldberger|Yoav Goldberg","title":"Unsupervised Distillation of Syntactic Information from Contextualized Word Representations"},{"content":{"abstract":"Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier\u2019s decision. We use this framework to compare several explainers, including gradient methods, erasure, and attention mechanisms, in terms of their communication success. In addition, we reinterpret these methods in the light of classical feature selection, and use this as inspiration for new embedded explainers, through the use of selective, sparse attention. Experiments in text classification and natural language inference, using different configurations of explainers and laypeople (including both machines and humans), reveal an advantage of attention-based explainers over gradient and erasure methods, and show that selective attention is a simpler alternative to stochastic rationalizers. Human experiments show strong results on text classification with post-hoc explainers trained to optimize communication success.","authors":["Marcos Treviso","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Explanation Game: Towards Prediction Explainability through Sparse Communication","tldr":"Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier\u2019s decision. We use this framework to compare s...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.10","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-10","speakers":"Marcos Treviso|Andr\u00e9 F. T. Martins","title":"The Explanation Game: Towards Prediction Explainability through Sparse Communication"},{"content":{"abstract":"Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM (Shen et al., 2019), which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that (1) the model has reasonably consistent parsing behaviors across different restarts, (2) the model struggles with the internal structures of complex noun phrases, (3) the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling.","authors":["Yian Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?","tldr":"Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM (Shen et al., 2019), which is trained on language modelling and has near-state-of-the-art performance...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.11","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-11","speakers":"Yian Zhang","title":"Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?"},{"content":{"abstract":"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models\u2019 performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.","authors":["Chuanrong Li","Lin Shengshuo","Zeyu Liu","Xinyi Wu","Xuhui Zhou","Shane Steinert-Threlkeld"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets","tldr":"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.12","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-12","speakers":"Chuanrong Li|Lin Shengshuo|Zeyu Liu|Xinyi Wu|Xuhui Zhou|Shane Steinert-Threlkeld","title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets"},{"content":{"abstract":"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets.","authors":["Hande Celikkanat","Sami Virpioja","J\u00f6rg Tiedemann","Marianna Apidianaki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations","tldr":"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of em...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.13","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-13","speakers":"Hande Celikkanat|Sami Virpioja|J\u00f6rg Tiedemann|Marianna Apidianaki","title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations"},{"content":{"abstract":"The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.","authors":["David Yenicelik","Florian Schmidt","Yannic Kilcher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How does BERT capture semantics? A closer look at polysemous words","tldr":"The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polys...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.15","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-15","speakers":"David Yenicelik|Florian Schmidt|Yannic Kilcher","title":"How does BERT capture semantics? A closer look at polysemous words"},{"content":{"abstract":"We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on lexical entailment and negation. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.","authors":["Atticus Geiger","Kyle Richardson","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation","tldr":"We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systemati...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.16","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-16","speakers":"Atticus Geiger|Kyle Richardson|Christopher Potts","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"content":{"abstract":"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT\u2019s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.","authors":["Jaspreet Singh","Jonas Wallat","Avishek Anand"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT","tldr":"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it capture...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.17","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-17","speakers":"Jaspreet Singh|Jonas Wallat|Avishek Anand","title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT"},{"content":{"abstract":"Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to investigate for evidence of compositional reasoning over numerical data in various natural language number systems. By using both grammaticality judgment and value comparison classification tasks in English, Japanese, Danish, and French, we find evidence that the information encoded in these pretrained models\u2019 embeddings is sufficient for grammaticality judgments but generally not for value comparisons. We analyze possible reasons for this and discuss how our tasks could be extended in further studies.","authors":["Devin Johnson","Denise Mak","Andrew Barker","Lexi Loessberg-Zahl"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models","tldr":"Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual prob...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.18","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-18","speakers":"Devin Johnson|Denise Mak|Andrew Barker|Lexi Loessberg-Zahl","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"content":{"abstract":"Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task structure, while probing task evaluations often look at only a few attributes and models. We address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.","authors":["Andrew Runge","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Neural Entity Representations for Semantic Information","tldr":"Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task struc...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.20","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-20","speakers":"Andrew Runge|Eduard Hovy","title":"Exploring Neural Entity Representations for Semantic Information"},{"content":{"abstract":"Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.","authors":["John Morris"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples","tldr":"Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is dete...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.22","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-22","speakers":"John Morris","title":"Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples"},{"content":{"abstract":"How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model\u2019s output is changed in the way predicted by our analysis.","authors":["Paul Soulos","R. Thomas McCoy","Tal Linzen","Paul Smolensky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks","tldr":"How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.23","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-23","speakers":"Paul Soulos|R. Thomas McCoy|Tal Linzen|Paul Smolensky","title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks"},{"content":{"abstract":"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing (NLP) models. Very recent work suggests that the self-attention in the Transformer encodes syntactic information; Here, we show that self-attention scores encode semantics by considering sentiment analysis tasks. In contrast to gradient-based feature attribution methods, we propose a simple and effective Layer-wise Attention Tracing (LAT) method to analyze structured attention weights. We apply our method to Transformer models trained on two tasks that have surface dissimilarities, but share common semantics\u2014sentiment analysis of movie reviews and time-series valence prediction in life story narratives. Across both tasks, words with high aggregated attention weights were rich in emotional semantics, as quantitatively validated by an emotion lexicon labeled by human annotators. Our results show that structured attention weights encode rich semantics in sentiment analysis, and match human interpretations of semantics.","authors":["Zhengxuan Wu","Thanh-Son Nguyen","Desmond Ong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis","tldr":"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing (NLP) models. Very recent work suggests that the self-attention in the Transformer encodes synta...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.24","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-24","speakers":"Zhengxuan Wu|Thanh-Son Nguyen|Desmond Ong","title":"Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis"},{"content":{"abstract":"Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT\u2019s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.","authors":["Tristan Thrush","Ethan Wilcox","Roger Levy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization","tldr":"Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We addr...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.25","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-25","speakers":"Tristan Thrush|Ethan Wilcox|Roger Levy","title":"Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization"},{"content":{"abstract":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results.","authors":["Xikun Zhang","Deepak Ramachandran","Ian Tenney","Yanai Elazar","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do Language Embeddings capture Scales?","tldr":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.27","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-27","speakers":"Xikun Zhang|Deepak Ramachandran|Ian Tenney|Yanai Elazar|Dan Roth","title":"Do Language Embeddings capture Scales?"},{"content":{"abstract":"With the increase in the use of AI systems, a need for explanation systems arises. Building an explanation system requires a definition of explanation. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences. We study multiple perspectives and aspects of explainability of recommendations or predictions made by AI systems, and provide a generic definition of explanation. The proposed definition is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.","authors":["Tejaswani Verma","Christoph Lingenfelder","Dietrich Klakow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Defining Explanation in an AI Context","tldr":"With the increase in the use of AI systems, a need for explanation systems arises. Building an explanation system requires a definition of explanation. However, the natural language term explanation is difficult to define formally as it includes mult...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.29","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-29","speakers":"Tejaswani Verma|Christoph Lingenfelder|Dietrich Klakow","title":"Defining Explanation in an AI Context"},{"content":{"abstract":"We study the behavior of several black-box search algorithms used for generating adversarial examples for natural language processing (NLP) tasks. We perform a fine-grained analysis of three elements relevant to search: search algorithm, search space, and search budget. When new search algorithms are proposed in past work, the attack search space is often modified alongside the search algorithm. Without ablation studies benchmarking the search algorithm change with the search space held constant, one cannot tell if an increase in attack success rate is a result of an improved search algorithm or a less restrictive search space. Additionally, many previous studies fail to properly consider the search algorithms\u2019 run-time cost, which is essential for downstream tasks like adversarial training. Our experiments provide a reproducible benchmark of search algorithms across a variety of search spaces and query budgets to guide future research in adversarial NLP. Based on our experiments, we recommend greedy attacks with word importance ranking when under a time constraint or attacking long inputs, and either beam search or particle swarm optimization otherwise.","authors":["Jin Yong Yoo","John Morris","Eli Lifland","Yanjun Qi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples","tldr":"We study the behavior of several black-box search algorithms used for generating adversarial examples for natural language processing (NLP) tasks. We perform a fine-grained analysis of three elements relevant to search: search algorithm, search space...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.30","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-30","speakers":"Jin Yong Yoo|John Morris|Eli Lifland|Yanjun Qi","title":"Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples"},{"content":{"abstract":"Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the ability of these models to generalize these capacities to novel words. This type of generalization is exhibited by humans, and is intimately related to morphology\u2014humans are in many cases able to identify inflections of novel words in the appropriate context. This type of morphological capacity has not been previously tested in BERT models, and is important for morphologically-rich languages, which are under-studied in the literature regarding BERT\u2019s linguistic capacities. In this work, we investigate this by considering monolingual and multilingual BERT models\u2019 abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch. We find that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of BERT models.","authors":["Coleman Haley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"This is a BERT. Now there are several of them. Can they generalize to novel words?","tldr":"Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the a...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.31","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-31","speakers":"Coleman Haley","title":"This is a BERT. Now there are several of them. Can they generalize to novel words?"},{"content":{"abstract":"Public health surveillance and tracking virus via social media can be a useful digital tool for contact tracing and preventing the spread of the virus. Nowadays, large volumes of COVID-19 tweets can quickly be processed in real-time to offer information to researchers. Nonetheless, due to the absence of labeled data for COVID-19, the preliminary supervised classifier or semi-supervised self-labeled methods will not handle non-spherical data with adequate accuracy. With the seasonal influenza and novel Coronavirus having many similar symptoms, we propose using few shot learning to fine-tune a semi-supervised model built on unlabeled COVID-19 and previously labeled influenza dataset that can provide in- sights into COVID-19 that have not been investigated. The experimental results show the efficacy of the proposed model with an accuracy of 86%, identification of Covid-19 related discussion using recently collected tweets.","authors":["Brandon Lwowski","Peyman Najafirad"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19 Surveillance through Twitter using Self-Supervised and Few Shot Learning","tldr":"Public health surveillance and tracking virus via social media can be a useful digital tool for contact tracing and preventing the spread of the virus. Nowadays, large volumes of COVID-19 tweets can quickly be processed in real-time to offer informat...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.1","presentation_id":"38939841","rocketchat_channel":"paper-nlp-covid19-emnlp-1","speakers":"Brandon Lwowski|Peyman Najafirad","title":"COVID-19 Surveillance through Twitter using Self-Supervised and Few Shot Learning"},{"content":{"abstract":"The Covid-19 pandemic urged the scientific community to join efforts at an unprecedented scale, leading to faster than ever dissemination of data and results, which in turn motivated more research works. This paper presents and discusses information retrieval models aimed at addressing the challenge of searching the large number of publications that stem from these studies. The model presented, based on classical baselines followed by an interaction based neural ranking model, was evaluated and evolved within the TREC Covid challenge setting. Results on this dataset show that, when starting with a strong baseline, our light neural ranking model can achieve results that are comparable to other model architectures that use very large number of parameters.","authors":["Tiago Almeida","S\u00e9rgio Matos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Frugal neural reranking: evaluation on the Covid-19 literature","tldr":"The Covid-19 pandemic urged the scientific community to join efforts at an unprecedented scale, leading to faster than ever dissemination of data and results, which in turn motivated more research works. This paper presents and discusses information ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.10","presentation_id":"38939845","rocketchat_channel":"paper-nlp-covid19-emnlp-10","speakers":"Tiago Almeida|S\u00e9rgio Matos","title":"Frugal neural reranking: evaluation on the Covid-19 literature"},{"content":{"abstract":"Ever since the COVID-19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioinformaticians, nurses, data scientists, and all of the affiliated relevant disciplines have been mobilized to help discover efficient treatments for the infected population, as well as a vaccine solution to prevent further the virus spread. In this combat against the virus responsible for the pandemic, key for any advancements is the timely, accurate, peer-reviewed, and efficient communication of any novel research findings. In this paper we present a novel framework to address the information need of filtering efficiently the scientific bibliography for relevant literature around COVID-19. The contributions of the paper are summarized in the following: we define and describe the information need that encompasses the major requirements for COVID-19 articles relevancy, we present and release an expert-curated benchmark set for the task, and we analyze the performance of several state-of-the-art machine learning classifiers that may distinguish the relevant from the non-relevant COVID-19 literature.","authors":["Zubair Afzal","Vikrant Yadav","Olga Fedorova","Vaishnavi Kandala","Janneke van de Loo","Saber A. Akhondi","Pascal Coupet","George Tsatsaronis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles","tldr":"Ever since the COVID-19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioi...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.16","presentation_id":"38939846","rocketchat_channel":"paper-nlp-covid19-emnlp-16","speakers":"Zubair Afzal|Vikrant Yadav|Olga Fedorova|Vaishnavi Kandala|Janneke van de Loo|Saber A. Akhondi|Pascal Coupet|George Tsatsaronis","title":"CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles"},{"content":{"abstract":"We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.","authors":["Alexandre B\u00e9rard","Zae Myung Kim","Vassilina Nikoulina","Eunjeong Lucy Park","Matthias Gall\u00e9"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Multilingual Neural Machine Translation Model for Biomedical Data","tldr":"We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large am...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2","presentation_id":"38939842","rocketchat_channel":"paper-nlp-covid19-emnlp-2","speakers":"Alexandre B\u00e9rard|Zae Myung Kim|Vassilina Nikoulina|Eunjeong Lucy Park|Matthias Gall\u00e9","title":"A Multilingual Neural Machine Translation Model for Biomedical Data"},{"content":{"abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of pandemic-related discussion. Next, we examine the volume of activity in order to determine whether the number of people discussing mental health has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","authors":["Laura Biester","Katie Matton","Janarthanan Rajendran","Emily Mower Provost","Rada Mihalcea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","tldr":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to bette...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.20","presentation_id":"38939847","rocketchat_channel":"paper-nlp-covid19-emnlp-20","speakers":"Laura Biester|Katie Matton|Janarthanan Rajendran|Emily Mower Provost|Rada Mihalcea","title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums"},{"content":{"abstract":"Social media is a rich source where we can learn about people\u2019s reactions to social issues. As COVID-19 has significantly impacted on people\u2019s lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people\u2019s reactions and concerns about COVID-19 in North America, especially focusing on Canada. We analyze COVID-19 related tweets using topic modeling and aspect-based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID-19. We also examine people\u2019s sentiment about COVID-19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","authors":["Hyeju Jang","Emily Rempel","Giuseppe Carenini","Naveed Janjua"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploratory Analysis of COVID-19 Related Tweets in North America to Inform Public Health Institutes","tldr":"Social media is a rich source where we can learn about people\u2019s reactions to social issues. As COVID-19 has significantly impacted on people\u2019s lives, it is essential to capture how people react to public health interventions and understand their conc...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.28","presentation_id":"38939848","rocketchat_channel":"paper-nlp-covid19-emnlp-28","speakers":"Hyeju Jang|Emily Rempel|Giuseppe Carenini|Naveed Janjua","title":"Exploratory Analysis of COVID-19 Related Tweets in North America to Inform Public Health Institutes"},{"content":{"abstract":"Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on heterogeneous meanings in communities across the United States and that these disparate meanings shaped communities\u2019 response to the virus during the early, vital stages of the outbreak in the U.S. Using word embeddings, we demonstrate that counties where residents socially distanced less on average (as measured by residential mobility) more semantically associated the virus in their COVID discourse with concepts of fraud, the political left, and more benign illnesses like the flu. We also show that the different meanings the virus took on in different communities explains a substantial fraction of what we call the \u201c\u201dTrump Gap\u201d, or the empirical tendency for more Trump-supporting counties to socially distance less. This work demonstrates that community-level processes of meaning-making in part determined behavioral responses to the COVID-19 pandemic and that these processes can be measured unobtrusively using Twitter.","authors":["Austin Van Loon","Sheridan Stewart","Brandon Waldon","Shrinidhi K Lakshmikanth","Ishan Shah","Sharath Chandra Guntuku","Garrick Sherman","James Zou","Johannes Eichstaedt"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Explaining the Trump Gap in Social Distancing Using COVID Discourse","tldr":"Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on hetero...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.31","presentation_id":"38939849","rocketchat_channel":"paper-nlp-covid19-emnlp-31","speakers":"Austin Van Loon|Sheridan Stewart|Brandon Waldon|Shrinidhi K Lakshmikanth|Ishan Shah|Sharath Chandra Guntuku|Garrick Sherman|James Zou|Johannes Eichstaedt","title":"Explaining the Trump Gap in Social Distancing Using COVID Discourse"},{"content":{"abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce Expressive Interviewing \u2013 an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\u2019s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","authors":["Charles Welch","Allison Lahnala","Veronica Perez-Rosas","Siqi Shen","Sarah Seraj","Larry An","Kenneth Resnicow","James Pennebaker","Rada Mihalcea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","tldr":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolatio...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.35","presentation_id":"38939850","rocketchat_channel":"paper-nlp-covid19-emnlp-35","speakers":"Charles Welch|Allison Lahnala|Veronica Perez-Rosas|Siqi Shen|Sarah Seraj|Larry An|Kenneth Resnicow|James Pennebaker|Rada Mihalcea","title":"Expressive Interviewing: A Conversational System for Coping with COVID-19"},{"content":{"abstract":"The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misinformation detection datasets are not effective for evaluating systems designed to detect misinformation on this topic. Misinformation detection can be divided into two sub-tasks: (i) retrieval of misconceptions relevant to posts being checked for veracity, and (ii) stance detection to identify whether the posts Agree, Disagree, or express No Stance towards the retrieved misconceptions. To facilitate research on this task, we release COVIDLies (https://ucinlp.github.io/covid19 ), a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon.","authors":["Tamanna Hossain","Robert L. Logan IV","Arjuna Ugarte","Yoshitomo Matsubara","Sean Young","Sameer Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVIDLies: Detecting COVID-19 Misinformation on Social Media","tldr":"The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misi...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.37","presentation_id":"38939851","rocketchat_channel":"paper-nlp-covid19-emnlp-37","speakers":"Tamanna Hossain|Robert L. Logan IV|Arjuna Ugarte|Yoshitomo Matsubara|Sean Young|Sameer Singh","title":"COVIDLies: Detecting COVID-19 Misinformation on Social Media"},{"content":{"abstract":"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories.","authors":["Akiko Aizawa","Frederic Bergeron","Junjie Chen","Fei Cheng","Katsuhiko Hayashi","Kentaro Inui","Hiroyoshi Ito","Daisuke Kawahara","Masaru Kitsuregawa","Hirokazu Kiyomaru","Masaki Kobayashi","Takashi Kodama","Sadao Kurohashi","Qianying Liu","Masaki Matsubara","Yusuke Miyao","Atsuyuki Morishima","Yugo Murawaki","Kazumasa Omura","Haiyue Song","Eiichiro Sumita","Shinji Suzuki","Ribeka Tanaka","Yu Tanaka","Masashi Toyoda","Nobuhiro Ueda","Honai Ueoka","Masao Utiyama","Ying Zhong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A System for Worldwide COVID-19 Information Aggregation","tldr":"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g.,...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.45","presentation_id":"38939852","rocketchat_channel":"paper-nlp-covid19-emnlp-45","speakers":"Akiko Aizawa|Frederic Bergeron|Junjie Chen|Fei Cheng|Katsuhiko Hayashi|Kentaro Inui|Hiroyoshi Ito|Daisuke Kawahara|Masaru Kitsuregawa|Hirokazu Kiyomaru|Masaki Kobayashi|Takashi Kodama|Sadao Kurohashi|Qianying Liu|Masaki Matsubara|Yusuke Miyao|Atsuyuki Morishima|Yugo Murawaki|Kazumasa Omura|Haiyue Song|Eiichiro Sumita|Shinji Suzuki|Ribeka Tanaka|Yu Tanaka|Masashi Toyoda|Nobuhiro Ueda|Honai Ueoka|Masao Utiyama|Ying Zhong","title":"A System for Worldwide COVID-19 Information Aggregation"},{"content":{"abstract":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depres- sion, supported by the literature. We propose a methodology for providing insight into tem- poral mental health dynamics to be utilised for strategic decision-making.","authors":["Tom Tabak","Matthew Purver"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Temporal Mental Health Dynamics on Social Media","tldr":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global CO...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.47","presentation_id":"38939853","rocketchat_channel":"paper-nlp-covid19-emnlp-47","speakers":"Tom Tabak|Matthew Purver","title":"Temporal Mental Health Dynamics on Social Media"},{"content":{"abstract":"The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, \u201dpivot\u201d languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.","authors":["Antonios Anastasopoulos","Alessandro Cattelan","Zi-Yi Dou","Marcello Federico","Christian Federmann","Dmitriy Genzel","Franscisco Guzm\u00e1n","Junjie Hu","Macduff Hughes","Philipp Koehn","Rosie Lazar","Will Lewis","Graham Neubig","Mengmeng Niu","Alp \u00d6ktem","Eric Paquin","Grace Tang","Sylwia Tur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TICO-19: the Translation Initiative for COvid-19","tldr":"The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collab...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.50","presentation_id":"38939854","rocketchat_channel":"paper-nlp-covid19-emnlp-50","speakers":"Antonios Anastasopoulos|Alessandro Cattelan|Zi-Yi Dou|Marcello Federico|Christian Federmann|Dmitriy Genzel|Franscisco Guzm\u00e1n|Junjie Hu|Macduff Hughes|Philipp Koehn|Rosie Lazar|Will Lewis|Graham Neubig|Mengmeng Niu|Alp \u00d6ktem|Eric Paquin|Grace Tang|Sylwia Tur","title":"TICO-19: the Translation Initiative for COvid-19"},{"content":{"abstract":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. In this paper, we analyse Dutch public sentiment on governmental COVID-19 measures from text data collected across three online media sources (Twitter, Reddit and Nu.nl) from February to September 2020. We apply sentiment analysis methods to analyse polarity over time, as well as to identify stance towards two specific pandemic policies regarding social distancing and wearing face masks. The presented preliminary results provide valuable insights into the narratives shown in vast social media text data, which help understand the influence of COVID-19 measures on the general public.","authors":["Shihan Wang","Marijn Schraagen","Erik Tjong Kim Sang","Mehdi Dastani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media","tldr":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-dat...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.53","presentation_id":"38939855","rocketchat_channel":"paper-nlp-covid19-emnlp-53","speakers":"Shihan Wang|Marijn Schraagen|Erik Tjong Kim Sang|Mehdi Dastani","title":"Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media"},{"content":{"abstract":"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well.","authors":["Jinhyuk Lee","Sean S. Yi","Minbyul Jeong","Mujeen Sung","WonJin Yoon","Yonghwa Choi","Miyoung Ko","Jaewoo Kang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Answering Questions on COVID-19 in Real-Time","tldr":"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our e...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.6","presentation_id":"38939843","rocketchat_channel":"paper-nlp-covid19-emnlp-6","speakers":"Jinhyuk Lee|Sean S. Yi|Minbyul Jeong|Mujeen Sung|WonJin Yoon|Yonghwa Choi|Miyoung Ko|Jaewoo Kang","title":"Answering Questions on COVID-19 in Real-Time"},{"content":{"abstract":"A dataset of COVID-19-related scientific literature is compiled, combining the articles from several online libraries and selecting those with open access and full text available. Then, hierarchical nonnegative matrix factorization is used to organize literature related to the novel coronavirus into a tree structure that allows researchers to search for relevant literature based on detected topics. We discover eight major latent topics and 52 granular subtopics in the body of literature, related to vaccines, genetic structure and modeling of the disease and patient studies, as well as related diseases and virology. In order that our tool may help current researchers, an interactive website is created that organizes available literature using this hierarchical structure.","authors":["Rachel Grotheer","Longxiu Huang","Yihuan Huang","Alona Kryshchenko","Oleksandr Kryshchenko","Pengyu Li","Xia Li","Elizaveta Rebrova","Kyung Ha","Deanna Needell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19 Literature Topic-Based Search via Hierarchical NMF","tldr":"A dataset of COVID-19-related scientific literature is compiled, combining the articles from several online libraries and selecting those with open access and full text available. Then, hierarchical nonnegative matrix factorization is used to organiz...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.60","presentation_id":"38939856","rocketchat_channel":"paper-nlp-covid19-emnlp-60","speakers":"Rachel Grotheer|Longxiu Huang|Yihuan Huang|Alona Kryshchenko|Oleksandr Kryshchenko|Pengyu Li|Xia Li|Elizaveta Rebrova|Kyung Ha|Deanna Needell","title":"COVID-19 Literature Topic-Based Search via Hierarchical NMF"},{"content":{"abstract":"Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topic-based methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compare traditional topic models based on word tokens with topic models based on medical concepts, and propose several ways to improve topic coherence and specificity.","authors":["Yulia Otmakhova","Karin Verspoor","Timothy Baldwin","Simon \u0160uster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration","tldr":"Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topic-based methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compa...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.63","presentation_id":"38939857","rocketchat_channel":"paper-nlp-covid19-emnlp-63","speakers":"Yulia Otmakhova|Karin Verspoor|Timothy Baldwin|Simon \u0160uster","title":"Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration"},{"content":{"abstract":"We present a Question Answering (QA) system that won one of the tasks of the Kaggle CORD-19 Challenge, according to the qualitative evaluation of experts. The system is a combination of an Information Retrieval module and a reading comprehension module that finds the answers in the retrieved passages. In this paper we present a quantitative and qualitative analysis of the system. The quantitative evaluation using manually annotated datasets contradicted some of our design choices, e.g. the fact that using QuAC for fine-tuning provided better answers over just using SQuAD. We analyzed this mismatch with an additional A/B test which showed that the system using QuAC was indeed preferred by users, confirming our intuition. Our analysis puts in question the suitability of automatic metrics and its correlation to user preferences. We also show that automatic metrics are highly dependent on the characteristics of the gold standard, such as the average length of the answers.","authors":["Arantxa Otegi","Jon Ander Campos","Gorka Azkune","Aitor Soroa","Eneko Agirre"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatic Evaluation vs. User Preference in Neural Textual QuestionAnswering over COVID-19 Scientific Literature","tldr":"We present a Question Answering (QA) system that won one of the tasks of the Kaggle CORD-19 Challenge, according to the qualitative evaluation of experts. The system is a combination of an Information Retrieval module and a reading comprehension modu...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.64","presentation_id":"38939858","rocketchat_channel":"paper-nlp-covid19-emnlp-64","speakers":"Arantxa Otegi|Jon Ander Campos|Gorka Azkune|Aitor Soroa|Eneko Agirre","title":"Automatic Evaluation vs. User Preference in Neural Textual QuestionAnswering over COVID-19 Scientific Literature"},{"content":{"abstract":"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.","authors":["Lin Miao","Mark Last","Marina Litvak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures","tldr":"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at contain...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.65","presentation_id":"38939859","rocketchat_channel":"paper-nlp-covid19-emnlp-65","speakers":"Lin Miao|Mark Last|Marina Litvak","title":"Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures"},{"content":{"abstract":"We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.","authors":["Dan Su","Yan Xu","Tiezheng Yu","Farhad Bin Siddique","Elham Barezi","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management","tldr":"We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.9","presentation_id":"38939844","rocketchat_channel":"paper-nlp-covid19-emnlp-9","speakers":"Dan Su|Yan Xu|Tiezheng Yu|Farhad Bin Siddique|Elham Barezi|Pascale Fung","title":"CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management"},{"content":{"abstract":"With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific researchers. To this end, we developed a pipeline that creates an implicit feedback matrix based on Named Entity Recognition (NER) on a corpus of documents, using multidisciplinary ontologies for recognizing and linking the entities. Our hypothesis is that by using ontologies from different fields in the NER phase, we can improve the results for state-of-the-art collaborative-filtering recommender systems applied to the dataset created. The tests performed using the COVID-19 Open Research Dataset (CORD-19) dataset show that when using four ontologies, the results for precision@k, for example, reach the 80%, whereas when using only one ontology, the results for precision@k drops to 20%, for the same users. Furthermore, the use of multi-fields entities may help in the discovery of new items, even if the researchers do not have items from that field in their set of preferences.","authors":["Marcia Afonso Barros","Andre Lamurias","Diana Sousa","Pedro Ruas","Francisco M. Couto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19: A Semantic-Based Pipeline for Recommending Biomedical Entities","tldr":"With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific rese...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.20","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-20","speakers":"Marcia Afonso Barros|Andre Lamurias|Diana Sousa|Pedro Ruas|Francisco M. Couto","title":"COVID-19: A Semantic-Based Pipeline for Recommending Biomedical Entities"},{"content":{"abstract":"Coronavirus Disease of 2019 (COVID-19) created dire consequences globally and triggered an intense scientific effort from different domains. The resulting publications created a huge text collection in which finding the studies related to a biomolecule of interest is challenging for general purpose search engines because the publications are rich in domain specific terminology. Here, we present Vapur: an online COVID-19 search engine specifically designed to find related protein - chemical pairs. Vapur is empowered with a relation-oriented inverted index that is able to retrieve and group studies for a query biomolecule with respect to its related entities. The inverted index of Vapur is automatically created with a BioNLP pipeline and integrated with an online user interface. The online interface is designed for the smooth traversal of the current literature by domain researchers and is publicly available at https://tabilab.cmpe.boun.edu.tr/vapur/.","authors":["Abdullatif K\u00f6ksal","Hilal D\u00f6nmez","R\u0131za \u00d6z\u00e7elik","Elif Ozkirimli","Arzucan \u00d6zg\u00fcr"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Vapur: A Search Engine to Find Related Protein - Compound Pairs in COVID-19 Literature","tldr":"Coronavirus Disease of 2019 (COVID-19) created dire consequences globally and triggered an intense scientific effort from different domains. The resulting publications created a huge text collection in which finding the studies related to a biomolecu...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.21","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-21","speakers":"Abdullatif K\u00f6ksal|Hilal D\u00f6nmez|R\u0131za \u00d6z\u00e7elik|Elif Ozkirimli|Arzucan \u00d6zg\u00fcr","title":"Vapur: A Search Engine to Find Related Protein - Compound Pairs in COVID-19 Literature"},{"content":{"abstract":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of $500$ sentences that were manually selected by the researchers from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of $100,959$ sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","authors":["Alejandro Piad-Morffis","Suilan Estevez-Velarde","Ernesto Luis Estevanell-Valladares","Yoan Guti\u00e9rrez","Andr\u00e9s Montoyo","Rafael Mu\u00f1oz","Yudivi\u00e1n Almeida-Cruz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowledge Discovery in COVID-19 Research Literature","tldr":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of $...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.22","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-22","speakers":"Alejandro Piad-Morffis|Suilan Estevez-Velarde|Ernesto Luis Estevanell-Valladares|Yoan Guti\u00e9rrez|Andr\u00e9s Montoyo|Rafael Mu\u00f1oz|Yudivi\u00e1n Almeida-Cruz","title":"Knowledge Discovery in COVID-19 Research Literature"},{"content":{"abstract":"The COVID-19 pandemic has thrown natural life out of gear across the globe. Strict measures are deployed to curb the spread of the virus that is causing it, and the most effective of them have been social isolation. This has led to wide-spread gloom and depression across society but more so among the young and the elderly. There are currently more than 200 million college students in 186 countries worldwide, affected due to the pandemic. The mode of education has changed suddenly, with the rapid adaptation of e-learning, whereby teaching is undertaken remotely and on digital platforms. This study presents insights gathered from social media posts that were posted by students and young adults during the COVID times. Using statistical and NLP techniques, we analyzed the behavioural issues reported by users themselves in their posts in depression related communities on Reddit. We present methodologies to systematically analyze content using linguistic techniques to find out the stress-inducing factors. Online education, losing jobs, isolation from friends and abusive families emerge as key stress factors","authors":["Sachin Thukral","Suyash Sangwan","Arnab Chatterjee","Lipika Dey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying pandemic-related stress factors from social-media posts \u2013 Effects on students and young-adults","tldr":"The COVID-19 pandemic has thrown natural life out of gear across the globe. Strict measures are deployed to curb the spread of the virus that is causing it, and the most effective of them have been social isolation. This has led to wide-spread gloom ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.23","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-23","speakers":"Sachin Thukral|Suyash Sangwan|Arnab Chatterjee|Lipika Dey","title":"Identifying pandemic-related stress factors from social-media posts \u2013 Effects on students and young-adults"},{"content":{"abstract":"The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restrictions like lockdown and social distancing, it became important to understand the emotional response of the public towards the pandemic. In this paper, we study the reaction of Saudi Arabia citizens towards the pandemic. We utilize a collection of Arabic tweets that were sent during 2020, primarily through hashtags that were originated from Saudi Arabia. Our results showed that people had kept a positive reaction towards the pandemic. This positive reaction was at its highest at the beginning of the COVID-19 crisis and started to decline as time passes. Overall, the results showed that people were so supportive of each other through this pandemic. This research can help researchers and policymakers in understanding the emotional effect of a pandemic on societies.","authors":["Aseel Addawood","Alhanouf Alsuwailem","Ali Alohali","Dalal Alajaji","Mashail Alturki","Jaida Alsuhaibani","Fawziah Aljabli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tracking And Understanding Public Reaction During COVID-19: Saudi Arabia As A Use Case","tldr":"The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restric...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.24","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-24","speakers":"Aseel Addawood|Alhanouf Alsuwailem|Ali Alohali|Dalal Alajaji|Mashail Alturki|Jaida Alsuhaibani|Fawziah Aljabli","title":"Tracking And Understanding Public Reaction During COVID-19: Saudi Arabia As A Use Case"},{"content":{"abstract":"Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help with symptoms. In this work, we mined a large twitter dataset of 424 million tweets of COVID-19 chatter to identify discourse around drug mentions. While seemingly a straightforward task, due to the informal nature of language use in Twitter, we demonstrate the need of machine learning alongside traditional automated methods to aid in this task. By applying these complementary methods, we are able to recover almost 15% additional data, making misspelling handling a needed task as a pre-processing step when dealing with social media data.","authors":["Ramya Tekumalla","Juan M Banda"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Characterizing drug mentions in COVID-19 Twitter Chatter","tldr":"Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help wit...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.25","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-25","speakers":"Ramya Tekumalla|Juan M Banda","title":"Characterizing drug mentions in COVID-19 Twitter Chatter"},{"content":{"abstract":"Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, and feelings about a wide range of issues. In this study, using more than 530,000 original tweets in Persian/Farsi on COVID-19, we analyzed the topics discussed among users, who are mainly Iranians, to gauge and track the response to the pandemic and how it evolved over time. We applied a combination of manual annotation of a random sample of tweets and topic modeling tools to classify the contents and frequency of each category of topics. We identified the top 25 topics among which living experience under home quarantine emerged as a major talking point. We additionally categorized the broader content of tweets that shows satire, followed by news, is the dominant tweet type among Iranian users. While this framework and methodology can be used to track public response to ongoing developments related to COVID-19, a generalization of this framework can become a useful framework to gauge Iranian public reaction to ongoing policy measures or events locally and internationally.","authors":["Pedram Hosseini","Poorya Hosseini","David Broniatowski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in Iran using NLP","tldr":"Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.26","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-26","speakers":"Pedram Hosseini|Poorya Hosseini|David Broniatowski","title":"Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in Iran using NLP"},{"content":{"abstract":"The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition and normalisation in parallel to help find relevant publications and to aid in downstream NLP tasks such as text summarisation. In our approach, we are using a dictionary-based system for its high recall in conjunction with two models based on BioBERT for their accuracy. Their outputs are combined according to different strategies depending on the entity type. In addition, we are using a manually crafted dictionary to increase performance for new concepts related to COVID-19. We have previously evaluated our work on the CRAFT corpus, and make the output of our pipeline available on two visualisation platforms.","authors":["Nico Colic","Lenz Furrer","Fabio Rinaldi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Annotating the Pandemic: Named Entity Recognition and Normalisation in COVID-19 Literature","tldr":"The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.27","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-27","speakers":"Nico Colic|Lenz Furrer|Fabio Rinaldi","title":"Annotating the Pandemic: Named Entity Recognition and Normalisation in COVID-19 Literature"},{"content":{"abstract":"In a recent project, the Language Application Grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid \u201cAskMe\u201d query and retrieval engine. We describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications.","authors":["Keith Suderman","Nancy Ide","Verhagen Marc","Brent Cochran","James Pustejovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AskMe: A LAPPS Grid-based NLP Query and Retrieval System for Covid-19 Literature","tldr":"In a recent project, the Language Application Grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid \u201cAskMe\u201d...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.28","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-28","speakers":"Keith Suderman|Nancy Ide|Verhagen Marc|Brent Cochran|James Pustejovsky","title":"AskMe: A LAPPS Grid-based NLP Query and Retrieval System for Covid-19 Literature"},{"content":{"abstract":"Understanding scientific articles related to COVID-19 requires broad knowledge about concepts such as symptoms, diseases and medicine. Given the very large and ever-growing scientific articles related to COVID-19, it is a daunting task even for experts to recognize the large set of concepts mentioned in these articles. In this paper, we address the problem of concept wikification for COVID-19, which is to automatically recognize mentions of concepts related to COVID-19 in text and resolve them into Wikipedia titles. We develop an approach to curate a COVID-19 concept wikification dataset by mining Wikipedia text and the associated intra-Wikipedia links. We also develop an end-to-end system for concept wikification for COVID-19. Preliminary experiments show very encouraging results. Our dataset, code and pre-trained model are available at github.com/panlybero/Covid19_wikification.","authors":["Panagiotis Lymperopoulos","Haoling Qiu","Bonan Min"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Concept Wikification for COVID-19","tldr":"Understanding scientific articles related to COVID-19 requires broad knowledge about concepts such as symptoms, diseases and medicine. Given the very large and ever-growing scientific articles related to COVID-19, it is a daunting task even for exper...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.29","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-29","speakers":"Panagiotis Lymperopoulos|Haoling Qiu|Bonan Min","title":"Concept Wikification for COVID-19"},{"content":{"abstract":"Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature.","authors":["Philip Resnik","Katherine E. Goodman","Mike Moran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Developing a Curated Topic Model for COVID-19 Medical Research Literature","tldr":"Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desid...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.30","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-30","speakers":"Philip Resnik|Katherine E. Goodman|Mike Moran","title":"Developing a Curated Topic Model for COVID-19 Medical Research Literature"},{"content":{"abstract":"We release a dataset of over 2,100 COVID19 related Frequently asked Question-Answer pairs scraped from over 40 trusted websites. We include an additional 24, 000 questions pulled from online sources that have been aligned by experts with existing answered questions from our dataset. This paper describes our efforts in collecting the dataset and summarizes the resulting data. Our dataset is automatically updated daily and available at https://github.com/JHU-COVID-QA/ scraping-qas. So far, this data has been used to develop a chatbot providing users information about COVID-19. We encourage others to build analytics and tools upon this dataset as well.","authors":["Adam Poliak","Max Fleming","Cash Costello","Kenton W Murray","Mahsa Yarmohammadi","Shivani Pandya","Darius Irani","Milind Agarwal","Udit Sharma","Shuo Sun","Nicola Ivanov","Lingxi Shang","Kaushik Srinivasan","Seolhwa Lee","Xu Han","Smisha Agarwal","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Collecting Verified COVID-19 Question Answer Pairs","tldr":"We release a dataset of over 2,100 COVID19 related Frequently asked Question-Answer pairs scraped from over 40 trusted websites. We include an additional 24, 000 questions pulled from online sources that have been aligned by experts with existing ans...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.31","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-31","speakers":"Adam Poliak|Max Fleming|Cash Costello|Kenton W Murray|Mahsa Yarmohammadi|Shivani Pandya|Darius Irani|Milind Agarwal|Udit Sharma|Shuo Sun|Nicola Ivanov|Lingxi Shang|Kaushik Srinivasan|Seolhwa Lee|Xu Han|Smisha Agarwal|Jo\u00e3o Sedoc","title":"Collecting Verified COVID-19 Question Answer Pairs"},{"content":{"abstract":"The number of unique terms in the scientific literature used to refer to either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase rapidly despite well-established standardized terms. This high degree of term variation makes high recall identification of these important entities difficult. In this manuscript we present an extensive dictionary of terms used in the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based approach to iteratively generate new term variants, then locate these variants in a large text corpus. We compare our dictionary to an extensive collection of terminological resources, demonstrating that our resource provides a substantial number of additional terms. We use our dictionary to analyze the usage of SARS-CoV-2 and COVID-19 terms over time and show that the number of unique terms continues to grow rapidly. Our dictionary is freely available at https://github.com/ncbi-nlp/CovidTermVar.","authors":["Robert Leaman","Zhiyong Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and SARS-CoV-2","tldr":"The number of unique terms in the scientific literature used to refer to either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase rapidly despite well-established standardized terms. This high degree of term variation makes hig...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.32","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-32","speakers":"Robert Leaman|Zhiyong Lu","title":"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and SARS-CoV-2"},{"content":{"abstract":"To combat misinformation regarding COVID- 19 during this unprecedented pandemic, we propose a conversational agent that answers questions related to COVID-19. We adapt the Poly-encoder (Humeau et al., 2020) model for informational retrieval from FAQs. We show that after fine-tuning, the Poly-encoder can achieve a higher F1 score. We make our code publicly available for other researchers to use.","authors":["Seolhwa Lee","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using the Poly-encoder for a COVID-19 Question Answering System","tldr":"To combat misinformation regarding COVID- 19 during this unprecedented pandemic, we propose a conversational agent that answers questions related to COVID-19. We adapt the Poly-encoder (Humeau et al., 2020) model for informational retrieval from FAQs...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.33","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-33","speakers":"Seolhwa Lee|Jo\u00e3o Sedoc","title":"Using the Poly-encoder for a COVID-19 Question Answering System"},{"content":{"abstract":"With the rapid development of COVID-19 around the world, people are requested to maintain \u201csocial distance\u201d and \u201cstay at home\u201d. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter and Sina Weibo. People generate posts to share information, express opinions and seek help during the pandemic outbreak, and these kinds of data on social media are valuable for studies to prevent COVID-19 transmissions, such as early warning and outbreaks detection. Therefore, in this paper, we release a novel and fine-grained large-scale COVID-19 social media dataset collected from Sina Weibo, named Weibo-COV, contains more than 40 million posts ranging from December 1, 2019 to April 30, 2020. Moreover, this dataset includes comprehensive information nuggets like post-level information, interactive information, location information, and repost network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and rapid researches to suppress the spread of this pandemic.","authors":["Yong Hu","Heyan Huang","Anfan Chen","Xian-Ling Mao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Weibo-COV: A Large-Scale COVID-19 Social Media Dataset from Weibo","tldr":"With the rapid development of COVID-19 around the world, people are requested to maintain \u201csocial distance\u201d and \u201cstay at home\u201d. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.34","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-34","speakers":"Yong Hu|Heyan Huang|Anfan Chen|Xian-Ling Mao","title":"Weibo-COV: A Large-Scale COVID-19 Social Media Dataset from Weibo"},{"content":{"abstract":"In this paper, we present an iterative graph-based approach for the detection of symptoms of COVID-19, the pathology of which seems to be evolving. More generally, the method can be applied to finding context-specific words and texts (e.g. symptom mentions) in large imbalanced corpora (e.g. all tweets mentioning }#COVID-19). Given the novelty of COVID-19, we also test if the proposed approach generalizes to the problem of detecting Adverse Drug Reaction (ADR). We find that the approach applied to Twitter data can detect symptom mentions substantially before to their being reported by the Centers for Disease Control (CDC).","authors":["Roshan Santosh","H. Schwartz","Johannes Eichstaedt","Lyle Ungar","Sharath Chandra Guntuku"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Emerging Symptoms of COVID-19 using Context-based Twitter Embeddings","tldr":"In this paper, we present an iterative graph-based approach for the detection of symptoms of COVID-19, the pathology of which seems to be evolving. More generally, the method can be applied to finding context-specific words and texts (e.g. symptom me...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.35","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-35","speakers":"Roshan Santosh|H. Schwartz|Johannes Eichstaedt|Lyle Ungar|Sharath Chandra Guntuku","title":"Detecting Emerging Symptoms of COVID-19 using Context-based Twitter Embeddings"},{"content":{"abstract":"As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter, there are issues with their interpretability. We propose a novel use of learned feature importance which improves upon the performance of prior state-of-the-art text classification techniques, while producing more easily interpretable decisions. We also discuss both technical and practical challenges that remain for this task.","authors":["David Hardage","Peyman Najafirad"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI: Ongoing Applied Research","tldr":"As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identifica...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.36","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-36","speakers":"David Hardage|Peyman Najafirad","title":"Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI: Ongoing Applied Research"},{"content":{"abstract":"As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboard for the real-time classification, geolocation and interactive visualization of COVID-19 tweets that addresses these issues. We also describe a novel L2 classification layer that outperforms linear layers on a dataset of respiratory virus tweets.","authors":["Andrei Mircea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Real-time Classification, Geolocation and Interactive Visualization of COVID-19 Information Shared on Social Media to Better Understand Global Developments","tldr":"As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboar...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.37","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-37","speakers":"Andrei Mircea","title":"Real-time Classification, Geolocation and Interactive Visualization of COVID-19 Information Shared on Social Media to Better Understand Global Developments"}]
