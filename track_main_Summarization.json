[{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1023.png","content":{"abstract":"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.","authors":["Arthur Bra\u017einskas","Mirella Lapata","Ivan Titov"],"demo_url":"","keywords":["opinion summarization","automatic text","summary production","summarization mode"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.337","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.965","main.471","main.3012","main.3581","main.2506"],"title":"Few-Shot Learning for Opinion Summarization","tldr":"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the ...","track":"Summarization"},"forum":"main.1023","id":"main.1023","presentation_id":"38938830"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1091.png","content":{"abstract":"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.","authors":["Dayiheng Liu","Yeyun Gong","Yu Yan","Jie Fu","Bo Shao","Daxin Jiang","Jiancheng Lv","Nan Duan"],"demo_url":"","keywords":["news generation","single generation","multi-source decoder","keyphrases"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.505","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.3088","main.652","main.714","main.471","main.1023"],"title":"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation","tldr":"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. Howeve...","track":"Summarization"},"forum":"main.1091","id":"main.1091","presentation_id":"38938842"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1123.png","content":{"abstract":"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.","authors":["Yang Deng","Wenxuan Zhang","Wai Lam"],"demo_url":"","keywords":["question-driven summarization","question-driven method","multi-hop generator","multi-hop"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.547","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2125","main.1648","main.2761","main.693","main.3054"],"title":"Multi-hop Inference for Question-driven Summarization","tldr":"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive sum...","track":"Summarization"},"forum":"main.1123","id":"main.1123","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1835.png","content":{"abstract":"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work.  Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.","authors":["Jonathan Pilault","Raymond Li","Sandeep Subramanian","Chris Pal"],"demo_url":"","keywords":["neural summarization","summarization tasks","extractive step","transformer model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.748","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2650","main.2125","main.2506","main.471","main.714"],"title":"On Extractive and Abstractive Neural Document Summarization with Transformer Language Models","tldr":"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the trans...","track":"Summarization"},"forum":"main.1835","id":"main.1835","presentation_id":"38938995"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.1952.png","content":{"abstract":"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.","authors":["Liqiang Xiao","Lu Wang","Hao He","Yaohui Jin"],"demo_url":"","keywords":["modeling importance","summarization","statistical methods","information theory"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.293","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2650","main.471","main.143","main.3437","main.714"],"title":"Modeling Content Importance for Summarization with Pre-trained Language Models","tldr":"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importan...","track":"Summarization"},"forum":"main.1952","id":"main.1952","presentation_id":"38939016"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2125.png","content":{"abstract":"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.","authors":["Yue Dong","Shuohang Wang","Zhe Gan","Yu Cheng","Jackie Chi Kit Cheung","Jingjing Liu"],"demo_url":"","keywords":["news summarization","factual inconsistency","pre-trained systems","extractive strategies"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.749","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2437","main.2506","main.1835","main.2650","main.965"],"title":"Multi-Fact Correction in Abstractive Text Summarization","tldr":"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: ...","track":"Summarization"},"forum":"main.2125","id":"main.2125","presentation_id":"38939056"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2437.png","content":{"abstract":"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.","authors":["Meng Cao","Yue Dong","Jiapeng Wu","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["factual evaluation","artificial correction","neural systems","self-supervised methods"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.506","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2125","main.2506","main.714","main.2650","main.1835"],"title":"Factual Error Correction for Abstractive Summarization Models","tldr":"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries fo...","track":"Summarization"},"forum":"main.2437","id":"main.2437","presentation_id":"38939120"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2470.png","content":{"abstract":"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.","authors":["Shrey Desai","Jiacheng Xu","Greg Durrett"],"demo_url":"","keywords":["compressive systems","compressions","rouge","pre-trained model"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.507","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2650","TACL.2411","main.2307","main.2506","main.471"],"title":"Compressive Summarization with Plausibility and Salience Modeling","tldr":"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose ...","track":"Summarization"},"forum":"main.2470","id":"main.2470","presentation_id":"38939125"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2506.png","content":{"abstract":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.  Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.","authors":["Wojciech Kryscinski","Bryan McCann","Caiming Xiong","Richard Socher"],"demo_url":"","keywords":["assessing algorithms","natural inference","fact checking","auxiliary tasks"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.750","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.2125","main.1835","main.1023","main.2437","main.2470"],"title":"Evaluating the Factual Consistency of Abstractive Text Summarization","tldr":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying...","track":"Summarization"},"forum":"main.2506","id":"main.2506","presentation_id":"38939131"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2650.png","content":{"abstract":"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS and BART on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.","authors":["Jiacheng Xu","Shrey Desai","Greg Durrett"],"demo_url":"","keywords":["analyzing models","seqseq models","summarization decoders","pegasus"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.508","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1835","main.2470","main.2125","main.3437","main.471"],"title":"Understanding Neural Abstractive Summarization Models via Uncertainty","tldr":"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and white...","track":"Summarization"},"forum":"main.2650","id":"main.2650","presentation_id":"38939166"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2809.png","content":{"abstract":"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).  For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard benchmark datasets. We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques. Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models. To our knowledge, this is one of the first works that use GANs for keyphrase generation. We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.","authors":["Avinash Swaminathan","Haimin Zhang","Debanjan Mahata","Rakesh Gosangi","Rajiv Ratn Shah","Amanda Stent"],"demo_url":"","keywords":["generation keyphrases","keyphrase generation","keyphrase approach","generative gans"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.645","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.2914","main.2313","demo.118","main.1490","main.3126"],"title":"A Preliminary Exploration of GANs for Keyphrase Generation","tldr":"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).  For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated key...","track":"Summarization"},"forum":"main.2809","id":"main.2809","presentation_id":"38939201"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2849.png","content":{"abstract":"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities'', \"the capital cities'' and \"two European cities''. Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.","authors":["Cl\u00e9ment Jumel","Annie Louis","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["generation","paraphrasing","semantic entities","tesa"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.646","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.911","main.327","main.3010","main.1528","main.3216"],"title":"TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization","tldr":"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities'', \"the capital cities'' an...","track":"Summarization"},"forum":"main.2849","id":"main.2849","presentation_id":"38939209"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.2900.png","content":{"abstract":"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.","authors":["Kexiang Wang","Baobao Chang","Zhifang Sui"],"demo_url":"","keywords":["multi-document summarization","mds task","combinatorial impact","multi-document mds"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.32","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.540","main.286","main.3012","main.1123","main.471"],"title":"A Spectral Method for Unsupervised Multi-Document Summarization","tldr":"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called s...","track":"Summarization"},"forum":"main.2900","id":"main.2900","presentation_id":"38939222"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3012.png","content":{"abstract":"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.","authors":["Hanlu Wu","Tengfei Ma","Lingfei Wu","Tariro Manyumwa","Shouling Ji"],"demo_url":"","keywords":["summarization task","document system","rouge","unsupervised learning"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.294","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.965","main.1023","main.3552","main.2125","main.471"],"title":"Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning","tldr":"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated refe...","track":"Summarization"},"forum":"main.3012","id":"main.3012","presentation_id":"38939249"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3111.png","content":{"abstract":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.","authors":["Zhengjue Wang","Zhibin Duan","Hao Zhang","Chaojie Wang","Long Tian","Bo Chen","Mingyuan Zhou"],"demo_url":"","keywords":["abstractive summarization","document understanding","summary generation","ta"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.35","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3398","main.471","main.1835","main.965","main.714"],"title":"Friendly Topic Assistant for Transformer Based Abstractive Summarization","tldr":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are be...","track":"Summarization"},"forum":"main.3111","id":"main.3111","presentation_id":"38939270"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3389.png","content":{"abstract":"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.","authors":["Sangwoo Cho","Kaiqiang Song","Chen Li","Dong Yu","Hassan Foroosh","Fei Liu"],"demo_url":"","keywords":["highlighting","summarization","abstractive summarizers","determinantal processes"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.509","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.965","main.2650","main.2125","main.1023","main.2367"],"title":"Better Highlighting: Creating Sub-Sentence Summary Highlights","tldr":"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be...","track":"Summarization"},"forum":"main.3389","id":"main.3389","presentation_id":"38939323"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3398.png","content":{"abstract":"We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.","authors":["Shashi Narayan","Joshua Maynez","Jakub Adamek","Daniele Pighin","Blaz Bratanic","Ryan McDonald"],"demo_url":"","keywords":["extractive summarization","stepwise summarization","sentence filtering","rotowire generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.339","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.3111","main.1835","main.2511","main.3437","main.1618"],"title":"Stepwise Extractive Summarization and Planning with Structured Transformers","tldr":"We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer ...","track":"Summarization"},"forum":"main.3398","id":"main.3398","presentation_id":"38939328"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3437.png","content":{"abstract":"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.","authors":["Ruipeng Jia","Yanan Cao","Hengzhu Tang","Fang Fang","Cong Cao","Shi Wang"],"demo_url":"","keywords":["sentence-level summarization","node task","network mining","text summarization"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.295","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1835","main.2761","main.714","main.1023"],"title":"Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network","tldr":"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is...","track":"Summarization"},"forum":"main.3437","id":"main.3437","presentation_id":"38939335"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3464.png","content":{"abstract":"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning. In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer's performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.","authors":["Logan Lebanoff","Franck Dernoncourt","Doo Soon Kim","Lidan Wang","Walter Chang","Fei Liu"],"demo_url":"","keywords":["sentence fusion","transformer","modeling correspondence","summarization systems"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.338","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.2650","main.1835","main.2125","main.965","main.3437"],"title":"Learning to Fuse Sentences with Transformers for Summarization","tldr":"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusi...","track":"Summarization"},"forum":"main.3464","id":"main.3464","presentation_id":"38939343"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3552.png","content":{"abstract":"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).","authors":["Manik Bhandari","Pranav Narayan Gour","Atabak Ashfaq","Pengfei Liu","Graham Neubig"],"demo_url":"","keywords":["manual evaluation","text-generation tasks","text summarization","top-scoring systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.751","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.3012","main.965","main.1023","main.2125","main.714"],"title":"Re-evaluating Evaluation in Text Summarization","tldr":"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 yea...","track":"Summarization"},"forum":"main.3552","id":"main.3552","presentation_id":"38939364"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.3581.png","content":{"abstract":"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on \\emph{arbitrary} aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.","authors":["Bowen Tan","Lianhui Qin","Eric Xing","Zhiting Hu"],"demo_url":"","keywords":["aspect-based summarization","weak method","aspect scheme","supervision data"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.510","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.1023","main.693","main.2476","main.3012","main.983"],"title":"Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach","tldr":"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of s...","track":"Summarization"},"forum":"main.3581","id":"main.3581","presentation_id":"38939371"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.471.png","content":{"abstract":"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.  In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.","authors":["Ryosuke Kohita","Akifumi Wachi","Yang Zhao","Ryuki Tachibana"],"demo_url":"","keywords":["abstractive textsummarization","unsupervised summarization","unsupervised summarizers","unsupervised methods"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.34","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1023","main.714","main.1835","main.2650","main.965"],"title":"Q-learning with Language Model for Edit-based Unsupervised Summarization","tldr":"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.  In this paper, we...","track":"Summarization"},"forum":"main.471","id":"main.471","presentation_id":"38938716"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.504.png","content":{"abstract":"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on  state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.","authors":["Thomas Scialom","Paul-Alexis Dray","Sylvain Lamprier","Benjamin Piwowarski","Jacopo Staiano"],"demo_url":"","keywords":["mlsum","cross-lingual analyses","large-scale dataset","online newspapers"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.647","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.875","main.3116","main.3257","main.1049","main.871"],"title":"MLSUM: The Multilingual Summarization Corpus","tldr":"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with Engli...","track":"Summarization"},"forum":"main.504","id":"main.504","presentation_id":"38938723"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.540.png","content":{"abstract":"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models.","authors":["Yao Lu","Yue Dong","Laurent Charlin"],"demo_url":"","keywords":["multi-document summarization","multi-document task","multi-xscience","extreme summarization"],"material":null,"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.648","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g4J","start_time":"Wed, 18 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.2900","main.693","main.1123","main.3581","main.714"],"title":"Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles","tldr":"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challen...","track":"Summarization"},"forum":"main.540","id":"main.540","presentation_id":"38938728"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.628.png","content":{"abstract":"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which  reduces manual annotation cost.","authors":["Nayu Liu","Xian Sun","Hongfeng Yu","Wenkai Zhang","Guangluan Xu"],"demo_url":"","keywords":["multimodal summarization","multimodal tasks","multiencoder-decoder frameworks","multistage network"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.144","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 04:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g1G","start_time":"Tue, 17 Nov 2020 02:00:00 GMT"}],"similar_paper_uids":["main.702","main.3013","main.2853","main.3174","main.471"],"title":"Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos","tldr":"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods la...","track":"Summarization"},"forum":"main.628","id":"main.628","presentation_id":""},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.645.png","content":{"abstract":"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers---remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.","authors":["Jiaao Chen","Diyi Yang"],"demo_url":"","keywords":["text summarization","nlp","summarizing text","human-humanmachine interaction"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.336","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 17:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z7D","start_time":"Tue, 17 Nov 2020 16:00:00 GMT"}],"similar_paper_uids":["main.1522","main.215","main.1702","main.699","main.128"],"title":"Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization","tldr":"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human...","track":"Summarization"},"forum":"main.645","id":"main.645","presentation_id":"38938747"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.675.png","content":{"abstract":"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not  come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.","authors":["Rishi Bommasani","Claire Cardie"],"demo_url":"","keywords":["dataset construction","large-scale evaluation","large-scale datasets","summarization research"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.649","program":"main","sessions":[{"end_time":"Wed, 18 Nov 2020 20:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g5I","start_time":"Wed, 18 Nov 2020 18:00:00 GMT"}],"similar_paper_uids":["main.3012","main.3648","main.1389","demo.107","main.3552"],"title":"Intrinsic Evaluation of Summarization Datasets","tldr":"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sou...","track":"Summarization"},"forum":"main.675","id":"main.675","presentation_id":"38938755"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.693.png","content":{"abstract":"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.","authors":["Yumo Xu","Mirella Lapata"],"demo_url":"","keywords":["modeling interactions","query summarization","assembling summaries","question answering"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.296","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.3581","main.1123","main.2587","TACL.2095","main.1023"],"title":"Coarse-to-Fine Query Focused Multi-Document Summarization","tldr":"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant su...","track":"Summarization"},"forum":"main.693","id":"main.693","presentation_id":"38938758"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.702.png","content":{"abstract":"A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.","authors":["Mingzhe Li","Xiuying Chen","Shen Gao","Zhangming Chan","Dongyan Zhao","Rui Yan"],"demo_url":"","keywords":["video-based summarization","human evaluations","vmsmo","dual-interaction-based summarizer"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.752","program":"main","sessions":[{"end_time":"Thu, 19 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z16C","start_time":"Thu, 19 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.628","main.3088","main.471","main.1085","main.3111"],"title":"VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles","tldr":"A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatic...","track":"Summarization"},"forum":"main.702","id":"main.702","presentation_id":"38938761"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.714.png","content":{"abstract":"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.","authors":["Yanyan Zou","Xingxing Zhang","Wei Lu","Furu Wei","Ming Zhou"],"demo_url":"","keywords":["abstractive summarization","sequence-to-sequence problem","sentence reordering","next generation"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.297","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 12:00:00 GMT","hosts":null,"link":"https://www.virtualchair.net/events/emnlp2020","session_name":"g2K","start_time":"Tue, 17 Nov 2020 10:00:00 GMT"}],"similar_paper_uids":["main.471","main.2125","main.1835","main.2650","main.1023"],"title":"Pre-training for Abstractive Document Summarization by Reinstating Source Text","tldr":"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents ...","track":"Summarization"},"forum":"main.714","id":"main.714","presentation_id":"38938762"},{"card_image_path":"https://emnlp2020-public.s3.amazonaws.com/paper_images/main.965.png","content":{"abstract":"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.","authors":["Dandan Huang","Leyang Cui","Sen Yang","Guangsheng Bao","Kun Wang","Jun Xie","Yue Zhang"],"demo_url":"","keywords":["text summarization","deep learning","automatic summarizers","summarization systems"],"material":null,"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.emnlp-main.33","program":"main","sessions":[{"end_time":"Tue, 17 Nov 2020 01:00:00 GMT","hosts":null,"link":"https://zoom.us","session_name":"z3A","start_time":"Tue, 17 Nov 2020 00:00:00 GMT"}],"similar_paper_uids":["main.1023","main.3012","main.2125","main.3389","main.471"],"title":"What Have We Achieved on Text Summarization?","tldr":"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human profes...","track":"Summarization"},"forum":"main.965","id":"main.965","presentation_id":"38938815"}]
