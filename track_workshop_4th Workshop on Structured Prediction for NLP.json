[{"content":{"abstract":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.","authors":["Federico L\u00f3pez","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification","tldr":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.490","presentation_id":"38940646","rocketchat_channel":"paper-spnlp20-490","speakers":"Federico L\u00f3pez|Michael Strube","title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification"},{"content":{"abstract":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structured prediction problem, consisting in inferring the most probable instance of the semantic model given the text. In this work, we focus on the challenging sub-problem of cardinality prediction that consists in predicting the number of distinct individuals of each class in the semantic model. We show that cardinality prediction can successfully be approached by modeling the overall task as a joint inference problem, predicting the number of individuals of certain classes while at the same time extracting their properties. We approach this task with probabilistic graphical models computing the maximum-a-posteriori instance of the semantic model. Our main contribution lies on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury.","authors":["Hendrik ter Horst","Philipp Cimiano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension","tldr":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structure...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.10","presentation_id":"38940161","rocketchat_channel":"paper-spnlp20-10","speakers":"Hendrik ter Horst|Philipp Cimiano","title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension"},{"content":{"abstract":"","authors":["Yuntian Deng","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cascaded Text Generation with Markov Transformers","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.11","presentation_id":"38940151","rocketchat_channel":"paper-spnlp20-11","speakers":"Yuntian Deng|Alexander Rush","title":"Cascaded Text Generation with Markov Transformers"},{"content":{"abstract":"","authors":["Justin Chiu","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scaling Hidden Markov Language Models","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.12","presentation_id":"38940160","rocketchat_channel":"paper-spnlp20-12","speakers":"Justin Chiu|Alexander Rush","title":"Scaling Hidden Markov Language Models"},{"content":{"abstract":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.","authors":["Anh Duong Trinh","Robert J. Ross","John D. Kelleher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking","tldr":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, su...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.14","presentation_id":"38940154","rocketchat_channel":"paper-spnlp20-14","speakers":"Anh Duong Trinh|Robert J. Ross|John D. Kelleher","title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking"},{"content":{"abstract":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.","authors":["Ning Shi","Ziheng Zeng","Haotian Zhang","Yichen Gong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.159","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent Inference in Text Editing","tldr":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying dec...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1463","presentation_id":"38940648","rocketchat_channel":"paper-spnlp20-1463","speakers":"Ning Shi|Ziheng Zeng|Haotian Zhang|Yichen Gong","title":"Recurrent Inference in Text Editing"},{"content":{"abstract":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper, we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. The ability of our end-to-end method to retrieve structured information is assessed on a large set of business documents. We show that it performs competitively with a standard word classifier without requiring costly word level supervision.","authors":["Cl\u00e9ment Sage","Alex Aussem","V\u00e9ronique Eglin","Haytham Elghazel","J\u00e9r\u00e9my Espinas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks","tldr":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.16","presentation_id":"38940153","rocketchat_channel":"paper-spnlp20-16","speakers":"Cl\u00e9ment Sage|Alex Aussem|V\u00e9ronique Eglin|Haytham Elghazel|J\u00e9r\u00e9my Espinas","title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks"},{"content":{"abstract":"","authors":["Zihao Deng","Sijia Wang","Brendan Juba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntactically restricted self-attention for Semantic Role Labeling","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.17","presentation_id":"38940162","rocketchat_channel":"paper-spnlp20-17","speakers":"Zihao Deng|Sijia Wang|Brendan Juba","title":"Syntactically restricted self-attention for Semantic Role Labeling"},{"content":{"abstract":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an end-to-end deep learning framework of the quality estimation and automatic post-editing of the machine translation output. Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model. To imitate the behavior of human translators, we design three efficient delegation modules \u2013 quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them. We examine this approach with the English\u2013German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance. We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation.","authors":["Ke Wang","Jiayi Wang","Niyu Ge","Yangbin Shi","Yu Zhao","Kai Fan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.197","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing","tldr":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by p...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1774","presentation_id":"38940649","rocketchat_channel":"paper-spnlp20-1774","speakers":"Ke Wang|Jiayi Wang|Niyu Ge|Yangbin Shi|Yu Zhao|Kai Fan","title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing"},{"content":{"abstract":"","authors":["Manuel Widmoser","Maria Pacheco","Jean Honorio","Dan Goldwasser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Randomized Deep Structured Prediction for Argumentation Mining","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.19","presentation_id":"38940158","rocketchat_channel":"paper-spnlp20-19","speakers":"Manuel Widmoser|Maria Pacheco|Jean Honorio|Dan Goldwasser","title":"Randomized Deep Structured Prediction for Argumentation Mining"},{"content":{"abstract":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.","authors":["Noe Casas","Jos\u00e9 A. R. Fonollosa","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation","tldr":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradig...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2","presentation_id":"38940163","rocketchat_channel":"paper-spnlp20-2","speakers":"Noe Casas|Jos\u00e9 A. R. Fonollosa|Marta R. Costa-juss\u00e0","title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation"},{"content":{"abstract":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.","authors":["Nikolaos Manginas","Ilias Chalkidis","Prodromos Malakasiotis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations","tldr":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.20","presentation_id":"38940156","rocketchat_channel":"paper-spnlp20-20","speakers":"Nikolaos Manginas|Ilias Chalkidis|Prodromos Malakasiotis","title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations"},{"content":{"abstract":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both cost-augmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.","authors":["Lifu Tu","Richard Yuanzhe Pang","Kevin Gimpel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks","tldr":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structu...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.21","presentation_id":"38940143","rocketchat_channel":"paper-spnlp20-21","speakers":"Lifu Tu|Richard Yuanzhe Pang|Kevin Gimpel","title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks"},{"content":{"abstract":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.","authors":["Shucheng Li","Lingfei Wu","Shiwei Feng","Fangli Xu","Fengyuan Xu","Sheng Zhong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.255","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem","tldr":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as se...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2146","presentation_id":"38940650","rocketchat_channel":"paper-spnlp20-2146","speakers":"Shucheng Li|Lingfei Wu|Shiwei Feng|Fangli Xu|Fengyuan Xu|Sheng Zhong","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"content":{"abstract":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, \u201cSome person was born in some location at some time.\u201d We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.","authors":["Yunmo Chen","Tongfei Chen","Seth Ebner","Aaron Steven White","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reading the Manual: Event Extraction as Definition Comprehension","tldr":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.22","presentation_id":"38940159","rocketchat_channel":"paper-spnlp20-22","speakers":"Yunmo Chen|Tongfei Chen|Seth Ebner|Aaron Steven White|Benjamin Van Durme","title":"Reading the Manual: Event Extraction as Definition Comprehension"},{"content":{"abstract":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.","authors":["Daivik Swarup","Ahsaas Bajaj","Sheshera Mysore","Tim O\u2019Gorman","Rajarshi Das","Andrew McCallum"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.270","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text","tldr":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different way...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2220","presentation_id":"38940651","rocketchat_channel":"paper-spnlp20-2220","speakers":"Daivik Swarup|Ahsaas Bajaj|Sheshera Mysore|Tim O\u2019Gorman|Rajarshi Das|Andrew McCallum","title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text"},{"content":{"abstract":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.","authors":["Vikas Raunak","Siddharth Dalmia","Vivek Gupta","Florian Metze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.276","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Long-Tailed Phenomena in Neural Machine Translation","tldr":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2284","presentation_id":"38940652","rocketchat_channel":"paper-spnlp20-2284","speakers":"Vikas Raunak|Siddharth Dalmia|Vivek Gupta|Florian Metze","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"content":{"abstract":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y\u02c6 given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y\u02c6, y* | x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets. Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.","authors":["Jason Lee","Dustin Tran","Orhan Firat","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Discrepancy between Density Estimation and Sequence Generation","tldr":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.23","presentation_id":"38940144","rocketchat_channel":"paper-spnlp20-23","speakers":"Jason Lee|Dustin Tran|Orhan Firat|Kyunghyun Cho","title":"On the Discrepancy between Density Estimation and Sequence Generation"},{"content":{"abstract":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a target-to-source translation model and a language model. By applying Bayes\u2019 rule strategically, we reformulate this approach as a log-linear combination of translation, sentence-level and document-level language model probabilities. In addition to using static coefficients for each term, this formulation alternatively allows for the learning of dynamic per-token weights to more finely control the impact of the language models. Using both static or dynamic coefficients leads to improvements over a context-agnostic baseline and a context-aware concatenation model.","authors":["S\u00e9bastien Jean","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation","tldr":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a ta...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.24","presentation_id":"38940157","rocketchat_channel":"paper-spnlp20-24","speakers":"S\u00e9bastien Jean|Kyunghyun Cho","title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation"},{"content":{"abstract":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.","authors":["Alireza Mohammadshahi","James Henderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.294","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing","tldr":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dep...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2417","presentation_id":"38940653","rocketchat_channel":"paper-spnlp20-2417","speakers":"Alireza Mohammadshahi|James Henderson","title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing"},{"content":{"abstract":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for Natural Language Understanding without relying on a parser. Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our method is interpretable due to the bias introduced by the KR approach.","authors":["Arindam Mitra","Sanjay Narayana","Chitta Baral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective","tldr":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launc...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.26","presentation_id":"38940152","rocketchat_channel":"paper-spnlp20-26","speakers":"Arindam Mitra|Sanjay Narayana|Chitta Baral","title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective"},{"content":{"abstract":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2018) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness.","authors":["Renato Negrinho","Matthew R. Gormley","Geoff Gordon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.406","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Investigation of Beam-Aware Training in Supertagging","tldr":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.3373","presentation_id":"38940654","rocketchat_channel":"paper-spnlp20-3373","speakers":"Renato Negrinho|Matthew R. Gormley|Geoff Gordon","title":"An Empirical Investigation of Beam-Aware Training in Supertagging"},{"content":{"abstract":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.","authors":["Abhinav Singh","Patrick Xia","Guanghui Qin","Mahsa Yarmohammadi","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models","tldr":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where eac...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.4","presentation_id":"38940142","rocketchat_channel":"paper-spnlp20-4","speakers":"Abhinav Singh|Patrick Xia|Guanghui Qin|Mahsa Yarmohammadi|Benjamin Van Durme","title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models"},{"content":{"abstract":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset for navigation domain.","authors":["Ke Tran","Ming Tan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations","tldr":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabiliti...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.7","presentation_id":"38940155","rocketchat_channel":"paper-spnlp20-7","speakers":"Ke Tran|Ming Tan","title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations"},{"content":{"abstract":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.","authors":["Youngbin Ro","Yukyung Lee","Pilsung Kang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.99","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT","tldr":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.957","presentation_id":"38940647","rocketchat_channel":"paper-spnlp20-957","speakers":"Youngbin Ro|Yukyung Lee|Pilsung Kang","title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT"}]
