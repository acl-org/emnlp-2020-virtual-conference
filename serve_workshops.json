[{"abstract":"CoNLL is a yearly conference organized by SIGNLL (ACL's Special Interest Group on Natural Language Learning). The main focus of CoNLL is on theoretically, cognitively and scientifically motivated approaches to computational linguistics, rather than on work driven by particular engineering applications. ","blocks":[{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 10:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"}],"id":"WS-1","livestream":null,"organizers":"Raquel Fern\u00e1ndez and Tal Linzen","papers":[{"content":{"abstract":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.","authors":["Francois Meyer","Martha Lewis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modelling Lexical Ambiguity with Density Matrices","tldr":"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelate...","track":"CoNLL 2020"},"id":"WS-1.100","presentation_id":"38939483","rocketchat_channel":"paper-conll-100","speakers":"Francois Meyer|Martha Lewis","title":"Modelling Lexical Ambiguity with Density Matrices"},{"content":{"abstract":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network\u2019s architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","authors":["William Havard","Laurent Besacier","Jean-Pierre Chevrot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech","tldr":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and the...","track":"CoNLL 2020"},"id":"WS-1.101","presentation_id":"38939484","rocketchat_channel":"paper-conll-101","speakers":"William Havard|Laurent Besacier|Jean-Pierre Chevrot","title":"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech"},{"content":{"abstract":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the \u201cLarge-scale online biomedical semantic indexing\u201d track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach.","authors":["Dusan Grujicic","Gorjan Radevski","Tinne Tuytelaars","Matthew Blaschko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to ground medical text in a 3D human atlas","tldr":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to r...","track":"CoNLL 2020"},"id":"WS-1.108","presentation_id":"38939485","rocketchat_channel":"paper-conll-108","speakers":"Dusan Grujicic|Gorjan Radevski|Tinne Tuytelaars|Matthew Blaschko","title":"Learning to ground medical text in a 3D human atlas"},{"content":{"abstract":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional types. The multilinear maps of words compose with each other to form sentence representations. We extend the skipgram algorithm from vectors to multi- linear maps to learn these representations and instantiate it on unary and binary maps for transitive verbs. These are evaluated on verb and sentence similarity and disambiguation tasks and a subset of the SICK relatedness dataset. Our model performs better than previous type- driven models and is competitive with state of the art representation learning methods such as BERT and neural sentence encoders.","authors":["Gijs Wijnholds","Mehrnoosh Sadrzadeh","Stephen Clark"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Representation Learning for Type-Driven Composition","tldr":"This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional...","track":"CoNLL 2020"},"id":"WS-1.109","presentation_id":"38939486","rocketchat_channel":"paper-conll-109","speakers":"Gijs Wijnholds|Mehrnoosh Sadrzadeh|Stephen Clark","title":"Representation Learning for Type-Driven Composition"},{"content":{"abstract":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.","authors":["Romain Couillet","Yagmur Gizem Cinar","Eric Gaussier","Muhammad Imran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word Representations Concentrate and This is Good News!","tldr":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p ...","track":"CoNLL 2020"},"id":"WS-1.113","presentation_id":"38939487","rocketchat_channel":"paper-conll-113","speakers":"Romain Couillet|Yagmur Gizem Cinar|Eric Gaussier|Muhammad Imran","title":"Word Representations Concentrate and This is Good News!"},{"content":{"abstract":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, \u201cLazImpa\u201d, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.","authors":["Mathieu Rita","Rahma Chaabouni","Emmanuel Dupoux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently","tldr":"Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission o...","track":"CoNLL 2020"},"id":"WS-1.115","presentation_id":"38939488","rocketchat_channel":"paper-conll-115","speakers":"Mathieu Rita|Rahma Chaabouni|Emmanuel Dupoux","title":"\u201cLazImpa\u201d: Lazy and Impatient neural agents learn to communicate efficiently"},{"content":{"abstract":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.","authors":["Alex Tamkin","Trisha Singh","Davide Giovanardi","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.125","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Transferability in Pretrained Language Models","tldr":"How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different ...","track":"CoNLL 2020"},"id":"WS-1.1165_F","presentation_id":"38940643","rocketchat_channel":"paper-conll-1165_F","speakers":"Alex Tamkin|Trisha Singh|Davide Giovanardi|Noah Goodman","title":"Investigating Transferability in Pretrained Language Models"},{"content":{"abstract":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic competence, which includes basic linguistic skills encompassing both referential phenomena and generic knowledge, in particular a) the ability to denote, b) the mastery of the lexicon, or c) the ability to model one\u2019s language use on others. Even though each of those faculties has been extensively tested individually, there is still no computational model that would account for their joint acquisition under the conditions experienced by a human. In this paper, I focus on one particular aspect of this problem: the amount of linguistic data available to the child or machine. I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-the-art performance. I argue that both the nature of the data and the way it is presented to the system matter to acquisition.","authors":["Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Re-solve it: simulating the acquisition of core semantic competences from small data","tldr":"Many tasks are considered to be \u2018solved\u2019 in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic comp...","track":"CoNLL 2020"},"id":"WS-1.127","presentation_id":"38939489","rocketchat_channel":"paper-conll-127","speakers":"Aur\u00e9lie Herbelot","title":"Re-solve it: simulating the acquisition of core semantic competences from small data"},{"content":{"abstract":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training and evaluation of Named Entity Linking (NEL) tools, since different annotation styles correspond to divergent views on the entities present in the same texts. Such divergence is particularly present in texts from the media domain that contain references to creative works. In this work we present a corpus of 1000 annotated documents selected from the media domain. Each document is presented with multiple gold standard annotations representing various annotation styles. This corpus is used to evaluate a series of Named Entity Linking tools in order to understand the impact of the differences in annotation styles on the reported accuracy when processing highly ambiguous entities such as names of creative works. Relaxed annotation guidelines that include overlap styles lead to better results across all tools.","authors":["Adrian M.P. Brasoveanu","Albert Weichselbraun","Lyndon Nixon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works","tldr":"Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training an...","track":"CoNLL 2020"},"id":"WS-1.128","presentation_id":"38939490","rocketchat_channel":"paper-conll-128","speakers":"Adrian M.P. Brasoveanu|Albert Weichselbraun|Lyndon Nixon","title":"In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works"},{"content":{"abstract":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to motivate two new metrics that address the issues with the standard test, and which distinguish between class-wise offset concentration (similar directions between pairs of words drawn from different broad classes, such as France-London, China-Ottawa,...) and pairing consistency (the existence of a regular transformation between correctly-matched pairs such as France:Paris::China:Beijing). We show that, while the standard analogy test is flawed, several popular word embeddings do nevertheless encode linguistic regularities.","authors":["Louis Fournier","Emmanuel Dupoux","Ewan Dunbar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analogies minus analogy test: measuring regularities in word embeddings","tldr":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to mot...","track":"CoNLL 2020"},"id":"WS-1.136","presentation_id":"38939491","rocketchat_channel":"paper-conll-136","speakers":"Louis Fournier|Emmanuel Dupoux|Ewan Dunbar","title":"Analogies minus analogy test: measuring regularities in word embeddings"},{"content":{"abstract":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.","authors":["Maria A. Rodriguez","Paola Merlo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word associations and the distance properties of context-aware word embeddings","tldr":"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embeddin...","track":"CoNLL 2020"},"id":"WS-1.137","presentation_id":"38939492","rocketchat_channel":"paper-conll-137","speakers":"Maria A. Rodriguez|Paola Merlo","title":"Word associations and the distance properties of context-aware word embeddings"},{"content":{"abstract":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on \u00c6Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear \u03bb-calculus with an accuracy of as high as 70%.","authors":["Konstantinos Kogkalidis","Michael Moortgat","Richard Moot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Proof Nets","tldr":"Linear logic and the linear \u03bb-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation o...","track":"CoNLL 2020"},"id":"WS-1.14","presentation_id":"38939465","rocketchat_channel":"paper-conll-14","speakers":"Konstantinos Kogkalidis|Michael Moortgat|Richard Moot","title":"Neural Proof Nets"},{"content":{"abstract":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims\u2019 topics and their possible negative impacts are the main factors affecting their check-worthiness.","authors":["Yavuz Selim Kartal","Mucahid Kutlu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales","tldr":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, avai...","track":"CoNLL 2020"},"id":"WS-1.142","presentation_id":"38939493","rocketchat_channel":"paper-conll-142","speakers":"Yavuz Selim Kartal|Mucahid Kutlu","title":"TrClaim-19: The First Collection for Turkish Check-Worthy Claim Detection with Annotator Rationales"},{"content":{"abstract":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.","authors":["Forrest Davis","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse structure interacts with reference but not syntax in neural language models","tldr":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different ...","track":"CoNLL 2020"},"id":"WS-1.144","presentation_id":"38939494","rocketchat_channel":"paper-conll-144","speakers":"Forrest Davis|Marten van Schijndel","title":"Discourse structure interacts with reference but not syntax in neural language models"},{"content":{"abstract":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.","authors":["Robert Hawkins","Minae Kwon","Dorsa Sadigh","Noah Goodman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Adaptation for Efficient Machine Communication","tldr":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly a...","track":"CoNLL 2020"},"id":"WS-1.147","presentation_id":"38939495","rocketchat_channel":"paper-conll-147","speakers":"Robert Hawkins|Minae Kwon|Dorsa Sadigh|Noah Goodman","title":"Continual Adaptation for Efficient Machine Communication"},{"content":{"abstract":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing explicit representations for objects and their relations by extracting scene graphs from the images. Utilizing an embedding of this scene graph enables our model to more explicitly reason over objects and their relations during story generation, compared to the global features from an object classifier used in previous work. We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features and show that our approach outperforms previous systems. Our experiments also indicate that our models obtain competitive results on reference-based metrics.","authors":["Xudong Hong","Rakshith Shetty","Asad Sayeed","Khushboo Mehra","Vera Demberg","Bernt Schiele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings","tldr":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing expl...","track":"CoNLL 2020"},"id":"WS-1.149","presentation_id":"38939496","rocketchat_channel":"paper-conll-149","speakers":"Xudong Hong|Rakshith Shetty|Asad Sayeed|Khushboo Mehra|Vera Demberg|Bernt Schiele","title":"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings"},{"content":{"abstract":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TaxiNLI, a new dataset, that has 10k examples from the MNLI dataset with these taxonomic labels. Through various experiments on TaxiNLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies\u2014a large jump over the previous models\u2014some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.","authors":["Pratik Joshi","Somak Aditya","Aalok Sathe","Monojit Choudhury"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TaxiNLI: Taking a Ride up the NLU Hill","tldr":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remain...","track":"CoNLL 2020"},"id":"WS-1.15","presentation_id":"38939466","rocketchat_channel":"paper-conll-15","speakers":"Pratik Joshi|Somak Aditya|Aalok Sathe|Monojit Choudhury","title":"TaxiNLI: Taking a Ride up the NLU Hill"},{"content":{"abstract":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover, historical variations can be present in aged documents, which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models, and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets, and does not degrade the results for modern datasets.","authors":["Emanuela Boros","Ahmed Hamdi","Elvys Linhares Pontes","Luis Adri\u00e1n Cabrera-Diego","Jose G. Moreno","Nicolas Sidere","Antoine Doucet"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents","tldr":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this ...","track":"CoNLL 2020"},"id":"WS-1.152","presentation_id":"38939497","rocketchat_channel":"paper-conll-152","speakers":"Emanuela Boros|Ahmed Hamdi|Elvys Linhares Pontes|Luis Adri\u00e1n Cabrera-Diego|Jose G. Moreno|Nicolas Sidere|Antoine Doucet","title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents"},{"content":{"abstract":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.","authors":["Steven Derby","Paul Miller","Barry Devereux"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models","tldr":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure...","track":"CoNLL 2020"},"id":"WS-1.155","presentation_id":"38939498","rocketchat_channel":"paper-conll-155","speakers":"Steven Derby|Paul Miller|Barry Devereux","title":"Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models"},{"content":{"abstract":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.","authors":["Satwik Bhattamishra","Arkil Patel","Navin Goyal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling","tldr":"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their po...","track":"CoNLL 2020"},"id":"WS-1.156","presentation_id":"38939499","rocketchat_channel":"paper-conll-156","speakers":"Satwik Bhattamishra|Arkil Patel|Navin Goyal","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"content":{"abstract":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a partition that places images into equivalence classes based on player position. To model this task, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these models generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.","authors":["Allen Nie","Reuben Cohn-Gordon","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.173","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pragmatic Issue-Sensitive Image Captioning","tldr":"Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the playe...","track":"CoNLL 2020"},"id":"WS-1.1597_F","presentation_id":"38940644","rocketchat_channel":"paper-conll-1597_F","speakers":"Allen Nie|Reuben Cohn-Gordon|Christopher Potts","title":"Pragmatic Issue-Sensitive Image Captioning"},{"content":{"abstract":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames the task as an inference problem for a general statistical model consisting of observed data (potentially cognate pairs of words), latent variables (the cognacy status of pairs) and unknown global parameters (which sounds correspond between languages). We then give a specific instance of such a model along with an expectation-maximisation algorithm to infer its parameters. We evaluate our system on a corpus of 8140 cognate sets, finding the performance of our method to be comparable to the state of the art. We additionally carry out qualitative analysis demonstrating advantages it has over existing systems. We also suggest several ways our work could be extended within the general theoretical framework we propose.","authors":["Roddy MacSween","Andrew Caines"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Expectation Maximisation Algorithm for Automated Cognate Detection","tldr":"In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames ...","track":"CoNLL 2020"},"id":"WS-1.162","presentation_id":"38939500","rocketchat_channel":"paper-conll-162","speakers":"Roddy MacSween|Andrew Caines","title":"An Expectation Maximisation Algorithm for Automated Cognate Detection"},{"content":{"abstract":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.","authors":["Debasmita Bhattacharya","Marten van Schijndel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filler-gaps that neural networks fail to generalize","tldr":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap de...","track":"CoNLL 2020"},"id":"WS-1.168","presentation_id":"38939501","rocketchat_channel":"paper-conll-168","speakers":"Debasmita Bhattacharya|Marten van Schijndel","title":"Filler-gaps that neural networks fail to generalize"},{"content":{"abstract":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.","authors":["Qile Zhu","Haidar Khan","Saleh Soltan","Stephen Rawls","Wael Hamza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding","tldr":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, ...","track":"CoNLL 2020"},"id":"WS-1.177","presentation_id":"38939502","rocketchat_channel":"paper-conll-177","speakers":"Qile Zhu|Haidar Khan|Saleh Soltan|Stephen Rawls|Wael Hamza","title":"Don\u2019t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding"},{"content":{"abstract":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments. We use SuspectGuilt to train and assess predictive models which validate the usefulness of the corpus, and show that these models benefit from genre pretraining and joint supervision from the text-level ratings and span-level annotations. Such models might be used as tools for understanding the societal effects of crime reporting.","authors":["Elisa Kreiss","Zijian Wang","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives","tldr":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime ...","track":"CoNLL 2020"},"id":"WS-1.18","presentation_id":"38939467","rocketchat_channel":"paper-conll-18","speakers":"Elisa Kreiss|Zijian Wang|Christopher Potts","title":"Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives"},{"content":{"abstract":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network (RNN) controller a Nondeterministic Stack RNN. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.","authors":["Brian DuSell","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Context-free Languages with Nondeterministic Stack RNNs","tldr":"We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang\u2019s algorithm for simulating nondeterministic pushdown automata. We call the combination of this dat...","track":"CoNLL 2020"},"id":"WS-1.183","presentation_id":"38939503","rocketchat_channel":"paper-conll-183","speakers":"Brian DuSell|David Chiang","title":"Learning Context-free Languages with Nondeterministic Stack RNNs"},{"content":{"abstract":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representation of the scaffolding and dynamics that guide a coherent narrative. This paper introduces a new model that integrates explicit narrative structure with neural language models, formalizing narrative modeling as a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in which the latent dynamics of the system (i.e. how the state vector transforms over time) is controlled by top-level discrete switching variables. The switching variables represent narrative structure (e.g., sentiment or discourse states), while the latent state vector encodes information on the current state of the narrative. This probabilistic formulation allows us to control generation, and can be learned in a semi-supervised fashion using both labeled and unlabeled data. Additionally, we derive a Gibbs sampler for our model that can \u201cfill in\u201d arbitrary parts of the narrative, guided by the switching variables. Our filled-in (English language) narratives outperform several baselines on both automatic and human evaluations","authors":["Noah Weber","Leena Shekhar","Heeyoung Kwon","Niranjan Balasubramanian","Nathanael Chambers"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Narrative Text in a Switching Dynamical System","tldr":"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representa...","track":"CoNLL 2020"},"id":"WS-1.185","presentation_id":"38939504","rocketchat_channel":"paper-conll-185","speakers":"Noah Weber|Leena Shekhar|Heeyoung Kwon|Niranjan Balasubramanian|Nathanael Chambers","title":"Generating Narrative Text in a Switching Dynamical System"},{"content":{"abstract":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. This task is inspired bycomputational and cognitive studies of eventunderstanding, which suggest that understand-ing processes of events is often directed by rec-ognizing the goals, plans or intentions of theprotagonist(s). We develop a large dataset con-taining over 60k event processes, featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10\u02c63\u223c10\u02c64)label vocabularies. We then propose a hybridlearning framework,P2GT, which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. As our experiments indi-cate,P2GTsupports identifying the intent ofprocesses, as well as the fine semantic type ofthe affected object. It also demonstrates the ca-pability of handling few-shot cases, and stronggeneralizability on out-of-domain processes.","authors":["Muhao Chen","Hongming Zhang","Haoyu Wang","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.43","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Are You Trying to Do? Semantic Typing of Event Processes","tldr":"This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object ...","track":"CoNLL 2020"},"id":"WS-1.189","presentation_id":"38939505","rocketchat_channel":"paper-conll-189","speakers":"Muhao Chen|Hongming Zhang|Haoyu Wang|Dan Roth","title":"What Are You Trying to Do? Semantic Typing of Event Processes"},{"content":{"abstract":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information from it. The corpus has been constructed with two main tasks in mind. The first one, to extract entities about outbreaks such as disease, host, location among others. The second one, to retrieve relations among entities, for instance, in such geographic location fifteen cases of a given disease were reported. Overall, our goal is to offer resources and tools to perform an automated analysis so as to support early detection of disease outbreaks and therefore diminish their spreading.","authors":["Antonella Dellanzo","Viviana Cotik","Jose Ochoa-Luna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.44","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America","tldr":"In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information f...","track":"CoNLL 2020"},"id":"WS-1.195","presentation_id":"38939506","rocketchat_channel":"paper-conll-195","speakers":"Antonella Dellanzo|Viviana Cotik|Jose Ochoa-Luna","title":"A Corpus for Outbreak Detection of Diseases Prevalent in Latin America"},{"content":{"abstract":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.","authors":["Nora Kassner","Benno Krojer","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.45","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?","tldr":"How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present...","track":"CoNLL 2020"},"id":"WS-1.202","presentation_id":"38939507","rocketchat_channel":"paper-conll-202","speakers":"Nora Kassner|Benno Krojer|Hinrich Sch\u00fctze","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"content":{"abstract":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.","authors":["Mark Anderson","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Frailty of Universal POS Tags for Neural UD Parsers","tldr":"We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear ...","track":"CoNLL 2020"},"id":"WS-1.21","presentation_id":"38939468","rocketchat_channel":"paper-conll-21","speakers":"Mark Anderson|Carlos G\u00f3mez-Rodr\u00edguez","title":"On the Frailty of Universal POS Tags for Neural UD Parsers"},{"content":{"abstract":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent\u2019s language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs.","authors":["Tanmay Parekh","Emily Ahn","Yulia Tsvetkov","Alan W Black"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.46","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues","tldr":"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings....","track":"CoNLL 2020"},"id":"WS-1.218","presentation_id":"38939508","rocketchat_channel":"paper-conll-218","speakers":"Tanmay Parekh|Emily Ahn|Yulia Tsvetkov|Alan W Black","title":"Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues"},{"content":{"abstract":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-motor behaviour of typing. Most work to date has focused solely on the timing of keypresses without reference to the linguistic content. In this paper we argue that the identity of the key combinations being produced should impact how they are handled by people with PD, and provide evidence that natural language processing methods can thus be of help in identifying signs of disease. We test the performance of a bi-directional LSTM with convolutional features in distinguishing people with PD from age-matched controls typing in English and Spanish, both in clinics and online.","authors":["Neil Dhir","Mathias Edman","\u00c1lvaro Sanchez Ferro","Tom Stafford","Colin Bannard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.47","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network","tldr":"There is urgent need for non-intrusive tests that can detect early signs of Parkinson\u2019s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-m...","track":"CoNLL 2020"},"id":"WS-1.221","presentation_id":"38939509","rocketchat_channel":"paper-conll-221","speakers":"Neil Dhir|Mathias Edman|\u00c1lvaro Sanchez Ferro|Tom Stafford|Colin Bannard","title":"Identifying robust markers of Parkinson\u2019s disease in typing behaviour using a CNN-LSTM network"},{"content":{"abstract":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models\u2019 generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it\u2019s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models.","authors":["Tianyu Liu","Zheng Xin","Xiaoan Ding","Baobao Chang","Zhifang Sui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference","tldr":"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is i...","track":"CoNLL 2020"},"id":"WS-1.222","presentation_id":"38939510","rocketchat_channel":"paper-conll-222","speakers":"Tianyu Liu|Zheng Xin|Xiaoan Ding|Baobao Chang|Zhifang Sui","title":"An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference"},{"content":{"abstract":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.","authors":["Tiwalayo Eisape","Noga Zaslavsky","Roger Levy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cloze Distillation Improves Psychometric Predictive Power","tldr":"Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human n...","track":"CoNLL 2020"},"id":"WS-1.226","presentation_id":"38939511","rocketchat_channel":"paper-conll-226","speakers":"Tiwalayo Eisape|Noga Zaslavsky|Roger Levy","title":"Cloze Distillation Improves Psychometric Predictive Power"},{"content":{"abstract":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. We find that a highly augmented model shows highest accuracy in predicting held-out forms, and investigate other properties of interest learned by our models\u2019 representations. We outline extensions to this architecture that can better capture variation in Indo-Aryan sound change.","authors":["Chundra Cathcart","Taraka Rama"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.50","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping","tldr":"This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. ...","track":"CoNLL 2020"},"id":"WS-1.234","presentation_id":"38939512","rocketchat_channel":"paper-conll-234","speakers":"Chundra Cathcart|Taraka Rama","title":"Disentangling dialects: a neural approach to Indo-Aryan historical phonology and subgrouping"},{"content":{"abstract":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign language as a manual gesture recognition problem. However, signers use other articulators: facial expressions, head and body position and movement to convey linguistic information. Given the important role of non-manual markers, this paper proposes a dataset and presents a use case to stress the importance of including non-manual features to improve the recognition accuracy of signs. To the best of our knowledge no prior publicly available dataset exists that explicitly focuses on non-manual components responsible for the grammar of sign languages. To this end, the proposed dataset contains 28250 videos of signs of high resolution and quality, with annotation of manual and non-manual components. We conducted a series of evaluations in order to investigate whether non-manual components would improve signs\u2019 recognition accuracy. We release the dataset to encourage SLR researchers and help advance current progress in this area toward real-time sign language interpretation. Our dataset will be made publicly available at https://krslproject.github.io/krsl-corpus","authors":["Alfarabi Imashev","Medet Mukushev","Vadim Kimmelman","Anara Sandygulova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.51","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL","tldr":"The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign...","track":"CoNLL 2020"},"id":"WS-1.247","presentation_id":"38939513","rocketchat_channel":"paper-conll-247","speakers":"Alfarabi Imashev|Medet Mukushev|Vadim Kimmelman|Anara Sandygulova","title":"A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL"},{"content":{"abstract":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor\u2019s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance.","authors":["Tomasz Dwojak","Micha\u0142 Pietruszka","\u0141ukasz Borchmann","Jakub Ch\u0142\u0119dowski","Filip Grali\u0144ski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.52","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Dataset Recycling to Multi-Property Extraction and Beyond","tldr":"This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introdu...","track":"CoNLL 2020"},"id":"WS-1.258","presentation_id":"38939514","rocketchat_channel":"paper-conll-258","speakers":"Tomasz Dwojak|Micha\u0142 Pietruszka|\u0141ukasz Borchmann|Jakub Ch\u0142\u0119dowski|Filip Grali\u0144ski","title":"From Dataset Recycling to Multi-Property Extraction and Beyond"},{"content":{"abstract":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published neurolinguistic studies of the N400. We find that surprisal can predict N400 amplitude in a wide range of cases, and the cases where it cannot do so provide valuable insight into the neurocognitive processes underlying the response.","authors":["James Michaelov","Benjamin Bergen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How well does surprisal explain N400 amplitude under different experimental conditions?","tldr":"We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty\u2014the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published n...","track":"CoNLL 2020"},"id":"WS-1.259","presentation_id":"38939515","rocketchat_channel":"paper-conll-259","speakers":"James Michaelov|Benjamin Bergen","title":"How well does surprisal explain N400 amplitude under different experimental conditions?"},{"content":{"abstract":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems. Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.","authors":["Leshem Choshen","Dmitry Nikolaev","Yevgeni Berzak","Omri Abend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Classifying Syntactic Errors in Learner Language","tldr":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation sch...","track":"CoNLL 2020"},"id":"WS-1.26","presentation_id":"38939469","rocketchat_channel":"paper-conll-26","speakers":"Leshem Choshen|Dmitry Nikolaev|Yevgeni Berzak|Omri Abend","title":"Classifying Syntactic Errors in Learner Language"},{"content":{"abstract":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowledge. However, designing probing tasks for lesser-resourced languages is tricky, because these often lack largescale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in English. To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study. We show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcomes. We then probe embeddings in a multilingual setup with design choices that lie in a \u2018stable region\u2019, as we identify for English, and find that results on English do not transfer to other languages. Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future.","authors":["Steffen Eger","Johannes Daxenberger","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation","tldr":"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations\u2014e.g., to increase interpretability of their results\u2014probing tasks have been designed which query them for linguistic knowled...","track":"CoNLL 2020"},"id":"WS-1.28","presentation_id":"38939470","rocketchat_channel":"paper-conll-28","speakers":"Steffen Eger|Johannes Daxenberger|Iryna Gurevych","title":"How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation"},{"content":{"abstract":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of these approaches focus strictly on leveraging the co-occurrences of relationship word pairs within sentences. In this paper, we investigate the hypothesis that examples of a lexical relation in a corpus are fundamental to a neural word embedding\u2019s ability to complete analogies involving the relation. Our experiments, in which we remove all known examples of a relation from training corpora, show only marginal degradation in analogy completion performance involving the removed relation. This finding enhances our understanding of neural word embeddings, showing that co-occurrence information of a particular semantic relation is the not the main source of their structural regularity.","authors":["Hsiao-Yu Chiang","Jose Camacho-Collados","Zachary Pardos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding the Source of Semantic Regularities in Word Embeddings","tldr":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of ...","track":"CoNLL 2020"},"id":"WS-1.29","presentation_id":"38939471","rocketchat_channel":"paper-conll-29","speakers":"Hsiao-Yu Chiang|Jose Camacho-Collados|Zachary Pardos","title":"Understanding the Source of Semantic Regularities in Word Embeddings"},{"content":{"abstract":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric element one has propagated in the limited body of computational work on one-anaphora resolution, making this task harder than it is. In the present paper, we resolve this by drawing from an adequate linguistic analysis of the word one in different syntactic environments - once again highlighting the significance of linguistic theory in Natural Language Processing (NLP) tasks. We prepare an annotated corpus marking actual instances of one-anaphora with their textual antecedents, and use the annotations to experiment with state-of-the art neural models for one-anaphora resolution. Apart from presenting a strong neural baseline for this task, we contribute a gold-standard corpus, which is, to the best of our knowledge, the biggest resource on one-anaphora till date.","authors":["Payal Khullar","Arghya Bhattacharya","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Finding The Right One and Resolving it","tldr":"One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric...","track":"CoNLL 2020"},"id":"WS-1.38","presentation_id":"38939472","rocketchat_channel":"paper-conll-38","speakers":"Payal Khullar|Arghya Bhattacharya|Manish Shrivastava","title":"Finding The Right One and Resolving it"},{"content":{"abstract":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.","authors":["Jonathan Malmaud","Roger Levy","Yevgeni Berzak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension","tldr":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking d...","track":"CoNLL 2020"},"id":"WS-1.49","presentation_id":"38939473","rocketchat_channel":"paper-conll-49","speakers":"Jonathan Malmaud|Roger Levy|Yevgeni Berzak","title":"Bridging Information-Seeking Human Gaze and Machine Reading Comprehension"},{"content":{"abstract":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","authors":["John P. Lalor","Hong Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation","tldr":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic...","track":"CoNLL 2020"},"id":"WS-1.510_F","presentation_id":"38940641","rocketchat_channel":"paper-conll-510_F","speakers":"John P. Lalor|Hong Yu","title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation"},{"content":{"abstract":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction to syntactically and semantically sound language stimuli. In this study, we asked: how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain\u2019s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain\u2019s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.","authors":["Maryam Hashemzadeh","Greta Kaufeld","Martha White","Andrea E. Martin","Alona Fyshe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.57","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?","tldr":"The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain\u2019s reaction t...","track":"CoNLL 2020"},"id":"WS-1.561_F","presentation_id":"38940642","rocketchat_channel":"paper-conll-561_F","speakers":"Maryam Hashemzadeh|Greta Kaufeld|Martha White|Andrea E. Martin|Alona Fyshe","title":"From Language to Language-ish: How Brain-Like is an LSTM\u2019s Representation of Nonsensical Language Stimuli?"},{"content":{"abstract":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the dataset and evaluate it with state-of-the-art summarisation methods.","authors":["Yifan Chen","Tamara Polajnar","Colin Batchelor","Simone Teufel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Corpus of Very Short Scientific Summaries","tldr":"We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first...","track":"CoNLL 2020"},"id":"WS-1.59","presentation_id":"38939474","rocketchat_channel":"paper-conll-59","speakers":"Yifan Chen|Tamara Polajnar|Colin Batchelor|Simone Teufel","title":"A Corpus of Very Short Scientific Summaries"},{"content":{"abstract":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training an LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model\u2019s generated output (its \u2018babbling\u2019), compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.","authors":["Ludovica Pannitto","Aur\u00e9lie Herbelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data","tldr":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a chil...","track":"CoNLL 2020"},"id":"WS-1.61","presentation_id":"38939475","rocketchat_channel":"paper-conll-61","speakers":"Ludovica Pannitto|Aur\u00e9lie Herbelot","title":"Recurrent babbling: evaluating the acquisition of grammar from limited input data"},{"content":{"abstract":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively. Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost. We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions. We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.","authors":["Jacqueline van Arkel","Marieke Woensdregt","Mark Dingemanse","Mark Blokpoel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis","tldr":"How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators...","track":"CoNLL 2020"},"id":"WS-1.63","presentation_id":"38939476","rocketchat_channel":"paper-conll-63","speakers":"Jacqueline van Arkel|Marieke Woensdregt|Mark Dingemanse|Mark Blokpoel","title":"A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis"},{"content":{"abstract":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.","authors":["Cory Shain","Micha Elsner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Acquiring language from speech by learning to remember and predict","tldr":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we pro...","track":"CoNLL 2020"},"id":"WS-1.69","presentation_id":"38939477","rocketchat_channel":"paper-conll-69","speakers":"Cory Shain|Micha Elsner","title":"Acquiring language from speech by learning to remember and predict"},{"content":{"abstract":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where the questioner is not given the context from which answers are drawn, but must reason pragmatically about how to acquire new information, given the shared conversation history. We identify two core challenges: (1) formally defining the informativeness of potential questions, and (2) exploring the prohibitively large space of potential questions to find the good candidates. To generate pragmatic questions, we use reinforcement learning to optimize an informativeness metric we propose, combined with a reward function designed to promote more specific questions. We demonstrate that the resulting pragmatic questioner substantially improves the informativeness and specificity of questions generated over a baseline model, as evaluated by our metrics as well as humans.","authors":["Peng Qi","Yuhao Zhang","Christopher D. Manning"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations","tldr":"We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where t...","track":"CoNLL 2020"},"id":"WS-1.69_F","presentation_id":"38940640","rocketchat_channel":"paper-conll-69_F","speakers":"Peng Qi|Yuhao Zhang|Christopher D. Manning","title":"Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations"},{"content":{"abstract":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.","authors":["Hongyu Gong","Suma Bhat","Pramod Viswanath"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enriching Word Embeddings with Temporal and Spatial Information","tldr":"The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may requ...","track":"CoNLL 2020"},"id":"WS-1.7","presentation_id":"38939463","rocketchat_channel":"paper-conll-7","speakers":"Hongyu Gong|Suma Bhat|Pramod Viswanath","title":"Enriching Word Embeddings with Temporal and Spatial Information"},{"content":{"abstract":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus. We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning. We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data. Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-of-the-art models.","authors":["Frederick Reiss","Hong Xu","Bryan Cutler","Karthik Muthuraman","Zachary Eichenberger"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus","tldr":"The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth ...","track":"CoNLL 2020"},"id":"WS-1.70","presentation_id":"38939478","rocketchat_channel":"paper-conll-70","speakers":"Frederick Reiss|Hong Xu|Bryan Cutler|Karthik Muthuraman|Zachary Eichenberger","title":"Identifying Incorrect Labels in the CoNLL-2003 Corpus"},{"content":{"abstract":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT\u2019s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.","authors":["Gabriella Chronis","Katrin Erk"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships","tldr":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These e...","track":"CoNLL 2020"},"id":"WS-1.73","presentation_id":"38939479","rocketchat_channel":"paper-conll-73","speakers":"Gabriella Chronis|Katrin Erk","title":"When is a bishop not like a rook? When it\u2019s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships"},{"content":{"abstract":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - MQA-RC, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models. However, we show this relationship does not hold true for the XLNet models \u2013 despite the fact that the XLNet performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.","authors":["Ekta Sood","Simon Tannert","Diego Frassinelli","Andreas Bulling","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension","tldr":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new metho...","track":"CoNLL 2020"},"id":"WS-1.8","presentation_id":"38939464","rocketchat_channel":"paper-conll-8","speakers":"Ekta Sood|Simon Tannert|Diego Frassinelli|Andreas Bulling|Ngoc Thang Vu","title":"Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension"},{"content":{"abstract":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty \u2014 entropy-based UID, surprisal-based UID, and pointwise mutual information \u2014 to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.","authors":["Brennan Gonering","Emily Morgan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Processing effort is a poor predictor of cross-linguistic word order frequency","tldr":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Densit...","track":"CoNLL 2020"},"id":"WS-1.83","presentation_id":"38939480","rocketchat_channel":"paper-conll-83","speakers":"Brennan Gonering|Emily Morgan","title":"Processing effort is a poor predictor of cross-linguistic word order frequency"},{"content":{"abstract":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions (about 70%) is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.","authors":["Maja Popovi\u0107"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relations between comprehensibility and adequacy errors in machine translation output","tldr":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all ma...","track":"CoNLL 2020"},"id":"WS-1.88","presentation_id":"38939481","rocketchat_channel":"paper-conll-88","speakers":"Maja Popovi\u0107","title":"Relations between comprehensibility and adequacy errors in machine translation output"},{"content":{"abstract":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier\u2019s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","authors":["Hartger Veeman","Marc Allassonni\u00e8re-Tang","Aleksandrs Berdicevskis","Ali Basirat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment","tldr":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two e...","track":"CoNLL 2020"},"id":"WS-1.96","presentation_id":"38939482","rocketchat_channel":"paper-conll-96","speakers":"Hartger Veeman|Marc Allassonni\u00e8re-Tang|Aleksandrs Berdicevskis|Ali Basirat","title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment"},{"content":{"abstract":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a graph notation. To this end, we designed a novel Plain Graph Notation (PGN) that handles various graphs universally. Then, our parser predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our parser can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the parser tied for 1st place in both cross-framework and cross-lingual tracks.","authors":["Hiroaki Ozaki","Gaku Morio","Yuta Koreeda","Terufumi Morishita","Toshinori Miyoshi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer","tldr":"This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text...","track":"CoNLL 2020"},"id":"WS-1.Shared1","presentation_id":"38941228","rocketchat_channel":"paper-conll-Shared1","speakers":"Hiroaki Ozaki|Gaku Morio|Yuta Koreeda|Terufumi Morishita|Toshinori Miyoshi","title":"Hitachi at MRP 2020: Text-to-Graph-Notation Transducer"},{"content":{"abstract":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. Our solution consists of two sub-systems: transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) and iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.","authors":["Longxu Dou","Yunlong Feng","Yuqiu Ji","Wanxiang Che","Ting Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser","tldr":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, A...","track":"CoNLL 2020"},"id":"WS-1.Shared2","presentation_id":"38941229","rocketchat_channel":"paper-conll-Shared2","speakers":"Longxu Dou|Yunlong Feng|Yuqiu Ji|Wanxiang Che|Ting Liu","title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser"},{"content":{"abstract":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.","authors":["Ofir Arviv","Ruixiang Cui","Daniel Hershcovich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers","tldr":"This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respe...","track":"CoNLL 2020"},"id":"WS-1.Shared3","presentation_id":"38941230","rocketchat_channel":"paper-conll-Shared3","speakers":"Ofir Arviv|Ruixiang Cui|Daniel Hershcovich","title":"HUJI-KU at MRP 2020: Two Transition-based Neural Parsers"},{"content":{"abstract":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the abstract meaning representation framework and propose a joint state model for the graph-sequence iterative inference of (Cai and Lam, 2020) for a simplified graph-sequence inference. In our joint state model, we update only a single joint state vector during the graph-sequence inference process instead of keeping the dual state vectors, and all other components are exactly the same as in (Cai and Lam, 2020).","authors":["Seung-Hoon Na","Jinwoo Min"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference","tldr":"This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the...","track":"CoNLL 2020"},"id":"WS-1.Shared4","presentation_id":"38941231","rocketchat_channel":"paper-conll-Shared4","speakers":"Seung-Hoon Na|Jinwoo Min","title":"JBNU at MRP 2020: AMR Parsing Using a Joint State Model for Graph-Sequence Iterative Inference"},{"content":{"abstract":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.","authors":["David Samuel","Milan Straka"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN","tldr":"We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the ...","track":"CoNLL 2020"},"id":"WS-1.Shared5","presentation_id":"38941232","rocketchat_channel":"paper-conll-Shared5","speakers":"David Samuel|Milan Straka","title":"\u00daFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN"},{"content":{"abstract":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.","authors":["Daniel Zeman","Jan Hajic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FGD at MRP 2020: Prague Tectogrammatical Graphs","tldr":"Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG ...","track":"CoNLL 2020"},"id":"WS-1.Shared6","presentation_id":"38941233","rocketchat_channel":"paper-conll-Shared6","speakers":"Daniel Zeman|Jan Hajic","title":"FGD at MRP 2020: Prague Tectogrammatical Graphs"},{"content":{"abstract":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.","authors":["Lasha Abzianidze","Johan Bos","Stephan Oepen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs","tldr":"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpreta...","track":"CoNLL 2020"},"id":"WS-1.Shared7","presentation_id":"38941234","rocketchat_channel":"paper-conll-Shared7","speakers":"Lasha Abzianidze|Johan Bos|Stephan Oepen","title":"DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs"},{"content":{"abstract":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu","authors":["Stephan Oepen","Omri Abend","Lasha Abzianidze","Johan Bos","Jan Hajic","Daniel Hershcovich","Bin Li","Tim O\u2019Gorman","Nianwen Xue","Daniel Zeman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.conll-shared.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing","tldr":"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the ...","track":"CoNLL 2020"},"id":"WS-1.Shared8","presentation_id":"38941235","rocketchat_channel":"paper-conll-Shared8","speakers":"Stephan Oepen|Omri Abend|Lasha Abzianidze|Johan Bos|Jan Hajic|Daniel Hershcovich|Bin Li|Tim O\u2019Gorman|Nianwen Xue|Daniel Zeman","title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing"}],"prerecorded_talks":[{"presentation_id":"38940786","speakers":"Emmanuel Dupoux","title":"Learning Language Like Infants Do: Self Supervised Learning From Raw Audio"},{"presentation_id":"38940787","speakers":"Kristina Toutanova","title":"Toward Progress in Text Representations for Question Answering"}],"rocketchat_channel":"workshop-conll","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 12:45:00 GMT","hosts":"TBD","link":"","session_name":"<a href=\"http://mrp.nlpl.eu/2020/index.php?page=16\">Shared Task: Meaning Representation Parsing</a> (zoom and gather.town)","start_time":"Thu, 19 Nov 2020 10:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 13:00:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break","start_time":"Thu, 19 Nov 2020 12:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:00:00 GMT","hosts":"TBD","link":"","session_name":"Session 1: Mixed topics (gather.town)","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:50:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break / watch pre-recorded Toutanova talk \"Toward Progress in Text Representations for Question Answering\"","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:00:00 GMT","hosts":"TBD","link":"","session_name":"Opening remarks and best paper awards","start_time":"Thu, 19 Nov 2020 15:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:30:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk Q&A: Kristina Toutanova (Zoom)","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:45:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break","start_time":"Thu, 19 Nov 2020 16:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Session 2: Interaction and multimodality (gather.town)","start_time":"Thu, 19 Nov 2020 16:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break","start_time":"Thu, 19 Nov 2020 18:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":"TBD","link":"","session_name":"Session 3: Interpretability of NLP models (gather.town)","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:00:00 GMT","hosts":"TBD","link":"","session_name":"Session 4: Resources and techniques (gather.town)","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break / watch pre-recorded Dupoux talk \"Learning Language Like Infants Do: Self Supervised Learning From Raw Video\"","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:30:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk Q&A: Emmanuel Dupoux (Zoom)","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:45:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break","start_time":"Fri, 20 Nov 2020 16:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Session 5: Language acquisition and psycholinguistics (gather.town)","start_time":"Fri, 20 Nov 2020 16:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"TBD","link":"","session_name":"Coffee break","start_time":"Fri, 20 Nov 2020 18:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:00:00 GMT","hosts":"TBD","link":"","session_name":"Session 6: Mixed topics + Findings papers (gather.town)","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"}],"title":"CoNLL 2020","website":"https://www.conll.org","zoom_links":["https://zoom.us"]},{"abstract":"A two day conference on machine translation with shared tasks.","blocks":[{"end_time":"Thu, 19 Nov 2020 20:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 09:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:15:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"}],"id":"WS-2","livestream":null,"organizers":"Barry Haddow, Philipp Koehn, Ond\u0159ej Bojar, Christian Federmann, Yvette Graham, Matthias Huck, Christof Monz and Lucia Specia","papers":[{"content":{"abstract":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese->English.","authors":["Tanfang Chen","Weiwei Wang","Wenyang Wei","Xing Shi","Xiangang Li","Jieping Ye","Kevin Knight"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.7.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiDi's Machine Translation System for WMT2020","tldr":"This paper describes the DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese->English. In this direction, we use the Transformer as our baseline model and integrate several tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1","presentation_id":"38939543","rocketchat_channel":"paper-wmt-1","speakers":"Tanfang Chen|Weiwei Wang|Wenyang Wei|Xing Shi|Xiangang Li|Jieping Ye|Kevin Knight","title":"DiDi's Machine Translation System for WMT2020"},{"content":{"abstract":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7% and 5% relative improvement, over the baseline, in sacreBLEU score on the test set for Pashto and Khmer respectively.","authors":["Muhammad ElNokrashy","Amr Hendy","Mohamed Abdelghaffar","Mohamed Afify","Ahmed Tawfik","Hany Hassan Awadalla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.106.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions","tldr":"This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.100","presentation_id":"38939612","rocketchat_channel":"paper-wmt-100","speakers":"Muhammad ElNokrashy|Amr Hendy|Mohamed Abdelghaffar|Mohamed Afify|Ahmed Tawfik|Hany Hassan Awadalla","title":"Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions"},{"content":{"abstract":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting. We find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the UNMT system trained on it. Furthermore, we show that document-level data selection should be preferred for training the XLM model when possible. Finally, we show that there is a trade-off between quality and quantity of the data used to train UNMT systems.","authors":["Lukas Edman","Antonio Toral","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.130.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian","tldr":"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German\u2013Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.101","presentation_id":"38939613","rocketchat_channel":"paper-wmt-101","speakers":"Lukas Edman|Antonio Toral|Gertjan van Noord","title":"Data Selection for Unsupervised Translation of German\u2013Upper Sorbian"},{"content":{"abstract":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","authors":["Art\u016brs Stafanovi\u010ds","M\u0101rcis Pinnis","Toms Bergmanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.73.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations","tldr":"When translating The secretary asked for details. to a language with grammatical gender, it might be necessary to determine the gender of the subject secretary. If the sentence does not contain the necessary information, it is not always possible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.102","presentation_id":"38939614","rocketchat_channel":"paper-wmt-102","speakers":"Art\u016brs Stafanovi\u010ds|M\u0101rcis Pinnis|Toms Bergmanis","title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations"},{"content":{"abstract":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model trained with the Fairseq sequence modeling toolkit. Our final system is an ensemble of two transformer models, which ranked first in WMT20 evaluation. One model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are same alphabet languages. The other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.","authors":["Kamalkumar Rathinasamy","Amanpreet Singh","Balaguru Sivasambagupta","Prajna Prasad Neerchal","Vani Sivasankaran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.52.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task","tldr":"This paper describes Infosys\u2019s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.103","presentation_id":"38939615","rocketchat_channel":"paper-wmt-103","speakers":"Kamalkumar Rathinasamy|Amanpreet Singh|Balaguru Sivasambagupta|Prajna Prasad Neerchal|Vani Sivasankaran","title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model, Transformer. We applied various strategies in order to improve our baseline MT systems, e.g. onolin- gual sentence selection for creating synthetic training data, mining monolingual sentences for adapting our MT systems to the task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks.","authors":["Venkatesh Parthasarathy","Akshai Ramesh","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.27.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT System Description for the WMT20 News Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.104","presentation_id":"38939616","rocketchat_channel":"paper-wmt-104","speakers":"Venkatesh Parthasarathy|Akshai Ramesh|Rejwanul Haque|Andy Way","title":"The ADAPT System Description for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical terminologies, and using the state-of-the-art neural MT (NMT) model: Transformer. In order to improve our baseline NMT system, we employ a number of methods, e.g. \u201cpseudo\u201d parallel data selection, monolingual data selection for synthetic corpus creation, mining monolingual sentences for adapting our NMT systems to this task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that systematic addition of the aforementioned techniques to the baseline yields an excellent performance in the English-to-Basque translation task.","authors":["Prashant Nayak","Rejwanul Haque","Andy Way"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.91.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes the ADAPT Centre\u2019s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical ter...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.105","presentation_id":"38939617","rocketchat_channel":"paper-wmt-105","speakers":"Prashant Nayak|Rejwanul Haque|Andy Way","title":"The ADAPT\u2019s Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English into French, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from English into German; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions: zero-shot and few-shot domain adaptation.","authors":["Sadaf Abdul Rauf","Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez","Minh Quang Pham","Fran\u00e7ois Yvon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.86.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LIMSI @ WMT 2020","tldr":"This paper describes LIMSI's submissions to the translation shared tasks at WMT'20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English int...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.107","presentation_id":"38939618","rocketchat_channel":"paper-wmt-107","speakers":"Sadaf Abdul Rauf|Jos\u00e9 Carlos Rosales N\u00fa\u00f1ez|Minh Quang Pham|Fran\u00e7ois Yvon","title":"LIMSI @ WMT 2020"},{"content":{"abstract":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions, from Russian to English and from English to Russian. We used official training data, 102 million parallel corpora and 10 million monolingual corpora. Our baseline systems are Transformer models trained with the Sockeye sequence modeling toolkit, supplemented by bi-text data filtering schemes, back-translations, reordering and other related processing methods. The BLEU value of our translation result from Russian to English is 35.7, ranking 5th, while from English to Russian is 39.8, ranking 2th.","authors":["ariel Xv"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.35.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Russian-English Bidirectional Machine Translation System","tldr":"This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.109","presentation_id":"38939619","rocketchat_channel":"paper-wmt-109","speakers":"ariel Xv","title":"Russian-English Bidirectional Machine Translation System"},{"content":{"abstract":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish -> Portuguese.","authors":["Lovish Madaan","Soumya Sharma","Parag Singla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.46.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task","tldr":"In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi <-> Marathi and Spanish <-> Portuguese. We try out three different model settings for the translation task and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.113","presentation_id":"38939620","rocketchat_channel":"paper-wmt-113","speakers":"Lovish Madaan|Soumya Sharma|Parag Singla","title":"Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task"},{"content":{"abstract":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.","authors":["Keshaw Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.136.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20","tldr":"In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transforme...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.114","presentation_id":"38939621","rocketchat_channel":"paper-wmt-114","speakers":"Keshaw Singh","title":"Adobe AMPS\u2019s Submission for Very Low Resource Supervised Translation Task at WMT20"},{"content":{"abstract":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and data augmentation, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our system significantly outperforms all other baselines and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our submission can achieve +5.56 BLEU and -4.57 TER with respect to the official MT baseline.","authors":["Jiayi Wang","Ke Wang","Kai Fan","Yuqi Zhang","Jun Lu","Xin Ge","Yangbin Shi","Yu Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.84.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT","tldr":"The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba\u2019s submissions to the WMT 2020 APE Shared Ta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.115","presentation_id":"38939622","rocketchat_channel":"paper-wmt-115","speakers":"Jiayi Wang|Ke Wang|Kai Fan|Yuqi Zhang|Jun Lu|Xin Ge|Yangbin Shi|Yu Zhao","title":"Alibaba\u2019s Submission for the WMT 2020 APE Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT"},{"content":{"abstract":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation.","authors":["Vandan Mujadia","Dipti Sharma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.48.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMT based Similar Language Translation for Hindi - Marathi","tldr":"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.116","presentation_id":"38939623","rocketchat_channel":"paper-wmt-116","speakers":"Vandan Mujadia|Dipti Sharma","title":"NMT based Similar Language Translation for Hindi - Marathi"},{"content":{"abstract":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En->Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta->En and En->Ta respectively, and 27.9 and 13.0 for Iu->En and En->Iu respectively.","authors":["Peng-Jen Chen","Ann Lee","Changhan Wang","Naman Goyal","Angela Fan","Mary Williamson","Jiatao Gu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.8.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Facebook AI's WMT20 News Translation Task Submission","tldr":"This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil <-> English and Inuktitut <-> English, where there are limited out-of-domain bitext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.117","presentation_id":"38939624","rocketchat_channel":"paper-wmt-117","speakers":"Peng-Jen Chen|Ann Lee|Changhan Wang|Naman Goyal|Angela Fan|Mary Williamson|Jiatao Gu","title":"Facebook AI's WMT20 News Translation Task Submission"},{"content":{"abstract":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.","authors":["Rupjyoti Baruah","Rajesh Kumar Mundotiya","Amit Kumar","Anil kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.126.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLPRL System for Very Low Resource Supervised Machine Translation","tldr":"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.118","presentation_id":"38939625","rocketchat_channel":"paper-wmt-118","speakers":"Rupjyoti Baruah|Rajesh Kumar Mundotiya|Amit Kumar|Anil kumar Singh","title":"NLPRL System for Very Low Resource Supervised Machine Translation"},{"content":{"abstract":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT models to learn tag placement via training data augmentation. We study the interactions of tag representation, data augmentation size, tag complexity, and language pair to show the drawbacks and benefits of each method. We construct and release new test sets containing tagged data for three language pairs of varying difficulty.","authors":["Greg Hanneman","Georgiana Dinu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.138.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Should Markup Tags Be Translated?","tldr":"The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT mode...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.119","presentation_id":"38939626","rocketchat_channel":"paper-wmt-119","speakers":"Greg Hanneman|Georgiana Dinu","title":"How Should Markup Tags Be Translated?"},{"content":{"abstract":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples in order to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 12,432 language pairs that provides competitive translation quality for all language pairs.","authors":["Markus Freitag","Orhan Firat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.66.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Complete Multilingual Neural Machine Translation","tldr":"Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.12","presentation_id":"38939550","rocketchat_channel":"paper-wmt-12","speakers":"Markus Freitag|Orhan Firat","title":"Complete Multilingual Neural Machine Translation"},{"content":{"abstract":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on news data and fine-tune them on in-domain and pseudo-in-domain web crawled data. Our baseline systems are transformer-big models that are pre-trained on the WMT'19 News Translation task and fine-tuned on pseudo-in-domain web crawled data and in-domain task data. We also experiment with (i) adaptation using speaker and domain tags and (ii) using different types and amounts of preceding context. We observe that contrarily to expectations, exploiting context degrades the results (and on analysis the data is not highly contextual). However using domain tags does improve scores according to the automatic evaluation. Our final primary systems use domain tags and are ensembles of 4 models, with noisy channel reranking of outputs. Our en-de system was ranked second in the shared task while our de-en system outperformed all the other systems.","authors":["Nikita Moghe","Christian Hardmeier","Rachel Bawden"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.58.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task","tldr":"This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.121","presentation_id":"38939627","rocketchat_channel":"paper-wmt-121","speakers":"Nikita Moghe|Christian Hardmeier|Rachel Bawden","title":"The University of Edinburgh-Uppsala University's Submission to the WMT 2020 Chat Translation Task"},{"content":{"abstract":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking results significantly improve the results on the training sets but decrease the test set results. RTMs can achieve to become the 5th among 13 models in ru-en subtask and 5th in the multilingual track of sentence-level Task 1 based on MAE.","authors":["Ergun Bi\u00e7ici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.114.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RTM Ensemble Learning Results at Quality Estimation Task","tldr":"We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking re...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.122","presentation_id":"38939628","rocketchat_channel":"paper-wmt-122","speakers":"Ergun Bi\u00e7ici","title":"RTM Ensemble Learning Results at Quality Estimation Task"},{"content":{"abstract":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our submission to the WMT20 Chat Translation Task, which consists of two language directions, English \u2013> German and German \u2013> English. The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network. Our system ensembles three models, each with different hyperparameters. Despite being trained on a very small corpus, our model produces surprisingly good results.","authors":["Roweida Mohammed","Mahmoud Al-Ayyoub","Malak Abdullah"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.59.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"JUST System for WMT20 Chat Translation Task","tldr":"Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.123","presentation_id":"38939629","rocketchat_channel":"paper-wmt-123","speakers":"Roweida Mohammed|Mahmoud Al-Ayyoub|Malak Abdullah","title":"JUST System for WMT20 Chat Translation Task"},{"content":{"abstract":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glass-box approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multi-lingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs inTask 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a light-weight alternative to the neural-based models.","authors":["Marina Fomicheva","Shuo Sun","Lisa Yankovskaya","Fr\u00e9d\u00e9ric Blain","Vishrav Chaudhary","Mark Fishel","Francisco Guzm\u00e1n","Lucia Specia"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.116.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task","tldr":"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b)...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.124","presentation_id":"38939630","rocketchat_channel":"paper-wmt-124","speakers":"Marina Fomicheva|Shuo Sun|Lisa Yankovskaya|Fr\u00e9d\u00e9ric Blain|Vishrav Chaudhary|Mark Fishel|Francisco Guzm\u00e1n|Lucia Specia","title":"BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (English token count). Our submission focuses on filtering Pashto-English, building on previously successful methods to produce two sets of scores: LASER_LM, a combination of the LASER similarity scores provided in the shared task and perplexity scores from language models, and DCCEF_DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring.","authors":["Felicia Koerner","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.109.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task","tldr":"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year\u2019s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.125","presentation_id":"38939631","rocketchat_channel":"paper-wmt-125","speakers":"Felicia Koerner|Philipp Koehn","title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task"},{"content":{"abstract":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We participated in ten translation directions for the English/Spanish, English/Portuguese, English/Russian, English/Italian, and English/French language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources.","authors":["Felipe Soares","Delton Vaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.95.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts","tldr":"This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We particip...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.128","presentation_id":"38939632","rocketchat_channel":"paper-wmt-128","speakers":"Felipe Soares|Delton Vaz","title":"UoS Participation in the WMT20 Translation of Biomedical Abstracts"},{"content":{"abstract":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and build our baseline systems to be morphologically motivated sub-word unit-based Transformer base models that we train using the Marian machine translation toolkit. Additionally, we experiment with different parallel and monolingual data selection schemes, as well as sampled back-translation. Our final models are ensembles of Transformer base and Transformer big models which feature right-to-left re-ranking.","authors":["Rihards Kri\u0161lauks","M\u0101rcis Pinnis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.15.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tilde at WMT 2020: News Task Systems","tldr":"This paper describes Tilde's submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.133","presentation_id":"38939633","rocketchat_channel":"paper-wmt-133","speakers":"Rihards Kri\u0161lauks|M\u0101rcis Pinnis","title":"Tilde at WMT 2020: News Task Systems"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.","authors":["Rebecca Knowles","Samuel Larkin","Darlene Stewart","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.132.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications","tldr":"We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.135","presentation_id":"38939634","rocketchat_channel":"paper-wmt-135","speakers":"Rebecca Knowles|Samuel Larkin|Darlene Stewart|Patrick Littell","title":"NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications"},{"content":{"abstract":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology.","authors":["Prajit Dhar","Arianna Bisazza","Gertjan van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.9.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020","tldr":"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.136","presentation_id":"38939635","rocketchat_channel":"paper-wmt-136","speakers":"Prajit Dhar|Arianna Bisazza|Gertjan van Noord","title":"Linguistically Motivated Subwords for English-Tamil Translation: University of Groningen\u2019s Submission to WMT-2020"},{"content":{"abstract":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World's languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.","authors":["J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.139.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT","tldr":"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that coll...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.137","presentation_id":"38939636","rocketchat_channel":"paper-wmt-137","speakers":"J\u00f6rg Tiedemann","title":"The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT"},{"content":{"abstract":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.","authors":["Minh Quang Pham","Jitao Xu","Josep Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.63.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Priming Neural Machine Translation","tldr":"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.138","presentation_id":"38939637","rocketchat_channel":"paper-wmt-138","speakers":"Minh Quang Pham|Jitao Xu|Josep Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"Priming Neural Machine Translation"},{"content":{"abstract":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for Hindi\u2194Marathi each and 1 NMT systems were developed for Hindi\u2194Marathi using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.","authors":["Atul Kr. Ojha","Priya Rani","Akanksha Bansal","Bharathi Raja Chakravarthi","Ritesh Kumar","John P. McCrae"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.49.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020","tldr":"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi\u2194Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.139","presentation_id":"38939638","rocketchat_channel":"paper-wmt-139","speakers":"Atul Kr. Ojha|Priya Rani|Akanksha Bansal|Bharathi Raja Chakravarthi|Ritesh Kumar|John P. McCrae","title":"NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020"},{"content":{"abstract":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese\u2013English and Hindi\u2013English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40% smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40% of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.","authors":["Raj Dabre","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.61.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models","tldr":"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.14","presentation_id":"38939551","rocketchat_channel":"paper-wmt-14","speakers":"Raj Dabre|Atsushi Fujita","title":"Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models"},{"content":{"abstract":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.","authors":["Rebecca Knowles","Darlene Stewart","Samuel Larkin","Patrick Littell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.13.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NRC Systems for the 2020 Inuktitut-English News Translation Task","tldr":"We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetune...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.141","presentation_id":"38939639","rocketchat_channel":"paper-wmt-141","speakers":"Rebecca Knowles|Darlene Stewart|Samuel Larkin|Patrick Littell","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"content":{"abstract":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the models and used back-translation for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each model. We also investigate the role of mutual intelligibility in model performance.","authors":["Ife Adebara","El Moatez Billah Nagoudi","Muhammad Abdul Mageed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.42.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers","tldr":"In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.142","presentation_id":"38939640","rocketchat_channel":"paper-wmt-142","speakers":"Ife Adebara|El Moatez Billah Nagoudi|Muhammad Abdul Mageed","title":"Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers"},{"content":{"abstract":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.","authors":["Ivana Kvapil\u00edkov\u00e1","Tom Kocmi","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.133.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20","tldr":"This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.143","presentation_id":"38939641","rocketchat_channel":"paper-wmt-143","speakers":"Ivana Kvapil\u00edkov\u00e1|Tom Kocmi|Ond\u0159ej Bojar","title":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20"},{"content":{"abstract":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.","authors":["Rachel Bawden","Alexandra Birch","Radina Dobreva","Arturo Oncevay","Antonio Valerio Miceli Barone","Philip Williams"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.5.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task","tldr":"We describe the University of Edinburgh\u2019s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.144","presentation_id":"38939642","rocketchat_channel":"paper-wmt-144","speakers":"Rachel Bawden|Alexandra Birch|Radina Dobreva|Arturo Oncevay|Antonio Valerio Miceli Barone|Philip Williams","title":"The University of Edinburgh\u2019s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task"},{"content":{"abstract":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.","authors":["Jo\u00e3o Moura","miguel vera","Daan van Stigt","Fabio Kepler","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.119.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task","tldr":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitte...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.146","presentation_id":"38939643","rocketchat_channel":"paper-wmt-146","speakers":"Jo\u00e3o Moura|miguel vera|Daan van Stigt|Fabio Kepler|Andr\u00e9 F. T. Martins","title":"IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en\u2192ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.","authors":["Karen Hambardzumyan","Hovhannes Tamoyan","Hrant Khachatrian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.88.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs","tldr":"This report describes YerevaNN\u2019s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.147","presentation_id":"38939644","rocketchat_channel":"paper-wmt-147","speakers":"Karen Hambardzumyan|Hovhannes Tamoyan|Hrant Khachatrian","title":"YerevaNN's Systems for WMT20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs"},{"content":{"abstract":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available in our GitHub repository.","authors":["Alexandre Lopes","Rodrigo Nogueira","Roberto Lotufo","Helio Pedrini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.90.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation","tldr":"Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Port...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.149","presentation_id":"38939645","rocketchat_channel":"paper-wmt-149","speakers":"Alexandre Lopes|Rodrigo Nogueira|Roberto Lotufo|Helio Pedrini","title":"Lite Training Strategies for Portuguese-English and English-Portuguese Translation"},{"content":{"abstract":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we implemented a noising module that simulates four types of post-editing errors, and we introduced this module into a Transformer-based multi-source APE model. Our noising module implants errors into texts on the target side of parallel corpora during the training phase to make synthetic MT outputs, increasing the entire number of training samples. We also generated additional training data using the parallel corpora and NMT model that were released for the Quality Estimation task, and we used these data to train our APE model. Experimental results on the WMT20 English-German APE data set show improvements over the baseline in terms of both the TER and BLEU scores: our primary submission achieved an improvement of -3.15 TER and +4.01 BLEU, and our contrastive submission achieved an improvement of -3.34 TER and +4.30 BLEU.","authors":["WonKee Lee","Jaehun Shin","Baikjin Jung","Jihyung Lee","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.83.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Noising Scheme for Data Augmentation in Automatic Post-Editing","tldr":"This paper describes POSTECH's submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we imple...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.150","presentation_id":"38939646","rocketchat_channel":"paper-wmt-150","speakers":"WonKee Lee|Jaehun Shin|Baikjin Jung|Jihyung Lee|Jong-Hyeok Lee","title":"Noising Scheme for Data Augmentation in Automatic Post-Editing"},{"content":{"abstract":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard Transformer, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 BLEU on the agent side (en\u2192de), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.","authors":["Calvin Bao","Yow-Ting Shiue","Chujun Song","Jie Li","Marine Carpuat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.56.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation","tldr":"This paper describes the University of Maryland's submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model tr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.153","presentation_id":"38939647","rocketchat_channel":"paper-wmt-153","speakers":"Calvin Bao|Yow-Ting Shiue|Chujun Song|Jie Li|Marine Carpuat","title":"The University of Maryland's Submissions to the WMT20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation"},{"content":{"abstract":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose strategies from previous years\u2019 efforts with larger datasets and also train models with precomputed word alignments under various settings in an effort to improve translation quality.","authors":["Jeremy Gwinnup","Tim Anderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.20.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The AFRL WMT20 News\u00ad Translation Systems","tldr":"This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.155","presentation_id":"38939648","rocketchat_channel":"paper-wmt-155","speakers":"Jeremy Gwinnup|Tim Anderson","title":"The AFRL WMT20 News\u00ad Translation Systems"},{"content":{"abstract":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre/post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.","authors":["Ankur Kejriwal","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.108.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20","tldr":"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language model...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.156","presentation_id":"38939649","rocketchat_channel":"paper-wmt-156","speakers":"Ankur Kejriwal|Philipp Koehn","title":"An exploratory approach to the Parallel Corpus Filtering shared task WMT20"},{"content":{"abstract":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95% of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our models do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and robustness.","authors":["David Wan","Chris Kedzie","Faisal Ladhak","Marine Carpuat","Kathleen McKeown"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.141.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Terminology Constraints in Automatic Post-Editing","tldr":"Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.157","presentation_id":"38939650","rocketchat_channel":"paper-wmt-157","speakers":"David Wan|Chris Kedzie|Faisal Ladhak|Marine Carpuat|Kathleen McKeown","title":"Incorporating Terminology Constraints in Automatic Post-Editing"},{"content":{"abstract":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system's performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.","authors":["Qu Cui","Xiang Geng","Shujian Huang","Jiajun CHEN"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.115.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NJU's submission to the WMT20 QE Shared Task","tldr":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.158","presentation_id":"38939651","rocketchat_channel":"paper-wmt-158","speakers":"Qu Cui|Xiang Geng|Shujian Huang|Jiajun CHEN","title":"NJU's submission to the WMT20 QE Shared Task"},{"content":{"abstract":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.","authors":["Brian Thompson","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.67.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity","tldr":"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.16","presentation_id":"38939552","rocketchat_channel":"paper-wmt-16","speakers":"Brian Thompson|Matt Post","title":"Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity"},{"content":{"abstract":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1\u2019s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.","authors":["Chi-kiu Lo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.99.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation","tldr":"We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.160","presentation_id":"38939652","rocketchat_channel":"paper-wmt-160","speakers":"Chi-kiu Lo","title":"Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation"},{"content":{"abstract":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2\u2019s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.","authors":["Chi-kiu Lo","Samuel Larkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.100.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model","tldr":"We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with con...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.161","presentation_id":"38939653","rocketchat_channel":"paper-wmt-161","speakers":"Chi-kiu Lo|Samuel Larkin","title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model"},{"content":{"abstract":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation.","authors":["Vikrant Goyal","Anoop Kunchukuttan","Rahul Kejriwal","Siddharth Jain","Amit Bhagwat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.19.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20","tldr":"We describe our submission for the English\u2192Tamil and Tamil\u2192English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares conta...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.162","presentation_id":"38939654","rocketchat_channel":"paper-wmt-162","speakers":"Vikrant Goyal|Anoop Kunchukuttan|Rahul Kejriwal|Siddharth Jain|Amit Bhagwat","title":"Contact Relatedness can help improve multilingual NMT: Microsoft STCI-MT @ WMT20"},{"content":{"abstract":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.","authors":["Minh Quang Pham","Josep Maria Crego","Fran\u00e7ois Yvon","Jean Senellart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.72.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation","tldr":"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.163","presentation_id":"38939655","rocketchat_channel":"paper-wmt-163","speakers":"Minh Quang Pham|Josep Maria Crego|Fran\u00e7ois Yvon|Jean Senellart","title":"A Study of Residual Adapters for Multi-Domain Neural Machine Translation"},{"content":{"abstract":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.","authors":["Sumbal Naz","Sadaf Abdul Rauf","Noor-e- Hira","Sami Ul Haq"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.92.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FJWU participation for the WMT20 Biomedical Translation Task","tldr":"This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.167","presentation_id":"38939656","rocketchat_channel":"paper-wmt-167","speakers":"Sumbal Naz|Sadaf Abdul Rauf|Noor-e- Hira|Sami Ul Haq","title":"FJWU participation for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.","authors":["Zuchao Li","Hai Zhao","Rui Wang","Kehai Chen","Masao Utiyama","Eiichiro Sumita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.22.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on su...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.168","presentation_id":"38939657","rocketchat_channel":"paper-wmt-168","speakers":"Zuchao Li|Hai Zhao|Rui Wang|Kehai Chen|Masao Utiyama|Eiichiro Sumita","title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers' LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.","authors":["Chi-kiu Lo","Eric Joanis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.110.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models","tldr":"The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting se...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.169","presentation_id":"38939658","rocketchat_channel":"paper-wmt-169","speakers":"Chi-kiu Lo|Eric Joanis","title":"Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models"},{"content":{"abstract":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.","authors":["Tom Kocmi","Tomasz Limisiewicz","Gabriel Stanovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.39.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Gender Coreference and Bias Evaluation at WMT 2020","tldr":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.170","presentation_id":"38939659","rocketchat_channel":"paper-wmt-170","speakers":"Tom Kocmi|Tomasz Limisiewicz|Gabriel Stanovsky","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"content":{"abstract":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN<->FR,CS,DE,FI system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.","authors":["Annette Rios","Mathias M\u00fcller","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.64.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation","tldr":"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.171","presentation_id":"38939660","rocketchat_channel":"paper-wmt-171","speakers":"Annette Rios|Mathias M\u00fcller|Rico Sennrich","title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation"},{"content":{"abstract":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech \u2194 English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.","authors":["Ulrich Germann","Roman Grundkiewicz","Martin Popel","Radina Dobreva","Nikolay Bogoychev","Kenneth Heafield"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.17.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task","tldr":"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower te...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.172","presentation_id":"38939661","rocketchat_channel":"paper-wmt-172","speakers":"Ulrich Germann|Roman Grundkiewicz|Martin Popel|Radina Dobreva|Nikolay Bogoychev|Kenneth Heafield","title":"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task"},{"content":{"abstract":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-English. All our submissions are MarianNMT-based neural systems. We use more data compared to last year and update our back-translations with better models from the previous year. We show competitive results in terms of BLEU in most directions.","authors":["Alexander Molchanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.25.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PROMT Systems for WMT 2020 Shared News Translation Task","tldr":"This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.175","presentation_id":"38939662","rocketchat_channel":"paper-wmt-175","speakers":"Alexander Molchanov","title":"PROMT Systems for WMT 2020 Shared News Translation Task"},{"content":{"abstract":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.","authors":["Ulrich Germann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.18.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks","tldr":"This paper describes the University of Edinburgh's submission of German <-> English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness....","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.177","presentation_id":"38939663","rocketchat_channel":"paper-wmt-177","speakers":"Ulrich Germann","title":"The University of Edinburgh\u2019s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks"},{"content":{"abstract":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensemble technique on different transformer architectures (Deep, Hybrid, Big, Large Transformers). To enlarge the in-domain bilingual corpus, we use back-translation of monolingual in-domain data in the target language as additional in-domain training data. Our systems in German->English and English->German are ranked 1st and 3rd respectively according to the official evaluation results in terms of BLEU scores.","authors":["Xing Wang","Zhaopeng Tu","Longyue Wang","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.97.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task","tldr":"This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German<->English, English<->German, Chinese<->English and English<->Chinese. We implement our system with model ensem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.178","presentation_id":"38939664","rocketchat_channel":"paper-wmt-178","speakers":"Xing Wang|Zhaopeng Tu|Longyue Wang|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task"},{"content":{"abstract":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed.","authors":["Christian Roest","Lukas Edman","Gosse Minnema","Kevin Kelly","Jennifer Spenader","Antonio Toral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.29.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training","tldr":"Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English\u2013Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of corr...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.179","presentation_id":"38939665","rocketchat_channel":"paper-wmt-179","speakers":"Christian Roest|Lukas Edman|Gosse Minnema|Kevin Kelly|Jennifer Spenader|Antonio Toral","title":"Machine Translation for English\u2013Inuktitut with Segmentation, Data Acquisition and Pre-Training"},{"content":{"abstract":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.","authors":["Eleftherios Avramidis","Vivien Macketanz","Ursula Strohriegel","Aljoscha Burchardt","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.38.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation","tldr":"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized i...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.18","presentation_id":"38939553","rocketchat_channel":"paper-wmt-18","speakers":"Eleftherios Avramidis|Vivien Macketanz|Ursula Strohriegel|Aljoscha Burchardt|Sebastian M\u00f6ller","title":"Fine-grained linguistic evaluation for state-of-the-art Machine Translation"},{"content":{"abstract":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.","authors":["Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.14.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI Submission for the Inuktitut Language in WMT News 2020","tldr":"This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut\u2013English in both translation directions. Our system combines transfer learning from a Czech\u2013English high-resource language pair a...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.180","presentation_id":"38939666","rocketchat_channel":"paper-wmt-180","speakers":"Tom Kocmi","title":"CUNI Submission for the Inuktitut Language in WMT News 2020"},{"content":{"abstract":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike the simulated scenarios used in particular in much of the unsupervised MT work in the past). We were able to obtain most of the digital data available for Upper Sorbian, a minority language of Germany, which was the original motivation for the Unsupervised MT shared task. As we were defining the task, we also obtained a small amount of parallel data (about 60000 parallel sentences), allowing us to offer a Very Low Resource Supervised MT task as well. Six primary systems participated in the unsupervised shared task, two of these systems used additional data beyond the data released by the organizers. Ten primary systems participated in the very low resource supervised task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.","authors":["Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.80.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT","tldr":"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.181","presentation_id":"38939667","rocketchat_channel":"paper-wmt-181","speakers":"Alexander Fraser","title":"Findings of the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT"},{"content":{"abstract":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.","authors":["Martin Popel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.28.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training","tldr":"We describe our two NMT systems submitted to the WMT\u00a02020 shared task in English<->Czech and English<->Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating mu...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.183","presentation_id":"38939668","rocketchat_channel":"paper-wmt-183","speakers":"Martin Popel","title":"CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training"},{"content":{"abstract":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.","authors":["Lei Zhou","Liang Ding","Koichi Takeda"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.125.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns","tldr":"This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Spec...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.185","presentation_id":"38939669","rocketchat_channel":"paper-wmt-185","speakers":"Lei Zhou|Liang Ding|Koichi Takeda","title":"Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns"},{"content":{"abstract":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese \u2192 English task.","authors":["Shuangzhi Wu","Xing Wang","Longyue Wang","Fangxu Liu","Jun Xie","Zhaopeng Tu","Shuming Shi","Mu Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.34.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task","tldr":"This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English \u2194 Chinese and English \u2192 German language pairs. Our systems are built on deep Transf...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.187","presentation_id":"38939670","rocketchat_channel":"paper-wmt-187","speakers":"Shuangzhi Wu|Xing Wang|Longyue Wang|Fangxu Liu|Jun Xie|Zhaopeng Tu|Shuming Shi|Mu Li","title":"Tencent Neural Machine Translation Systems for the WMT20 News Translation Task"},{"content":{"abstract":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German-to-English primary system is ranked the second in terms of BLEU scores.","authors":["Longyue Wang","Zhaopeng Tu","Xing Wang","Li Ding","Liang Ding","Shuming Shi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.60.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task","tldr":"This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.188","presentation_id":"38939671","rocketchat_channel":"paper-wmt-188","speakers":"Longyue Wang|Zhaopeng Tu|Xing Wang|Li Ding|Liang Ding|Shuming Shi","title":"Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task"},{"content":{"abstract":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into German and Chinese by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source/domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year\u2019s results are not directly comparable with last year\u2019s round. However, on both language directions, participants\u2019 submissions show considerable improvements over the baseline results. On English-German, the top ranked system improves over the baseline by -11.35 TER and +16.68 BLEU points, while on EnglishChinese the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year\u2019s round.","authors":["Rajen Chatterjee","Markus Freitag","Matteo Negri","Marco Turchi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.75.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing","tldr":"We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \u201cblack-box\u201d machine translation system by learning from existing human corrections of different senten...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.189","presentation_id":"38939672","rocketchat_channel":"paper-wmt-189","speakers":"Rajen Chatterjee|Markus Freitag|Matteo Negri|Marco Turchi","title":"Findings of the WMT 2020 Shared Task on Automatic Post-Editing"},{"content":{"abstract":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We release our preprocessed dataset to encourage evaluations that stress-test systems under multiple data conditions.","authors":["Kelly Marchisio","Kevin Duh","Philipp Koehn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.68.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"When Does Unsupervised Machine Translation Work?","tldr":"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar doma...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.19","presentation_id":"38939554","rocketchat_channel":"paper-wmt-19","speakers":"Kelly Marchisio|Kevin Duh|Philipp Koehn","title":"When Does Unsupervised Machine Translation Work?"},{"content":{"abstract":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how to improve BLEU by using diverse automatically paraphrased references (Bawden et al., 2020), extending experiments to the multilingual setting for the WMT2020 metrics shared task and for three base metrics. We compare their capacity to exploit up to 100 additional synthetic references. We find that gains are possible when using additional, automatically paraphrased references, although they are not systematic. However, segment-level correlations, particularly into English, are improved for all three metrics and even with higher numbers of paraphrased references.","authors":["Rachel Bawden","Biao Zhang","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.98.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task","tldr":"We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.190","presentation_id":"38939673","rocketchat_channel":"paper-wmt-190","speakers":"Rachel Bawden|Biao Zhang|Andre T\u00e4ttar|Matt Post","title":"ParBLEU: Augmenting Metrics with Automatic Paraphrases for the WMT\u201920 Metrics Shared Task"},{"content":{"abstract":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En->De for agent utterances and De->En for customer messages. We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments (DDA) to evaluate the agent translations.","authors":["M. Amin Farajian","Ant\u00f3nio V. Lopes","Andr\u00e9 F. T. Martins","Sameen Maruf","Gholamreza Haffari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.3.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Chat Translation","tldr":"We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German c...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.191","presentation_id":"38939674","rocketchat_channel":"paper-wmt-191","speakers":"M. Amin Farajian|Ant\u00f3nio V. Lopes|Andr\u00e9 F. T. Martins|Sameen Maruf|Gholamreza Haffari","title":"Findings of the WMT 2020 Shared Task on Chat Translation"},{"content":{"abstract":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight language pairs. Five language pairs were previously addressed in past editions of the shared task, namely, English/German, English/French, English/Spanish, English/Portuguese, and English/Chinese. Three additional languages pairs were also introduced this year: English/Russian, English/Italian, and English/Basque. The task addressed the evaluation of both scientific abstracts (all language pairs) and terminologies (English/Basque only). We received submissions from a total of 20 teams. For recurring language pairs, we observed an improvement in the translations in terms of automatic scores and qualitative evaluations, compared to previous years.","authors":["Rachel Bawden","Giorgio Maria Di Nunzio","Cristian Grozea","Inigo Jauregi Unanue","Antonio Jimeno Yepes","Nancy Mah","David Martinez","Aur\u00e9lie N\u00e9v\u00e9ol","Mariana Neves","Maite Oronoz","Olatz Perez-de-Vi\u00f1aspre","Massimo Piccardi","Roland Roller","Amy Siu","Philippe Thomas","Federica Vezzani","Maika Vicente Navarro","Dina Wiemann","Lana Yeganova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.76.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages","tldr":"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight lan...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.192","presentation_id":"38939675","rocketchat_channel":"paper-wmt-192","speakers":"Rachel Bawden|Giorgio Maria Di Nunzio|Cristian Grozea|Inigo Jauregi Unanue|Antonio Jimeno Yepes|Nancy Mah|David Martinez|Aur\u00e9lie N\u00e9v\u00e9ol|Mariana Neves|Maite Oronoz|Olatz Perez-de-Vi\u00f1aspre|Massimo Piccardi|Roland Roller|Amy Siu|Philippe Thomas|Federica Vezzani|Maika Vicente Navarro|Dina Wiemann|Lana Yeganova","title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages"},{"content":{"abstract":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs \u2013 English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for \u201dcatastrophic errors\u201d. We received 59 submissions by 11 participating teams from a variety of types of institutions.","authors":["Lucia Specia","Zhenhao Li","Juan Pino","Vishrav Chaudhary","Francisco Guzm\u00e1n","Graham Neubig","Nadir Durrani","Yonatan Belinkov","Philipp Koehn","Hassan Sajjad","Paul Michel","Xian Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.4.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness","tldr":"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.193","presentation_id":"38939676","rocketchat_channel":"paper-wmt-193","speakers":"Lucia Specia|Zhenhao Li|Juan Pino|Vishrav Chaudhary|Francisco Guzm\u00e1n|Graham Neubig|Nadir Durrani|Yonatan Belinkov|Philipp Koehn|Hassan Sajjad|Paul Michel|Xian Li","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"content":{"abstract":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with open domain texts, direct assessment annotations, and multiple language pairs: English-German, English-Chinese, Russian-English, Romanian-English, Estonian-English, Sinhala-English and Nepali-English data for the sentence-level subtasks, English-German and English-Chinese for the word-level subtask, and English-French data for the document-level subtask. In addition, we made neural machine translation models available to participants. 19 participating teams from 27 institutions submitted altogether 1374 systems to different task variants and language pairs.","authors":["Lucia Specia","Fr\u00e9d\u00e9ric Blain","Marina Fomicheva","Erick Fonseca","Vishrav Chaudhary","Francisco Guzm\u00e1n","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.79.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Quality Estimation","tldr":"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with ope...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.194","presentation_id":"38939677","rocketchat_channel":"paper-wmt-194","speakers":"Lucia Specia|Fr\u00e9d\u00e9ric Blain|Marina Fomicheva|Erick Fonseca|Vishrav Chaudhary|Francisco Guzm\u00e1n|Andr\u00e9 F. T. Martins","title":"Findings of the WMT 2020 Shared Task on Quality Estimation"},{"content":{"abstract":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto\u2013English and Khmer\u2013English and also included the challenge of sentence alignment from document pairs.","authors":["Philipp Koehn","Vishrav Chaudhary","Ahmed El-Kishky","Naman Goyal","Peng-Jen Chen","Francisco Guzm\u00e1n"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.78.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment","tldr":"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of s...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.195","presentation_id":"38939678","rocketchat_channel":"paper-wmt-195","speakers":"Philipp Koehn|Vishrav Chaudhary|Ahmed El-Kishky|Naman Goyal|Peng-Jen Chen|Francisco Guzm\u00e1n","title":"Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment"},{"content":{"abstract":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Marta R. Costa-juss\u00e0","Fethi Bougares","Olivier Galibert"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.2.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the First Shared Task on Lifelong Learning Machine Translation","tldr":"A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test dat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.196","presentation_id":"38939679","rocketchat_channel":"paper-wmt-196","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Marta R. Costa-juss\u00e0|Fethi Bougares|Olivier Galibert","title":"Findings of the First Shared Task on Lifelong Learning Machine Translation"},{"content":{"abstract":"This is the main findings paper","authors":["Lo\u00efc Barrault","Magdalena Biesialska","Ond\u0159ej Bojar","Marta R. Costa-juss\u00e0","Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Matthias Huck","Eric Joanis","Tom Kocmi","Philipp Koehn","Chi-kiu Lo","Nikola Ljube\u0161i\u0107","Christof Monz","Makoto Morishita","Masaaki Nagata","Toshiaki Nakazawa","Santanu Pal","Matt Post","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.1.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20)","tldr":"This is the main findings paper...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.197","presentation_id":"38939680","rocketchat_channel":"paper-wmt-197","speakers":"Lo\u00efc Barrault|Magdalena Biesialska|Ond\u0159ej Bojar|Marta R. Costa-juss\u00e0|Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Matthias Huck|Eric Joanis|Tom Kocmi|Philipp Koehn|Chi-kiu Lo|Nikola Ljube\u0161i\u0107|Christof Monz|Makoto Morishita|Masaaki Nagata|Toshiaki Nakazawa|Santanu Pal|Matt Post|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"content":{"abstract":"","authors":["Christian Federmann","Yvette Graham","Roman Grundkiewicz","Barry Haddow","Tom Kocmi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1971","presentation_id":"38940635","rocketchat_channel":"paper-wmt-1971","speakers":"Christian Federmann|Yvette Graham|Roman Grundkiewicz|Barry Haddow|Tom Kocmi","title":"Findings of the 2020 Conference on Machine Translation (WMT20): News Translation Task"},{"content":{"abstract":"","authors":["Magdalena Biesialska","Marta R. Costa-juss\u00e0","Nikola Ljube\u0161i\u0107","Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1972","presentation_id":"38940636","rocketchat_channel":"paper-wmt-1972","speakers":"Magdalena Biesialska|Marta R. Costa-juss\u00e0|Nikola Ljube\u0161i\u0107|Santanu Pal|Marcos Zampieri","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Similar Language Translation Task"},{"content":{"abstract":"","authors":["Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites","tldr":null,"track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.1973","presentation_id":"38940637","rocketchat_channel":"paper-wmt-1973","speakers":"Ond\u0159ej Bojar","title":"Findings of the 2020 Conference on Machine Translation (WMT20): Test Suites"},{"content":{"abstract":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 \"zero-shot\" language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.","authors":["Thibault Sellam","Amy Pu","Hyung Won Chung","Sebastian Gehrmann","Qijun Tan","Markus Freitag","Dipanjan Das","Ankur Parikh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.102.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task","tldr":"The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.198","presentation_id":"38939681","rocketchat_channel":"paper-wmt-198","speakers":"Thibault Sellam|Amy Pu|Hyung Won Chung|Sebastian Gehrmann|Qijun Tan|Markus Freitag|Dipanjan Das|Ankur Parikh","title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task"},{"content":{"abstract":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining mod- ule adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions.","authors":["Runxin Xu","Zhuo Zhi","Jun Cao","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.112.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Volctrans Parallel Corpus Filtering System for WMT 2020","tldr":"In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.2","presentation_id":"38939544","rocketchat_channel":"paper-wmt-2","speakers":"Runxin Xu|Zhuo Zhi|Jun Cao|Mingxuan Wang|Lei Li","title":"Volctrans Parallel Corpus Filtering System for WMT 2020"},{"content":{"abstract":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.","authors":["Pere Verg\u00e9s Boncompte","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.54.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages","tldr":"In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.20","presentation_id":"38939555","rocketchat_channel":"paper-wmt-20","speakers":"Pere Verg\u00e9s Boncompte|Marta R. Costa-juss\u00e0","title":"Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages"},{"content":{"abstract":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to-German and English-to-Chinese. Our approach is based on multi-task fine-tuned cross-lingual language models (XLM), initially pre-trained and further domain-adapted through intermediate training using the translation language model (TLM) approach complemented with a novel self-supervised learning task which aim is to model errors inherent to machine translation outputs. Results obtained on both word and sentence-level QE show that the proposed intermediate training method is complementary to language model domain adaptation and outperforms the fine-tuning only approach.","authors":["Raphael Rubino"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.121.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation","tldr":"This paper describes the NICT Kyoto submission for the WMT\u201920 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.21","presentation_id":"38939556","rocketchat_channel":"paper-wmt-21","speakers":"Raphael Rubino","title":"NICT Kyoto Submission for the WMT\u201920 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation"},{"content":{"abstract":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.","authors":["Sheila Castilho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.137.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation","tldr":"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of \u201chuman parity\u201d (Toral et al., 2018; L\u00e4ubli et al., 2018) with document-level human evaluations have been published. Yet,...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.22","presentation_id":"38939557","rocketchat_channel":"paper-wmt-22","speakers":"Sheila Castilho","title":"On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation"},{"content":{"abstract":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English \u2194 Czech, English \u2194 Russian, French \u2192 German and Tamil \u2192 English), third in 2 directions (English \u2192 German, English \u2192 Japanese), and fourth in 2 directions (English \u2192 Pashto and and English \u2192 Tamil).","authors":["Tingxun Shi","Shiyu Zhao","Xiaopu Li","Xiaoxue Wang","Qian Zhang","Di Ai","Dawei Dang","Xue Zhengshan","JIE HAO"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.30.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OPPO's Machine Translation Systems for WMT20","tldr":"In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.23","presentation_id":"38939558","rocketchat_channel":"paper-wmt-23","speakers":"Tingxun Shi|Shiyu Zhao|Xiaopu Li|Xiaoxue Wang|Qian Zhang|Di Ai|Dawei Dang|Xue Zhengshan|JIE HAO","title":"OPPO's Machine Translation Systems for WMT20"},{"content":{"abstract":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.","authors":["Aizhan Imankulova","Masahiro Kaneko","Tosho Hirasawa","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.70.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Multimodal Simultaneous Neural Machine Translation","tldr":"Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence transla...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.26","presentation_id":"38939559","rocketchat_channel":"paper-wmt-26","speakers":"Aizhan Imankulova|Masahiro Kaneko|Tosho Hirasawa|Mamoru Komachi","title":"Towards Multimodal Simultaneous Neural Machine Translation"},{"content":{"abstract":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of techniques such as back-translation and fine-tuning, which are already widely adopted in translation tasks. We attempted to develop new methods for both synthetic data filtering and reranking. However, the methods turned out to be ineffective, and they provided us with no significant improvement over the baseline. We analyze these negative results to provide insights for future studies.","authors":["Shun Kiyono","Takumi Ito","Ryuto Konno","Makoto Morishita","Jun Suzuki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.12.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task","tldr":"In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT'20 news translation task. We participated in this task in two language pairs and four language directions: English <\u2013> German and English <\u2013> Japanese. Our system consists of tech...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.3","presentation_id":"38939545","rocketchat_channel":"paper-wmt-3","speakers":"Shun Kiyono|Takumi Ito|Ryuto Konno|Makoto Morishita|Jun Suzuki","title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task"},{"content":{"abstract":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.","authors":["Mat\u012bss Rikters","Ryokan Ri","Tong Li","Toshiaki Nakazawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.74.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document-aligned Japanese-English Conversation Parallel Corpus","tldr":"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.31","presentation_id":"38939560","rocketchat_channel":"paper-wmt-31","speakers":"Mat\u012bss Rikters|Ryokan Ri|Tong Li|Toshiaki Nakazawa","title":"Document-aligned Japanese-English Conversation Parallel Corpus"},{"content":{"abstract":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of -3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of -5.29 and a BLEU score of +7.32 for the En-Zh subtask.","authors":["Jihyung Lee","WonKee Lee","Jaehun Shin","Baikjin Jung","Young-Kil Kim","Jong-Hyeok Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.82.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model","tldr":"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, wh...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.32","presentation_id":"38939561","rocketchat_channel":"paper-wmt-32","speakers":"Jihyung Lee|WonKee Lee|Jaehun Shin|Baikjin Jung|Young-Kil Kim|Jong-Hyeok Lee","title":"POSTECH-ETRI\u2019s Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model"},{"content":{"abstract":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two training techniques, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.","authors":["Inigo Jauregi Unanue","Massimo Piccardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.89.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation","tldr":"This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.35","presentation_id":"38939562","rocketchat_channel":"paper-wmt-35","speakers":"Inigo Jauregi Unanue|Massimo Piccardi","title":"Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation"},{"content":{"abstract":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.","authors":["Vil\u00e9m Zouhar","Tereza Vojt\u011bchov\u00e1","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.41.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WMT20 Document-Level Markable Error Exploration","tldr":"Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused o...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.36","presentation_id":"38939563","rocketchat_channel":"paper-wmt-36","speakers":"Vil\u00e9m Zouhar|Tereza Vojt\u011bchov\u00e1|Ond\u0159ej Bojar","title":"WMT20 Document-Level Markable Error Exploration"},{"content":{"abstract":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.","authors":["Jind\u0159ich Libovick\u00fd","Viktor Hangya","Helmut Schmid","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.131.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task","tldr":"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data wit...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.37","presentation_id":"38939564","rocketchat_channel":"paper-wmt-37","speakers":"Jind\u0159ich Libovick\u00fd|Viktor Hangya|Helmut Schmid|Alexander Fraser","title":"The LMU Munich System for the WMT20 Very Low Resource Supervised MT Task"},{"content":{"abstract":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.","authors":["Jin Xu","Yinuo Guo","Junfeng Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.104.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA","tldr":"Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that ther...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.39","presentation_id":"38939565","rocketchat_channel":"paper-wmt-39","speakers":"Jin Xu|Yinuo Guo|Junfeng Hu","title":"Incorporate Semantic Structures into Machine Translation Evaluation via UCCA"},{"content":{"abstract":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuktitut and Inuktitut to English. For each, we trained a single-direction model. However, directions including English, Polish and Czech were derived from a common multilingual base, which was later fine-tuned on each particular direction. For all the translation directions, we used a similar training regime, with iterative training corpora improvement through back-translation and model ensembling. For the En \u2192 Cs direction, we additionally leveraged document-level information by re-ranking the beam output with a separate model.","authors":["Mateusz Krubi\u0144ski","Marcin Chochowski","Bart\u0142omiej Boczek","Miko\u0142aj Koszowski","Adam Dobrowolski","Marcin Szyma\u0144ski","Pawe\u0142 Przybysz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.16.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task","tldr":"This paper describes the submission to the WMT20 shared news translation task by Samsung R&D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuk...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.40","presentation_id":"38939566","rocketchat_channel":"paper-wmt-40","speakers":"Mateusz Krubi\u0144ski|Marcin Chochowski|Bart\u0142omiej Boczek|Miko\u0142aj Koszowski|Adam Dobrowolski|Marcin Szyma\u0144ski|Pawe\u0142 Przybysz","title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task"},{"content":{"abstract":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -> Czech, English -> German, and English -> Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year.","authors":["Yves Scherrer","Alessandro Raganato","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.40.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The MUCOW word sense disambiguation test suite at WMT 2020","tldr":"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.42","presentation_id":"38939567","rocketchat_channel":"paper-wmt-42","speakers":"Yves Scherrer|Alessandro Raganato|J\u00f6rg Tiedemann","title":"The MUCOW word sense disambiguation test suite at WMT 2020"},{"content":{"abstract":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling. The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na \u0308\u0131ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.","authors":["Shruti Bhosale","Kyra Yee","Sergey Edunov","Michael Auli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.69.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling","tldr":"Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy ch...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.43","presentation_id":"38939568","rocketchat_channel":"paper-wmt-43","speakers":"Shruti Bhosale|Kyra Yee|Sergey Edunov|Michael Auli","title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling"},{"content":{"abstract":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Randomised Trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. To train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. In addition we re-score the output of Bicleaner using character-level language models and n-gram saturation.","authors":["Miquel Espl\u00e0-Gomis","V\u00edctor M. S\u00e1nchez-Cartagena","Jaume Zaragoza-Bernabeu","Felipe S\u00e1nchez-Mart\u00ednez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.107.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task","tldr":"This paper describes the joint submission of Universitat d'Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Ra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.44","presentation_id":"38939569","rocketchat_channel":"paper-wmt-44","speakers":"Miquel Espl\u00e0-Gomis|V\u00edctor M. S\u00e1nchez-Cartagena|Jaume Zaragoza-Bernabeu|Felipe S\u00e1nchez-Mart\u00ednez","title":"Bicleaner at WMT 2020: Universitat d'Alacant-Prompsit's submission to the parallel corpus filtering shared task"},{"content":{"abstract":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020 News Translation corpora, and fine-tuned on the APE corpus. Bottleneck Adapter Layers are integrated into the model to prevent over-fitting. We further collect external translations as the augmented MT candidates to improve the performance. The experiment demonstrates that pre-trained NMT models are effective when fine-tuning with the APE corpus of a limited size, and the performance can be further improved with external MT augmentation. Our system achieves competitive results on both directions in the final evaluation.","authors":["Hao Yang","Minghan Wang","Daimeng Wei","Hengchao Shang","Jiaxin Guo","Zongyao Li","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.85.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.45","presentation_id":"38939570","rocketchat_channel":"paper-wmt-45","speakers":"Hao Yang|Minghan Wang|Daimeng Wei|Hengchao Shang|Jiaxin Guo|Zongyao Li|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word/sentence-level sub-tasks for both of En-De/Zh language pairs.","authors":["Minghan Wang","Hao Yang","Hengchao Shang","Daimeng Wei","Jiaxin Guo","Lizhi Lei","Ying Qin","Shimin Tao","Shiliang Sun","Yimeng Chen","Liangyou Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.123.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task","tldr":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-editing Effort Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.46","presentation_id":"38939571","rocketchat_channel":"paper-wmt-46","speakers":"Minghan Wang|Hao Yang|Hengchao Shang|Daimeng Wei|Jiaxin Guo|Lizhi Lei|Ying Qin|Shimin Tao|Shiliang Sun|Yimeng Chen|Liangyou Li","title":"HW-TSC's Participation at WMT 2020 Automatic Post Editing Shared Task"},{"content":{"abstract":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut->English and Tamil->English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.","authors":["Yuhao Zhang","Ziyang Wang","Runzhe Cao","Binghao Wei","Weiqiao Shan","Shuhan Zhou","Abudurexiti Reheman","Tao Zhou","Xin Zeng","Laohu Wang","Yongyu Mu","Jingnan Zhang","Xiaoqian Liu","Xuanjun Zhou","Yinqiao Li","Bei Li","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.37.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans Machine Translation Systems for WMT20","tldr":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese<->English, English->Chinese, Inuktitut->English and Tamil->English total five tasks and rank first in Japanese<->English...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.47","presentation_id":"38939572","rocketchat_channel":"paper-wmt-47","speakers":"Yuhao Zhang|Ziyang Wang|Runzhe Cao|Binghao Wei|Weiqiao Shan|Shuhan Zhou|Abudurexiti Reheman|Tao Zhou|Xin Zeng|Laohu Wang|Yongyu Mu|Jingnan Zhang|Xiaoqian Liu|Xuanjun Zhou|Yinqiao Li|Bei Li|Tong Xiao|Jingbo Zhu","title":"The NiuTrans Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the baseline and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our models such as Back Translation, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.","authors":["Daimeng Wei","Hengchao Shang","Zhanglin Wu","Zhengzhe Yu","Liangyou Li","Jiaxin Guo","Minghan Wang","Hao Yang","Lizhi Lei","Ying Qin","Shiliang Sun"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.31.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task","tldr":"This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the b...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.48","presentation_id":"38939573","rocketchat_channel":"paper-wmt-48","speakers":"Daimeng Wei|Hengchao Shang|Zhanglin Wu|Zhengzhe Yu|Liangyou Li|Jiaxin Guo|Minghan Wang|Hao Yang|Lizhi Lei|Ying Qin|Shiliang Sun","title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task"},{"content":{"abstract":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared to last year, for some language pairs we dedicated a lot more resources to training, and tried to follow standard best practices to build competitive systems which can achieve good results in the rankings. By using deep and complex architectures we sacrificed direct re-usability of our systems in production environments but evaluation showed that this approach could result in better models that significantly outperform baseline architectures. We submitted two systems to the zero shot robustness task. These submissions are described briefly in this paper as well.","authors":["Csaba Oravecz","Katina Bontcheva","L\u00e1szl\u00f3 Tihanyi","David Kolovratnik","Bhavani Bhaskar","Adrien Lardilleux","Szymon Klocek","Andreas Eisele"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.26.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"eTranslation's Submissions to the WMT 2020 News Translation Task","tldr":"The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team\u2019s participation last year we developed systems for 5 language pairs with various strategies. Compared t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.49","presentation_id":"38939574","rocketchat_channel":"paper-wmt-49","speakers":"Csaba Oravecz|Katina Bontcheva|L\u00e1szl\u00f3 Tihanyi|David Kolovratnik|Bhavani Bhaskar|Adrien Lardilleux|Szymon Klocek|Andreas Eisele","title":"eTranslation's Submissions to the WMT 2020 News Translation Task"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.118.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language mod...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.5","presentation_id":"38939546","rocketchat_channel":"paper-wmt-5","speakers":"Dongjun Lee","title":"Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation"},{"content":{"abstract":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model using monolingual data from both the languages by jointly pre-training the encoder and decoder and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).","authors":["Salam Michael Singh","Thoudam Doren Singh","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.135.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020","tldr":"We describe NITS-CNLP's submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.51","presentation_id":"38939575","rocketchat_channel":"paper-wmt-51","speakers":"Salam Michael Singh|Thoudam Doren Singh|Sivaji Bandyopadhyay","title":"The NITS-CNLP System for the Unsupervised MT Task at WMT 2020"},{"content":{"abstract":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English<->French, English->German and English->Italian.","authors":["Wei Peng","Jianfeng Liu","Minghan Wang","Liangyou Li","Xupeng Meng","Hao Yang","Qun Liu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.93.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Huawei's Submissions to the WMT20 Biomedical Translation Task","tldr":"This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine tran...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.52","presentation_id":"38939576","rocketchat_channel":"paper-wmt-52","speakers":"Wei Peng|Jianfeng Liu|Minghan Wang|Liangyou Li|Xupeng Meng|Hao Yang|Qun Liu","title":"Huawei's Submissions to the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire documents (or bilingual dialogues) at once. We use the same ensemble of such models as our primary submission to all three tasks and achieve competitive results. We also experiment with language model pre-training techniques and evaluate their impact on robustness to noise and out-of-domain translation. For German, Spanish, Italian, and French to English translation in the Biomedical Task, we also submit our recently released multilingual Covid19NMT model.","authors":["Alexandre Berard","Ioan Calapodescu","Vassilina Nikoulina","Jerin Philip"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.57.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020","tldr":"This paper describes Naver Labs Europe's participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire docume...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.53","presentation_id":"38939577","rocketchat_channel":"paper-wmt-53","speakers":"Alexandre Berard|Ioan Calapodescu|Vassilina Nikoulina|Jerin Philip","title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020"},{"content":{"abstract":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems.","authors":["Maja Popovi\u0107","Alberto Poncelas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.51.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation between similar South-Slavic languages","tldr":"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian\u2013Slovenian and Serbian\u2013Slovenian language pairs in both translation dir...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.54","presentation_id":"38939578","rocketchat_channel":"paper-wmt-54","speakers":"Maja Popovi\u0107|Alberto Poncelas","title":"Neural Machine Translation between similar South-Slavic languages"},{"content":{"abstract":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.","authors":["Xiangpeng Wei","Ping Guo","Yunpeng Li","Xingsheng Zhang","Luxi Xing","Yue Hu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.32.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIE's Neural Machine Translation Systems for WMT20","tldr":"In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.57","presentation_id":"38939579","rocketchat_channel":"paper-wmt-57","speakers":"Xiangpeng Wei|Ping Guo|Yunpeng Li|Xingsheng Zhang|Luxi Xing|Yue Hu","title":"IIE's Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. a) Dual Bilingual GPT-2 model, b) Dual Conditional Cross-Entropy Model and c) IBM word alignment model. The scores of these models are combined by using a positive-unlabeled (PU) learning model and a brute-force search to obtain additional gains. Besides, a few simple but efficient rules are adopted to evaluate the quality and the diversity of the corpus. In the alignment-filtering task, the extraction pipeline of bilingual sentence pairs includes the following steps: bilingual lexicon mining, language identification, sentence segmentation and sentence alignment. The final result shows that, in both filtering and alignment tasks, our system significantly outperforms the LASER-based system.","authors":["Jun Lu","Xin Ge","Yangbin Shi","Yuqi Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.111.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task","tldr":"This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.58","presentation_id":"38939580","rocketchat_channel":"paper-wmt-58","speakers":"Jun Lu|Xin Ge|Yangbin Shi|Yuqi Zhang","title":"Alibaba Submission to the WMT20 Parallel Corpus Filtering Task"},{"content":{"abstract":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE<cit.>), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.","authors":["Liwei Wu","Xiao Pan","Zehui Lin","Yaoming ZHU","Mingxuan Wang","Lei Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.33.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Volctrans Machine Translation System for WMT20","tldr":"This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer <cit.>, into which we also employed new architectures (bigger or...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.59","presentation_id":"38939581","rocketchat_channel":"paper-wmt-59","speakers":"Liwei Wu|Xiao Pan|Zehui Lin|Yaoming ZHU|Mingxuan Wang|Lei Li","title":"The Volctrans Machine Translation System for WMT20"},{"content":{"abstract":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our system improves the NMT output by -3.95 and +4.50 in terms of TER and BLEU, respectively.","authors":["Dongjun Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.81.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Transformers for Neural Automatic Post-Editing","tldr":"In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (M...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.6","presentation_id":"38939547","rocketchat_channel":"paper-wmt-6","speakers":"Dongjun Lee","title":"Cross-Lingual Transformers for Neural Automatic Post-Editing"},{"content":{"abstract":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian\u2192German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German\u2192Upper Sorbian and 35.2 on Upper Sorbian\u2192German.","authors":["Alexandra Chronopoulou","Dario Stojanovski","Viktor Hangya","Alexander Fraser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.128.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task","tldr":"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German\u2194Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.60","presentation_id":"38939582","rocketchat_channel":"paper-wmt-60","speakers":"Alexandra Chronopoulou|Dario Stojanovski|Viktor Hangya|Alexander Fraser","title":"The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task"},{"content":{"abstract":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.","authors":["Danielle Saunders","Bill Byrne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.94.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task","tldr":"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.62","presentation_id":"38939583","rocketchat_channel":"paper-wmt-62","speakers":"Danielle Saunders|Bill Byrne","title":"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task"},{"content":{"abstract":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit systems for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.","authors":["Sourav Dutta","Jesujoba Alabi","Saptarashmi Bandyopadhyay","Dana Ruiter","Josef van Genabith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.129.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian","tldr":"This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.65","presentation_id":"38939584","rocketchat_channel":"paper-wmt-65","speakers":"Sourav Dutta|Jesujoba Alabi|Saptarashmi Bandyopadhyay|Dana Ruiter|Josef van Genabith","title":"UdS-DFKI@WMT20: Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian"},{"content":{"abstract":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.","authors":["Jingjing Huo","Christian Herold","Yingbo Gao","Leonard Dahlmann","Shahram Khadivi","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.71.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diving Deep into Context-Aware Neural Machine Translation","tldr":"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectur...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.68","presentation_id":"38939585","rocketchat_channel":"paper-wmt-68","speakers":"Jingjing Huo|Christian Herold|Yingbo Gao|Leonard Dahlmann|Shahram Khadivi|Hermann Ney","title":"Diving Deep into Context-Aware Neural Machine Translation"},{"content":{"abstract":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This approach allows the flexible combination of a number of independent component models which are further augmented with back-translation, distillation, fine-tuning with in-domain data, Monte-Carlo Tree Search decoding, and improved uncertainty estimation. In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques. Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set (newstest 2019).","authors":["Lei Yu","Laurent Sartran","Po-Sen Huang","Wojciech Stokowiec","Domenic Donato","Srivatsan Srinivasan","Alek Andreev","Wang Ling","Sona Mokra","Agustin Dal Lago","Yotam Doron","Susannah Young","Phil Blunsom","Chris Dyer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.36.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020","tldr":"This paper describes the DeepMind submission to the Chinese\u2192English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This app...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.69","presentation_id":"38939586","rocketchat_channel":"paper-wmt-69","speakers":"Lei Yu|Laurent Sartran|Po-Sen Huang|Wojciech Stokowiec|Domenic Donato|Srivatsan Srinivasan|Alek Andreev|Wang Ling|Sona Mokra|Agustin Dal Lago|Yotam Doron|Susannah Young|Phil Blunsom|Chris Dyer","title":"The DeepMind Chinese\u2013English Document Translation System at WMT2020"},{"content":{"abstract":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems.","authors":["Santanu Pal","Marcos Zampieri"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.50.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages","tldr":"In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language f...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.70","presentation_id":"38939587","rocketchat_channel":"paper-wmt-70","speakers":"Santanu Pal|Marcos Zampieri","title":"Neural Machine Translation for Similar Languages: The Case of Indo-Aryan Languages"},{"content":{"abstract":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of \u00a02x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x\u201311x across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average)","authors":["Biao Zhang","Ivan Titov","Rico Sennrich"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.62.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fast Interleaved Bidirectional Sequence Generation","tldr":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decodin...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.71","presentation_id":"38939588","rocketchat_channel":"paper-wmt-71","speakers":"Biao Zhang|Ivan Titov|Rico Sennrich","title":"Fast Interleaved Bidirectional Sequence Generation"},{"content":{"abstract":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -> German and was ranked second for German -> Upper Sorbian according to BLEU scores. For English\u2013Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.","authors":["Yves Scherrer","Stig-Arne Gr\u00f6nroos","Sami Virpioja"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.134.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks","tldr":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For bot...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.72","presentation_id":"38939589","rocketchat_channel":"paper-wmt-72","speakers":"Yves Scherrer|Stig-Arne Gr\u00f6nroos|Sami Virpioja","title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks"},{"content":{"abstract":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes to train statistical models. Using optimal tokenization scheme among these we created synthetic source side text with back translation. And prune synthetic text with language model scores. This synthetic data was then used along with training data in various settings to build translation models. We also report configuration of the submitted systems and results produced by them.","authors":["Saumitra Yadav","Manish Shrivastava"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.55.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020","tldr":"In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi\u21d0\u21d2Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes t...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.73","presentation_id":"38939590","rocketchat_channel":"paper-wmt-73","speakers":"Saumitra Yadav|Manish Shrivastava","title":"A3-108 Machine Translation System for Similar Language Translation Shared Task 2020"},{"content":{"abstract":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.","authors":["Ander Corral","Xabier Saralegi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.87.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation","tldr":"This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and E...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.74","presentation_id":"38939591","rocketchat_channel":"paper-wmt-74","speakers":"Ander Corral|Xabier Saralegi","title":"Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation"},{"content":{"abstract":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.","authors":["Farhan Dhanani","Muhammad Rafi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.43.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attention Transformer Model for Translation of Similar Languages","tldr":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.75","presentation_id":"38939592","rocketchat_channel":"paper-wmt-75","speakers":"Farhan Dhanani|Muhammad Rafi","title":"Attention Transformer Model for Translation of Similar Languages"},{"content":{"abstract":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.","authors":["Markus Freitag","George Foster","David Grangier","Colin Cherry"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.140.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Human-Paraphrased References Improve Neural Machine Translation","tldr":"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores th...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.76","presentation_id":"38939593","rocketchat_channel":"paper-wmt-76","speakers":"Markus Freitag|George Foster|David Grangier|Colin Cherry","title":"Human-Paraphrased References Improve Neural Machine Translation"},{"content":{"abstract":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive transfer from high resource languages. We use iterative backtranslation to fine-tune the system and benefit from the monolingual data available. In order to measure the effectivity of such methods, we compare our results to a bilingual baseline system.","authors":["Carlos Escolano","Marta R. Costa-juss\u00e0","Jos\u00e9 A. R. Fonollosa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.10.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT","tldr":"In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive tra...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.77","presentation_id":"38939594","rocketchat_channel":"paper-wmt-77","speakers":"Carlos Escolano|Marta R. Costa-juss\u00e0|Jos\u00e9 A. R. Fonollosa","title":"The TALP-UPC System Description for WMT20 News Translation Task: Multilingual Adaptation for Low Resource MT"},{"content":{"abstract":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system- and segment-level are analyzed in detail and their shortcomings are pointed out.","authors":["Peter Stanchev","Weiyue Wang","Hermann Ney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.103.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Better Evaluation of Metrics for Machine Translation","tldr":"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlati...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.8","presentation_id":"38939548","rocketchat_channel":"paper-wmt-8","speakers":"Peter Stanchev|Weiyue Wang|Hermann Ney","title":"Towards a Better Evaluation of Metrics for Machine Translation"},{"content":{"abstract":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for domain Adaptation.","authors":["Luis A. Men\u00e9ndez-Salazar","Grigori Sidorov","Marta R. Costa-Juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.47.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The IPN-CIC team system submission for the WMT 2020 similar language task","tldr":"This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.80","presentation_id":"38939595","rocketchat_channel":"paper-wmt-80","speakers":"Luis A. Men\u00e9ndez-Salazar|Grigori Sidorov|Marta R. Costa-Juss\u00e0","title":"The IPN-CIC team system submission for the WMT 2020 similar language task"},{"content":{"abstract":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese\u2192English system achieves 36.9 case-sensitive BLEU score, which is thehighest among all submissions.","authors":["Fandong Meng","Jianhao Yan","Yijin Liu","Yuan Gao","Xianfeng Zeng","Qinsong Zeng","Peng Li","Ming Chen","Jie Zhou","Sifan Liu","Hao Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.24.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WeChat Neural Machine Translation Systems for WMT20","tldr":"We participate in the WMT 2020 shared newstranslation task on Chinese\u2192English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.81","presentation_id":"38939596","rocketchat_channel":"paper-wmt-81","speakers":"Fandong Meng|Jianhao Yan|Yijin Liu|Yuan Gao|Xianfeng Zeng|Qinsong Zeng|Peng Li|Ming Chen|Jie Zhou|Sifan Liu|Hao Zhou","title":"WeChat Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for \u201cattaching\u201d dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.","authors":["Xing Jie Zhong","David Chiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.65.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation","tldr":"Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.82","presentation_id":"38939597","rocketchat_channel":"paper-wmt-82","speakers":"Xing Jie Zhong|David Chiang","title":"Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation"},{"content":{"abstract":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system\u2019s performance.","authors":["Tucker Berckmann","Berkan Hiziroglu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.127.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Low-Resource Translation as Language Modeling","tldr":"We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.83","presentation_id":"38939598","rocketchat_channel":"paper-wmt-83","speakers":"Tucker Berckmann|Berkan Hiziroglu","title":"Low-Resource Translation as Language Modeling"},{"content":{"abstract":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compare to the baseline model.","authors":["Jiwan Kim","Soyoon Park","Sangha Kim","Yoonjung Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.11.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task","tldr":"This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model wi...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.84","presentation_id":"38939599","rocketchat_channel":"paper-wmt-84","speakers":"Jiwan Kim|Soyoon Park|Sangha Kim|Yoonjung Choi","title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task"},{"content":{"abstract":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from monolingual data, and combining many NMT systems through n-best list reranking improve translation quality. However, while we observed such improvements on the validation data, we did not observed similar improvements on the test data. Our analysis reveals that the presence of translationese texts in the validation data led us to take decisions in building NMT systems that were not optimal to obtain the best results on the test data.","authors":["Benjamin Marie","Raphael Rubino","Atsushi Fujita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.23.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combination of Neural Machine Translation Systems at WMT20","tldr":"This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese->English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from mono...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.86","presentation_id":"38939600","rocketchat_channel":"paper-wmt-86","speakers":"Benjamin Marie|Raphael Rubino|Atsushi Fujita","title":"Combination of Neural Machine Translation Systems at WMT20"},{"content":{"abstract":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task.","authors":["Chi Hu","Hui Liu","Kai Feng","Chen Xu","Nuo Xu","Zefan Zhou","Shiqin Yan","Yingfeng Luo","Chenglong Wang","Xia Meng","Tong Xiao","Jingbo Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.117.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task","tldr":"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. R...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.87","presentation_id":"38939601","rocketchat_channel":"paper-wmt-87","speakers":"Chi Hu|Hui Liu|Kai Feng|Chen Xu|Nuo Xu|Zefan Zhou|Shiqin Yan|Yingfeng Luo|Chenglong Wang|Xia Meng|Tong Xiao|Jingbo Zhu","title":"The NiuTrans System for the WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.","authors":["Akifumi Nakamachi","Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.120.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TMUOU Submission for WMT20 Quality Estimation Shared Task","tldr":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.88","presentation_id":"38939602","rocketchat_channel":"paper-wmt-88","speakers":"Akifumi Nakamachi|Hiroki Shimanaka|Tomoyuki Kajiwara|Mamoru Komachi","title":"TMUOU Submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU scores in the directions of English to Pashto, Pashto to English and Khmer to English (13.1, 23.1 and 25.5 respectively) among all the participants. Our submitted systems are unconstrained and focus on mBART (Multilingual Bidirectional and Auto-Regressive Transformers), back-translation and forward-translation. Also, we apply rules, language model and RoBERTa model to filter monolingual, parallel sentences and synthetic sentences. Besides, we validate the difference of the vocabulary built from monolingual data and parallel data.","authors":["Chao Bei","Hao Zong","Qingmin Liu","Conghu Yuan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.6.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GTCOM Neural Machine Translation Systems for WMT20","tldr":"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU sco...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.89","presentation_id":"38939603","rocketchat_channel":"paper-wmt-89","speakers":"Chao Bei|Hao Zong|Qingmin Liu|Conghu Yuan","title":"GTCOM Neural Machine Translation Systems for WMT20"},{"content":{"abstract":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems.","authors":["Xabier Soto","Olatz Perez-de-Vi\u00f1aspre","Gorka Labaka","Maite Oronoz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.96.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation","tldr":"In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora rece...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.9","presentation_id":"38939549","rocketchat_channel":"paper-wmt-9","speakers":"Xabier Soto|Olatz Perez-de-Vi\u00f1aspre|Gorka Labaka|Maite Oronoz","title":"Ixamed's submission description for WMT20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation"},{"content":{"abstract":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art.","authors":["Ricardo Rei","Craig Stewart","Ana C Farinha","Alon Lavie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.101.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task","tldr":"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the \u201cQE as a Metric\u201d track. Accordingly, we...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.90","presentation_id":"38939604","rocketchat_channel":"paper-wmt-90","speakers":"Ricardo Rei|Craig Stewart|Ana C Farinha|Alon Lavie","title":"Unbabel\u2019s Participation in the WMT20 Metrics Shared Task"},{"content":{"abstract":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translation directions for this language pair. The trained model is evaluated on the corpus provided by shared task organizers, using BLEU, RIBES, and TER scores. There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively.","authors":["Amit Kumar","Rupjyoti Baruah","Rajesh Kumar Mundotiya","Anil Kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.44.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task","tldr":"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi \u2013 Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translat...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.91","presentation_id":"38939605","rocketchat_channel":"paper-wmt-91","speakers":"Amit Kumar|Rupjyoti Baruah|Rajesh Kumar Mundotiya|Anil Kumar Singh","title":"Transformer-based Neural Machine Translation System for Hindi \u2013 Marathi: WMT20 Shared Task"},{"content":{"abstract":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing neural machine translation system to perform the same task. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years\u2019 state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.","authors":["Haluk A\u00e7ar\u00e7i\u00e7ek","Talha \u00c7olako\u011flu","p\u0131nar ece aktan hatipo\u011flu","Chong Hsuan Huang","Wei Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.105.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning","tldr":"This paper illustrates Huawei\u2019s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the fil...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.92","presentation_id":"38939606","rocketchat_channel":"paper-wmt-92","speakers":"Haluk A\u00e7ar\u00e7i\u00e7ek|Talha \u00c7olako\u011flu|p\u0131nar ece aktan hatipo\u011flu|Chong Hsuan Huang|Wei Peng","title":"Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning"},{"content":{"abstract":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.","authors":["Tharindu Ranasinghe","Constantin Orasan","Ruslan Mitkov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.122.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TransQuest at WMT2020: Sentence-Level Direct Assessment","tldr":"This paper presents the team TransQuest\u2019s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural ...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.93","presentation_id":"38939607","rocketchat_channel":"paper-wmt-93","speakers":"Tharindu Ranasinghe|Constantin Orasan|Ruslan Mitkov","title":"TransQuest at WMT2020: Sentence-Level Direct Assessment"},{"content":{"abstract":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.","authors":["Sami Ul Haq","Sadaf Abdul Rauf","Arsalan Shaukat","Abdullah Saeed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.53.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document Level NMT of Low-Resource Languages with Backtranslation","tldr":"This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi\u2212Hindi. Our system is an ext...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.94","presentation_id":"38939608","rocketchat_channel":"paper-wmt-94","speakers":"Sami Ul Haq|Sadaf Abdul Rauf|Arsalan Shaukat|Abdullah Saeed","title":"Document Level NMT of Low-Resource Languages with Backtranslation"},{"content":{"abstract":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications: using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a Pearson correlation of 0.664, ranking first (tied) on English-Chinese.","authors":["Haijiang Wu","Zixuan Wang","Qingsong Ma","Xinjie Wen","Ruichen Wang","Xiaoli Wang","Yulin Zhang","Zhipeng Yao","Siyao Peng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.124.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tencent submission for WMT20 Quality Estimation Shared Task","tldr":"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator m...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.96","presentation_id":"38939609","rocketchat_channel":"paper-wmt-96","speakers":"Haijiang Wu|Zixuan Wang|Qingsong Ma|Xinjie Wen|Ruichen Wang|Xiaoli Wang|Yulin Zhang|Zhipeng Yao|Siyao Peng","title":"Tencent submission for WMT20 Quality Estimation Shared Task"},{"content":{"abstract":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).","authors":["Yujin Baek","Zae Myung Kim","Jihyung Moon","Hyunjoong Kim","Eunjeong Park"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.113.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PATQUEST: Papago Translation Quality Estimation","tldr":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former foc...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.97","presentation_id":"38939610","rocketchat_channel":"paper-wmt-97","speakers":"Yujin Baek|Zae Myung Kim|Jihyung Moon|Hyunjoong Kim|Eunjeong Park","title":"PATQUEST: Papago Translation Quality Estimation"},{"content":{"abstract":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We have participated in WMT20 shared task of similar language translation on Hindi-Marathi pair. The main challenge of this task is by utilization of monolingual data and similarity features of similar language pair to overcome the limitation of available parallel data. In this work, we have implemented NMT based model that simultaneously learns bilingual embedding from both the source and target language pairs. Our model has achieved Hindi to Marathi bilingual evaluation understudy (BLEU) score of 11.59, rank-based intuitive bilingual evaluation score (RIBES) score of 57.76 and translation edit rate (TER) score of 79.07 and Marathi to Hindi BLEU score of 15.44, RIBES score of 61.13 and TER score of 75.96.","authors":["Sahinur Rahman Laskar","Abdullah Faiz Ur Rahman Khilji","Partha Pakray","Sivaji Bandyopadhyay"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.45.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hindi-Marathi Cross Lingual Model","tldr":"Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We hav...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.99","presentation_id":"38939611","rocketchat_channel":"paper-wmt-99","speakers":"Sahinur Rahman Laskar|Abdullah Faiz Ur Rahman Khilji|Partha Pakray|Sivaji Bandyopadhyay","title":"Hindi-Marathi Cross Lingual Model"},{"content":{"abstract":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.","authors":["Fran\u00e7ois Hernandez","Vincent Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.21.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Ubiqus English-Inuktitut System for WMT20","tldr":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. T...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.21","presentation_id":"","rocketchat_channel":"paper-wmt-21","speakers":"Fran\u00e7ois Hernandez|Vincent Nguyen","title":"The Ubiqus English-Inuktitut System for WMT20"},{"content":{"abstract":"This is a placeholder for the metrics task paper as the rsults are not available yet","authors":["Nitika Mathur","Johnny Wei","Qingsong Ma","Ond\u0159ej Bojar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.statmt.org/wmt20/pdf/2020.wmt-1.77.pdf","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Results of the WMT20 Metrics Shared Task","tldr":"This is a placeholder for the metrics task paper as the rsults are not available yet...","track":"Fifth Conference on Machine Translation (WMT20)"},"id":"WS-2.WS-2.2020.wmt-1.77","presentation_id":"","rocketchat_channel":"paper-wmt-77","speakers":"Nitika Mathur|Johnny Wei|Qingsong Ma|Ond\u0159ej Bojar","title":"Results of the WMT20 Metrics Shared Task"}],"prerecorded_talks":[{"presentation_id":"38940745","speakers":"Masakhane","title":"\"Low-resourcedness\" Beyond Data"}],"rocketchat_channel":"workshop-wmt","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 10:00:00 GMT","hosts":"zoom","link":"","session_name":"Welcome Speech","start_time":"Thu, 19 Nov 2020 09:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 11:00:00 GMT","hosts":"zoom","link":"","session_name":"Shared Tasks Overview 1a","start_time":"Thu, 19 Nov 2020 10:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 12:30:00 GMT","hosts":"GT","link":"","session_name":"Poster Session 1a","start_time":"Thu, 19 Nov 2020 11:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 13:00:00 GMT","hosts":"n/a","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 12:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:00:00 GMT","hosts":"zoom","link":"","session_name":"Full Research Paper Session 1","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:00:00 GMT","hosts":"n/a","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 14:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:00:00 GMT","hosts":"zoom","link":"","session_name":"Shared Tasks Overview 1b","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:30:00 GMT","hosts":"GT","link":"","session_name":"Poster Session 1b","start_time":"Thu, 19 Nov 2020 17:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"n/a","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 18:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:00:00 GMT","hosts":"zoom","link":"","session_name":"Full Research Paper Session 2","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:00:00 GMT","hosts":"zoom","link":"","session_name":"Full Research Paper Session 3","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:00:00 GMT","hosts":"zoom","link":"","session_name":"Shared Tasks Overview 2a","start_time":"Fri, 20 Nov 2020 10:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 12:30:00 GMT","hosts":"GT","link":"","session_name":"Poster Session 2a","start_time":"Fri, 20 Nov 2020 11:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:00:00 GMT","hosts":"n/a","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 12:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:00:00 GMT","hosts":"livestream over zoom??","link":"","session_name":"Invited Talk","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:30:00 GMT","hosts":"zoom","link":"","session_name":"Panel Discussion","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"zoom","link":"","session_name":"Shared Tasks Overview 2b","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:30:00 GMT","hosts":"GT","link":"","session_name":"Poster Session 2b","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"n/a","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 18:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":"zoom","link":"","session_name":"Full Research Paper Session 4","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:15:00 GMT","hosts":"zoom","link":"","session_name":"Closing","start_time":"Fri, 20 Nov 2020 20:00:00 GMT"}],"title":"Fifth Conference on Machine Translation (WMT20)","website":"http:/www.statmt.org/wmt20","zoom_links":["https://zoom.us"]},{"abstract":"Let's take a break from chasing leaderboards! Your negative results might save others time, or poke holes in things we take for granted.","blocks":[{"end_time":"Fri, 20 Nov 2020 00:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"}],"id":"WS-3","livestream":null,"organizers":"Anna Rogers, Joao Sedoc and Anna Rumshisky","papers":[{"content":{"abstract":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good performance on a dataset might not indicate performance that correlates well with human\u2019s expectations from models that \u201cunderstand\u201d language. In this work we consider a top performing model on several Multiple Choice Question Answering (MCQA) datasets, and evaluate it against a set of expectations one might have from such a model, using a series of zero-information perturbations of the model\u2019s inputs. Our results show that the model clearly falls short of our expectations, and motivates a modified training approach that forces the model to better attend to the inputs. We show that the new training paradigm leads to a model that performs on par with the original model while better satisfying our expectations.","authors":["Krunal Shah","Nitish Gupta","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.317","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What do we expect from Multiple-choice QA Systems?","tldr":"The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models\u2019 language understanding abilities. However, using various perturbations, multiple recent works have shown that good perf...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2575","presentation_id":"38940132","rocketchat_channel":"paper-insights-2575","speakers":"Krunal Shah|Nitish Gupta|Dan Roth","title":"What do we expect from Multiple-choice QA Systems?"},{"content":{"abstract":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several research challenges which are not tackled well for domain specific corpus found in industries. In this paper, we have highlighted these problems through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model. Our experiments have lead to interesting findings that showed: 1) Largest substring from the left that is found in the vocabulary (in-vocab) is always chosen at every sub-word unit that can lead to suboptimal tokenization choices, 2) Semantic meaning of a vocabulary word deteriorates when found as a substring in an Out-Of-Vocabulary (OOV) word, and 3) Minor misspellings in words are inadequately handled. We believe that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.","authors":["Anmol Nayak","Hariprasad Timmapathini","Karthikeyan Ponnalagu","Vijendran Gopalan Venkoparao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words","tldr":"BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several rese...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.1","presentation_id":"38940788","rocketchat_channel":"paper-insights-1","speakers":"Anmol Nayak|Hariprasad Timmapathini|Karthikeyan Ponnalagu|Vijendran Gopalan Venkoparao","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"content":{"abstract":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.","authors":["Prasanna Parthasarathi","Sharan Narang","Arvind Neelakantan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Task-Level Dialogue Composition of Generative Transformer Model","tldr":"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.12","presentation_id":"38940793","rocketchat_channel":"paper-insights-12","speakers":"Prasanna Parthasarathi|Sharan Narang|Arvind Neelakantan","title":"On Task-Level Dialogue Composition of Generative Transformer Model"},{"content":{"abstract":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to determine the veracity of news. Our experiments find that the success of these detectors can be limited since they are rarely sensitive to semantic perturbations and are very sensitive to syntactic perturbations. Also, we would like to open-source our code and believe it could be a useful diagnostic tool for evaluating models aimed at fighting machine-generated fake news.","authors":["Meghana Moorthy Bhat","Srinivasan Parthasarathy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study","tldr":"We empirically study the effectiveness of machine-generated fake news detectors by understanding the model\u2019s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to de...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.19","presentation_id":"38940794","rocketchat_channel":"paper-insights-19","speakers":"Meghana Moorthy Bhat|Srinivasan Parthasarathy","title":"How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study"},{"content":{"abstract":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.","authors":["Ashwin Geet D\u2019Sa","Irina Illina","Dominique Fohr","Dietrich Klakow","Dana Ruiter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification","tldr":"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of lab...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.20","presentation_id":"38940795","rocketchat_channel":"paper-insights-20","speakers":"Ashwin Geet D\u2019Sa|Irina Illina|Dominique Fohr|Dietrich Klakow|Dana Ruiter","title":"Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification"},{"content":{"abstract":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p <0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.","authors":["Catherine Finegan-Dollak","Ashish Verma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layout-Aware Text Representations Harm Clustering Documents by Type","tldr":"Clustering documents by type\u2014grouping invoices with invoices and articles with articles\u2014is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document lay...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.22","presentation_id":"38940796","rocketchat_channel":"paper-insights-22","speakers":"Catherine Finegan-Dollak|Ashish Verma","title":"Layout-Aware Text Representations Harm Clustering Documents by Type"},{"content":{"abstract":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Networks (CapsNets) are a relatively new architecture in NLP. Due to their novelty, CapsNets are being used more and more in NLP tasks. However, their usefulness is still mostly untested.In this paper, we compare three neural network architectures\u2014LSTM, CNN, and CapsNet\u2014on a part of speech tagging task. We compare these architectures in both high- and low-resource training conditions and find that no architecture consistently performs the best. Our analysis shows that our CapsNet performs nearly as well as a more complex LSTM under certain training conditions, but not others, and that our CapsNet almost always outperforms our CNN. We also find that our CapsNet implementation shows faster prediction times than the LSTM for Scottish Gaelic but not for Spanish, highlighting the effect that the choice of languages can have on the models.","authors":["Andrew Zupon","Faiz Rafique","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios","tldr":"Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Network...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.23","presentation_id":"38940797","rocketchat_channel":"paper-insights-23","speakers":"Andrew Zupon|Faiz Rafique|Mihai Surdeanu","title":"An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios"},{"content":{"abstract":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve around claims made by people about a given topic of interest. Fact-checking portals offer partially structured information that can assist such analysis. However, exploiting the network structure of such online discourse data is as of yet under-explored. We study the effectiveness of using neural-graph embedding features for claim topic prediction and their complementarity with text embeddings. We show that graph embeddings are modestly complementary with text embeddings, but the low performance of graph embedding features alone indicate that the model fails to capture topological features pertinent of the topic prediction task.","authors":["Valentina Beretta","S\u00e9bastien Harispe","Katarina Boland","Luke Lo Seen","Konstantin Todorov","Andon Tchechmedjiev"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?","tldr":"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve aroun...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.24","presentation_id":"38940798","rocketchat_channel":"paper-insights-24","speakers":"Valentina Beretta|S\u00e9bastien Harispe|Katarina Boland|Luke Lo Seen|Konstantin Todorov|Andon Tchechmedjiev","title":"Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?"},{"content":{"abstract":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB\u201905 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.","authors":["Piotr Szyma\u0144ski","Piotr \u017belasko","Mikolaj Morzy","Adrian Szymczak","Marzena \u017by\u0142a-Hoppe","Joanna Banaszczak","Lukasz Augustyniak","Jan Mizgajski","Yishay Carmiel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.295","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WER we are and WER we think we are","tldr":"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Re...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.2436","presentation_id":"38940634","rocketchat_channel":"paper-insights-2436","speakers":"Piotr Szyma\u0144ski|Piotr \u017belasko|Mikolaj Morzy|Adrian Szymczak|Marzena \u017by\u0142a-Hoppe|Joanna Banaszczak|Lukasz Augustyniak|Jan Mizgajski|Yishay Carmiel","title":"WER we are and WER we think we are"},{"content":{"abstract":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge.","authors":["Zhengzhong Liang","Mihai Surdeanu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?","tldr":"Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.26","presentation_id":"38940799","rocketchat_channel":"paper-insights-26","speakers":"Zhengzhong Liang|Mihai Surdeanu","title":"Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?"},{"content":{"abstract":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data\u2014data built by minimally editing a set of seed examples to yield counterfactual labels\u2014to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.","authors":["William Huang","Haokun Liu","Samuel R. Bowman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data","tldr":"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain e...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.27","presentation_id":"38940800","rocketchat_channel":"paper-insights-27","speakers":"William Huang|Haokun Liu|Samuel R. Bowman","title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data"},{"content":{"abstract":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive empirical investigation indicates otherwise. In this paper, we establish that ensemble summary for single document using NMF is no better than the best base model summary.","authors":["Alka Khurana","Vasudha Bhatnagar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NMF Ensembles? Not for Text Summarization!","tldr":"Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive emp...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.29","presentation_id":"38940801","rocketchat_channel":"paper-insights-29","speakers":"Alka Khurana|Vasudha Bhatnagar","title":"NMF Ensembles? Not for Text Summarization!"},{"content":{"abstract":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two scorers differ in their handling of them, finding that one approach produces F1 scores approximately 0.5 points higher on the CoNLL 2003 English development and test sets. We propose best practices to increase the replicability of NER evaluations by increasing transparency regarding the handling of improper label sequences.","authors":["Constantine Lignos","Marjan Kamyab"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come","tldr":"We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.30","presentation_id":"38940802","rocketchat_channel":"paper-insights-30","speakers":"Constantine Lignos|Marjan Kamyab","title":"If You Build Your Own NER Scorer, Non-replicable Results Will Come"},{"content":{"abstract":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.","authors":["Jatin Ganhotra","Robert Moore","Sachindra Joshi","Kahini Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.358","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effects of Naturalistic Variation in Goal-Oriented Dialog","tldr":"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fix...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3004","presentation_id":"38940807","rocketchat_channel":"paper-insights-3004","speakers":"Jatin Ganhotra|Robert Moore|Sachindra Joshi|Kahini Wadhawan","title":"Effects of Naturalistic Variation in Goal-Oriented Dialog"},{"content":{"abstract":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.","authors":["Gaurav Arora","Chirag Jain","Manas Chaturvedi","Krupal Modi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HINT3: Raising the bar for Intent Detection in the Wild","tldr":"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-wor...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.31","presentation_id":"38940803","rocketchat_channel":"paper-insights-31","speakers":"Gaurav Arora|Chirag Jain|Manas Chaturvedi|Krupal Modi","title":"HINT3: Raising the bar for Intent Detection in the Wild"},{"content":{"abstract":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier\u2019s predictions can be steered to the poison target class with success rates of >80\\% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.","authors":["Alvin Chan","Yi Tay","Yew-Soon Ong","Aston Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.373","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder","tldr":"This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a \u2018backdoor poisoning\u2019 attack on NLP models. Our poisoning attack utilizes conditional adversarially regula...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3106","presentation_id":"38940808","rocketchat_channel":"paper-insights-3106","speakers":"Alvin Chan|Yi Tay|Yew-Soon Ong|Aston Zhang","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"content":{"abstract":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei andZou, 2019) and Back-Translation (Sennrichet al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XLNet, and RoBERTa. We observe a negative result, finding that techniques which previously reported strong improvements for non-pretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.","authors":["Shayne Longpre","Yu Wang","Chris DuBois"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.394","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?","tldr":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.3296","presentation_id":"38940806","rocketchat_channel":"paper-insights-3296","speakers":"Shayne Longpre|Yu Wang|Chris DuBois","title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?"},{"content":{"abstract":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action \u2014 e.g., \u201cI started a new book I bought last week\u201d, where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods.","authors":["Yanai Elazar","Victoria Basmov","Shauli Ravfogel","Yoav Goldberg","Reut Tsarfaty"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Extraordinary Failure of Complement Coercion Crowdsourcing","tldr":"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied acti...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.33","presentation_id":"38940804","rocketchat_channel":"paper-insights-33","speakers":"Yanai Elazar|Victoria Basmov|Shauli Ravfogel|Yoav Goldberg|Reut Tsarfaty","title":"The Extraordinary Failure of Complement Coercion Crowdsourcing"},{"content":{"abstract":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment with a multi-task learning approach for explicitly incorporating the structured elements of dictionary entries, such as user-assigned tags and usage examples, when learning embeddings for dictionary headwords. Our work generalizes several existing models for learning word embeddings from dictionaries. However, we find that the most effective representations overall are learned by simply training with a skip-gram objective over the concatenated text of all entries in the dictionary, giving no particular focus to the structure of the entries.","authors":["Steven Wilson","Walid Magdy","Barbara McGillivray","Gareth Tyson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Embedding Structured Dictionary Entries","tldr":"Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment ...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.34","presentation_id":"38940805","rocketchat_channel":"paper-insights-34","speakers":"Steven Wilson|Walid Magdy|Barbara McGillivray|Gareth Tyson","title":"Embedding Structured Dictionary Entries"},{"content":{"abstract":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as unstable model behaviour and some noise within the dataset itself. We then experiment with two approaches for integrating information from knowledge graphs: (i) concatenating knowledge graph triples to text passages and (ii) encoding knowledge with a Graph Neural Network. Neither of these approaches show a clear improvement and we hypothesize that this may be due to a combination of inaccuracies in the knowledge graph, imprecision in entity linking, and the models\u2019 inability to capture additional information from knowledge graphs.","authors":["Daria Dzendzik","Carl Vogel","Jennifer Foster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!","tldr":"In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as uns...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.4","presentation_id":"38940789","rocketchat_channel":"paper-insights-4","speakers":"Daria Dzendzik|Carl Vogel|Jennifer Foster","title":"Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated!"},{"content":{"abstract":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied. We propose a systematic approach aimed at evaluating data selection in scenarios of increasing complexity. Specifically, we compare the case in which source and target tasks are the same while source and target domains are different, against the more challenging scenario where both tasks and domains are different. We run a number of experiments on semantic sequence tagging tasks, which are relatively less investigated in data selection, and conclude that data selection has more benefit on the scenario when the tasks are the same, while in case of different (although related) tasks from distant domains, a combination of data selection and multi-task learning is ineffective for most cases.","authors":["Samuel Louvan","Bernardo Magnini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks","tldr":"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied....","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.6","presentation_id":"38940790","rocketchat_channel":"paper-insights-6","speakers":"Samuel Louvan|Bernardo Magnini","title":"How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks"},{"content":{"abstract":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search (ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS to NLP tasks, our results are mixed \u2013 we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.","authors":["Ansel MacLaughlin","Jwala Dhamala","Anoop Kumar","Sriram Venkatapathy","Ragav Venkatesan","Rahul Gupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks","tldr":"Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and c...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.7","presentation_id":"38940791","rocketchat_channel":"paper-insights-7","speakers":"Ansel MacLaughlin|Jwala Dhamala|Anoop Kumar|Sriram Venkatapathy|Ragav Venkatesan|Rahul Gupta","title":"Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks"},{"content":{"abstract":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.","authors":["Silvia Terragni","Debora Nozza","Elisabetta Fersini","Messina Enza"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.insights-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models","tldr":"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. Whil...","track":"Workshop on Insights from Negative Results in NLP"},"id":"WS-3.8","presentation_id":"38940792","rocketchat_channel":"paper-insights-8","speakers":"Silvia Terragni|Debora Nozza|Elisabetta Fersini|Messina Enza","title":"Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models"}],"prerecorded_talks":[{"presentation_id":"38940632","speakers":"Byron C. Wallace","title":"Negative results yield interesting questions, or: A bunch of stuff that didn't work"},{"presentation_id":"38940633","speakers":"Rada Mihalcea","title":"The Ups and Downs of Word Embeddings"}],"rocketchat_channel":"workshop-insights","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 15:15:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>Opening remarks</b>","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:00:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"<b>Invited talk: <a href=\"http://web.eecs.umich.edu/~mihalcea/\">Rada Mihalcea</a>:</b> The ups and downs of word embeddings<br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-invited-mihalcea\">RocketChat</a><br/>\n<i>Word embeddings have largely been a \u201csuccess story\u201d in our field. They have enabled progress in numerous language processing applications, and have facilitated the application of large-scale language analyses in other domains, such as social sciences and humanities. While less talked about, word embeddings also have many shortcomings \u2013 instability, lack of transparency, biases, and more. In this talk, I will review the \u201cups\u201d and \u201cdowns\u201d of word embeddings, discuss tradeoffs, and chart potential future research directions to address some of the downsides of these word representations.</i>","start_time":"Thu, 19 Nov 2020 15:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:15:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"<b>Q&A with Rada Mihalcea</b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-invited-mihalcea\">RocketChat</a>","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:45:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>Thematic session: representation learning </b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-representations\">RocketChat</a> <br/>\n\u2022 Embedding Structured Dictionary Entries (Steven Wilson, Walid Magdy, Barbara McGillivray and Gareth Tyson) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.34.html\">Paper</a><br/>\n\u2022 Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About? (Valentina Beretta, S\u00e9bastien Harispe, Katarina Boland, Luke Lo Seen, Konstantin Todorov and Andon Tchechmedjiev) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.24.html\">Paper</a><br/>\n\u2022 Layout-Aware Text Representations Harm Clustering Documents by Type (Catherine Finegan-Dollak and Ashish Verma) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.22.html\">Paper</a>","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:15:00 GMT","hosts":"Joao Sedoc","link":"","session_name":"<b>Thematic session: dialogue </b>  <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-dialogue\">RocketChat</a> <br/>\n\u2022 On Task-Level Dialogue Composition of Generative Transformer Model (Prasanna Parthasarathi, Sharan Narang and Arvind Neelakantan) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.12.html\">Paper</a><br/>\n\u2022 HINT3: Raising the bar for Intent Detection in the Wild (Gaurav Arora, Chirag Jain, Manas Chaturvedi and Krupal Modi) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.31.html\">Paper</a><br/>\n\u2022 Effects of Naturalistic Variation in Goal-Oriented Dialog (Jatin Ganhotra, Robert Moore, Sachindra Joshi and Kahini Wadhawan <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.3004.html\">Paper</a>","start_time":"Thu, 19 Nov 2020 16:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:00:00 GMT","hosts":"n/a","link":"","session_name":"<b>Social break / meal time.</b> Gather.town (room N)","start_time":"Thu, 19 Nov 2020 17:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"<b>Invited talk: <a href=\"http://www.byronwallace.com/\">Byron Wallace</a>:</b>Negative results yield interesting questions, or: a bunch of stuff that didn\u2019t work  <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-invited-wallace\">RocketChat</a><br/>\n<i>I will discuss recent projects in which ideas did not pan out as expected, but where these initial negative results led to (arguably) more interesting questions. My hope is that these case studies of negative results \u2014 which ultimately led to work we viewed as compelling enough to warrant write-up \u2014 will foster discussion about when \u201cnegative\u201d results are nonetheless interesting, and about the kinds of questions we ask in empirical NLP research.</i>","start_time":"Thu, 19 Nov 2020 18:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"<b>Q&A with Byron Wallace</b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-invited-wallace\">RocketChat</a>","start_time":"Thu, 19 Nov 2020 18:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:30:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>Thematic session: question answering </b><br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-from-qa\">RocketChat</a><br/>\n\u2022 Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules? (Zhengzhong Liang and Mihai Surdeanu) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.26.html\">Paper</a><br/>\n\u2022 What do we expect from Multiple-choice QA Systems? (Krunal Shah, Nitish Gupta and Dan Roth) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.2575.html\">Paper</a><br/>\n\u2022 Q. Can Knowledge Graphs be used to Answer Boolean Questions? A. It\u2019s complicated! (Daria Dzendzik, Carl Vogel and Jennifer Foster) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.4.html\">Paper</a>","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:00:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>Thematic session: natural language inference </b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-from-nli\">RocketChat</a><br/>\n\u2022 Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data (William Huang, Haokun Liu and Samuel R. Bowman) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.27.html\">Paper</a><br/>\n\u2022 The Extraordinary Failure of Complement Coercion Crowdsourcing (Yanai Elazar, Victoria Basmov, Shauli Ravfogel, Yoav Goldberg and Reut Tsarfat) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.33.html\">Paper</a><br/>\n\u2022 Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder (Alvin Chan, Yi Tay, Yew-Soon Ong and Aston Zhang) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.106.html\">Paper</a>","start_time":"Thu, 19 Nov 2020 19:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:30:00 GMT","hosts":"Joao Sedoc","link":"","session_name":"<b>Thematic session: lessons learned the hard way</b> <br/><a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-lessons\">RocketChat</a> <br/>\n\u2022 Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks (Ansel MacLaughlin, Jwala Dhamala, Anoop Kumar, Sriram Venkatapathy, Ragav Venkatesan and Rahul Gupta) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.7.html\">Paper</a><br/>\n\u2022 NMF Ensembles? Not for Text Summarization! (Alka Khurana and Vasudha Bhatnagar) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.29.html\">Paper</a><br/>\n\u2022 If You Build Your Own NER Scorer, Non-replicable Results Will Come (Constantine Lignos and Marjan Kamyab) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.30.html\">Paper</a>","start_time":"Thu, 19 Nov 2020 20:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":"n/a","link":"","session_name":"<b>Social break / meal time.</b> Gather.town (room N)","start_time":"Thu, 19 Nov 2020 20:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:00:00 GMT","hosts":"n/a","link":"","session_name":"<b>Interactive Orals</b> <br/>\n\u2022 Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words (Anmol Nayak, Hariprasad Timmapathini, Karthikeyan Ponnalagu and Vijendran Gopalan Venkoparao) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.1.html\">Paper</a> <br/>\n\u2022 How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks (Samuel Louvan and Bernardo Magnini) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.6.html\">Paper</a><br/>\n\u2022 Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models (Silvia Terragni, Debora Nozza, Elisabetta Fersini and Messina Enza) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.8.html\">Paper</a><br/>\n\u2022 How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study (Meghana Moorthy Bhat and Srinivasan Parthasarathy) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.19.html\">Paper</a><br/>\n\u2022 Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification (Ashwin Geet D\u2019Sa, Irina Illina, Dominique Fohr and Dietrich Klakow) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.20.html\">Paper</a><br/>\n\u2022 An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios (Andrew Zupon, Faiz Rafique and Mihai Surdeanu) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.23.html\">Paper</a><br/>\n\u2022 How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers? (Shayne Longpre, Yu Wang and Christopher DuBois) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.3296.html\">Paper</a><br/>\n\u2022 WER we are and WER we think we are (Piotr Szyma\u0144ski, Piotr \u017belasko, Mikolaj Morzy, Adrian Szymczak, Marzena \u017by\u0142a-Hoppe, Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski and Yishay Carmiel) <a href=\"https://virtual.2020.emnlp.org/paper_WS-3.2436.html\">Paper</a><br/>","start_time":"Thu, 19 Nov 2020 21:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:45:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>The frustrations of leaderboardism. Panel discussion with Kawin Ethayarajh, Jesse Dodge and Rachael Tatman </b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-panel\">RocketChat</a><br/>\n<i>Leaderboards do not only drive progress in NLP: the bias towards publication of positive, and particularly state-of-the-art results implicitly encourages the development of highly specialized and brittle systems. If the reported success cannot be reproduced, or does not generalize well, the main result is much frustration by the developers who pick up academic papers in search of something that would actually work.</i>","start_time":"Thu, 19 Nov 2020 22:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:00:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>QA with the panelists </b> <br/> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/insights-panel\">RocketChat</a>","start_time":"Thu, 19 Nov 2020 22:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:15:00 GMT","hosts":"Anna Rogers","link":"","session_name":"<b>Closing remarks </b> <a href=\"https://zoom.us?pwd=VGRLTnpVZyswOHJiZ3R1UzNrTWYxZz09\">Zoom</a>","start_time":"Thu, 19 Nov 2020 23:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 00:00:00 GMT","hosts":"Joao Sedoc","link":"","session_name":"<b>Virtual happy hour</b> Gather.town (room N)","start_time":"Thu, 19 Nov 2020 23:15:00 GMT"}],"title":"Workshop on Insights from Negative Results in NLP","website":"https://insights-workshop.github.io/","zoom_links":["https://zoom.us"]},{"abstract":"N/A","blocks":[{"end_time":"Thu, 19 Nov 2020 12:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 08:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"}],"id":"WS-4","livestream":null,"organizers":"Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton and Mikhail Burtsev","papers":[{"content":{"abstract":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring (FIRE) for this task. In this method, a context filter and a knowledge filter are first built, which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. Besides, the entries irrelevant to the conversation are discarded by the knowledge filter. After that, iteratively referring is performed between context and response representations as well as between knowledge and response representations, in order to collect deep matching features for scoring response candidates. Experimental results show that FIRE outperforms previous methods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with original and revised personas respectively, and margins larger than 3.1% on the CMU_DoG dataset in terms of top-1 accuracy. We also show that FIRE is more interpretable by visualizing the knowledge grounding process.","authors":["Jia-Chen Gu","Zhenhua Ling","Quan Liu","Zhigang Chen","Xiaodan Zhu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.127","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots","tldr":"The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method n...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1175","presentation_id":"38940705","rocketchat_channel":"paper-scai-1175","speakers":"Jia-Chen Gu|Zhenhua Ling|Quan Liu|Zhigang Chen|Xiaodan Zhu","title":"Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots"},{"content":{"abstract":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete response at a stroke. This tends to generate high-frequency function words with less semantic information rather than low-frequency content words with more semantic information. To address this issue, we propose a content-aware model with two-stage decoding process named Two-stage Dialogue Generation (TSDG). We separate the decoding process of content words and function words so that content words can be generated independently without the interference of function words. Experimental results on two datasets indicate that our model significantly outperforms several competitive generative models in terms of automatic and human evaluation.","authors":["Junsheng Kong","Zhicheng Zhong","Yi Cai","Xin Wu","Da Ren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.192","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process","tldr":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete r...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1735","presentation_id":"38940706","rocketchat_channel":"paper-scai-1735","speakers":"Junsheng Kong|Zhicheng Zhong|Yi Cai|Xin Wu|Da Ren","title":"TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process"},{"content":{"abstract":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse data sets. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications.","authors":["Matthew Henderson","I\u00f1igo Casanueva","Nikola Mrk\u0161i\u0107","Pei-Hao Su","Tsung-Hsien Wen","Ivan Vuli\u0107"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.196","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ConveRT: Efficient and Accurate Conversational Representations from Transformers","tldr":"General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers)...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1761-ws4","presentation_id":"38940707","rocketchat_channel":"paper-scai-1761-ws4","speakers":"Matthew Henderson|I\u00f1igo Casanueva|Nikola Mrk\u0161i\u0107|Pei-Hao Su|Tsung-Hsien Wen|Ivan Vuli\u0107","title":"ConveRT: Efficient and Accurate Conversational Representations from Transformers"},{"content":{"abstract":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator\u2019s goal is to convert the feedback into a response that answers the user\u2019s previous utterance and to fool the discriminator which distinguishes feedback from natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94%to 75.96% in ranking correct responses on the PERSONACHATdataset, a large improvement given that the original model is already trained on 131k samples.","authors":["Makesh Narsimhan Sreedhar","Kun Ni","Siva Reddy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.221","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback","tldr":"The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissati...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.1947","presentation_id":"38940708","rocketchat_channel":"paper-scai-1947","speakers":"Makesh Narsimhan Sreedhar|Kun Ni|Siva Reddy","title":"Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback"},{"content":{"abstract":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-based representation (e.g. \u201cI have two cats.\u201d). We argue that these representations remain superficial w.r.t. the complexity of human personality. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, values, and beliefs to drive language generation. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.","authors":["Thomas Scialom","Serra Sinem Tekiro\u011flu","Jacopo Staiano","Marco Guerini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.238","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Toward Stance-based Personas for Opinionated Dialogues","tldr":"In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-ba...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2041","presentation_id":"38940704","rocketchat_channel":"paper-scai-2041","speakers":"Thomas Scialom|Serra Sinem Tekiro\u011flu|Jacopo Staiano|Marco Guerini","title":"Toward Stance-based Personas for Opinionated Dialogues"},{"content":{"abstract":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn\u2019s contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features. On dialogues sampled from 28 Alexa domains, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27% (0.43 -> 0.70) and 7% (0.63 -> 0.70) improvement in linear correlation performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.","authors":["Praveen Kumar Bodigutla","Aditya Tiwari","Spyros Matsoukas","Josep Valls-Vargas","Lazaros Polymenakos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.347","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations","tldr":"Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which redu...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2889","presentation_id":"38940709","rocketchat_channel":"paper-scai-2889","speakers":"Praveen Kumar Bodigutla|Aditya Tiwari|Spyros Matsoukas|Josep Valls-Vargas|Lazaros Polymenakos","title":"Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations"},{"content":{"abstract":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing the first-stage of passage retrieval. Given an input question, it uses a BERT-based classifier (trained with weak supervision) to de-contextualize the input by selecting relevant terms from the dialog history. Using the question and the selected terms, it issues a query to a search engine to perform the first-stage of passage retrieval. On the other hand, MVR is responsible for contextualized passage reranking. It first constructs multiple views of the information need embedded within an input question. The views are based on the dialog history and the top documents obtained in the first-stage of retrieval. It then uses each view to rerank passages using BERT (fine-tuned for passage ranking). Finally, MVR performs a fusion over the rankings produced by the individual views. Experiments show that the above combination improves first-state retrieval as well as the overall accuracy in a reranking pipeline. On the key metric of NDCG@3, the proposed combination achieves a relative performance improvement of 14.8% over the state-of-the-art baseline and is also able to surpass the Oracle.","authors":["Vaibhav Kumar","Jamie Callan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.354","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Making Information Seeking Easier: An Improved Pipeline for Conversational Search","tldr":"This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing ...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2957","presentation_id":"38940710","rocketchat_channel":"paper-scai-2957","speakers":"Vaibhav Kumar|Jamie Callan","title":"Making Information Seeking Easier: An Improved Pipeline for Conversational Search"},{"content":{"abstract":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves over a strong Transformer baseline as measured by human and automatic quality scores and lexical diversity. We also find SMRT is comparable to pretraining in human evaluation quality, and outperforms pretraining on automatic quality and lexical diversity, without requiring related-domain dialog data.","authors":["Huda Khayrallah","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.403","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training","tldr":"Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple re...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.3361","presentation_id":"38940711","rocketchat_channel":"paper-scai-3361","speakers":"Huda Khayrallah|Jo\u00e3o Sedoc","title":"SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training"},{"content":{"abstract":"","authors":["Tba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper1","presentation_id":"38940061","rocketchat_channel":"paper-scai-paper1","speakers":"Tba","title":"TBA"},{"content":{"abstract":"The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting (QR) of the conversational context allows to shed more light on this phenomenon and also use it to evaluate robustness of different answer selection approaches. We introduce a simple framework that enables an automated analysis of the conversational question answering (QA) performance using question rewrites, and present the results of this analysis on the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover sensitivity to question formulation of the popular state-of-the-art question answering approaches. Our results demonstrate that the reading comprehension model is insensitive to question formulation, while the passage ranking changes dramatically with a little variation in the input question. The benefit of QR is that it allows us to pinpoint and group such cases automatically. We show how to use this methodology to verify whether QA models are really learning the task or just finding shortcuts in the dataset, and better understand the frequent types of error they make.","authors":["Svitlana Vakulenko","Shayne Longpre","Zhucheng Tu","Raviteja Anantha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering","tldr":"The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting (QR) of the conversational context allows to shed more light on thi...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper2","presentation_id":"38940062","rocketchat_channel":"paper-scai-paper2","speakers":"Svitlana Vakulenko|Shayne Longpre|Zhucheng Tu|Raviteja Anantha","title":"A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering"},{"content":{"abstract":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders models from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the model with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over state-of-the-art models.","authors":["Eyal Ben-David","Orgad Keller","Eric Malmi","Idan Szpektor","Roi Reichart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.135","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Semantically Driven Sentence Fusion: Modeling and Evaluation","tldr":"Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders mod...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper3","presentation_id":"38940063","rocketchat_channel":"paper-scai-paper3","speakers":"Eyal Ben-David|Orgad Keller|Eric Malmi|Idan Szpektor|Roi Reichart","title":"Semantically Driven Sentence Fusion: Modeling and Evaluation"},{"content":{"abstract":"","authors":["Tba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper4","presentation_id":"38940064","rocketchat_channel":"paper-scai-paper4","speakers":"Tba","title":"TBA"},{"content":{"abstract":"","authors":["Marco Guerini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TBA","tldr":null,"track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.paper5","presentation_id":"38940065","rocketchat_channel":"paper-scai-paper5","speakers":"Marco Guerini","title":"TBA"},{"content":{"abstract":"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response candidates) for which a neural ranker is ineffective and (ii) improving neural ranking for such instances. To address both challenges we resort to slice-based learning for which the goal is to improve effectiveness of neural models for slices (subsets) of data. We address challenge (i) by proposing different slicing functions (SFs) that select slices of the dataset\u2014based on prior work we heuristically capture different failures of neural rankers. Then, for challenge (ii) we adapt a neural ranking model to learn slice-aware representations, i.e. the adapted model learns to represent the question and responses differently based on the model\u2019s prediction of which slices they belong to. Our experimental results (the source code and data are available at https://github.com/Guzpenha/slice_based_learning) across three different ranking tasks and four corpora show that slice-based learning improves the effectiveness by an average of 2% over a neural ranker that is not slice-aware.","authors":["Gustavo Penha","Claudia Hauff"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Slice-Aware Neural Ranking","tldr":"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response ...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2020.scai-1.1","presentation_id":"","rocketchat_channel":"paper-scai-1","speakers":"Gustavo Penha|Claudia Hauff","title":"Slice-Aware Neural Ranking"},{"content":{"abstract":"Conversational Question Answering (ConvQA) is a Conversational Search task in a simplified setting, where an answer must be extracted from a given passage. Neural language models, such as BERT, fine-tuned on large-scale ConvQA datasets such as CoQA and QuAC have been used to address this task. Recently, Multi-Task Learning (MTL) has emerged as a particularly interesting approach for developing ConvQA models, where the objective is to enhance the performance of a primary task by sharing the learned structure across several related auxiliary tasks. However, existing ConvQA models that leverage MTL have not investigated the dynamic adjustment of the relative importance of the different tasks during learning, nor the resulting impact on the performance of the learned models. In this paper, we first study the effectiveness and efficiency of dynamic MTL methods including Evolving Weighting, Uncertainty Weighting, and Loss-Balanced Task Weighting, compared to static MTL methods such as the uniform weighting of tasks. Furthermore, we propose a novel hybrid dynamic method combining Abridged Linear for the main task with a Loss-Balanced Task Weighting (LBTW) for the auxiliary tasks, so as to automatically fine-tune task weighting during learning, ensuring that each of the task\u2019s weights is adjusted by the relative importance of the different tasks. We conduct experiments using QuAC, a large-scale ConvQA dataset. Our results demonstrate the effectiveness of our proposed method, which significantly outperforms both the single-task learning and static task weighting methods with improvements ranging from +2.72% to +3.20% in F1 scores. Finally, our findings show that the performance of using MTL in developing ConvQA model is sensitive to the correct selection of the auxiliary tasks as well as to an adequate balancing of the loss rates of these tasks during training by using LBTW.","authors":["Sarawoot Kongyoung","Craig Macdonald","Iadh Ounis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.scai-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering","tldr":"Conversational Question Answering (ConvQA) is a Conversational Search task in a simplified setting, where an answer must be extracted from a given passage. Neural language models, such as BERT, fine-tuned on large-scale ConvQA datasets such as CoQA a...","track":"Search-Oriented Conversational AI (SCAI) 2"},"id":"WS-4.2020.scai-1.3","presentation_id":"","rocketchat_channel":"paper-scai-3","speakers":"Sarawoot Kongyoung|Craig Macdonald|Iadh Ounis","title":"Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering"}],"prerecorded_talks":[{"presentation_id":"38940066","speakers":"Hang Li, Tianqiao Liu, Yu Kang, Guowei Xu, Wenbiao Ding, Zitao Liu","title":"Clarifying Questions in Conversations with Augmented Syntax Features and Side Information"},{"presentation_id":"38940075","speakers":"Tba","title":"TBA"},{"presentation_id":"38940067","speakers":"Hang Li, Tianqiao Liu, Yu Kang, Guowei Xu, Wenbiao Ding, Zitao Liu","title":"Clarifying Questions in Conversations with Augmented Syntax Features and Side Information - TAL ML Team's Solution for ConvAI3 Challenge"},{"presentation_id":"38940068","speakers":"Jian Wang, Wenjie Li","title":"Clarifying Questions for Conversational Search Systems: Team Soda\u2019s Solution for the ClariQ Challenge"},{"presentation_id":"38940069","speakers":"Jian Wang, Wenjie Li","title":"TBA"},{"presentation_id":"38940070","speakers":"Wenjie Ou, Yue Lin","title":"TBA"},{"presentation_id":"38940071","speakers":"Tba","title":"TBA"},{"presentation_id":"38940072","speakers":"Tba","title":"TBA"},{"presentation_id":"38940073","speakers":"Tba","title":"TBA"},{"presentation_id":"38940074","speakers":"Tba","title":"TBA"},{"presentation_id":"38940054","speakers":"Verena Rieser","title":"Response Generation and Retrieval for Multimodal Conversational AI"},{"presentation_id":"38940055","speakers":"Ivan Vuli\u0107","title":"Data-Efficient Natural Language Understanding for Task-Oriented Dialogue"},{"presentation_id":"38940056","speakers":"Lee, Sungjin","title":"Towards Self-Learning for Large Scale Conversational Agents"},{"presentation_id":"38940057","speakers":"Jason Weston","title":"BlenderBot, Knowledge and Search"},{"presentation_id":"38940058","speakers":"Y-Lan Boureau","title":"Better-behaved Conversational Agents"},{"presentation_id":"38940059","speakers":"Jian-Yun Nie","title":"Smooth Training for Open-Domain Question Answering"},{"presentation_id":"38940060","speakers":"Thomas Wolf","title":"Transfer Learning and tools for Conversational Agents"}],"rocketchat_channel":"workshop-scai","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 08:10:00 GMT","hosts":"M. Burtsev, A. Chuklin, J. Dalton","link":"","session_name":"Welcome Speech #1 (Europe and Asia)","start_time":"Thu, 19 Nov 2020 08:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 08:40:00 GMT","hosts":"A. Chuklin","link":"","session_name":"Invited Talk by Ivan Vuli\u0107 (30 min video)","start_time":"Thu, 19 Nov 2020 08:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 09:10:00 GMT","hosts":"A. Chuklin","link":"","session_name":"Invited Talk by Thomas Wolf (30 min video)","start_time":"Thu, 19 Nov 2020 08:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 09:40:00 GMT","hosts":"A. Chuklin","link":"","session_name":"Invited Talk by Verena Rieser (30 min video)","start_time":"Thu, 19 Nov 2020 09:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 10:20:00 GMT","hosts":"J. Dalton & M. Burtsev","link":"","session_name":"Q&A & Panel Discussion w/ V. Rieser, I. Vuli\u0107 and T. Wolf","start_time":"Thu, 19 Nov 2020 09:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 10:40:00 GMT","hosts":"A. Chuklin","link":"","session_name":"Q&A with SCAI paper authors #1","start_time":"Thu, 19 Nov 2020 10:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 11:00:00 GMT","hosts":"M. Alliannejadi","link":"","session_name":"ClariQ Challenge Presentation and Winner Announcement","start_time":"Thu, 19 Nov 2020 10:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 12:00:00 GMT","hosts":"M. Alliannejadi & M. Burtsev","link":"","session_name":"Coffe Hour & Poster Session #1","start_time":"Thu, 19 Nov 2020 11:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:10:00 GMT","hosts":"J. Dalton, J. Kiseleva, A.Chuklin, M. Burtsev","link":"","session_name":"Welcome Speech #2 (Europe and Americas)","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:30:00 GMT","hosts":"J. Dalton","link":"","session_name":"Q&A with SCAI paper authors #2","start_time":"Thu, 19 Nov 2020 16:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"J. Dalton & A. Chuklin","link":"","session_name":"Social Hour & Poster Session #2","start_time":"Thu, 19 Nov 2020 16:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:00:00 GMT","hosts":"J. Kiseleva","link":"","session_name":"Invited Talk by Jason Weston (30 min video)","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:30:00 GMT","hosts":"J. Kiseleva","link":"","session_name":"Invited Talk by Jian-Yun Nie (30 min video)","start_time":"Thu, 19 Nov 2020 18:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"J. Kiseleva","link":"","session_name":"Invited Talk by Y-Lan Boureau (30 min video)","start_time":"Thu, 19 Nov 2020 18:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:30:00 GMT","hosts":"J. Kiseleva","link":"","session_name":"Invited Talk by Sungjin Lee (30 min video)","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:10:00 GMT","hosts":"J. Kiseleva","link":"","session_name":"Q&A & Panel Discussion w/ Y. Boureau, S. Lee, J. Nie, J. Weston","start_time":"Thu, 19 Nov 2020 19:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":"J. Kiseleva & J. Dalton","link":"","session_name":"Social Hour","start_time":"Thu, 19 Nov 2020 20:10:00 GMT"}],"title":"Search-Oriented Conversational AI (SCAI) 2","website":"http://scai.info","zoom_links":["https://zoom.us"]},{"abstract":"The CMCL workshop provides a venue for computational research on cognitive theories of language processing, representation and acquisition.","blocks":[{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"}],"id":"WS-5","livestream":null,"organizers":"Emmanuele Chersoni, Cassandra L. Jacobs, Yohei Oseki, Laurent Pr\u00e9vot and Enrico Santus","papers":[{"content":{"abstract":"By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model\u2019s ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell\u2019s analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.","authors":["Yiding Hao","Simon Mendelsohn","Rachel Sterneck","Randi Martinez","Robert Frank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling","tldr":"By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a c...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.16","presentation_id":"38939682","rocketchat_channel":"paper-cmcl2020-16","speakers":"Yiding Hao|Simon Mendelsohn|Rachel Sterneck|Randi Martinez|Robert Frank","title":"Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling"},{"content":{"abstract":"Different aspects of language processing have been shown to be sensitive to priming but the findings of studies examining priming effects in adolescents with Autism Spectrum Disorder (ASD) and Developmental Language Disorder (DLD) have been inconclusive. We present a study analysing visual and implicit semantic priming in adolescents with ASD and DLD. Based on a dataset of fictional and script-like narratives, we evaluate how often and how extensively, content of two different priming sources is used by the participants. The first priming source was visual, consisting of images shown to the participants to assist them with their storytelling. The second priming source originated from commonsense knowledge, using crowdsourced data containing prototypical script elements. Our results show that individuals with ASD are less sensitive to both types of priming, but show typical usage of primed cues when they use them at all. In contrast, children with DLD show mostly average priming sensitivity, but exhibit an over-proportional use of the priming cues.","authors":["Michaela Regneri","Diane King","Fahreen Walji","Olympia Palikara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Images and Imagination: Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder","tldr":"Different aspects of language processing have been shown to be sensitive to priming but the findings of studies examining priming effects in adolescents with Autism Spectrum Disorder (ASD) and Developmental Language Disorder (DLD) have been inconclus...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.7","presentation_id":"38939683","rocketchat_channel":"paper-cmcl2020-7","speakers":"Michaela Regneri|Diane King|Fahreen Walji|Olympia Palikara","title":"Images and Imagination: Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder"},{"content":{"abstract":"Word order flexibility is one of the distinctive features of SOV languages. In this work, we investigate whether the order and relative distance of preverbal dependents in Hindi, an SOV language, is affected by factors motivated by efficiency considerations during comprehension/production. We investigate the influence of Head\u2013Dependent Mutual Information (HDMI), similarity-based interference, accessibility and case-marking. Results show that preverbal dependents remain close to the verbal head when the HDMI between the verb and its dependent is high. This demonstrates the influence of locality constraints on dependency distance and word order in an SOV language. Additionally, dependency distance were found to be longer when the dependent was animate, when it was case-marked and when it was semantically similar to other preverbal dependents. Together the results highlight the crosslinguistic generalizability of these factors and provide evidence for a functionally motivated account of word order in SOV languages such as Hindi.","authors":["Kartik Sharma","Richard Futrell","Samar Husain"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Determines the Order of Verbal Dependents in Hindi? Effects of Efficiency in Comprehension and Production","tldr":"Word order flexibility is one of the distinctive features of SOV languages. In this work, we investigate whether the order and relative distance of preverbal dependents in Hindi, an SOV language, is affected by factors motivated by efficiency conside...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.1","presentation_id":"","rocketchat_channel":"paper-cmcl2020-1","speakers":"Kartik Sharma|Richard Futrell|Samar Husain","title":"What Determines the Order of Verbal Dependents in Hindi? Effects of Efficiency in Comprehension and Production"},{"content":{"abstract":"We introduce a framework in which production-rule based computational cognitive modeling and Reinforcement Learning can systematically interact and inform each other. We focus on linguistic applications because the sophisticated rule-based cognitive models needed to capture linguistic behavioral data promise to provide a stringent test suite for RL algorithms, connecting RL algorithms to both accuracy and reaction-time experimental data. Thus, we open a path towards assembling an experimentally rigorous and cognitively realistic benchmark for RL algorithms. We extend our previous work on lexical decision tasks and tabular RL algorithms (Brasoveanu and Dotla\u010dil, 2020b) with a discussion of neural-network based approaches, and a discussion of how parsing can be formalized as an RL problem.","authors":["Adrian Brasoveanu","Jakub Dotlacil"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Production-based Cognitive Models as a Test Suite for Reinforcement Learning Algorithms","tldr":"We introduce a framework in which production-rule based computational cognitive modeling and Reinforcement Learning can systematically interact and inform each other. We focus on linguistic applications because the sophisticated rule-based cognitive ...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.3","presentation_id":"","rocketchat_channel":"paper-cmcl2020-3","speakers":"Adrian Brasoveanu|Jakub Dotlacil","title":"Production-based Cognitive Models as a Test Suite for Reinforcement Learning Algorithms"},{"content":{"abstract":"Continuous vector word representations (or word embeddings) have shown success in capturing semantic relations between words, as evidenced with evaluation against behavioral data of adult performance on semantic tasks (Pereira et al. 2016). Adult semantic knowledge is the endpoint of a language acquisition process; thus, a relevant question is whether these models can also capture emerging word representations of young language learners. However, the data of semantic knowledge of children is scarce or non-existent for some age groups. In this paper, we propose to bridge this gap by using Age of Acquisition norms to evaluate word embeddings learnt from child-directed input. We present two methods that evaluate word embeddings in terms of (a) the semantic neighbourhood density of learnt words, and (b) the convergence to adult word associations. We apply our methods to bag-of-words models, and we find that (1) children acquire words with fewer semantic neighbours earlier, and (2) young learners only attend to very local context. These findings provide converging evidence for validity of our methods in understanding the prerequisite features for a distributional model of word learning.","authors":["Raquel G. Alhama","Caroline Rowland","Evan Kidd"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Word Embeddings for Language Acquisition","tldr":"Continuous vector word representations (or word embeddings) have shown success in capturing semantic relations between words, as evidenced with evaluation against behavioral data of adult performance on semantic tasks (Pereira et al. 2016). Adult sem...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.4","presentation_id":"","rocketchat_channel":"paper-cmcl2020-4","speakers":"Raquel G. Alhama|Caroline Rowland|Evan Kidd","title":"Evaluating Word Embeddings for Language Acquisition"},{"content":{"abstract":"The age of acquisition of a word is a psycholinguistic variable concerning the age at which a word is typically learned. It correlates with other psycholinguistic variables such as familiarity, concreteness, and imageability. Existing datasets for multiple languages also include linguistic variables such as the length and the frequency of lemmas in different corpora. There are substantial sets of normative values for English, but for other languages, such as Italian, the coverage is scarce. In this paper,a set of regression experiments investigates whether it is possible to guess the age of acquisition of Italian lemmas that have not been previously rated by humans. An intrinsic evaluation is proposed, correlating estimated Italian lemmas\u2019 AoA with English lemmas\u2019 AoA. An extrinsic evaluation - using AoA values as features for the classification of literary excerpts labeled by age appropriateness - shows how es-sential is lexical coverage for this task.","authors":["Irene Russo"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guessing the Age of Acquisition of Italian Lemmas through Linear Regression","tldr":"The age of acquisition of a word is a psycholinguistic variable concerning the age at which a word is typically learned. It correlates with other psycholinguistic variables such as familiarity, concreteness, and imageability. Existing datasets for mu...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.5","presentation_id":"","rocketchat_channel":"paper-cmcl2020-5","speakers":"Irene Russo","title":"Guessing the Age of Acquisition of Italian Lemmas through Linear Regression"},{"content":{"abstract":"The free association task has been very influential both in cognitive science and in computational linguistics. However, little research has been done to study how free associations develop in childhood. The current work focuses on the developmental hypothesis according to which free word associations emerge by mirroring the co-occurrence distribution of children\u2019s linguistic environment. I trained a distributional semantic model on a large corpus of child language and I tested if it could predict children\u2019s responses. The results largely supported the hypothesis: Co-occurrence-based similarity was a strong predictor of children\u2019s associative behavior even controlling for other possible predictors such as phonological similarity, word frequency, and word length. I discuss the findings in the light of theories of conceptual development.","authors":["Abdellah Fourtassi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Word Co-occurrence in Child-directed Speech Predicts Children\u2019s Free Word Associations","tldr":"The free association task has been very influential both in cognitive science and in computational linguistics. However, little research has been done to study how free associations develop in childhood. The current work focuses on the developmental ...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.6","presentation_id":"","rocketchat_channel":"paper-cmcl2020-6","speakers":"Abdellah Fourtassi","title":"Word Co-occurrence in Child-directed Speech Predicts Children\u2019s Free Word Associations"},{"content":{"abstract":"Interactive alignment is a major mechanism of linguistic coordination. Here we study the way this mechanism emerges in development across the lexical, syntactic, and conceptual levels. We leverage NLP tools to analyze a large-scale corpus of child-adult conversations between 2 and 5 years old. We found that, across development, children align consistently to adults above chance and that adults align consistently more to children than vice versa (even controlling for language production abilities). Besides these consistencies, we found a diversity of developmental trajectories across linguistic levels. These corpus-based findings provide strong support for an early onset of multi-level linguistic alignment in children and invites new experimental work.","authors":["Thomas Misiek","Benoit Favre","Abdellah Fourtassi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Development of Multi-level Linguistic Alignment in Child-adult Conversations","tldr":"Interactive alignment is a major mechanism of linguistic coordination. Here we study the way this mechanism emerges in development across the lexical, syntactic, and conceptual levels. We leverage NLP tools to analyze a large-scale corpus of child-ad...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.7","presentation_id":"","rocketchat_channel":"paper-cmcl2020-7","speakers":"Thomas Misiek|Benoit Favre|Abdellah Fourtassi","title":"Development of Multi-level Linguistic Alignment in Child-adult Conversations"},{"content":{"abstract":"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.","authors":["Kate McCurdy","Adam Lopez","Sharon Goldwater"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection","tldr":"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This sugges...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.8","presentation_id":"","rocketchat_channel":"paper-cmcl2020-8","speakers":"Kate McCurdy|Adam Lopez|Sharon Goldwater","title":"Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection"},{"content":{"abstract":"Case is an abstract grammatical feature that indicates argument relationship in a sentence. In English, cases are expressed on pronouns, as nominative case (e.g. I, he), accusative case (e.g. me, him) and genitive case (e.g. my, his). Children correctly use cased pronouns at a very young age. How do they acquire abstract case in the first place, when different cases are not associated with different meanings? This paper proposes that the distributional patterns in parents\u2019 input could be used to distinguish grammatical cases in English.","authors":["Xiaomeng Ma","Martin Chodorow","Virginia Valian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.cmcl-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition","tldr":"Case is an abstract grammatical feature that indicates argument relationship in a sentence. In English, cases are expressed on pronouns, as nominative case (e.g. I, he), accusative case (e.g. me, him) and genitive case (e.g. my, his). Children correc...","track":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)"},"id":"WS-5.2020.cmcl-1.9","presentation_id":"","rocketchat_channel":"paper-cmcl2020-9","speakers":"Xiaomeng Ma|Martin Chodorow|Virginia Valian","title":"Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition"}],"prerecorded_talks":[{"presentation_id":"38939684","speakers":"Suzanne Stevenson","title":"How Languages Carve Up the World: Cognitive Explanation through Computational Modeling"},{"presentation_id":"38939685","speakers":"Richard Futrell","title":"Information Processing, Communication, and Word Order"}],"rocketchat_channel":"workshop-cmcl2020","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 13:45:00 GMT","hosts":"Emmanuele Chersoni","link":"","session_name":"Introduction","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:45:00 GMT","hosts":"Yohei Oseki","link":"","session_name":"Keynote talk by Suzanne Stevenson","start_time":"Thu, 19 Nov 2020 13:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:15:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 14:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:15:00 GMT","hosts":"Laurent Pr\u00e9vot","link":"","session_name":"Session 1: Oral Presentations","start_time":"Thu, 19 Nov 2020 15:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:45:00 GMT","hosts":"TBD","link":"","session_name":"What Determines the Order of Verbal Dependents in Hindi? Effects of Efficiency in Comprehension and Production","start_time":"Thu, 19 Nov 2020 15:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:15:00 GMT","hosts":"TBD","link":"","session_name":"Images and Imagination: Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder","start_time":"Thu, 19 Nov 2020 15:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"TBD","link":"","session_name":"Lunch Break","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:45:00 GMT","hosts":"Nora Hollenstein","link":"","session_name":"Poster Booster","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"Nora Hollenstein","link":"","session_name":"Poster Session in gather.town","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Production-based Cognitive Models as a Test Suite for Reinforcement Learning Algorithms","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Evaluating Word Embeddings for Language Acquisition","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Guessing the Age of Acquisition of Italian Lemmas through Linear Regression","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Word Co-Occurrence in Child Directed Speech Predicts Children's Free Word Associations","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Development of Multilevel Linguistic Alignment in Child-Adult Conversations","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Conditioning, but on Which Distributions? Grammatical Gender in German Plural Inflection","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Probabilistic Weighting of Perspectives in Dyadic Communication (cross-submission)","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"TBD","link":"","session_name":"Inferring Symmetry in Natural Language (Findings of EMNLP)","start_time":"Thu, 19 Nov 2020 17:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:45:00 GMT","hosts":"Nora Hollenstein","link":"","session_name":"Session 2: Oral Presentations","start_time":"Thu, 19 Nov 2020 18:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:15:00 GMT","hosts":"TBD","link":"","session_name":"Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition","start_time":"Thu, 19 Nov 2020 18:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:45:00 GMT","hosts":"TBD","link":"","session_name":"Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling","start_time":"Thu, 19 Nov 2020 19:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:45:00 GMT","hosts":"Enrico Santus","link":"","session_name":"Keynote talk by Richard Futrell","start_time":"Thu, 19 Nov 2020 19:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":"Enrico Santus","link":"","session_name":"Closing Remarks","start_time":"Thu, 19 Nov 2020 20:45:00 GMT"}],"title":"Workshop on Cognitive Modeling and Computational Linguistics (CMCL)","website":"https://cmclorg.github.io/","zoom_links":["https://zoom.us"]},{"abstract":"Map natural language to meaning representations executed in databases, knowledge graphs, robotic environment and software applications.","blocks":[{"end_time":"Fri, 20 Nov 2020 01:35:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"}],"id":"WS-6","livestream":null,"organizers":"Ben Bogin, Srinivasan Iyer, Xi Victoria Lin, Panupong Pasupat, Alane Suhr, Pengcheng Yin, Tao Yu, Rui Zhang, Victor Zhong, Caiming Xiong and Dragomir Radev","papers":[{"content":{"abstract":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google Assistant on edge devices with limited memory budgets. We propose to learn compositional code embeddings to greatly reduce the sizes of BERT-base and RoBERTa-base. We also apply the technique to DistilBERT, ALBERT-base, and ALBERT-large, three already compressed BERT variants which attain similar state-of-the-art performances on semantic parsing with much smaller model sizes. We observe 95.15% 98.46% embedding compression rates and 20.47% 34.22% encoder compression rates, while preserving >97.5% semantic parsing performances. We provide the recipe for training and analyze the trade-off between code embedding sizes and downstream performances.","authors":["Prafull Prakash","Saurabh Kumar Shashidhar","Wenlong Zhao","Subendhu Rongali","Haidar Khan","Michael Kayser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.423","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings","tldr":"The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google A...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.3476","presentation_id":"38940116","rocketchat_channel":"paper-intex-sempar2020-3476","speakers":"Prafull Prakash|Saurabh Kumar Shashidhar|Wenlong Zhao|Subendhu Rongali|Haidar Khan|Michael Kayser","title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings"},{"content":{"abstract":"Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.","authors":["Siddharth Karamcheti","Dorsa Sadigh","Percy Liang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Adaptive Language Interfaces through Decomposition","tldr":"Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions th...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.10","presentation_id":"38939456","rocketchat_channel":"paper-intex-sempar2020-10","speakers":"Siddharth Karamcheti|Dorsa Sadigh|Percy Liang","title":"Learning Adaptive Language Interfaces through Decomposition"},{"content":{"abstract":"Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL\u2019s superior performance extends to well-formed text, achieving an 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.","authors":["Karthik Radhakrishnan","Arvind Srikantan","Xi Victoria Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ColloQL: Robust Text-to-SQL Over Search Queries","tldr":"Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.11","presentation_id":"38939457","rocketchat_channel":"paper-intex-sempar2020-11","speakers":"Karthik Radhakrishnan|Arvind Srikantan|Xi Victoria Lin","title":"ColloQL: Robust Text-to-SQL Over Search Queries"},{"content":{"abstract":"Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL shared task organized in the IntEx-SemPar workshop at EMNLP 2020. We have trained a number of Neural Machine Translation (NMT) models to efficiently generate the natural language responses from SQL. Our shuffled back-translation model has led to a BLEU score of 7.47 on the unknown test dataset. In this paper, we will discuss our methodologies to approach the problem and future directions to improve the quality of the generated natural language responses.","authors":["Saptarashmi Bandyopadhyay","Tianyang Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Natural Language Response Generation from SQL with Generalization and Back-translation","tldr":"Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.12","presentation_id":"38939458","rocketchat_channel":"paper-intex-sempar2020-12","speakers":"Saptarashmi Bandyopadhyay|Tianyang Zhao","title":"Natural Language Response Generation from SQL with Generalization and Back-translation"},{"content":{"abstract":"","authors":["Tao Yu","Chien-Sheng Wu","Xi Victoria Lin","Bailin Wang","Yi Chern Tan","Xinyi Yang","Dragomir Radev","Richard Socher","Caiming Xiong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GRAPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.13","presentation_id":"38939459","rocketchat_channel":"paper-intex-sempar2020-13","speakers":"Tao Yu|Chien-Sheng Wu|Xi Victoria Lin|Bailin Wang|Yi Chern Tan|Xinyi Yang|Dragomir Radev|Richard Socher|Caiming Xiong","title":"GRAPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing"},{"content":{"abstract":"","authors":["Yu Gu","Sue Kase","Michelle Vanni","Brian Sadler","Percy Liang","Xifeng Yan","Yu Su"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Re-thinking Open-domain Semantic Parsing","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.14","presentation_id":"38939460","rocketchat_channel":"paper-intex-sempar2020-14","speakers":"Yu Gu|Sue Kase|Michelle Vanni|Brian Sadler|Percy Liang|Xifeng Yan|Yu Su","title":"Re-thinking Open-domain Semantic Parsing"},{"content":{"abstract":"","authors":["Yusen Zhang","Xiangyu Dong","Shuaichen Chang","Tao Yu","Peng Shi","Rui Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL","tldr":null,"track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.15","presentation_id":"38939461","rocketchat_channel":"paper-intex-sempar2020-15","speakers":"Yusen Zhang|Xiangyu Dong|Shuaichen Chang|Tao Yu|Peng Shi|Rui Zhang","title":"Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL"},{"content":{"abstract":"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their \u201cblack box\u201d nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.","authors":["Saeedeh Shekarpour","Abhishek Nadgeri","Kuldeep Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph","tldr":"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide accep...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.7","presentation_id":"38939453","rocketchat_channel":"paper-intex-sempar2020-7","speakers":"Saeedeh Shekarpour|Abhishek Nadgeri|Kuldeep Singh","title":"QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph"},{"content":{"abstract":"Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit customer data handled by annotators. In this paper, we propose uncertainty and traffic-aware active learning, a novel active learning method that uses model confidence and utterance frequencies from customer traffic to select utterances for annotation. We show that our method significantly outperforms baselines on an internal customer dataset and the Facebook Task Oriented Parsing (TOP) dataset. On our internal dataset, our method achieves the same accuracy as random sampling with 2,000 fewer annotations.","authors":["Priyanka Sen","Emine Yilmaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncertainty and Traffic-Aware Active Learning for Semantic Parsing","tldr":"Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit c...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.8","presentation_id":"38939454","rocketchat_channel":"paper-intex-sempar2020-8","speakers":"Priyanka Sen|Emine Yilmaz","title":"Uncertainty and Traffic-Aware Active Learning for Semantic Parsing"},{"content":{"abstract":"Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to address TOP. In this paper, we propose a new format of meaning representation that is more compact and amenable to sequence-to-sequence (seq-to-seq) models. A simple copy-augmented seq-to-seq parser is built and evaluated over a public TOP dataset, resulting in 3.44% improvement over prior best seq-to-seq parser (exact match accuracy), which is also comparable to constituency parsers\u2019 performance.","authors":["Chaoting Xuan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.intexsempar-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog","tldr":"Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to add...","track":"Interactive and Executable Semantic Parsing (Int-Ex)"},"id":"WS-6.9","presentation_id":"38939455","rocketchat_channel":"paper-intex-sempar2020-9","speakers":"Chaoting Xuan","title":"Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog"}],"prerecorded_talks":[{"presentation_id":"38939462","speakers":"Jacob Andreas","title":"Invited Talk: Jacob Andreas"},{"presentation_id":"38939448","speakers":"Jonathan Berant","title":"Invited Talk: Jonathan Berant"},{"presentation_id":"38939449","speakers":"Yoav Artzi","title":"Invited Talk: Yoav Artzi"},{"presentation_id":"38939450","speakers":"Dilek Hakkani-T\u00fcr","title":"Invited Talk: Dilek Hakkani-T\u00fcr"},{"presentation_id":"38939451","speakers":"Alex Polozov","title":"Invited Talk: Alex Polozov"},{"presentation_id":"38939452","speakers":"Richard Socher","title":"Invited Talk: Richard Socher"}],"rocketchat_channel":"workshop-intex-sempar2020","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 16:30:00 GMT","hosts":"Victor Zhong, Victoria Lin, Rui Zhang","link":"","session_name":"Opening Remarks","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"Ben Bogin","link":"","session_name":"Invited Talk: Jacob Andreas","start_time":"Thu, 19 Nov 2020 16:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:30:00 GMT","hosts":"Ben Bogin","link":"","session_name":"Invited Talk: Jonathan Berant","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:50:00 GMT","hosts":"N/A","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 18:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"Rui Zhang","link":"","session_name":"Learning Adaptive Language Interfaces through Decomposition","start_time":"Thu, 19 Nov 2020 18:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:10:00 GMT","hosts":"Rui Zhang","link":"","session_name":"Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:20:00 GMT","hosts":"Rui Zhang","link":"","session_name":"Uncertainty and Traffic-Aware Active Learning for Semantic Parsing","start_time":"Thu, 19 Nov 2020 19:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:30:00 GMT","hosts":"Rui Zhang","link":"","session_name":"Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL","start_time":"Thu, 19 Nov 2020 19:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:30:00 GMT","hosts":"Rui Zhang","link":"","session_name":"Invited Talk: Yoav Artzi","start_time":"Thu, 19 Nov 2020 19:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:30:00 GMT","hosts":"N/A","link":"","session_name":"Poster Presentation in gather.town","start_time":"Thu, 19 Nov 2020 20:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:30:00 GMT","hosts":"Victoria Lin","link":"","session_name":"Invited Talk: Dilek Hakkani-T\u00fcr","start_time":"Thu, 19 Nov 2020 21:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:40:00 GMT","hosts":"Victoria Lin","link":"","session_name":"QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph","start_time":"Thu, 19 Nov 2020 22:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:50:00 GMT","hosts":"Victoria Lin","link":"","session_name":"ColloQL: Robust Text-to-SQL Over Search Queries","start_time":"Thu, 19 Nov 2020 22:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:00:00 GMT","hosts":"Victoria Lin","link":"","session_name":"GRAPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing","start_time":"Thu, 19 Nov 2020 22:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:10:00 GMT","hosts":"Victoria Lin","link":"","session_name":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases","start_time":"Thu, 19 Nov 2020 23:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:20:00 GMT","hosts":"Victoria Lin","link":"","session_name":"Natural Language Response Generation from SQL with Generalization and Back-translation","start_time":"Thu, 19 Nov 2020 23:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:30:00 GMT","hosts":"N/A","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 23:20:00 GMT"},{"end_time":"Fri, 20 Nov 2020 00:30:00 GMT","hosts":"Victor Zhong","link":"","session_name":"Invited Talk: Alex Polozov","start_time":"Thu, 19 Nov 2020 23:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 01:30:00 GMT","hosts":"Victor Zhong","link":"","session_name":"Invited Talk: Richard Socher","start_time":"Fri, 20 Nov 2020 00:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 01:35:00 GMT","hosts":"Victor Zhong, Victoria Lin, Rui Zhang","link":"","session_name":"Closing remarks","start_time":"Fri, 20 Nov 2020 01:30:00 GMT"}],"title":"Interactive and Executable Semantic Parsing (Int-Ex)","website":"https://intex-sempar.github.io/","zoom_links":["https://zoom.us"]},{"abstract":"SDP is a full day workshop that provides an interdisciplinary venue for researchers interested in any aspect of mining scientific literature. SDP includes a research track and three shared tasks: 6th CL-SciSumm, 1st LongSumm, 1st LaySumm.","blocks":[{"end_time":"Thu, 19 Nov 2020 22:10:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:45:00 GMT"}],"id":"WS-7","livestream":null,"organizers":"Muthu Kumar Chandrasekaran, Guy Feigenblat, Dayne Freitag, Tirthankar Ghosal, Michal Shmueli-Scheuer, Eduard Hovy, Petr Knoth, David Konopnicki, Philipp Mayr, Robert Patton, Dominika Tkaczyk and Anita de Waard","papers":[{"content":{"abstract":"Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global information. Specifically, we include citing article\u2019s title and abstract into context representation. We evaluate our model on datasets with different citation context sizes and demonstrate improvements with globally-enhanced context representations when citation contexts are smaller.","authors":["Zoran Medi\u0107","Jan Snajder"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information","tldr":"Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global informati...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.14","presentation_id":"38940720","rocketchat_channel":"paper-sdp2020-14","speakers":"Zoran Medi\u0107|Jan Snajder","title":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information"},{"content":{"abstract":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we study translational research at the level of scientific concepts for all scientific fields. We do this through text mining and predictive modeling using three corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28 million clinical trials. We extract scientific concepts (i.e., phrases) from corpora as instantiations of \u201cresearch ideas\u201d, create concept-level features as motivated by literature, and then follow the trajectories of over 450,000 new concepts (emerged from 1995-2014) to identify factors that lead only a small proportion of these ideas to be used in inventions and drug trials. Results from our analysis suggest several mechanisms that distinguish which scientific concept will be adopted in practice, and which will not. We also demonstrate that our derived features can be used to explain and predict knowledge transfer with high accuracy. Our work provides greater understanding of knowledge transfer for researchers, practitioners, and government agencies interested in encouraging translational research.","authors":["Hancheng Cao","Mengjie Cheng","Zhepeng Cen","Daniel McFarland","Xiang Ren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.158","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora","tldr":"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.1457","presentation_id":"38940721","rocketchat_channel":"paper-sdp2020-1457","speakers":"Hancheng Cao|Mengjie Cheng|Zhepeng Cen|Daniel McFarland|Xiang Ren","title":"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora"},{"content":{"abstract":"Our system participates in two shared tasks, CL-SciSumm 2020 and LongSumm 2020. In the CL-SciSumm shared task, based on our previous work, we apply more machine learning methods on position features and content features for facet classification in Task1B. And GCN is introduced in Task2 to perform extractive summarization. In the LongSumm shared task, we integrate both the extractive and abstractive summarization ways. Three methods were tested which are T5 Fine-tuning, DPPs Sampling, and GRU-GCN/GAT.","authors":["Lei Li","Yang Xie","Wei Liu","Yinan Liu","Yafei Jiang","Siya Qi","Xingyuan Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization","tldr":"Our system participates in two shared tasks, CL-SciSumm 2020 and LongSumm 2020. In the CL-SciSumm shared task, based on our previous work, we apply more machine learning methods on position features and content features for facet classification in Ta...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.15shared","presentation_id":"38940743","rocketchat_channel":"paper-sdp2020-15shared","speakers":"Lei Li|Yang Xie|Wei Liu|Yinan Liu|Yafei Jiang|Siya Qi|Xingyuan Li","title":"CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization"},{"content":{"abstract":"We introduce SciWING, an open-source soft-ware toolkit which provides access to state-of-the-art pre-trained models for scientific document processing (SDP) tasks, such as citation string parsing, logical structure recovery and citation intent classification. Compared to other toolkits, SciWING follows a full neural pipeline and provides a Python inter-face for SDP. When needed, SciWING provides fine-grained control for rapid experimentation with different models by swapping and stacking different modules. Transfer learning from general and scientific documents specific pre-trained transformers (i.e., BERT, SciBERT, etc.) can be performed. SciWING incorporates ready-to-use web and terminal-based applications and demonstrations to aid adoption and development. The toolkit is available from http://sciwing.io and the demos are available at http://rebrand.ly/sciwing-demo.","authors":["Abhinav Ramesh Kashyap","Min-Yen Kan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SciWING\u2013 A Software Toolkit for Scientific Document Processing","tldr":"We introduce SciWING, an open-source soft-ware toolkit which provides access to state-of-the-art pre-trained models for scientific document processing (SDP) tasks, such as citation string parsing, logical structure recovery and citation intent classi...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.17","presentation_id":"38940731","rocketchat_channel":"paper-sdp2020-17","speakers":"Abhinav Ramesh Kashyap|Min-Yen Kan","title":"SciWING\u2013 A Software Toolkit for Scientific Document Processing"},{"content":{"abstract":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in scientific papers focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of medical images in context. MedICaT consists of 217K images from 131K open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.","authors":["Sanjay Subramanian","Lucy Lu Wang","Ben Bogin","Sachin Mehta","Madeleine van Zuylen","Sravanthi Parasa","Sameer Singh","Matt Gardner","Hannaneh Hajishirzi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.191","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References","tldr":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describin...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.1728","presentation_id":"38940723","rocketchat_channel":"paper-sdp2020-1728","speakers":"Sanjay Subramanian|Lucy Lu Wang|Ben Bogin|Sachin Mehta|Madeleine van Zuylen|Sravanthi Parasa|Sameer Singh|Matt Gardner|Hannaneh Hajishirzi","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"content":{"abstract":"We focus on systems for TASK1 (TASK 1A and TASK 1B) of CL-SciSumm Shared Task 2020 in this paper. Task 1A is regarded as a binary classification task of sentence pairs. The strategies of domain-specific embedding and special tokens based on language models are proposed. Fusion of contextualized embedding and extra information is further explored in this article. We leverage Sembert to capture the structured semantic information. The joint of BERT-based model and classifiers without neural networks is also exploited. For the Task 1B, a language model with different weights for classes is fine-tuned to accomplish a multi-label classification task. The results show that extra information can improve the identification of cited text spans. The end-to-end trained models outperform models trained with two stages, and the averaged prediction of multi-models is more accurate than an individual one.","authors":["Ling Chai","Guizhen Fu","Yuan Ni"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP-PINGAN-TECH @ CL-SciSumm 2020","tldr":"We focus on systems for TASK1 (TASK 1A and TASK 1B) of CL-SciSumm Shared Task 2020 in this paper. Task 1A is regarded as a binary classification task of sentence pairs. The strategies of domain-specific embedding and special tokens based on language ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.18","presentation_id":"38941223","rocketchat_channel":"paper-sdp2020-18","speakers":"Ling Chai|Guizhen Fu|Yuan Ni","title":"NLP-PINGAN-TECH @ CL-SciSumm 2020"},{"content":{"abstract":"Lay summarization aims to generate lay summaries of scientific papers automatically. It is an essential task that can increase the relevance of science for all of society. In this paper, we build a lay summary generation system based on BART model. We leverage sentence labels as extra supervision signals to improve the performance of lay summarization. In the CL-LaySumm 2020 shared task, our model achieves 46.00 Rouge1-F1 score.","authors":["Tiezheng Yu","Dan Su","Wenliang Dai","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dimsum @LaySumm 20","tldr":"Lay summarization aims to generate lay summaries of scientific papers automatically. It is an essential task that can increase the relevance of science for all of society. In this paper, we build a lay summary generation system based on BART model. W...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.20shared","presentation_id":"38940741","rocketchat_channel":"paper-sdp2020-20shared","speakers":"Tiezheng Yu|Dan Su|Wenliang Dai|Pascale Fung","title":"Dimsum @LaySumm 20"},{"content":{"abstract":"Automatic prediction on the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures. We propose a multi-task shared structure encoding approach which automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naive multi-task methods.","authors":["Jiyi Li","Ayaka Sato","Kazuya Shimura","Fumiyo Fukumoto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-task Peer-Review Score Prediction","tldr":"Automatic prediction on the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.21","presentation_id":"38940727","rocketchat_channel":"paper-sdp2020-21","speakers":"Jiyi Li|Ayaka Sato|Kazuya Shimura|Fumiyo Fukumoto","title":"Multi-task Peer-Review Score Prediction"},{"content":{"abstract":"We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from the Open Source CORD-19 dataset by fully automating the procedure of information extraction using SciBERT. The best latent entity representations are then found by benchnmarking different KG embedding techniques on the task of link prediction using a Graph Convolution Network Auto Encoder (GCN-AE). We demonstrate the utility of ERLKG with respect to COVID-19 through multiple qualitative evaluations. Due to the lack of a gold standard, we propose a relatively large intrinsic evaluation dataset for COVID-19 and use it for validating the top two performing KG embedding techniques. We find TransD to be the best performing KG embedding technique with Pearson and Spearman correlation scores of 0.4348 and 0.4570 respectively. We demonstrate that a considerable number of ERLKG\u2019s top protein, chemical and disease predictions are currently in consideration for COVID-19 related research.","authors":["Sayantan Basu","Sinchani Chakraborty","Atif Hassan","Sana Siddique","Ashish Anand"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ERLKG: Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora","tldr":"We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.22","presentation_id":"38940725","rocketchat_channel":"paper-sdp2020-22","speakers":"Sayantan Basu|Sinchani Chakraborty|Atif Hassan|Sana Siddique|Ashish Anand","title":"ERLKG: Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora"},{"content":{"abstract":"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, is necessary to enable computers to understand and retrieve information from the documents. However, as we will show in this project, a mathematical notation can change its meaning even within the scope of a single paragraph. This flexibility makes it difficult to extract the exact meaning of a mathematical formula. In this project, we will propose a new task direction for grounding mathematical formulae. Particularly, we are addressing the widespread misconception of various research projects in mathematical information retrieval, which presume that mathematical notations have a fixed meaning within a single document. We manually annotated a long scientific paper to illustrate the task concept. Our high inter-annotator agreement shows that the task is well understood for humans. Our results indicate that it is worthwhile to grow the techniques for the proposed task to contribute to the further progress of mathematical language processing.","authors":["Takuto Asakura","Andr\u00e9 Greiner-Petter","Akiko Aizawa","Yusuke Miyao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Grounding of Formulae","tldr":"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.24","presentation_id":"38940733","rocketchat_channel":"paper-sdp2020-24","speakers":"Takuto Asakura|Andr\u00e9 Greiner-Petter|Akiko Aizawa|Yusuke Miyao","title":"Towards Grounding of Formulae"},{"content":{"abstract":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets (e.g., disease, mutation) that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: (a). document-query matching (b). keyword extraction and (c). facet-conditioned abstractive summarization. The outcomes of (b) and (c) are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component (a) directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST\u2019s TREC-PM track datasets (2017\u20132019) show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: https://github.com/bionlproc/text-summ-for-doc-retrieval.","authors":["Jiho Noh","Ramakanth Kavuluru"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.304","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization","tldr":"Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patie...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2502","presentation_id":"38940722","rocketchat_channel":"paper-sdp2020-2502","speakers":"Jiho Noh|Ramakanth Kavuluru","title":"Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization"},{"content":{"abstract":"Author name disambiguation (AND) algorithms identify a unique author entity record from all similar or same publication records in scholarly or similar databases. Typically, a clustering method is used that requires calculation of similarities between each possible record pair. However, the total number of pairs grows quadratically with the size of the author database making such clustering difficult for millions of records. One remedy is a blocking function that reduces the number of pairwise similarity calculations. Here, we introduce a new way of learning blocking schemes by using a conjunctive normal form (CNF) in contrast to the disjunctive normal form (DNF). We demonstrate on PubMed author records that CNF blocking reduces more pairs while preserving high pairs completeness compared to the previous methods that use a DNF and that the computation time is significantly reduced. In addition, we also show how to ensure that the method produces disjoint blocks so that much of the AND algorithm can be efficiently paralleled. Our CNF blocking method is tested on the entire PubMed database of 80 million author mentions and efficiently removes 82.17% of all author record pairs in 10 minutes.","authors":["Kunho Kim","Athar Sefid","C. Lee Giles"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning CNF Blocking for Large-scale Author Name Disambiguation","tldr":"Author name disambiguation (AND) algorithms identify a unique author entity record from all similar or same publication records in scholarly or similar databases. Typically, a clustering method is used that requires calculation of similarities betwee...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.26","presentation_id":"38940717","rocketchat_channel":"paper-sdp2020-26","speakers":"Kunho Kim|Athar Sefid|C. Lee Giles","title":"Learning CNF Blocking for Large-scale Author Name Disambiguation"},{"content":{"abstract":"Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information, it also has a wider use as an imperfect proxy for quality which has the advantage of being cheaply available for large volumes of scholarly documents. Previous work has dealt with number of citations prediction with relatively small training data sets, or larger datasets but with short, incomplete input text. In this work we leverage the open access ACL Anthology collection in combination with the Semantic Scholar bibliometric database to create a large corpus of scholarly documents with associated citation information and we propose a new citation prediction model called SChuBERT. In our experiments we compare SChuBERT with several state-of-the-art citation prediction models and show that it outperforms previous methods by a large margin. We also show the merit of using more training data and longer input for number of citations prediction.","authors":["Thomas van Dongen","Gideon Maillette de Buy Wenniger","Lambert Schomaker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction.","tldr":"Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information, it also has a wider use as an imperfect proxy for quality which has the advantage of bein...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.27","presentation_id":"38940730","rocketchat_channel":"paper-sdp2020-27","speakers":"Thomas van Dongen|Gideon Maillette de Buy Wenniger|Lambert Schomaker","title":"SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction."},{"content":{"abstract":"Training recurrent neural networks on long texts, in particular scholarly documents, causes problems for learning. While hierarchical attention networks (HANs) are effective in solving these problems, they still lose important information about the structure of the text. To tackle these problems, we propose the use of HANs combined with structure-tags which mark the role of sentences in the document. Adding tags to sentences, marking them as corresponding to title, abstract or main body text, yields improvements over the state-of-the-art for scholarly document quality prediction. The proposed system is applied to the task of accept/reject prediction on the PeerRead dataset and compared against a recent BiLSTM-based model and joint textual+visual model as well as against plain HANs. Compared to plain HANs, accuracy increases on all three domains.On the computation and language domain our new model works best overall, and increases accuracy 4.7% over the best literature result. We also obtain improvements when introducing the tags for prediction of the number of citations for 88k scientific publications that we compiled from the Allen AI S2ORC dataset. For our HAN-system with structure-tags we reach 28.5% explained variance, an improvement of 1.8% over our reimplementation of the BiLSTM-based model as well as 1.0% improvement over plain HANs.","authors":["Gideon Maillette de Buy Wenniger","Thomas van Dongen","Eleri Aedmaa","Herbert Teun Kruitbosch","Edwin A. Valentijn","Lambert Schomaker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structure-Tags Improve Text Classification for Scholarly Document Quality Prediction","tldr":"Training recurrent neural networks on long texts, in particular scholarly documents, causes problems for learning. While hierarchical attention networks (HANs) are effective in solving these problems, they still lose important information about the s...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.29","presentation_id":"38940732","rocketchat_channel":"paper-sdp2020-29","speakers":"Gideon Maillette de Buy Wenniger|Thomas van Dongen|Eleri Aedmaa|Herbert Teun Kruitbosch|Edwin A. Valentijn|Lambert Schomaker","title":"Structure-Tags Improve Text Classification for Scholarly Document Quality Prediction"},{"content":{"abstract":"Cydex is a platform that provides neural search infrastructure for domain-specific scholarly literature. The platform represents an abstraction of Covidex, our recently developed full-stack open-source search engine for the COVID-19 Open Research Dataset (CORD-19) from AI2. While Covidex takes advantage of the latest best practices for keyword search using the popular Lucene search library as well as state-of-the-art neural ranking models using T5, parts of the system were hard coded to only work with CORD-19. This paper describes our efforts to generalize Covidex into Cydex, which can be applied to scholarly literature in different domains. By decoupling corpus-specific configurations from the frontend implementation, we are able to demonstrate the generality of Cydex on two very different corpora: the ACL Anthology and a collection of hydrology abstracts. Our platform is entirely open source and available at cydex.ai.","authors":["Shane Ding","Edwin Zhang","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cydex: Neural Search Infrastructure for the Scholarly Literature","tldr":"Cydex is a platform that provides neural search infrastructure for domain-specific scholarly literature. The platform represents an abstraction of Covidex, our recently developed full-stack open-source search engine for the COVID-19 Open Research Dat...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.30","presentation_id":"38940734","rocketchat_channel":"paper-sdp2020-30","speakers":"Shane Ding|Edwin Zhang|Jimmy Lin","title":"Cydex: Neural Search Infrastructure for the Scholarly Literature"},{"content":{"abstract":"Automatically generating question answer (QA) pairs from the rapidly growing coronavirus-related literature is of great value to the medical community. Creating high quality QA pairs would allow researchers to build models to address scientific queries for answers which are not readily available in support of the ongoing fight against the pandemic. QA pair generation is, however, a very tedious and time consuming task requiring domain expertise for annotation and evaluation. In this paper we present our contribution in addressing some of the challenges of building a QA system without gold data. We first present a method to create QA pairs from a large semi-structured dataset through the use of transformer and rule-based models. Next, we propose a means of engaging subject matter experts (SMEs) for annotating the QA pairs through the usage of a web application. Finally, we demonstrate some experiments showcasing the effectiveness of leveraging active learning in designing a high performing model with a substantially lower annotation effort from the domain experts.","authors":["Rohan Bhambhoria","Luna Feng","Dawn Sepehr","John Chen","Conner Cowling","Sedef Kocak","Elham Dolatabadi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Smart System to Generate and Validate Question Answer Pairs for COVID-19 Literature","tldr":"Automatically generating question answer (QA) pairs from the rapidly growing coronavirus-related literature is of great value to the medical community. Creating high quality QA pairs would allow researchers to build models to address scientific queri...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.32","presentation_id":"38940713","rocketchat_channel":"paper-sdp2020-32","speakers":"Rohan Bhambhoria|Luna Feng|Dawn Sepehr|John Chen|Conner Cowling|Sedef Kocak|Elham Dolatabadi","title":"A Smart System to Generate and Validate Question Answer Pairs for COVID-19 Literature"},{"content":{"abstract":"Despite the advancements in search engine features, ranking methods, technologies, and the availability of programmable APIs, current-day open-access digital libraries still rely on crawl-based approaches for acquiring their underlying document collections. In this paper, we propose a novel search-driven framework for acquiring documents for such scientific portals. Within our framework, publicly-available research paper titles and author names are used as queries to a Web search engine. We were able to obtain ~267,000 unique research papers through our fully-automated framework using ~76,000 queries, resulting in almost 200,000 more papers than the number of queries. Moreover, through a combination of title and author name search, we were able to recover 78% of the original searched titles.","authors":["Krutarth Patel","Cornelia Caragea","Sujatha Das Gollapalli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Use of Web Search to Improve Scientific Collections","tldr":"Despite the advancements in search engine features, ranking methods, technologies, and the availability of programmable APIs, current-day open-access digital libraries still rely on crawl-based approaches for acquiring their underlying document colle...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.35","presentation_id":"38940728","rocketchat_channel":"paper-sdp2020-35","speakers":"Krutarth Patel|Cornelia Caragea|Sujatha Das Gollapalli","title":"On the Use of Web Search to Improve Scientific Collections"},{"content":{"abstract":"Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.","authors":["Seraphina Goldfarb-Tarrant","Alexander Robertson","Jasmina Lazic","Theodora Tsouloufi","Louise Donnison","Karen Smyth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scaling Systematic Literature Reviews with Machine Learning Pipelines","tldr":"Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.36","presentation_id":"38940729","rocketchat_channel":"paper-sdp2020-36","speakers":"Seraphina Goldfarb-Tarrant|Alexander Robertson|Jasmina Lazic|Theodora Tsouloufi|Louise Donnison|Karen Smyth","title":"Scaling Systematic Literature Reviews with Machine Learning Pipelines"},{"content":{"abstract":"In this paper, we tack lay summarization tasks, which aim to automatically produce lay summaries for scientific papers, to participate in the first CL-LaySumm 2020 in SDP workshop at EMNLP 2020. We present our approach of using Pre-training with Extracted Gap-sentences for Abstractive Summarization (PEGASUS; Zhang et al., 2019b) to produce the lay summary and combining those with the extractive summarization model using Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) and readability metrics that measure the readability of the sentence to further improve the quality of the summary. Our model achieves a remarkable performance on ROUGE metrics, demonstrating the produced summary is more readable while it summarizes the main points of the document.","authors":["Seungwon Kim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Pre-Trained Transformer for Better Lay Summarization","tldr":"In this paper, we tack lay summarization tasks, which aim to automatically produce lay summaries for scientific papers, to participate in the first CL-LaySumm 2020 in SDP workshop at EMNLP 2020. We present our approach of using Pre-training with Extr...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.37shared","presentation_id":"38940740","rocketchat_channel":"paper-sdp2020-37shared","speakers":"Seungwon Kim","title":"Using Pre-Trained Transformer for Better Lay Summarization"},{"content":{"abstract":"Acknowledgements are ubiquitous in scholarly papers. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing sentence structure. We develop an acknowledgement extraction system, AckExtract based on open-source text mining software and evaluate our method using manually labeled data. AckExtract uses the PDF of a scholarly paper as input and outputs acknowledgement entities. Results show an overall performance of F_1=0.92. We built a supplementary database by linking CORD-19 papers with acknowledgement entities extracted by AckExtract including persons and organizations and find that only up to 50\u201360% of named entities are actually acknowledged. We further analyze chronological trends of acknowledgement entities in CORD-19 papers. All codes and labeled data are publicly available at https://github.com/lamps-lab/ackextract.","authors":["Jian Wu","Pei Wang","Xin Wei","Sarah Rajtmajer","C. Lee Giles","Christopher Griffin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Acknowledgement Entity Recognition in CORD-19 Papers","tldr":"Acknowledgements are ubiquitous in scholarly papers. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing sentence structure....","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.39","presentation_id":"38940712","rocketchat_channel":"paper-sdp2020-39","speakers":"Jian Wu|Pei Wang|Xin Wei|Sarah Rajtmajer|C. Lee Giles|Christopher Griffin","title":"Acknowledgement Entity Recognition in CORD-19 Papers"},{"content":{"abstract":"We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data and trained CNNs to extract eight categories of both textual (titles, abstracts, headers, figure and table captions, and other texts) and non-textural content (figures and tables) from 30 years of IEEE VIS conference papers, of which a third were scanned bitmap PDFs. We curated this dataset and named it VISpaper-3K. We then showed our initial benchmark performance using VISpaper-3K over itself and CS-150 using YOLOv3 and Faster-RCNN. We open-source DeepPaperComposer of our training data generation and released the resulting annotation data VISpaper-3K to promote re-producible research.","authors":["Meng Ling","Jian Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers","tldr":"We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.40","presentation_id":"38940719","rocketchat_channel":"paper-sdp2020-40","speakers":"Meng Ling|Jian Chen","title":"DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers"},{"content":{"abstract":"The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.","authors":["Dongyeop Kang","Andrew Head","Risham Sidhu","Kyle Lo","Daniel Weld","Marti A. Hearst"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions","tldr":"The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate e...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.42","presentation_id":"38940724","rocketchat_channel":"paper-sdp2020-42","speakers":"Dongyeop Kang|Andrew Head|Risham Sidhu|Kyle Lo|Daniel Weld|Marti A. Hearst","title":"Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions"},{"content":{"abstract":"In this paper, we present the IIIT Bhagalpur and IIT Patna team\u2019s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries, respectively, for scientific articles. For the first two tasks, unsupervised systems are developed, while for the third one, we develop a supervised system.The performances of all the systems were evaluated on the associated datasets with the shared tasks in term of well-known ROUGE metric.","authors":["Saichethan Reddy","Naveen Saini","Sriparna Saha","Pushpak Bhattacharyya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIITBH-IITP@CL-SciSumm20, CL-LaySumm20, LongSumm20","tldr":"In this paper, we present the IIIT Bhagalpur and IIT Patna team\u2019s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.43shared","presentation_id":"38940739","rocketchat_channel":"paper-sdp2020-43shared","speakers":"Saichethan Reddy|Naveen Saini|Sriparna Saha|Pushpak Bhattacharyya","title":"IIITBH-IITP@CL-SciSumm20, CL-LaySumm20, LongSumm20"},{"content":{"abstract":"We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological database SABIO-RK, provide a definition of the task, and report findings from preliminary experiments. Rigorous evaluation proved challenging due to lack of gold-standard data and a difficult notion of correctness. Qualitative inspection of results, however, showed the feasibility and usefulness of the task","authors":["Mark-Christoph M\u00fcller","Sucheta Ghosh","Maja Rey","Ulrike Wittig","Wolfgang M\u00fcller","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain","tldr":"We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological d...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.44","presentation_id":"38940718","rocketchat_channel":"paper-sdp2020-44","speakers":"Mark-Christoph M\u00fcller|Sucheta Ghosh|Maja Rey|Ulrike Wittig|Wolfgang M\u00fcller|Michael Strube","title":"Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain"},{"content":{"abstract":"We present the systems we submitted for the shared tasks of the Workshop on Scholarly Document Processing at EMNLP 2020. Our approaches to the tasks are focused on exploiting large Transformer models pre-trained on huge corpora and adapting them to the different shared tasks. For tasks 1A and 1B of CL-SciSumm we are using different variants of the BERT model to tackle the tasks of \u201ccited text span\u201d and \u201cfacet\u201d identification. For the summarization tasks 2 of CL-SciSumm, LaySumm and LongSumm we make use of different variants of the PEGASUS model, with and without fine-tuning, adapted to the nuances of each one of those particular tasks.","authors":["Alexios Gidiotis","Stefanos Stefanidis","Grigorios Tsoumakas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AUTH @ CLSciSumm 20, LaySumm 20, LongSumm 20","tldr":"We present the systems we submitted for the shared tasks of the Workshop on Scholarly Document Processing at EMNLP 2020. Our approaches to the tasks are focused on exploiting large Transformer models pre-trained on huge corpora and adapting them to t...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.45","presentation_id":"38941222","rocketchat_channel":"paper-sdp2020-45","speakers":"Alexios Gidiotis|Stefanos Stefanidis|Grigorios Tsoumakas","title":"AUTH @ CLSciSumm 20, LaySumm 20, LongSumm 20"},{"content":{"abstract":"Automatic text summarization has been widely studied as an important task in natural language processing. Traditionally, various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summarization. Recently, deep learning based, specifically Transformer-based systems have been immensely popular. Summarization is a cognitively challenging task \u2013 extracting summary worthy sentences is laborious, and expressing semantics in brief when doing abstractive summarization is complicated. In this paper, we specifically look at the problem of summarizing scientific research papers from multiple domains. We differentiate between two types of summaries, namely, (a) LaySumm: A very short summary that captures the essence of the research paper in layman terms restricting overtly specific technical jargon and (b) LongSumm: A much longer detailed summary aimed at providing specific insights into various ideas touched upon in the paper. While leveraging latest Transformer-based models, our systems are simple, intuitive and based on how specific paper sections contribute to human summaries of the two types described above. Evaluations against gold standard summaries using ROUGE metrics prove the effectiveness of our approach. On blind test corpora, our system ranks first and third for the LongSumm and LaySumm tasks respectively.","authors":["Sayar Ghosh Roy","Nikhil Pinnaparaju","Risubh Jain","Manish Gupta","Vasudeva Varma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Summaformers @ LaySumm 20, LongSumm 20","tldr":"Automatic text summarization has been widely studied as an important task in natural language processing. Traditionally, various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.48_2shared","presentation_id":"38940742","rocketchat_channel":"paper-sdp2020-48_2shared","speakers":"Sayar Ghosh Roy|Nikhil Pinnaparaju|Risubh Jain|Manish Gupta|Vasudeva Varma","title":"Summaformers @ LaySumm 20, LongSumm 20"},{"content":{"abstract":"","authors":["Sayar Ghosh Roy","Nikhil Pinnaparaju","Risubh Jain","Manish Gupta\u2217","Vasudeva Varma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scientific Document Summarization for LaySumm '20 and LongSumm '2","tldr":null,"track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.48shared","presentation_id":"38940738","rocketchat_channel":"paper-sdp2020-48shared","speakers":"Sayar Ghosh Roy|Nikhil Pinnaparaju|Risubh Jain|Manish Gupta\u2217|Vasudeva Varma","title":"Scientific Document Summarization for LaySumm '20 and LongSumm '2"},{"content":{"abstract":"This work presents the entry by the team from Heidelberg University in the CL-SciSumm 2020 shared task at the Scholarly Document Processing workshop at EMNLP 2020. As in its previous iterations, the task is to highlight relevant parts in a reference paper, depending on a citance text excerpt from a citing paper. We participated in tasks 1A (citation identification) and 1B (citation context classification). Contrary to most previous works, we frame Task 1A as a search relevance problem, and introduce a 2-step re-ranking approach, which consists of a preselection based on BM25 in addition to positional document features, and a top-k re-ranking with BERT. For Task 1B, we follow previous submissions in applying methods that deal well with low resources and imbalanced classes.","authors":["Dennis Aumiller","Satya Almasian","Philip Hausner","Michael Gertz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UniHD@CL-SciSumm 2020: Citation Extraction as Search","tldr":"This work presents the entry by the team from Heidelberg University in the CL-SciSumm 2020 shared task at the Scholarly Document Processing workshop at EMNLP 2020. As in its previous iterations, the task is to highlight relevant parts in a reference ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.49","presentation_id":"38941224","rocketchat_channel":"paper-sdp2020-49","speakers":"Dennis Aumiller|Satya Almasian|Philip Hausner|Michael Gertz","title":"UniHD@CL-SciSumm 2020: Citation Extraction as Search"},{"content":{"abstract":"Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce two small ELECTRA based model named Bio-ELECTRA and Bio-ELECTRA++ that are eight times smaller than BERT Base and Bio-BERT and achieves comparable or better performance on biomedical question answering, yes/no question answer classification, question answer candidate ranking and relation extraction tasks. Bio-ELECTRA is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. Bio-ELECTRA++ is the further pre-trained version of Bio-ELECTRA trained on a corpus of open access full papers from PubMed Central. While, for biomedical named entity recognition, large BERT Base model outperforms Bio-ELECTRA++, Bio-ELECTRA and ELECTRA-Small++, with hyperparameter tuning Bio-ELECTRA++ achieves results comparable to BERT.","authors":["Ibrahim Burak Ozyurt"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining","tldr":"Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.5","presentation_id":"38940735","rocketchat_channel":"paper-sdp2020-5","speakers":"Ibrahim Burak Ozyurt","title":"On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining"},{"content":{"abstract":"In academic publications, citations are used to build context for a concept by highlighting relevant aspects from reference papers. Automatically identifying referenced snippets can help researchers swiftly isolate principal contributions of scientific works. In this paper, we exploit the underlying structure of scientific articles to predict reference paper spans and facets corresponding to a citation. We propose two methods to detect citation spans - keyphrase overlap, BERT along with structural priors. We fine-tune FastText embeddings and leverage textual, positional features to predict citation facets.","authors":["Anjana Umapathy","Karthik Radhakrishnan","Kinjal Jain","Rahul Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CiteQA@CLSciSumm 2020","tldr":"In academic publications, citations are used to build context for a concept by highlighting relevant aspects from reference papers. Automatically identifying referenced snippets can help researchers swiftly isolate principal contributions of scientif...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.54","presentation_id":"38941225","rocketchat_channel":"paper-sdp2020-54","speakers":"Anjana Umapathy|Karthik Radhakrishnan|Kinjal Jain|Rahul Singh","title":"CiteQA@CLSciSumm 2020"},{"content":{"abstract":"This paper presents our methods for the LongSumm 2020: Shared Task on Generating Long Summaries for Scientific Documents, where the task is to generatelong summaries given a set of scientific papers provided by the organizers. We explore 3 main approaches for this task: 1. An extractive approach using a BERT-based summarization model; 2. A two stage model that additionally includes an abstraction step using BART; and 3. A new multi-tasking approach on incorporating document structure into the summarizer. We found that our new multi-tasking approach outperforms the two other methods by large margins. Among 9 participants in the shared task, our best model ranks top according to Rouge-1 score (53.11%) while staying competitive in terms of Rouge-2.","authors":["Sajad Sotudeh Gharebagh","Arman Cohan","Nazli Goharian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GUIR @ LongSumm 2020: Learning to Generate Long Summaries from Scientific Documents","tldr":"This paper presents our methods for the LongSumm 2020: Shared Task on Generating Long Summaries for Scientific Documents, where the task is to generatelong summaries given a set of scientific papers provided by the organizers. We explore 3 main appro...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.56shared","presentation_id":"38940737","rocketchat_channel":"paper-sdp2020-56shared","speakers":"Sajad Sotudeh Gharebagh|Arman Cohan|Nazli Goharian","title":"GUIR @ LongSumm 2020: Learning to Generate Long Summaries from Scientific Documents"},{"content":{"abstract":"We study whether novel ideas in biomedical literature appear first in preprints or traditional journals. We develop a Bayesian method to estimate the time of appearance for a phrase in the literature, and apply it to a number of phrases, both automatically extracted and suggested by experts. We see that presently most phrases appear first in the traditional journals, but there is a number of phrases with the first appearance on preprint servers. A comparison of the general composition of texts from bioRxiv and traditional journals shows a growing trend of bioRxiv being predictive of traditional journals. We discuss the application of the method for related problems.","authors":["Swarup Satish","Zonghai Yao","Andrew Drozdov","Boris Veytsman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The impact of preprint servers in the formation of novel ideas","tldr":"We study whether novel ideas in biomedical literature appear first in preprints or traditional journals. We develop a Bayesian method to estimate the time of appearance for a phrase in the literature, and apply it to a number of phrases, both automat...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.6","presentation_id":"38940715","rocketchat_channel":"paper-sdp2020-6","speakers":"Swarup Satish|Zonghai Yao|Andrew Drozdov|Boris Veytsman","title":"The impact of preprint servers in the formation of novel ideas"},{"content":{"abstract":"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the multi-round TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the best systems. In round 3, we submitted the highest-scoring run that took advantage of previous training data and the second-highest fully automatic run. In rounds 4 and 5, we submitted the highest-scoring fully automatic runs.","authors":["Edwin Zhang","Nikhil Gupta","Raphael Tang","Xiao Han","Ronak Pradeep","Kuang Lu","Yue Zhang","Rodrigo Nogueira","Kyunghyun Cho","Hui Fang","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset","tldr":"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late Marc...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.60","presentation_id":"38940714","rocketchat_channel":"paper-sdp2020-60","speakers":"Edwin Zhang|Nikhil Gupta|Raphael Tang|Xiao Han|Ronak Pradeep|Kuang Lu|Yue Zhang|Rodrigo Nogueira|Kyunghyun Cho|Hui Fang|Jimmy Lin","title":"Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset"},{"content":{"abstract":"To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover and organize relevant literature. The system provides search at multiple levels of textual granularity, from sentences to aggregations across documents, both in natural language and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, question answering, search, analytics, expert search, and recommendations.","authors":["Marzieh Fadaee","Olga Gureenkova","Fernando Rejon Barrera","Carsten Schnober","Wouter Weerkamp","Jakub Zavrel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A New Neural Search and Insights Platform for Navigating and Organizing AI Research","tldr":"To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover ...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.61","presentation_id":"38940726","rocketchat_channel":"paper-sdp2020-61","speakers":"Marzieh Fadaee|Olga Gureenkova|Fernando Rejon Barrera|Carsten Schnober|Wouter Weerkamp|Jakub Zavrel","title":"A New Neural Search and Insights Platform for Navigating and Organizing AI Research"},{"content":{"abstract":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keywords of a given paper. We adapt the TextCNN architecture and automatically analyze its predictions using the Integrated Gradients method to highlight words and phrases that led to the recommendation of a scientific venue. We train and test our method on publications from the fields of artificial intelligence (AI) and medicine, both derived from the Semantic Scholar dataset. WTS achieves an Accuracy@5 of approximately 83% for AI papers and 95% in the field of medicine. It is open source and available for testing on https://wheretosubmit.ml.","authors":["Konstantin Kobs","Tobias Koopmann","Albin Zehe","David Fernes","Philipp Krop","Andreas Hotho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.78","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Where to Submit? Helping Researchers to Choose the Right Venue","tldr":"Whenever researchers write a paper, the same question occurs: \u201cWhere to submit?\u201d In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keyword...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.758","presentation_id":"38940736","rocketchat_channel":"paper-sdp2020-758","speakers":"Konstantin Kobs|Tobias Koopmann|Albin Zehe|David Fernes|Philipp Krop|Andreas Hotho","title":"Where to Submit? Helping Researchers to Choose the Right Venue"},{"content":{"abstract":"Expert search aims to find and rank experts based on a user\u2019s query. In academia, retrieving experts is an efficient way to navigate through a large amount of academic knowledge. Here, we study how different distributed representations of academic papers (i.e. embeddings) impact academic expert retrieval. We use the Microsoft Academic Graph dataset and experiment with different configurations of a document-centric voting model for retrieval. In particular, we explore the impact of the use of contextualized embeddings on search performance. We also present results for paper embeddings that incorporate citation information through retrofitting. Additionally, experiments are conducted using different techniques for assigning author weights based on author order. We observe that using contextual embeddings produced by a transformer model trained for sentence similarity tasks produces the most effective paper representations for document-centric expert retrieval. However, retrofitting the paper embeddings and using elaborate author contribution weighting strategies did not improve retrieval performance.","authors":["Mark Berger","Jakub Zavrel","Paul Groth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effective distributed representations for academic expert search","tldr":"Expert search aims to find and rank experts based on a user\u2019s query. In academia, retrieving experts is an efficient way to navigate through a large amount of academic knowledge. Here, we study how different distributed representations of academic pa...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.8","presentation_id":"38940716","rocketchat_channel":"paper-sdp2020-8","speakers":"Mark Berger|Jakub Zavrel|Paul Groth","title":"Effective distributed representations for academic expert search"},{"content":{"abstract":"Next to keeping up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges, computational work on enhancing search, summarization, and analysis of scholarly documents has flourished. However, the various strands of research on scholarly document processing remain fragmented. To reach to the broader NLP and AI/ML community, pool distributed efforts and enable shared access to published research, we held the 1st Workshop on Scholarly Document Processing at EMNLP 2020 as a virtual event. The SDP workshop consisted of a research track (including a poster session), two invited talks and three Shared Tasks (CL-SciSumm, Lay-Summ and LongSumm), geared towards easier access to scientific methods and results. Website: https://ornlcda.github.io/SDProc\n      ","authors":["Muthu Kumar Chandrasekaran","Guy Feigenblat","Dayne Freitag","Tirthankar Ghosal","Eduard Hovy","Philipp Mayr","Michal Shmueli-Scheuer","Anita de Waard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview of the First Workshop on Scholarly Document Processing (SDP)","tldr":"Next to keeping up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges, computational work on enhancing search, summarization, and analys...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.1","presentation_id":"","rocketchat_channel":"paper-sdp2020-1","speakers":"Muthu Kumar Chandrasekaran|Guy Feigenblat|Dayne Freitag|Tirthankar Ghosal|Eduard Hovy|Philipp Mayr|Michal Shmueli-Scheuer|Anita de Waard","title":"Overview of the First Workshop on Scholarly Document Processing (SDP)"},{"content":{"abstract":"arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greater. I will discuss the status and future of arXiv, and possibilities and plans to make more effective use of the research database to enhance ongoing research efforts.","authors":["Steinn Sigurdsson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The future of arXiv and knowledge discovery in open science","tldr":"arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greate...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.2","presentation_id":"","rocketchat_channel":"paper-sdp2020-2","speakers":"Steinn Sigurdsson","title":"The future of arXiv and knowledge discovery in open science"},{"content":{"abstract":"We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.","authors":["Muthu Kumar Chandrasekaran","Guy Feigenblat","Eduard Hovy","Abhilasha Ravichander","Michal Shmueli-Scheuer","Anita de Waard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm","tldr":"We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing t...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.24","presentation_id":"","rocketchat_channel":"paper-sdp2020-24","speakers":"Muthu Kumar Chandrasekaran|Guy Feigenblat|Eduard Hovy|Abhilasha Ravichander|Michal Shmueli-Scheuer|Anita de Waard","title":"Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm"},{"content":{"abstract":"The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and findings of the document. In the current paper, we present the participation of IITP-AI-NLP-ML team in three shared tasks, namely, CL-SciSumm 2020, LaySumm 2020, LongSumm 2020, which aims to generate medium, lay, and long summaries of the scientific articles, respectively. To solve CL-SciSumm 2020 and LongSumm 2020 tasks, three well-known clustering techniques are used, and then various sentence scoring functions, including textual entailment, are used to extract the sentences from each cluster for a summary generation. For LaySumm 2020, an encoder-decoder based deep learning model has been utilized. Performances of our developed systems are evaluated in terms of ROUGE measures on the associated datasets with the shared task.","authors":["Santosh Kumar Mishra","Harshavardhan Kundarapu","Naveen Saini","Sriparna Saha","Pushpak Bhattacharyya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IITP-AI-NLP-ML@ CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020","tldr":"The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.30","presentation_id":"","rocketchat_channel":"paper-sdp2020-30","speakers":"Santosh Kumar Mishra|Harshavardhan Kundarapu|Naveen Saini|Sriparna Saha|Pushpak Bhattacharyya","title":"IITP-AI-NLP-ML@ CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020"},{"content":{"abstract":"This document demonstrates our groups approach to the CL-SciSumm shared task 2020. There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation. In Task 1b, we use a SVM to classify the facet of a citation.","authors":["Artur Jurk","Maik Boltze","Georg Keller","Lorna Ulbrich","Anja Fischer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"1A-Team / Martin-Luther-Universit\u00e4t Halle-Wittenberg@CLSciSumm 20","tldr":"This document demonstrates our groups approach to the CL-SciSumm shared task 2020. There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.31","presentation_id":"","rocketchat_channel":"paper-sdp2020-31","speakers":"Artur Jurk|Maik Boltze|Georg Keller|Lorna Ulbrich|Anja Fischer","title":"1A-Team / Martin-Luther-Universit\u00e4t Halle-Wittenberg@CLSciSumm 20"},{"content":{"abstract":"This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity scores to identify spans of the reference text for the given citance. In Task 1b, we use a logistic regression to classifying the discourse facets.","authors":["Rong Huang","Kseniia Krylova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Team MLU@CL-SciSumm20: Methods for Computational Linguistics Scientific Citation Linkage","tldr":"This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity sco...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.32","presentation_id":"","rocketchat_channel":"paper-sdp2020-32","speakers":"Rong Huang|Kseniia Krylova","title":"Team MLU@CL-SciSumm20: Methods for Computational Linguistics Scientific Citation Linkage"},{"content":{"abstract":"This paper mainly introduces our methods for Task 1A and Task 1B of CL-SciSumm 2020. Task 1A is to identify reference text in reference paper. Traditional machine learning models and MLP model are used. We evaluate the performances of these models and submit the final results from the optimal model. Compared with previous work, we optimize the ratio of positive to negative examples after data sampling. In order to construct features for classification, we calculate similarities between reference text and candidate sentences based on sentence vectors. Accordingly, nine similarities are used, of which eight are chosen from what we used in CL-SciSumm 2019 and a new sentence similarity based on fastText is added. Task 1B is to classify the facets of reference text. Unlike the methods used in CL-SciSumm 2019, we construct inputs of models based on word vectors and add deep learning models for classification this year.","authors":["Heng Zhang","Lifan Liu","Ruping Wang","Shaohu Hu","Shutian Ma","Chengzhi Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IR&TM-NJUST@CLSciSumm 20","tldr":"This paper mainly introduces our methods for Task 1A and Task 1B of CL-SciSumm 2020. Task 1A is to identify reference text in reference paper. Traditional machine learning models and MLP model are used. We evaluate the performances of these models an...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.33","presentation_id":"","rocketchat_channel":"paper-sdp2020-33","speakers":"Heng Zhang|Lifan Liu|Ruping Wang|Shaohu Hu|Shutian Ma|Chengzhi Zhang","title":"IR&TM-NJUST@CLSciSumm 20"},{"content":{"abstract":"In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in scientific articles. The task is to generate abstractive and extractive summaries of a given scientific article. In the proposed approach, we are inspired by the concept of Argumentative Zoning (AZ) that de- fines the main rhetorical structure in scientific articles. We define two aspects that should be covered in scientific paper summary, namely Claim/Method and Conclusion/Result aspects. We use Solr index to expand the sentences of the paper abstract. We formulate each abstract sentence in a given publication as query to retrieve similar sentences from the text body of the document itself. We utilize a sentence selection algorithm described in previous literature to select sentences for the final summary that covers the two aforementioned aspects.","authors":["Alaa El-Ebshihy","Annisa Maulida Ningtyas","Linda Andersson","Florina Piroi","Andreas Rauber"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ARTU / TU Wien and Artificial Researcher@ LongSumm 20","tldr":"In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in scientific a...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.36","presentation_id":"","rocketchat_channel":"paper-sdp2020-36","speakers":"Alaa El-Ebshihy|Annisa Maulida Ningtyas|Linda Andersson|Florina Piroi|Andreas Rauber","title":"ARTU / TU Wien and Artificial Researcher@ LongSumm 20"},{"content":{"abstract":"The Scholarly Document Processing (SDP) workshop is to encourage more efforts on natural language understanding of scientific task. It contains three shared tasks and we participate in the LongSumm shared task. In this paper, we describe our text summarization system, SciSummPip, inspired by SummPip (Zhao et al., 2020) that is an unsupervised text summarization system for multi-document in News domain. Our SciSummPip includes a transformer-based language model SciBERT (Beltagy et al., 2019) for contextual sentence representation, content selection with PageRank (Page et al., 1999), sentence graph construction with both deep and linguistic information, sentence graph clustering and within-graph summary generation. Our work differs from previous method in that content selection and a summary length constraint is applied to adapt to the scientific domain. The experiment results on both training dataset and blind test dataset show the effectiveness of our method, and we empirically verify the robustness of modules used in SciSummPip with BERTScore (Zhang et al., 2019a).","authors":["Jiaxin Ju","Ming Liu","Longxiang Gao","Shirui Pan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Monash-Summ@LongSumm 20 SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline","tldr":"The Scholarly Document Processing (SDP) workshop is to encourage more efforts on natural language understanding of scientific task. It contains three shared tasks and we participate in the LongSumm shared task. In this paper, we describe our text sum...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.37","presentation_id":"","rocketchat_channel":"paper-sdp2020-37","speakers":"Jiaxin Ju|Ming Liu|Longxiang Gao|Shirui Pan","title":"Monash-Summ@LongSumm 20 SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline"},{"content":{"abstract":"We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed with the domain of the research article. We propose a two step divide-and-conquer approach. First, we judiciously select segments of the documents that are not overly pedantic and are likely to be of interest to the laity, and over-extract sentences from each segment using an unsupervised network based method. Next, we perform abstractive summarization on these extractions and systematically merge the abstractions. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. Our approach leverages state-of-the-art pre-trained deep neural network based models as zero-shot learners to achieve high scores on the task.","authors":["Rochana Chaturvedi","Saachi .","Jaspreet Singh Dhani","Anurag Joshi","Ankush Khanna","Neha Tomar","Swagata Duari","Alka Khurana","Vasudha Bhatnagar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sdp-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Divide and Conquer: From Complexity to Simplicity for Lay Summarization","tldr":"We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed...","track":"First Workshop on Scholarly Document Processing (SDP 2020)"},"id":"WS-7.2020.sdp-1.40","presentation_id":"","rocketchat_channel":"paper-sdp2020-40","speakers":"Rochana Chaturvedi|Saachi .|Jaspreet Singh Dhani|Anurag Joshi|Ankush Khanna|Neha Tomar|Swagata Duari|Alka Khurana|Vasudha Bhatnagar","title":"Divide and Conquer: From Complexity to Simplicity for Lay Summarization"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-sdp2020","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 14:00:00 GMT","hosts":"Philipp Mayr","link":"","session_name":"Opening Remarks","start_time":"Thu, 19 Nov 2020 13:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:15:00 GMT","hosts":"tbd","link":"","session_name":"Teaser for Shared Tasks (5mins each)","start_time":"Thu, 19 Nov 2020 14:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:35:00 GMT","hosts":"Tirthankar Ghosal","link":"","session_name":"Research Track: Session 1 COVID-19 document processing","start_time":"Thu, 19 Nov 2020 14:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:35:00 GMT","hosts":"TBD","link":"","session_name":"Wu et al.: Acknowledgement Entity Recognition in CORD-19 Papers","start_time":"Thu, 19 Nov 2020 14:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:55:00 GMT","hosts":"TBD","link":"","session_name":"Bhambhoria et al.: A Smart System to Generate and Validate Question Answer Pairs for COVID-19 Literature.","start_time":"Thu, 19 Nov 2020 14:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:15:00 GMT","hosts":"TBD","link":"","session_name":"Zhang et al.: Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset.","start_time":"Thu, 19 Nov 2020 14:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:35:00 GMT","hosts":"TBD","link":"","session_name":"Satish et al.: The impact of preprint servers in the formation of novel ideas.","start_time":"Thu, 19 Nov 2020 15:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:25:00 GMT","hosts":"Philipp Mayr","link":"","session_name":"Keynote 1 (incl. QA):  Kuansan Wang\nMitigating scholarly corpus biases with citations: A case study on CORD-19","start_time":"Thu, 19 Nov 2020 15:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:50:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 16:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:50:00 GMT","hosts":"Dayne Freitag","link":"","session_name":"Research Track: Session 2 SDP mixed session","start_time":"Thu, 19 Nov 2020 16:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:10:00 GMT","hosts":"TBD","link":"","session_name":"Berger et al.: Effective Distributed Representations for Academic Expert Search.","start_time":"Thu, 19 Nov 2020 16:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"TBD","link":"","session_name":"Kim et al.: Learning CNF Blocking for Large-scale Author Name Disambiguation.","start_time":"Thu, 19 Nov 2020 17:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:50:00 GMT","hosts":"TBD","link":"","session_name":"M\u00fcller: Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain.","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:30:00 GMT","hosts":"tbd","link":"","session_name":"Poster Pitches","start_time":"Thu, 19 Nov 2020 17:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 18:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:50:00 GMT","hosts":"Muthu","link":"","session_name":"Research Track: Session 3: Short papers and Findings","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:10:00 GMT","hosts":"TBD","link":"","session_name":"Ling & Chen: DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers","start_time":"Thu, 19 Nov 2020 19:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:20:00 GMT","hosts":"TBD","link":"","session_name":"Medic & Snajder: Improved Local Citation Recommendation Based on Context Enhanced with Global Information","start_time":"Thu, 19 Nov 2020 19:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:30:00 GMT","hosts":"TBD","link":"","session_name":"Cao et al.: Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora (Findings of EMNLP)","start_time":"Thu, 19 Nov 2020 19:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:40:00 GMT","hosts":"TBD","link":"","session_name":"Subramanian et al.: MedICaT: A Dataset of Medical Images, Captions, and Textual References (Findings of EMNLP)","start_time":"Thu, 19 Nov 2020 19:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:50:00 GMT","hosts":"TBD","link":"","session_name":"Noh et al.: Literature Retrieval for Precision Medicine with Neural Matching and Faceted Summarization (Findings of EMNLP)","start_time":"Thu, 19 Nov 2020 19:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:20:00 GMT","hosts":"Muthu, Anita, Guy, Michal","link":"","session_name":"Overview of Results of the Shared Tasks","start_time":"Thu, 19 Nov 2020 19:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:30:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 20:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:15:00 GMT","hosts":"Tirthankar Ghosal","link":"","session_name":"Keynote 2: Steinn Sigurdsson\nThe future of arXiv and knowledge discovery in open science","start_time":"Thu, 19 Nov 2020 20:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:00:00 GMT","hosts":"tbd","link":"","session_name":"Plenary Regroup & Panel (OC + Keynote Speakers)","start_time":"Thu, 19 Nov 2020 21:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:10:00 GMT","hosts":"TBD","link":"","session_name":"Closing","start_time":"Thu, 19 Nov 2020 22:00:00 GMT"}],"title":"First Workshop on Scholarly Document Processing (SDP 2020)","website":"https://ornlcda.github.io/SDProc/","zoom_links":["https://zoom.us","https://zoom.us","https://zoom.us"]},{"abstract":"Democratizing NLP!","blocks":[{"end_time":"Thu, 19 Nov 2020 17:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 03:30:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Thu, 19 Nov 2020 22:30:00 GMT"}],"id":"WS-9","livestream":null,"organizers":"Masato Hagiwara, Eunjeong Park, Dmitrijs Milajevs, Nelson F. Liu, Liling Tan and Geeticka Chauhan","papers":[{"content":{"abstract":"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.","authors":["Paul McCann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"fugashi, a Tool for Tokenizing Japanese in Python","tldr":"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without s...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.10","presentation_id":"38939744","rocketchat_channel":"paper-nlposs-10","speakers":"Paul McCann","title":"fugashi, a Tool for Tokenizing Japanese in Python"},{"content":{"abstract":"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.","authors":["Vincent Warmerdam","Thomas Kober","Rachael Tatman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Going Beyond T-SNE: Exposing whatlies in Text Embeddings","tldr":"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface tra...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.11","presentation_id":"38939745","rocketchat_channel":"paper-nlposs-11","speakers":"Vincent Warmerdam|Thomas Kober|Rachael Tatman","title":"Going Beyond T-SNE: Exposing whatlies in Text Embeddings"},{"content":{"abstract":"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.","authors":["Raphael Tang","Jaejun Lee","Afsaneh Razi","Julia Cambre","Ian Bicking","Jofish Kaye","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Howl: A Deployed, Open-Source Wake Word Detection System","tldr":"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on G...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.12","presentation_id":"38939746","rocketchat_channel":"paper-nlposs-12","speakers":"Raphael Tang|Jaejun Lee|Afsaneh Razi|Julia Cambre|Ian Bicking|Jofish Kaye|Jimmy Lin","title":"Howl: A Deployed, Open-Source Wake Word Detection System"},{"content":{"abstract":"We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages. By using pre-trained models from iNLTK for text classification on publicly available datasets, we significantly outperform previously reported results. On these datasets, we also show that by using pre-trained models and data augmentation from iNLTK, we can achieve more than 95% of the previous best performance by using less than 10% of the training data. iNLTK is already being widely used by the community and has 40,000+ downloads, 600+ stars and 100+ forks on GitHub. The library is available at https://github.com/goru001/inltk.","authors":["Gaurav Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"iNLTK: Natural Language Toolkit for Indic Languages","tldr":"We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages....","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.13","presentation_id":"38939747","rocketchat_channel":"paper-nlposs-13","speakers":"Gaurav Arora","title":"iNLTK: Natural Language Toolkit for Indic Languages"},{"content":{"abstract":"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each token is given a label and these labels are prefixed with special tokens such as B- or I-. After a model assigns labels to each token, these prefixes are used to group the tokens into spans. Properly parsing these annotations is critical for producing fair and comparable metrics; however, despite its importance, there is not an easy-to-use, standardized, programmatically integratable library to help work with span labeling. To remedy this, we introduce our open-source library, iobes. iobes is used for parsing, converting, and processing spans represented as token level decisions.","authors":["Brian Lester"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"iobes: Library for Span Level Processing","tldr":"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each t...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.14","presentation_id":"38939748","rocketchat_channel":"paper-nlposs-14","speakers":"Brian Lester","title":"iobes: Library for Span Level Processing"},{"content":{"abstract":"","authors":["Yada Pruksachatkun","Phil Yeres","Haokun Liu","Jason Phang","Phu Mon Htut","Alex Wang","Ian Tenney","Samuel R. Bowman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models","tldr":null,"track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.15","presentation_id":"38939749","rocketchat_channel":"paper-nlposs-15","speakers":"Yada Pruksachatkun|Phil Yeres|Haokun Liu|Jason Phang|Phu Mon Htut|Alex Wang|Ian Tenney|Samuel R. Bowman","title":"jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models"},{"content":{"abstract":"Despite the recent advances in applying language-independent approaches to various natural language processing tasks thanks to artificial intelligence, some language-specific tools are still essential to process a language in a viable manner. Kurdish language is a less-resourced language with a remarkable diversity in dialects and scripts and lacks basic language processing tools. To address this issue, we introduce a language processing toolkit to handle such a diversity in an efficient way. Our toolkit is composed of fundamental components such as text preprocessing, stemming, tokenization, lemmatization and transliteration and is able to get further extended by future developers. The project is publicly available.","authors":["Sina Ahmadi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KLPT \u2013 Kurdish Language Processing Toolkit","tldr":"Despite the recent advances in applying language-independent approaches to various natural language processing tasks thanks to artificial intelligence, some language-specific tools are still essential to process a language in a viable manner. Kurdish...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.16","presentation_id":"38939750","rocketchat_channel":"paper-nlposs-16","speakers":"Sina Ahmadi","title":"KLPT \u2013 Kurdish Language Processing Toolkit"},{"content":{"abstract":"Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Korean corpora, first describing institution-level resource development, then further iterate through a list of current open datasets for different types of tasks. We then propose a direction on how open-source dataset construction and releases should be done for less-resourced languages to promote research.","authors":["Won Ik Cho","Sangwhan Moon","Youngsook Song"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Open Korean Corpora: A Practical Report","tldr":"Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Kor...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.17","presentation_id":"38939751","rocketchat_channel":"paper-nlposs-17","speakers":"Won Ik Cho|Sangwhan Moon|Youngsook Song","title":"Open Korean Corpora: A Practical Report"},{"content":{"abstract":"This document describes shared development of finite-state description of two closely related but endangered minority languages, Erzya and Moksha. It touches upon morpholexical unity and diversity of the two languages and how this provides a motivation for shared open-source FST development. We describe how we have designed the transducers so that they can benefit from existing open-source infrastructures and are as reusable as possible.","authors":["Jack Rueter","Mika H\u00e4m\u00e4l\u00e4inen","Niko Partanen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Open-Source Morphology for Endangered Mordvinic Languages","tldr":"This document describes shared development of finite-state description of two closely related but endangered minority languages, Erzya and Moksha. It touches upon morpholexical unity and diversity of the two languages and how this provides a motivati...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.18","presentation_id":"38939752","rocketchat_channel":"paper-nlposs-18","speakers":"Jack Rueter|Mika H\u00e4m\u00e4l\u00e4inen|Niko Partanen","title":"Open-Source Morphology for Endangered Mordvinic Languages"},{"content":{"abstract":"We present Pimlico, an open source toolkit for building pipelines for processing large corpora. It is especially focused on processing linguistic corpora and provides wrappers around existing, widely used NLP tools. A particular goal is to ease distribution of reproducible and extensible experiments by making it easy to document and re-run all steps involved, including data loading, pre-processing, model training and evaluation. Once a pipeline is released, it is easy to adapt, for example, to run on a new dataset, or to re-run an experiment with different parameters. The toolkit takes care of many common challenges in writing and distributing corpus-processing code, such as managing data between the steps of a pipeline, installing required software and combining existing toolkits with new, task-specific code.","authors":["Mark Granroth-Wilding"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pimlico: A toolkit for corpus-processing pipelines and reproducible experiments","tldr":"We present Pimlico, an open source toolkit for building pipelines for processing large corpora. It is especially focused on processing linguistic corpora and provides wrappers around existing, widely used NLP tools. A particular goal is to ease distr...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.19","presentation_id":"38939753","rocketchat_channel":"paper-nlposs-19","speakers":"Mark Granroth-Wilding","title":"Pimlico: A toolkit for corpus-processing pipelines and reproducible experiments"},{"content":{"abstract":"We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to Python with additional improvements and functionality. PySBD passes 97.92% of the Golden Rule Set examplars for English, an improvement of 25% over the next best open source Python tool.","authors":["Nipun Sadvilkar","Mark Neumann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PySBD: Pragmatic Sentence Boundary Disambiguation","tldr":"We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unkno...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.20","presentation_id":"38939754","rocketchat_channel":"paper-nlposs-20","speakers":"Nipun Sadvilkar|Mark Neumann","title":"PySBD: Pragmatic Sentence Boundary Disambiguation"},{"content":{"abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","authors":["Daniel Deutsch","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","tldr":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the off...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.21","presentation_id":"38939755","rocketchat_channel":"paper-nlposs-21","speakers":"Daniel Deutsch|Dan Roth","title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics"},{"content":{"abstract":"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.","authors":["John Morris","Jin Yong Yoo","Yanjun Qi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TextAttack: Lessons learned in designing Python frameworks for NLP","tldr":"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused acro...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.22","presentation_id":"38939756","rocketchat_channel":"paper-nlposs-22","speakers":"John Morris|Jin Yong Yoo|Yanjun Qi","title":"TextAttack: Lessons learned in designing Python frameworks for NLP"},{"content":{"abstract":"From LDA to neural models, different topic modeling approaches have been proposed in the literature. However, their suitability and performance is not easy to compare, particularly when the algorithms are being used in the wild on heterogeneous datasets. In this paper, we introduce ToModAPI (TOpic MOdeling API), a wrapper library to easily train, evaluate and infer using different topic modeling algorithms through a unified interface. The library is extensible and can be used in Python environments or through a Web API.","authors":["Pasquale Lisena","Ismail Harrando","Oussama Kandakji","Raphael Troncy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TOMODAPI: A Topic Modeling API to Train, Use and Compare Topic Models","tldr":"From LDA to neural models, different topic modeling approaches have been proposed in the literature. However, their suitability and performance is not easy to compare, particularly when the algorithms are being used in the wild on heterogeneous datas...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.23","presentation_id":"38939757","rocketchat_channel":"paper-nlposs-23","speakers":"Pasquale Lisena|Ismail Harrando|Oussama Kandakji|Raphael Troncy","title":"TOMODAPI: A Topic Modeling API to Train, Use and Compare Topic Models"},{"content":{"abstract":"For the last 5 years, we have developed and maintained RSMTool \u2013 an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its cross-disciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper.","authors":["Nitin Madnani","Anastassia Loukina"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"User-centered & Robust NLP OSS: Lessons Learned from Developing & Maintaining RSMTool","tldr":"For the last 5 years, we have developed and maintained RSMTool \u2013 an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine l...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.24","presentation_id":"38939758","rocketchat_channel":"paper-nlposs-24","speakers":"Nitin Madnani|Anastassia Loukina","title":"User-centered & Robust NLP OSS: Lessons Learned from Developing & Maintaining RSMTool"},{"content":{"abstract":"The WordNet database of English (Fellbaum, 1998) is a key source of semantic information for research and development of natural language processing applications. As the sophistication of these applications increases with the use of large datasets, deep learning, and graph-based methods, so should the use of WordNet. To this end, we introduce WAFFLE: WordNet Applied to FreeForm Linguistic Exploration which makes WordNet available in an open source graph data structure. The WAFFLE graph relies on platform agnostic formats for robust interrogation and flexibility. Where existing implementations of WordNet offer dictionary-like lookup, single degree neighborhood operations, and path based similarity-scoring, the WAFFLE graph makes all nodes (semantic relation sets) and relationships queryable at scale, enabling local and global analysis of all relationships without the need for custom code. We demonstrate WAFFLE\u2019s ease of use, visualization capabilities, and scalable efficiency with common queries, operations, and interactions. WAFFLE is available at github.com/TRSS-NLP/WAFFLE.","authors":["Berk Ekmekci","Blake Howald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WAFFLE: A Graph for WordNet Applied to FreeForm Linguistic Exploration","tldr":"The WordNet database of English (Fellbaum, 1998) is a key source of semantic information for research and development of natural language processing applications. As the sophistication of these applications increases with the use of large datasets, d...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.25","presentation_id":"38939759","rocketchat_channel":"paper-nlposs-25","speakers":"Berk Ekmekci|Blake Howald","title":"WAFFLE: A Graph for WordNet Applied to FreeForm Linguistic Exploration"},{"content":{"abstract":"Conversational agents can be used to make diagnoses, classify mental states, promote health education, and provide emotional support. The benefits of adopting conversational agents include widespread access, increased treatment engagement, and improved patient relationships with the intervention. We propose here a framework to assist chat operators of mental healthcare services, instead of a fully automated conversational agent. This design eases to avoid the adverse effects of applying chatbots in mental healthcare. The proposed framework is capable of improving the quality and reducing the time of interactions via chat between a user and a chat operator. We also present a case study in the context of health promotion on reducing tobacco use. The proposed framework uses artificial intelligence, specifically natural language processing (NLP) techniques, to classify messages from chat users. A list of suggestions is offered to the chat operator, with topics to be discussed in the session. These suggestions were created based on service protocols and the classification of previous chat sessions. The operator can also edit the suggested messages. Data collected can be used in the future to improve the quality of the suggestions offered.","authors":["Thiago Madeira","Heder Bernardino","Jairo Francisco De Souza","Henrique Gomide","Nath\u00e1lia Munck Machado","Bruno Marcos Pinheiro da Silva","Alexandre Vieira Pereira Pacelli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Framework to Assist Chat Operators of Mental Healthcare Services","tldr":"Conversational agents can be used to make diagnoses, classify mental states, promote health education, and provide emotional support. The benefits of adopting conversational agents include widespread access, increased treatment engagement, and improv...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.4","presentation_id":"38939738","rocketchat_channel":"paper-nlposs-4","speakers":"Thiago Madeira|Heder Bernardino|Jairo Francisco De Souza|Henrique Gomide|Nath\u00e1lia Munck Machado|Bruno Marcos Pinheiro da Silva|Alexandre Vieira Pereira Pacelli","title":"A Framework to Assist Chat Operators of Mental Healthcare Services"},{"content":{"abstract":"Automating natural language understanding is a lifelong quest addressed for decades. With the help of advances in machine learning and particularly, deep learning, we are able to produce state of the art models that can imitate human interactions with languages. Unfortunately, these advances are controlled by the availability of language resources. Arabic advances in this field , although it has a great potential, are still limited. This is apparent in both research and development. In this paper, we showcase some NLP models we trained for Arabic. We also present our methodology and pipeline to build such models from data collection, data preprocessing, tokenization and model deployment. These tools help in the advancement of the field and provide a systematic approach for extending NLP tools to many languages.","authors":["Zaid Alyafeai","Maged Al-Shaibani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ARBML: Democritizing Arabic Natural Language Processing Tools","tldr":"Automating natural language understanding is a lifelong quest addressed for decades. With the help of advances in machine learning and particularly, deep learning, we are able to produce state of the art models that can imitate human interactions wit...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.5","presentation_id":"38939739","rocketchat_channel":"paper-nlposs-5","speakers":"Zaid Alyafeai|Maged Al-Shaibani","title":"ARBML: Democritizing Arabic Natural Language Processing Tools"},{"content":{"abstract":"The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components \u2013 parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network (GNN) libraries. Additionally, we discuss downstream usage and applications of the library, and how it can accelerate research for the NLP community.","authors":["Raeid Saqur","Ameet Deshpande"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes","tldr":"The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.6","presentation_id":"38939740","rocketchat_channel":"paper-nlposs-6","speakers":"Raeid Saqur|Ameet Deshpande","title":"CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes"},{"content":{"abstract":"The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been instrumental in the democratization of this technology. The maturity of the open-source NLP ecosystem however varies between languages. This work proposes a new open-source library aimed at bringing state-of-the-art NLP to Rust. Rust is a systems programming language for which the foundations required to build machine learning applications are available but still lacks ready-to-use, end-to-end NLP libraries. The proposed library, rust-bert, implements modern language models and ready-to-use pipelines (for example translation or summarization). This allows further development by the Rust community from both NLP experts and non-specialists. It is hoped that this library will accelerate the development of the NLP ecosystem in Rust. The library is under active development and available at https://github.com/guillaume-be/rust-bert.","authors":["Guillaume Becquin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End-to-end NLP Pipelines in Rust","tldr":"The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been i...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.7","presentation_id":"38939741","rocketchat_channel":"paper-nlposs-7","speakers":"Guillaume Becquin","title":"End-to-end NLP Pipelines in Rust"},{"content":{"abstract":"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.","authors":["Vaibhav Kumar","Tenzin Bhotia","Vaibhav Kumar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings","tldr":"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases wh...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.8","presentation_id":"38939742","rocketchat_channel":"paper-nlposs-8","speakers":"Vaibhav Kumar|Tenzin Bhotia|Vaibhav Kumar","title":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings"},{"content":{"abstract":"Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage.","authors":["Leonid Boytsov","Eric Nyberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Flexible retrieval with NMSLIB and FlexNeuART","tldr":"Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of d...","track":"Second Workshop for NLP Open Source Software (NLP-OSS)"},"id":"WS-9.9","presentation_id":"38939743","rocketchat_channel":"paper-nlposs-9","speakers":"Leonid Boytsov|Eric Nyberg","title":"Flexible retrieval with NMSLIB and FlexNeuART"}],"prerecorded_talks":[{"presentation_id":"38939735","speakers":"Chip Huyen","title":"(To be confirmed) Invited Talk 1 - Chip Huyen"},{"presentation_id":"38939736","speakers":"Spencer Kelly","title":"(To be confirmed) Invited Talk 2 - Spencer Kelly"},{"presentation_id":"38939737","speakers":"Thomas Wolf","title":"(To be confirmed) Invited Talk 3 -  Thomas Wolf"}],"rocketchat_channel":"workshop-nlposs","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 13:30:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Workshop Opening","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:30:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"On Typing: Historical and Potential Interactions in Word-processing - Spencer Kelly","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:30:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"An Introduction to Transfer Learning in NLP and HuggingFace - Thomas Wolf","start_time":"Thu, 19 Nov 2020 14:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:00:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Talks Session 1 (Watch on your own, authors encouraged to mend their slidelive chat but not complusory)","start_time":"Thu, 19 Nov 2020 15:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 00:00:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Talks Session 2 (Watch on your own, authors encouraged to mend their slidelive chat but not complusory)","start_time":"Thu, 19 Nov 2020 22:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 01:30:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Gather-town (Live) + Poster QnA","start_time":"Fri, 20 Nov 2020 00:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 03:00:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Principles of Good Machine Learning Systems Design - Chip Huyen","start_time":"Fri, 20 Nov 2020 02:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 03:30:00 GMT","hosts":"NLP-OSS Organziers","link":"","session_name":"Closing Remarks","start_time":"Fri, 20 Nov 2020 03:00:00 GMT"}],"title":"Second Workshop for NLP Open Source Software (NLP-OSS)","website":"https://nlposs.github.io/","zoom_links":["https://zoom.us"]},{"abstract":"Spatial Language Understanding, Learning/Reasoning on Spatial Semantics, (sub)Symbolic Representations and Grounding Language in Perception","blocks":[{"end_time":"Fri, 20 Nov 2020 02:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"}],"id":"WS-10","livestream":null,"organizers":"Malihe Alikhani, Jason Baldridge, Mohit Bansal, Archna Bhatia, Parisa Kordjamshidi and Marie-Francine Moens","papers":[{"content":{"abstract":"","authors":["Sayali Kulkarni","Shailee Jain","Mohammad Javad Hosseini","Jason Baldridge","Eugene Ie","Li Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Geocoding with multi-level loss for spatial language representation","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.11","presentation_id":"38940083","rocketchat_channel":"paper-splu2020-11","speakers":"Sayali Kulkarni|Shailee Jain|Mohammad Javad Hosseini|Jason Baldridge|Eugene Ie|Li Zhang","title":"Geocoding with multi-level loss for spatial language representation"},{"content":{"abstract":"","authors":["Roshanak Mirzaee","Hossein Rajaby Faghihi","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.12","presentation_id":"38940084","rocketchat_channel":"paper-splu2020-12","speakers":"Roshanak Mirzaee|Hossein Rajaby Faghihi|Parisa Kordjamshidi","title":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning"},{"content":{"abstract":"","authors":["Yue Zhang","Quan Guo","Parisa Kordjamshidi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations","tldr":null,"track":"Spatial Language Understanding"},"id":"WS-10.13","presentation_id":"38940085","rocketchat_channel":"paper-splu2020-13","speakers":"Yue Zhang|Quan Guo|Parisa Kordjamshidi","title":"Vision-and-Language Navigation by Reasoning over Spatial Configurations"},{"content":{"abstract":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers. Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The navigating agent models the guiding agent to simulate answers given candidate generated questions. The guiding agent in turn models the navigating agent to simulate navigation steps it would take to generate answers. We use the progress agents make towards the goal as a reinforcement learning reward signal to directly inform not only navigation actions, but also both question and answer generation. We demonstrate that RMM enables better generalization to novel environments. Interlocutor modelling may be a way forward for human-agent RMM where robots need to both ask and answer questions.","authors":["Homero Roman Roman","Yonatan Bisk","Jesse Thomason","Asli Celikyilmaz","Jianfeng Gao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.157","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"RMM: A Recursive Mental Model for Dialogue Navigation","tldr":"Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and ask...","track":"Spatial Language Understanding"},"id":"WS-10.1453","presentation_id":"38940095","rocketchat_channel":"paper-splu2020-1453","speakers":"Homero Roman Roman|Yonatan Bisk|Jesse Thomason|Asli Celikyilmaz|Jianfeng Gao","title":"RMM: A Recursive Mental Model for Dialogue Navigation"},{"content":{"abstract":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","authors":["Tsung-Yen Yang","Andrew Lan","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.172","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Robust and Interpretable Grounding of Spatial References with Relation Networks","tldr":"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for...","track":"Spatial Language Understanding"},"id":"WS-10.1595","presentation_id":"38940094","rocketchat_channel":"paper-splu2020-1595","speakers":"Tsung-Yen Yang|Andrew Lan|Karthik Narasimhan","title":"Robust and Interpretable Grounding of Spatial References with Relation Networks"},{"content":{"abstract":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.","authors":["Alberto Testoni","Claudio Greco","Tobias Bianchi","Mauricio Mazuecos","Agata Marcante","Luciana Benotti","Raffaella Bernardi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"They are not all alike: answering different spatial questions requires different grounding strategies","tldr":"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We b...","track":"Spatial Language Understanding"},"id":"WS-10.2","presentation_id":"38940076","rocketchat_channel":"paper-splu2020-2","speakers":"Alberto Testoni|Claudio Greco|Tobias Bianchi|Mauricio Mazuecos|Agata Marcante|Luciana Benotti|Raffaella Bernardi","title":"They are not all alike: answering different spatial questions requires different grounding strategies"},{"content":{"abstract":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ARRAMON. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language (English) instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work.","authors":["Hyounghun Kim","Abhaysinh Zala","Graham Burri","Hao Tan","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.348","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments","tldr":"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We ...","track":"Spatial Language Understanding"},"id":"WS-10.2904","presentation_id":"38940093","rocketchat_channel":"paper-splu2020-2904","speakers":"Hyounghun Kim|Abhaysinh Zala|Graham Burri|Hao Tan|Mohit Bansal","title":"ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments"},{"content":{"abstract":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while other features are more salient when assessing typicality. In this paper we explore the extent to which this is the case for English spatial prepositions and discuss the implications for pragmatic strategies and semantic models. We hypothesise that object-specific features \u2014 related to object properties and affordances \u2014 are more salient in categorisation, while geometric and physical relationships between objects are more salient in typicality judgements. In order to test this hypothesis we conducted a study using virtual environments to collect both category and typicality judgements in 3D scenes. Based on the collected data we cannot verify the hypothesis and conclude that object-specific features appear to be salient in both category and typicality judgements, further evidencing the need to include these types of features in semantic models.","authors":["Adam Richard-Bollans","Anthony Cohn","Luc\u00eda G\u00f3mez \u00c1lvarez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions","tldr":"Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while ot...","track":"Spatial Language Understanding"},"id":"WS-10.3","presentation_id":"38940077","rocketchat_channel":"paper-splu2020-3","speakers":"Adam Richard-Bollans|Anthony Cohn|Luc\u00eda G\u00f3mez \u00c1lvarez","title":"Categorisation, Typicality & Object-Specific Features in Spatial Referring Expressions"},{"content":{"abstract":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Currently, the best-performing models are able to complete less than 1% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information, the starting location in the virtual environment, is incorporated, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases, suggesting contextualized language models may provide strong planning modules for grounded virtual agents.","authors":["Peter Jansen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.395","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions","tldr":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \u201cput a hot piece of bread on a plate\u201d. Curre...","track":"Spatial Language Understanding"},"id":"WS-10.3302","presentation_id":"38940098","rocketchat_channel":"paper-splu2020-3302","speakers":"Peter Jansen","title":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions"},{"content":{"abstract":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our method with a user study, validating that our generated spatial arrangements align with human expectation.","authors":["Gorjan Radevski","Guillem Collell","Marie-Francine Moens","Tinne Tuytelaars"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.408","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Decoding Language Spatial Relations to 2D Spatial Arrangements","tldr":"We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-...","track":"Spatial Language Understanding"},"id":"WS-10.3382","presentation_id":"38940092","rocketchat_channel":"paper-splu2020-3382","speakers":"Gorjan Radevski|Guillem Collell|Marie-Francine Moens|Tinne Tuytelaars","title":"Decoding Language Spatial Relations to 2D Spatial Arrangements"},{"content":{"abstract":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with multiple visual features with different sizes of receptive fields, though the proper size of the receptive field of visual features intuitively varies depending on expressions. In this paper, we introduce a neural network architecture that modulates visual features with varying sizes of receptive field by linguistic features. We evaluate our architecture on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our architecture. Source code is available at https://github.com/Alab-NII/lcfp .","authors":["Taichi Iki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.420","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks","tldr":"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models cons...","track":"Spatial Language Understanding"},"id":"WS-10.3466","presentation_id":"38940091","rocketchat_channel":"paper-splu2020-3466","speakers":"Taichi Iki|Akiko Aizawa","title":"Language-Conditioned Feature Pyramids for Visual Selection Tasks"},{"content":{"abstract":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).","authors":["Hyeong Jin Shin","Jeong Yeon Park","Dae Bum Yuk","Jae Sung Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-based Spatial Information Extraction","tldr":"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018)...","track":"Spatial Language Understanding"},"id":"WS-10.5","presentation_id":"38940078","rocketchat_channel":"paper-splu2020-5","speakers":"Hyeong Jin Shin|Jeong Yeon Park|Dae Bum Yuk|Jae Sung Lee","title":"BERT-based Spatial Information Extraction"},{"content":{"abstract":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.","authors":["Chao Xu","Emmanuelle-Anna Dietz Saldanha","Dagmar Gromann","Beihai Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Cognitively Motivated Approach to Spatial Information Extraction","tldr":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing s...","track":"Spatial Language Understanding"},"id":"WS-10.6","presentation_id":"38940079","rocketchat_channel":"paper-splu2020-6","speakers":"Chao Xu|Emmanuelle-Anna Dietz Saldanha|Dagmar Gromann|Beihai Zhou","title":"A Cognitively Motivated Approach to Spatial Information Extraction"},{"content":{"abstract":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model\u2019s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues.","authors":["Takuma Udagawa","Takato Yamazaki","Akiko Aizawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.67","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions","tldr":"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize th...","track":"Spatial Language Understanding"},"id":"WS-10.676","presentation_id":"38940097","rocketchat_channel":"paper-splu2020-676","speakers":"Takuma Udagawa|Takato Yamazaki|Akiko Aizawa","title":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions"},{"content":{"abstract":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with respect to some anatomical structures. As the expressions result from the mental visualization of the radiologist\u2019s interpretations, they are varied and complex. The focus of this work is to automatically identify the spatial expression terms from three different radiology sub-domains. We propose a hybrid deep learning-based NLP method that includes \u2013 1) generating a set of candidate spatial triggers by exact match with the known trigger terms from the training data, 2) applying domain-specific constraints to filter the candidate triggers, and 3) utilizing a BERT-based classifier to predict whether a candidate trigger is a true spatial trigger or not. The results are promising, with an improvement of 24 points in the average F1 measure compared to a standard BERT-based sequence labeler.","authors":["Surabhi Datta","Kirk Roberts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports","tldr":"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with r...","track":"Spatial Language Understanding"},"id":"WS-10.7","presentation_id":"38940080","rocketchat_channel":"paper-splu2020-7","speakers":"Surabhi Datta|Kirk Roberts","title":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports"},{"content":{"abstract":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an image retrieval engine, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed method achieves a higher F1 value (89.67%) than a baseline method, demonstrating the effectiveness of using element-wise visual information.","authors":["Takuya Komada","Takashi Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition","tldr":"In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NE...","track":"Spatial Language Understanding"},"id":"WS-10.8","presentation_id":"38940081","rocketchat_channel":"paper-splu2020-8","speakers":"Takuya Komada|Takashi Inui","title":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition"},{"content":{"abstract":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the LiMiT dataset and share it publicly as a new resource for the research community.","authors":["Irene Manotas","Ngoc Phuoc An Vo","Vadim Sheinin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.88","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LiMiT: The Literal Motion in Text Dataset","tldr":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) datase...","track":"Spatial Language Understanding"},"id":"WS-10.857","presentation_id":"38940096","rocketchat_channel":"paper-splu2020-857","speakers":"Irene Manotas|Ngoc Phuoc An Vo|Vadim Sheinin","title":"LiMiT: The Literal Motion in Text Dataset"},{"content":{"abstract":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.","authors":["Harsh Mehta","Yoav Artzi","Jason Baldridge","Eugene Ie","Piotr Mirowski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.splu-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View","tldr":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively wi...","track":"Spatial Language Understanding"},"id":"WS-10.9","presentation_id":"38940082","rocketchat_channel":"paper-splu2020-9","speakers":"Harsh Mehta|Yoav Artzi|Jason Baldridge|Eugene Ie|Piotr Mirowski","title":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View"}],"prerecorded_talks":[{"presentation_id":"38940086","speakers":"James Pustejovsky, Brandeis University","title":"\tVisualizing Meaning: Semantic Simulation of Actions and Events"},{"presentation_id":"38940087","speakers":"Julia Hockenmaier, University of Illinois at Urbana-Champaign","title":"Collaborative Construction and Communication with Minecraft"},{"presentation_id":"38940088","speakers":"Douwe Kiela,  Facebook","title":"Rethinking Benchmarking in AI: Adversarial NLI, Dynabench and Hateful Memes"},{"presentation_id":"38940089","speakers":"Bonnie Dorr, Florida Institute for Human and Machine Cognition","title":"Lexical Conceptual Structure: Spatial Representations for AI Applications"},{"presentation_id":"38940090","speakers":"Yoav Artzi, Cornell University","title":"Few-shot Grounding, Mapping and Plan Generation for Instruction Following"}],"rocketchat_channel":"workshop-splu2020","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 14:00:00 GMT","hosts":"SpLU Organizers","link":"","session_name":"QA/Poster","start_time":"Thu, 19 Nov 2020 13:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:10:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Opening Talk","start_time":"Thu, 19 Nov 2020 14:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:00:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Invited talk 1: James Pustejovsky","start_time":"Thu, 19 Nov 2020 14:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:56:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Paper Presentations","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:14:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:28:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"BERT-based Spatial Information Extraction","start_time":"Thu, 19 Nov 2020 15:14:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:42:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"A Cognitively Motivated Approach to Spatial Information Extraction","start_time":"Thu, 19 Nov 2020 15:28:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:56:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Language-Conditioned Feature Pyramids for Visual Selection Tasks","start_time":"Thu, 19 Nov 2020 15:42:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:05:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 15:56:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:55:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"Invited talk 2: Julia Hockenmeir","start_time":"Thu, 19 Nov 2020 16:05:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:51:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"Paper Presentations","start_time":"Thu, 19 Nov 2020 16:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:09:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"They are not all alike: answering different spatial questions requires different grounding strategies","start_time":"Thu, 19 Nov 2020 16:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:23:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"Categorisation, Typicality and Object-Specific Features in Spatial Referring Expressions","start_time":"Thu, 19 Nov 2020 17:09:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:37:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions","start_time":"Thu, 19 Nov 2020 17:23:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:51:00 GMT","hosts":"Archna Bhatia","link":"","session_name":"Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions","start_time":"Thu, 19 Nov 2020 17:37:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 17:51:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:50:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Invited talk 3: Yoav Artzi","start_time":"Thu, 19 Nov 2020 18:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:46:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Paper Presentations","start_time":"Thu, 19 Nov 2020 18:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:04:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports","start_time":"Thu, 19 Nov 2020 18:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:18:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View","start_time":"Thu, 19 Nov 2020 19:04:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:32:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"SpaRTQA: A Textual Question Answering Benchmark for Spatial Reasoning","start_time":"Thu, 19 Nov 2020 19:18:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:46:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Decoding Language Spatial Relations to 2D Spatial Arrangements","start_time":"Thu, 19 Nov 2020 19:32:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:45:00 GMT","hosts":"SpLU Organizers","link":"","session_name":"QA/Poster","start_time":"Thu, 19 Nov 2020 19:46:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:35:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Invited talk 4: Bonnie Dorr","start_time":"Thu, 19 Nov 2020 20:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:31:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Paper Presentations","start_time":"Thu, 19 Nov 2020 21:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:49:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Geocoding with multi-level loss for spatial language representation","start_time":"Thu, 19 Nov 2020 21:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:03:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Vision-and-Language Navigation by Reasoning over Spatial Configurations","start_time":"Thu, 19 Nov 2020 21:49:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:17:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"LiMiT: The Literal Motion in Text Dataset","start_time":"Thu, 19 Nov 2020 22:03:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:31:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"ARRAMON: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments","start_time":"Thu, 19 Nov 2020 22:17:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:45:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 22:31:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:35:00 GMT","hosts":"Malihe Alikhani","link":"","session_name":"Invited Talk 5: Douwe Kiela","start_time":"Thu, 19 Nov 2020 22:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 00:03:00 GMT","hosts":"Malihe Alikhani","link":"","session_name":"Paper Presentations","start_time":"Thu, 19 Nov 2020 23:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:49:00 GMT","hosts":"Malihe Alikhani","link":"","session_name":"Robust and Interpretable Grounding of Spatial References with Relation Networks","start_time":"Thu, 19 Nov 2020 23:35:00 GMT"},{"end_time":"Fri, 20 Nov 2020 00:03:00 GMT","hosts":"Malihe Alikhani","link":"","session_name":"RMM: A Recursive Mental Model for Dialogue Navigation","start_time":"Thu, 19 Nov 2020 23:49:00 GMT"},{"end_time":"Fri, 20 Nov 2020 01:00:00 GMT","hosts":"Parisa Kordjamshidi","link":"","session_name":"Panel Discussion","start_time":"Fri, 20 Nov 2020 00:03:00 GMT"},{"end_time":"Fri, 20 Nov 2020 02:00:00 GMT","hosts":"SpLU Organizers","link":"","session_name":"QA/Poster","start_time":"Fri, 20 Nov 2020 01:00:00 GMT"}],"title":"Spatial Language Understanding","website":"https://spatial-language.github.io","zoom_links":["https://zoom.us"]},{"abstract":"Bringing together researchers interested in applying computational techniques to problems in linguistic typology and multilingual NLP.","blocks":[{"end_time":"Thu, 19 Nov 2020 23:10:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"}],"id":"WS-11","livestream":null,"organizers":"Yevgeni Berzak, Ryan Cotterell, Anna Korhonen, Roi Reichart, Eitan Grossman, Haim Dubossarsky, Arya D. McCarthy, Edoardo Maria Ponti, Ivan Vuli\u0107 and Ekaterina Vylomova","papers":[{"content":{"abstract":"","authors":["Isabel Papadimitriou","Ethan A. Chi","Richard Futrell","Kyle Mahowald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual BERT Learns Abstract Case Representations","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.10","presentation_id":"38939802","rocketchat_channel":"paper-sigtyp-10","speakers":"Isabel Papadimitriou|Ethan A. Chi|Richard Futrell|Kyle Mahowald","title":"Multilingual BERT Learns Abstract Case Representations"},{"content":{"abstract":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models\u2019 pretraining data and target language varieties.","authors":["Ethan C. Chau","Lucy H. Lin","Noah A. Smith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.118","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank","tldr":"Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar t...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.1093-WS11","presentation_id":"38940630","rocketchat_channel":"paper-sigtyp-1093-WS11","speakers":"Ethan C. Chau|Lucy H. Lin|Noah A. Smith","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"content":{"abstract":"","authors":["Harald Hammarstr\u00f6m"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Keyword Spotting: A quick-and-dirty method for extracting typological features of language from grammatical descriptions","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.11","presentation_id":"38939803","rocketchat_channel":"paper-sigtyp-11","speakers":"Harald Hammarstr\u00f6m","title":"Keyword Spotting: A quick-and-dirty method for extracting typological features of language from grammatical descriptions"},{"content":{"abstract":"This paper describes a workflow to impute missing values in a typological database, a sub- set of the World Atlas of Language Structures (WALS). Using a world-wide phylogeny de- rived from lexical data, the model assumes a phylogenetic continuous time Markov chain governing the evolution of typological val- ues. Data imputation is performed via a Max- imum Likelihood estimation on the basis of this model. As back-off model for languages whose phylogenetic position is unknown, a k- nearest neighbor classification based on geo- graphic distance is performed.","authors":["Gerhard J\u00e4ger"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Imputing typological values via phylogenetic inference","tldr":"This paper describes a workflow to impute missing values in a typological database, a sub- set of the World Atlas of Language Structures (WALS). Using a world-wide phylogeny de- rived from lexical data, the model assumes a phylogenetic continuous tim...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.12","presentation_id":"38939793","rocketchat_channel":"paper-sigtyp-12","speakers":"Gerhard J\u00e4ger","title":"Imputing typological values via phylogenetic inference"},{"content":{"abstract":"","authors":["Sonia Cristofaro","Guglielmo Inglese"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DEmA: the Pavia Diachronic Emergence of Alignment database","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.13","presentation_id":"38939804","rocketchat_channel":"paper-sigtyp-13","speakers":"Sonia Cristofaro|Guglielmo Inglese","title":"DEmA: the Pavia Diachronic Emergence of Alignment database"},{"content":{"abstract":"","authors":["Barend Beekhuizen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A dataset and metric to evaluate lexical extraction from parallel corpora","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.14","presentation_id":"38939805","rocketchat_channel":"paper-sigtyp-14","speakers":"Barend Beekhuizen","title":"A dataset and metric to evaluate lexical extraction from parallel corpora"},{"content":{"abstract":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings \u2013 both static and contextualized \u2013 for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners \u2013 even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.","authors":["Masoud Jalili Sabet","Philipp Dufter","Fran\u00e7ois Yvon","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.147","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings","tldr":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. Howeve...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.1409","presentation_id":"38940631","rocketchat_channel":"paper-sigtyp-1409","speakers":"Masoud Jalili Sabet|Philipp Dufter|Fran\u00e7ois Yvon|Hinrich Sch\u00fctze","title":"SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings"},{"content":{"abstract":"","authors":["Limor Raviv","Antje Meyer","Shiri Lev-Ari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The role of community size and network structure in shaping linguistic diversity: experimental evidence","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.16","presentation_id":"38939806","rocketchat_channel":"paper-sigtyp-16","speakers":"Limor Raviv|Antje Meyer|Shiri Lev-Ari","title":"The role of community size and network structure in shaping linguistic diversity: experimental evidence"},{"content":{"abstract":"The paper describes the Multitasking Self-attention based approach to constrained sub-task within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers (CITATION) model. The model uses Multitasking to compute values of all WALS features for a given input language simultaneously.\n\nResults show that our approach performs at par with the baseline approaches, even though our proposed approach requires only phylogenetic and geographical attributes namely Longitude, Latitude, Genus-index, Family-index and Country-index and do not use any of the known WALS features of the respective input language, to compute its missing WALS features.","authors":["Chinmay Choudhary"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NUIG: Multitasking Self-attention based approach to SigTyp 2020 Shared Task","tldr":"The paper describes the Multitasking Self-attention based approach to constrained sub-task within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers (CITATION) model. The model uses Multitasking to...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.17","presentation_id":"38939794","rocketchat_channel":"paper-sigtyp-17","speakers":"Chinmay Choudhary","title":"NUIG: Multitasking Self-attention based approach to SigTyp 2020 Shared Task"},{"content":{"abstract":"This paper enumerates SigTyP 2020 Shared Task on the prediction of typological features as performed by the KMI-Panlingua-IITKGP team. The task entailed the prediction of missing values in a particular language, provided, the name of the language family, its genus, location (in terms of latitude and longitude coordinates and name of the country where it is spoken) and a set of feature-value pair are available. As part of fulfillment of the aforementioned task, the team submitted 3 kinds of system - 2 rule-based and one hybrid system. Of these 3, one rule-based system generated the best performance on the test set. All the systems were \u2018constrained\u2019 in the sense that no additional dataset or information, other than those provided by the organisers, was used for developing the systems.","authors":["Ritesh Kumar","Deepak Alok","Akanksha Bansal","Bornini Lahiri","Atul Kr. Ojha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KMI-Panlingua-IITKGP @SIGTYP2020: Exploring rules and hybrid systems for automatic prediction of typological features","tldr":"This paper enumerates SigTyP 2020 Shared Task on the prediction of typological features as performed by the KMI-Panlingua-IITKGP team. The task entailed the prediction of missing values in a particular language, provided, the name of the language fam...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.18","presentation_id":"38939795","rocketchat_channel":"paper-sigtyp-18","speakers":"Ritesh Kumar|Deepak Alok|Akanksha Bansal|Bornini Lahiri|Atul Kr. Ojha","title":"KMI-Panlingua-IITKGP @SIGTYP2020: Exploring rules and hybrid systems for automatic prediction of typological features"},{"content":{"abstract":"","authors":["Michael Richter","Tariq Yousef"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information from Topic Contexts: The Prediction of Aspectual Coding of Verbs in Russian","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.2","presentation_id":"38939796","rocketchat_channel":"paper-sigtyp-2","speakers":"Michael Richter|Tariq Yousef","title":"Information from Topic Contexts: The Prediction of Aspectual Coding of Verbs in Russian"},{"content":{"abstract":"","authors":["Chiara Alzetta","Felice Dell'Orletta","Simonetta Montemagni","Giulia Venturi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncovering Typological Context-Sensitive Features","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.3","presentation_id":"38939797","rocketchat_channel":"paper-sigtyp-3","speakers":"Chiara Alzetta|Felice Dell'Orletta|Simonetta Montemagni|Giulia Venturi","title":"Uncovering Typological Context-Sensitive Features"},{"content":{"abstract":"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We employ frequentist inference to represent correlations between typological features and use this representation to train simple multi-class estimators that predict individual features. We describe two submitted ridge regression-based configurations which ranked second and third overall in the constrained task. Our best configuration achieved the microaveraged accuracy score of 0.66 on 149 test languages.","authors":["Alexander Gutkin","Richard Sproat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task","tldr":"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We ...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.4","presentation_id":"38939791","rocketchat_channel":"paper-sigtyp-4","speakers":"Alexander Gutkin|Richard Sproat","title":"NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task"},{"content":{"abstract":"","authors":["Alexander Gutkin","Martin Jansche","Lucy Skidmore"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Induction of Structured Phoneme Inventories","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.5","presentation_id":"38939798","rocketchat_channel":"paper-sigtyp-5","speakers":"Alexander Gutkin|Martin Jansche|Lucy Skidmore","title":"Towards Induction of Structured Phoneme Inventories"},{"content":{"abstract":"","authors":["Ahmet \u00dcst\u00fcn","Arianna Bisazza","Gosse Bouma","Gertjan Van Noord"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is Typology-Based Adaptation Effective for Multilingual Sequence Labelling?","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.6","presentation_id":"38939799","rocketchat_channel":"paper-sigtyp-6","speakers":"Ahmet \u00dcst\u00fcn|Arianna Bisazza|Gosse Bouma|Gertjan Van Noord","title":"Is Typology-Based Adaptation Effective for Multilingual Sequence Labelling?"},{"content":{"abstract":"We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing conditional probabilities and mutual information. The second approach is to train a neural predictor operating on precomputed language embeddings based on WALS features. Our submitted system combines the two approaches based on their self-estimated confidence scores. We reach the accuracy of 70.7% on the test data and rank first in the shared task.","authors":["Martin Vastl","Daniel Zeman","Rudolf Rosa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities: \u00daFAL Submission to the SIGTYP 2020 Shared Task","tldr":"We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.7","presentation_id":"38939792","rocketchat_channel":"paper-sigtyp-7","speakers":"Martin Vastl|Daniel Zeman|Rudolf Rosa","title":"Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities: \u00daFAL Submission to the SIGTYP 2020 Shared Task"},{"content":{"abstract":"","authors":["Aryaman Arora","Nathan Schneider"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SNACS Annotation of Case Markers and Adpositions in Hindi","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.8","presentation_id":"38939800","rocketchat_channel":"paper-sigtyp-8","speakers":"Aryaman Arora|Nathan Schneider","title":"SNACS Annotation of Case Markers and Adpositions in Hindi"},{"content":{"abstract":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.","authors":["Saurabh Kulshreshtha","Jose Luis Redondo Garcia","Ching-Yun Chang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.83","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study","tldr":"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved b...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.816","presentation_id":"38940629","rocketchat_channel":"paper-sigtyp-816","speakers":"Saurabh Kulshreshtha|Jose Luis Redondo Garcia|Ching-Yun Chang","title":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study"},{"content":{"abstract":"","authors":["Yushi Hu","Shane Settle","Karen Livescu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multilingual Jointly Trained Acoustic and Written Word Embeddings","tldr":null,"track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.9","presentation_id":"38939801","rocketchat_channel":"paper-sigtyp-9","speakers":"Yushi Hu|Shane Settle|Karen Livescu","title":"Multilingual Jointly Trained Acoustic and Written Word Embeddings"},{"content":{"abstract":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world\u2019s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.","authors":["Johannes Bjerva","Elizabeth Salesky","Sabrina J. Mielke","Aditi Chaudhary","Celano Giuseppe","Edoardo Maria Ponti","Ekaterina Vylomova","Ryan Cotterell","Isabelle Augenstein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sigtyp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SIGTYP 2020 Shared Task: Prediction of Typological Features","tldr":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world\u2019s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer lear...","track":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"},"id":"WS-11.2020.sigtyp-1.1","presentation_id":"","rocketchat_channel":"paper-sigtyp-1","speakers":"Johannes Bjerva|Elizabeth Salesky|Sabrina J. Mielke|Aditi Chaudhary|Celano Giuseppe|Edoardo Maria Ponti|Ekaterina Vylomova|Ryan Cotterell|Isabelle Augenstein","title":"SIGTYP 2020 Shared Task: Prediction of Typological Features"}],"prerecorded_talks":[{"presentation_id":"38939785","speakers":"Miriam Butt","title":"Invited talk"},{"presentation_id":"38939786","speakers":"Yulia Tsvetkov","title":"Invited talk"},{"presentation_id":"38939787","speakers":"Richard Sproat","title":"Invited talk"},{"presentation_id":"38939788","speakers":"Bill Croft","title":"Invited talk"},{"presentation_id":"38939789","speakers":"Harald Hammarstr\u00f6m","title":"Invited talk"},{"presentation_id":"38939790","speakers":"Johannes Bjerva, Elizabeth Salesky, Sabrina J. Mielke, Aditi Chaudhary, Giuseppe G. A. Celano, Edoardo M. Ponti, Ekaterina Vylomova, Ryan Cotterell, Isabelle Augenstein","title":"SIGTYP 2020 Shared Task: Prediction of Typological Features"}],"rocketchat_channel":"workshop-sigtyp","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 13:40:00 GMT","hosts":"TBD","link":"","session_name":"Opening Session","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:20:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Talk: Richard Sproat (Japan)","start_time":"Thu, 19 Nov 2020 13:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:30:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Q&A","start_time":"Thu, 19 Nov 2020 14:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:10:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Talk: Miriam Butt (Germany)","start_time":"Thu, 19 Nov 2020 14:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:20:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Q&A","start_time":"Thu, 19 Nov 2020 15:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:30:00 GMT","hosts":"TBD","link":"","session_name":"Coffee Break","start_time":"Thu, 19 Nov 2020 15:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:45:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Overview: Johannes Bjerva (Denmark)","start_time":"Thu, 19 Nov 2020 15:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:55:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Team 1: KMI-Panlingua-IITKGP (India)","start_time":"Thu, 19 Nov 2020 15:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:05:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Team 2: NEMO (UK+Japan)","start_time":"Thu, 19 Nov 2020 15:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:15:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Team 3:  \u00daFAL (Europe)","start_time":"Thu, 19 Nov 2020 16:05:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:25:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Team 4: Gerhard J\u00e4ger (Europe)","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:35:00 GMT","hosts":"TBD","link":"","session_name":"Shared Task Team 5: NUIG (Europe)","start_time":"Thu, 19 Nov 2020 16:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:15:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Talk: Harald Hammarstr\u00f6m (Germany)","start_time":"Thu, 19 Nov 2020 16:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:25:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Q&A","start_time":"Thu, 19 Nov 2020 17:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"TBD","link":"","session_name":"Coffee Break","start_time":"Thu, 19 Nov 2020 17:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:40:00 GMT","hosts":"TBD","link":"","session_name":"DEmA: the Pavia Diachronic Emergence of Alignment database (Europe/Italy)","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:50:00 GMT","hosts":"TBD","link":"","session_name":"A dataset and metric to evaluate lexical extraction from parallel corpora (Canada/Toronto)","start_time":"Thu, 19 Nov 2020 17:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:00:00 GMT","hosts":"TBD","link":"","session_name":"Keyword Spotting: A quick-and-dirty method for extracting typological features of language from grammatical descriptions (Europe/Germany)","start_time":"Thu, 19 Nov 2020 17:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:10:00 GMT","hosts":"TBD","link":"","session_name":"SNACS Annotation of Case Markers and Adpositions in Hindi (US/W,DC)","start_time":"Thu, 19 Nov 2020 18:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:20:00 GMT","hosts":"TBD","link":"","session_name":"Information from Topic Contexts: The Prediction of Aspectual Coding of Verbs in Russian (Europe/Germany)","start_time":"Thu, 19 Nov 2020 18:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:30:00 GMT","hosts":"TBD","link":"","session_name":"The role of community size and network structure in shaping linguistic diversity: experimental evidence (Europe;UK)","start_time":"Thu, 19 Nov 2020 18:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:05:00 GMT","hosts":"TBD","link":"","session_name":"gather.town","start_time":"Thu, 19 Nov 2020 18:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:20:00 GMT","hosts":"TBD","link":"","session_name":"SimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings (Europe/Germany)","start_time":"Thu, 19 Nov 2020 19:05:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:30:00 GMT","hosts":"TBD","link":"","session_name":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank (US/Pacific)","start_time":"Thu, 19 Nov 2020 19:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:45:00 GMT","hosts":"TBD","link":"","session_name":"Is Typology-Based Adaptation Effective for Multilingual Sequence Labelling? (Netherlands)","start_time":"Thu, 19 Nov 2020 19:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:00:00 GMT","hosts":"TBD","link":"","session_name":"Multilingual BERT Learns Abstract Case Representations (US/California)","start_time":"Thu, 19 Nov 2020 19:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:10:00 GMT","hosts":"TBD","link":"","session_name":"Towards Induction of Structured Phoneme Inventories (UK)","start_time":"Thu, 19 Nov 2020 20:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:20:00 GMT","hosts":"TBD","link":"","session_name":"Uncovering Typological Context-Sensitive Features (Europe/Italy)","start_time":"Thu, 19 Nov 2020 20:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:30:00 GMT","hosts":"TBD","link":"","session_name":"Multilingual Jointly Trained Acoustic and Written Word Embeddings (US/Chicago)","start_time":"Thu, 19 Nov 2020 20:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:45:00 GMT","hosts":"TBD","link":"","session_name":"Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study (US/UK)","start_time":"Thu, 19 Nov 2020 20:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:20:00 GMT","hosts":"TBD","link":"","session_name":"gather.town","start_time":"Thu, 19 Nov 2020 20:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:00:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Talk: Yulia Tsvetkov (US/Pennsylvania)","start_time":"Thu, 19 Nov 2020 21:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:10:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Q&A","start_time":"Thu, 19 Nov 2020 22:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 22:50:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Talk: Bill Croft (US/New Mexico)","start_time":"Thu, 19 Nov 2020 22:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:00:00 GMT","hosts":"TBD","link":"","session_name":"Keynote Q&A","start_time":"Thu, 19 Nov 2020 22:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 23:10:00 GMT","hosts":"TBD","link":"","session_name":"Closing","start_time":"Thu, 19 Nov 2020 23:00:00 GMT"}],"title":"SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology","website":"https://sigtyp.github.io/ws2020.html","zoom_links":["https://zoom.us"]},{"abstract":"Explore challenges and promises of clinical text: understand a patient's story, predict health outcomes, advance medical research, and more.","blocks":[{"end_time":"Thu, 19 Nov 2020 21:50:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:45:00 GMT"}],"id":"WS-12","livestream":null,"organizers":"Anna Rumshisky, Steven Bethard, Kirk Roberts and Tristan Naumann","papers":[{"content":{"abstract":"Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods. However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network (DCAN), integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art.","authors":["Shaoxiong Ji","Erik Cambria","Pekka Marttinen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text","tldr":"Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignme...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1","presentation_id":"38939807","rocketchat_channel":"paper-clinicalnlp-1","speakers":"Shaoxiong Ji|Erik Cambria|Pekka Marttinen","title":"Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text"},{"content":{"abstract":"Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) tasks. In this work, we present a novel extension to the Transformer architecture, by incorporating signature transform with the self-attention model. This architecture is added between embedding and prediction layers. Experiments on a new Swedish prescription data show the proposed architecture to be superior in two of the three information extraction tasks, comparing to baseline models. Finally, we evaluate two different embedding approaches between applying Multilingual BERT and translating the Swedish text to English then encode with a BERT model pretrained on clinical notes.","authors":["John Pougu\u00e9 Biyong","Bo Wang","Terry Lyons","Alejo Nevado-Holgado"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder","tldr":"Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.10","presentation_id":"38939813","rocketchat_channel":"paper-clinicalnlp-10","speakers":"John Pougu\u00e9 Biyong|Bo Wang|Terry Lyons|Alejo Nevado-Holgado","title":"Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder"},{"content":{"abstract":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinically incorrect radiology reports. In this work, we develop a radiology report generation model utilizing the transformer architecture that produces superior reports as measured by both standard language generation and clinical coherence metrics compared to competitive baselines. We then develop a method to differentiably extract clinical information from generated reports and utilize this differentiability to fine-tune our model to produce more clinically coherent reports.","authors":["Justin Lovelace","Bobak Mortazavi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.110","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning to Generate Clinically Coherent Chest X-Ray Reports","tldr":"Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinica...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1041","presentation_id":"38940177","rocketchat_channel":"paper-clinicalnlp-1041","speakers":"Justin Lovelace|Bobak Mortazavi","title":"Learning to Generate Clinically Coherent Chest X-Ray Reports"},{"content":{"abstract":"Reading comprehension style question-answering (QA) based on patient-specific documents represents a growing area in clinical NLP with plentiful applications. Bidirectional Encoder Representations from Transformers (BERT) and its derivatives lead the state-of-the-art accuracy on the task, but most evaluation has treated the data as a pre-mixture without systematically looking into the potential effect of imperfect train/test questions. The current study seeks to address this gap by experimenting with full versus partial train/test data consisting of paraphrastic questions. Our key findings include 1) training with all pooled question variants yielded best accuracy, 2) the accuracy varied widely, from 0.74 to 0.80, when trained with each single question variant, and 3) questions of similar lexical/syntactic structure tended to induce identical answers. The results suggest that how you ask questions matters in BERT-based QA, especially at the training stage.","authors":["Sungrim (Riea) Moon","Jungwei Fan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How You Ask Matters: The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset","tldr":"Reading comprehension style question-answering (QA) based on patient-specific documents represents a growing area in clinical NLP with plentiful applications. Bidirectional Encoder Representations from Transformers (BERT) and its derivatives lead the...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.11","presentation_id":"38939814","rocketchat_channel":"paper-clinicalnlp-11","speakers":"Sungrim (Riea) Moon|Jungwei Fan","title":"How You Ask Matters: The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset"},{"content":{"abstract":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall \u2018EDSS\u2019 score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS-BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS-BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state-of-the-art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule-based approaches.","authors":["Alister D\u2019Costa","Stefan Denkovski","Michal Malyska","Sae Young Moon","Brandon Rufino","Zhen Yang","Taylor Killian","Marzyeh Ghassemi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multiple Sclerosis Severity Classification From Clinical Text","tldr":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.12","presentation_id":"38939815","rocketchat_channel":"paper-clinicalnlp-12","speakers":"Alister D\u2019Costa|Stefan Denkovski|Michal Malyska|Sae Young Moon|Brandon Rufino|Zhen Yang|Taylor Killian|Marzyeh Ghassemi","title":"Multiple Sclerosis Severity Classification From Clinical Text"},{"content":{"abstract":"Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists\u2019 reporting style varies from one to another as sentences are written in a telegraphic format and do not follow general English grammar rules. In this work, we present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1% accuracy.","authors":["Morteza Pourreza Shahri","Amir Tahmasebi","Bingyang Ye","Henghui Zhu","Javed Aslam","Timothy Ferris"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Ensemble Approach to Automatic Structuring of Radiology Reports","tldr":"Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, speci...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.13","presentation_id":"38939816","rocketchat_channel":"paper-clinicalnlp-13","speakers":"Morteza Pourreza Shahri|Amir Tahmasebi|Bingyang Ye|Henghui Zhu|Javed Aslam|Timothy Ferris","title":"An Ensemble Approach to Automatic Structuring of Radiology Reports"},{"content":{"abstract":"Stroke is one of the leading causes of death and disability worldwide. Stroke is treatable, but it is prone to disability after treatment and must be prevented. To grasp the degree of disability caused by stroke, we use magnetic resonance imaging text records to predict stroke and measure the performance according to the document-level and sentence-level representation. As a result of the experiment, the document-level representation shows better performance.","authors":["Tak-Sung Heo","Chulho Kim","Jeong-Myeong Choi","Yeong-Seok Jeong","Yu-Seop Kim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records","tldr":"Stroke is one of the leading causes of death and disability worldwide. Stroke is treatable, but it is prone to disability after treatment and must be prevented. To grasp the degree of disability caused by stroke, we use magnetic resonance imaging tex...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.15","presentation_id":"38939817","rocketchat_channel":"paper-clinicalnlp-15","speakers":"Tak-Sung Heo|Chulho Kim|Jeong-Myeong Choi|Yeong-Seok Jeong|Yu-Seop Kim","title":"Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records"},{"content":{"abstract":"Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches that aim to accurately anchor temporal information on a timeline. Relative and incomplete time expressions (RI-Timexes) are expressions that require additional information for their temporal anchor to be resolved, but few studies have addressed this challenge specifically. In this study, we aimed to reproduce and verify a classification approach for identifying anchor dates and relations in clinical text, and propose a novel relation classification approach for this task.","authors":["Louise Dupuis","Nicol Bergou","Hegler Tissot","Sumithra Velupillai"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relative and Incomplete Time Expression Anchoring for Clinical Text","tldr":"Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches tha...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.16","presentation_id":"38939818","rocketchat_channel":"paper-clinicalnlp-16","speakers":"Louise Dupuis|Nicol Bergou|Hegler Tissot|Sumithra Velupillai","title":"Relative and Incomplete Time Expression Anchoring for Clinical Text"},{"content":{"abstract":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.","authors":["Jianmo Ni","Chun-Nan Hsu","Amilcare Gentili","Julian McAuley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.176","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays","tldr":"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists\u2019 workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such model...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1640","presentation_id":"38940178","rocketchat_channel":"paper-clinicalnlp-1640","speakers":"Jianmo Ni|Chun-Nan Hsu|Amilcare Gentili|Julian McAuley","title":"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays"},{"content":{"abstract":"One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks.","authors":["Zhi Wen","Xing Han Lu","Siva Reddy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining","tldr":"One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designe...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.17","presentation_id":"38939819","rocketchat_channel":"paper-clinicalnlp-17","speakers":"Zhi Wen|Xing Han Lu|Siva Reddy","title":"MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining"},{"content":{"abstract":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical notes only provide additional predictive power over structured information in readmission prediction. We further propose a probing framework to select parts of notes that enable more accurate predictions than using all notes, despite that the selected information leads to a distribution shift from the training data (\u201call notes\u201d). Finally, we demonstrate that models trained on the selected valuable information achieve even better predictive performance, with only 6.8%of all the tokens for readmission prediction.","authors":["Chao-Chun Hsu","Shantanu Karnwal","Sendhil Mullainathan","Ziad Obermeyer","Chenhao Tan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.187","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Characterizing the Value of Information in Medical Notes","tldr":"Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmis...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.1713","presentation_id":"38940179","rocketchat_channel":"paper-clinicalnlp-1713","speakers":"Chao-Chun Hsu|Shantanu Karnwal|Sendhil Mullainathan|Ziad Obermeyer|Chenhao Tan","title":"Characterizing the Value of Information in Medical Notes"},{"content":{"abstract":"We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extraction (Track 2) n2c2 data-set. We identify best practices for transfer learning, such as language-model fine-tuning and scalar mix. Our transfer learning models achieve strong performance in the overall task (F1=92.91%) as well as in ADE identification (F1=53.08%). Flair-based embeddings out-perform in the identification of context-dependent entities such as ADE. BERT-based embeddings out-perform in recognizing clinical terminology such as Drug and Form entities. ELMo-based embeddings deliver competitive performance in all entities. We develop a sentence-augmentation method for enhanced ADE identification benefiting BERT-based and ELMo-based models by up to 3.13% in F1 gains. Finally, we show that a simple ensemble of these models out-paces most current methods in ADE extraction (F1=55.77%).","authors":["Sankaran Narayanan","Kaivalya Mannam","Sreeranga P Rajan","P Venkat Rangan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Transfer Learning for Adverse Drug Event (ADE) and Medication Entity Extraction","tldr":"We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extracti...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.18","presentation_id":"38939820","rocketchat_channel":"paper-clinicalnlp-18","speakers":"Sankaran Narayanan|Kaivalya Mannam|Sreeranga P Rajan|P Venkat Rangan","title":"Evaluation of Transfer Learning for Adverse Drug Event (ADE) and Medication Entity Extraction"},{"content":{"abstract":"In this work, we propose a novel goal-oriented dialog task, automatic symptom detection. We build a system that can interact with patients through dialog to detect and collect clinical symptoms automatically, which can save a doctor\u2019s time interviewing the patient. Given a set of explicit symptoms provided by the patient to initiate a dialog for diagnosing, the system is trained to collect implicit symptoms by asking questions, in order to collect more information for making an accurate diagnosis. After getting the reply from the patient for each question, the system also decides whether current information is enough for a human doctor to make a diagnosis. To achieve this goal, we propose two neural models and a training pipeline for the multi-step reasoning task. We also build a knowledge graph as additional inputs to further improve model performance. Experiments show that our model significantly outperforms the baseline by 4%, discovering 67% of implicit symptoms on average with a limited number of questions.","authors":["Hongyin Luo","Shang-Wen Li","James Glass"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowledge Grounded Conversational Symptom Detection with Graph Memory Networks","tldr":"In this work, we propose a novel goal-oriented dialog task, automatic symptom detection. We build a system that can interact with patients through dialog to detect and collect clinical symptoms automatically, which can save a doctor\u2019s time interviewi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.19","presentation_id":"38939821","rocketchat_channel":"paper-clinicalnlp-19","speakers":"Hongyin Luo|Shang-Wen Li|James Glass","title":"Knowledge Grounded Conversational Symptom Detection with Graph Memory Networks"},{"content":{"abstract":"A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications.","authors":["Patrick Lewis","Myle Ott","Jingfei Du","Veselin Stoyanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art","tldr":"A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial t...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.20","presentation_id":"38939822","rocketchat_channel":"paper-clinicalnlp-20","speakers":"Patrick Lewis|Myle Ott|Jingfei Du|Veselin Stoyanov","title":"Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"},{"content":{"abstract":"Reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of risk factors that have been hypothesized to be important for understanding re-admission risk, including such factors as problems with substance abuse, ability to maintain work, relations with family. In this work, we develop Roberta-based models to predict the sentiment of sentences describing readmission risk factors in discharge summaries of patients with psychosis. We improve substantially on previous results by a scheme that shares information across risk factors while also allowing the model to learn risk factor-specific information.","authors":["Xiyu Ding","Mei-Hua Hall","Timothy Miller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries","tldr":"Reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of risk factors that have been hypothesized to be important for understanding re-admission risk, ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.21","presentation_id":"38939823","rocketchat_channel":"paper-clinicalnlp-21","speakers":"Xiyu Ding|Mei-Hua Hall|Timothy Miller","title":"Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries"},{"content":{"abstract":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce additional errors that can lead to potentially severe health outcomes. We propose a novel machine translation-based approach, PharmMT, to automatically and reliably simplify prescription directions into patient-friendly language, thereby significantly reducing pharmacist workload. We evaluate the proposed approach over a dataset consisting of over 530K prescriptions obtained from a large mail-order pharmacy. The end-to-end system achieves a BLEU score of 60.27 against the reference directions generated by pharmacists, a 39.6% relative improvement over the rule-based normalization. Pharmacists judged 94.3% of the simplified directions as usable as-is or with minimal changes. This work demonstrates the feasibility of a machine translation-based tool for simplifying prescription directions in real-life.","authors":["Jiazhao Li","Corey Lester","Xinyan Zhao","Yuting Ding","Yun Jiang","V.G.Vinod Vydiswaran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.251","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions","tldr":"The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2127","presentation_id":"38940180","rocketchat_channel":"paper-clinicalnlp-2127","speakers":"Jiazhao Li|Corey Lester|Xinyan Zhao|Yuting Ding|Yun Jiang|V.G.Vinod Vydiswaran","title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions"},{"content":{"abstract":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT\u2019s F1 score was lower by 4 points on average than medical BERT variants.","authors":["Macarious Abadeer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts","tldr":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.23","presentation_id":"38939824","rocketchat_channel":"paper-clinicalnlp-23","speakers":"Macarious Abadeer","title":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts"},{"content":{"abstract":"","authors":["Zixu Wang","Julia Ive","Sinead Moylett","Christoph Mueller","Rudolf Cardinal","Sumithra Velupillai","John O'Brien","Robert Stewart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer's Disease (AD) using Mental Health Records: a Classification Approach","tldr":null,"track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.25","presentation_id":"38939825","rocketchat_channel":"paper-clinicalnlp-25","speakers":"Zixu Wang|Julia Ive|Sinead Moylett|Christoph Mueller|Rudolf Cardinal|Sumithra Velupillai|John O'Brien|Robert Stewart","title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer's Disease (AD) using Mental Health Records: a Classification Approach"},{"content":{"abstract":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited efforts leveraging inter-dependencies among the two granularities. We instead propose a multi-grained joint deep network to concurrently learn the ADE entity recognition and ADE sentence classification tasks. Our joint approach takes advantage of their symbiotic relationship, with a transfer of knowledge between the two levels of granularity. Our dual-attention mechanism constructs multiple distinct representations of a sentence that capture both task-specific and semantic information in the sentence, providing stronger emphasis on the key elements essential for sentence classification. Our model improves state-of- art F1-score for both tasks: (i) entity recognition of ADE words (12.5% increase) and (ii) ADE sentence classification (13.6% increase) on MADE 1.0 benchmark of EHR notes.","authors":["Susmitha Wunnava","Xiao Qin","Tabassum Kakar","Xiangnan Kong","Elke Rundensteiner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.306","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events","tldr":"An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2509","presentation_id":"38940181","rocketchat_channel":"paper-clinicalnlp-2509","speakers":"Susmitha Wunnava|Xiao Qin|Tabassum Kakar|Xiangnan Kong|Elke Rundensteiner","title":"A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events"},{"content":{"abstract":"Automated Medication Regimen (MR) extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spans for frequency, route and change, corresponding to medications discussed in the conversation. We first describe a unique dataset of annotated doctor-patient conversations and then present a weakly supervised model architecture that can perform span extraction using noisy classification data. The model utilizes an attention bottleneck inside a classification model to perform the extraction. We experiment with several variants of attention scoring and projection functions and propose a novel transformer-based attention scoring function (TAScore). The proposed combination of TAScore and Fusedmax projection achieves a 10 point increase in Longest Common Substring F1 compared to the baseline of additive scoring plus softmax projection.","authors":["Dhruvesh Patel","Sandeep Konam","Sai Prabhakar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Weakly Supervised Medication Regimen Extraction from Medical Conversations","tldr":"Automated Medication Regimen (MR) extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spa...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.26","presentation_id":"38939826","rocketchat_channel":"paper-clinicalnlp-26","speakers":"Dhruvesh Patel|Sandeep Konam|Sai Prabhakar","title":"Weakly Supervised Medication Regimen Extraction from Medical Conversations"},{"content":{"abstract":"We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardized fashions using jargon not found in other medical specialties, complicating the already difficult task of manual data extraction. We examine the performance of several state-of-the-art neural methods for relation extraction of radiotherapy treatment details, with a goal of automating detailed information extraction. The neural systems perform at 0.82-0.88 macro-average F1, which approximates or in some cases exceeds the inter-annotator agreement. To the best of our knowledge, this is the first effort to develop models for radiotherapy relation extraction and one of the few efforts for relation extraction to describe cancer treatment in general.","authors":["Danielle Bitterman","Timothy Miller","David Harris","Chen Lin","Sean Finan","Jeremy Warner","Raymond Mak","Guergana Savova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extracting Relations between Radiotherapy Treatment Details","tldr":"We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardiz...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.27","presentation_id":"38939827","rocketchat_channel":"paper-clinicalnlp-27","speakers":"Danielle Bitterman|Timothy Miller|David Harris|Chen Lin|Sean Finan|Jeremy Warner|Raymond Mak|Guergana Savova","title":"Extracting Relations between Radiotherapy Treatment Details"},{"content":{"abstract":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient\u2019s medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","authors":["Anirudh Joshi","Namit Katariya","Xavier Amatriain","Anitha Kannan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.335","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures.","tldr":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2801","presentation_id":"38940182","rocketchat_channel":"paper-clinicalnlp-2801","speakers":"Anirudh Joshi|Namit Katariya|Xavier Amatriain|Anitha Kannan","title":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures."},{"content":{"abstract":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either medical domain-specific knowledge, or patients\u2019 prior diagnoses and clinical encounters. In this paper, we propose a novel model for automated clinical assessment generation (MCAG). MCAG is built on an innovative graph neural network, where rich clinical knowledge is incorporated into an end-to-end corpus-learning system. Our evaluation results against physician generated gold standard show that MCAG significantly improves the BLEU and rouge score compared with competitive baseline models. Further, physicians\u2019 evaluation showed that MCAG could generate high-quality assessments.","authors":["Zhichao Yang","Hong Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.336","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Accurate Electronic Health Assessment from Medical Graph","tldr":"One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic syste...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2804","presentation_id":"38940183","rocketchat_channel":"paper-clinicalnlp-2804","speakers":"Zhichao Yang|Hong Yu","title":"Generating Accurate Electronic Health Assessment from Medical Graph"},{"content":{"abstract":"In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 attributes, and 284 pairs of relations with clinical relevance. A trained medical doctor annotated these referrals, and then together with other three researchers, consolidated each of the annotations. The annotated corpus has nested entities, with 32.2% of entities embedded in other entities. We use this annotated corpus to obtain preliminary results for Named Entity Recognition (NER). The best results were achieved by using a biLSTM-CRF architecture using word embeddings trained over Spanish Wikipedia together with clinical embeddings computed by the group. NER models applied to this corpus can leverage statistics of diseases and pending procedures within this waiting list. This work constitutes the first annotated corpus using clinical narratives from Chile, and one of the few for the Spanish language. The annotated corpus, the clinical word embeddings, and the annotation guidelines are freely released to the research community.","authors":["Pablo B\u00e1ez","Fabi\u00e1n Villena","Mat\u00edas Rojas","Manuel Dur\u00e1n","Jocelyn Dunstan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Chilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in Spanish","tldr":"In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 at...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.29","presentation_id":"38939828","rocketchat_channel":"paper-clinicalnlp-29","speakers":"Pablo B\u00e1ez|Fabi\u00e1n Villena|Mat\u00edas Rojas|Manuel Dur\u00e1n|Jocelyn Dunstan","title":"The Chilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in Spanish"},{"content":{"abstract":"Loss of consciousness, so-called syncope, is a commonly occurring symptom associated with worse prognosis for a number of heart-related diseases. We present a comparison of methods for a diagnosis classification task in Norwegian clinical notes, targeting syncope, i.e. fainting cases. We find that an often neglected baseline with keyword matching constitutes a rather strong basis, but more advanced methods do offer some improvement in classification performance, especially a convolutional neural network model. The developed pipeline is planned to be used for quantifying unregistered syncope cases in Norway.","authors":["Ildiko Pilan","P\u00e5l H. Brekke","Fredrik A. Dahl","Tore Gundersen","Haldor Husby","\u00d8ystein Nytr\u00f8","Lilja \u00d8vrelid"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Classification of Syncope Cases in Norwegian Medical Records","tldr":"Loss of consciousness, so-called syncope, is a commonly occurring symptom associated with worse prognosis for a number of heart-related diseases. We present a comparison of methods for a diagnosis classification task in Norwegian clinical notes, targ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.3","presentation_id":"38939808","rocketchat_channel":"paper-clinicalnlp-3","speakers":"Ildiko Pilan|P\u00e5l H. Brekke|Fredrik A. Dahl|Tore Gundersen|Haldor Husby|\u00d8ystein Nytr\u00f8|Lilja \u00d8vrelid","title":"Classification of Syncope Cases in Norwegian Medical Records"},{"content":{"abstract":"With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for Portuguese, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in Brazilian Portuguese. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the baseline model in F1-score by 2.72%, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new model.","authors":["Elisa Terumi Rubel Schneider","Jo\u00e3o Vitor Andrioli de Souza","Julien Knafou","Lucas Emanuel Silva e Oliveira","Jenny Copara","Yohan Bonescki Gumiel","Lucas Ferro Antunes de Oliveira","Emerson Cabrera Paraiso","Douglas Teodoro","Cl\u00e1udia Maria Cabral Moro Barra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition","tldr":"With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity reco...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.30","presentation_id":"38939829","rocketchat_channel":"paper-clinicalnlp-30","speakers":"Elisa Terumi Rubel Schneider|Jo\u00e3o Vitor Andrioli de Souza|Julien Knafou|Lucas Emanuel Silva e Oliveira|Jenny Copara|Yohan Bonescki Gumiel|Lucas Ferro Antunes de Oliveira|Emerson Cabrera Paraiso|Douglas Teodoro|Cl\u00e1udia Maria Cabral Moro Barra","title":"BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition"},{"content":{"abstract":"A cancer registry is a critical and massive database for which various types of domain knowledge are needed and whose maintenance requires labor-intensive data curation. In order to facilitate the curation process for building a high-quality and integrated cancer registry database, we compiled a cross-hospital corpus and applied neural network methods to develop a natural language processing system for extracting cancer registry variables buried in unstructured pathology reports. The performance of the developed networks was compared with various baselines using standard micro-precision, recall and F-measure. Furthermore, we conducted experiments to study the feasibility of applying transfer learning to rapidly develop a well-performing system for processing reports from different sources that might be presented in different writing styles and formats. The results demonstrate that the transfer learning method enables us to develop a satisfactory system for a new hospital with only a few annotations and suggest more opportunities to reduce the burden of cancer registry curation.","authors":["Yan-Jie Lin","Hong-Jie Dai","You-Chen Zhang","Chung-Yang Wu","Yu-Cheng Chang","Pin-Jou Lu","Chih-Jen Huang","Yu-Tsang Wang","Hui-Min Hsieh","Kun-San Chao","Tsang-Wu Liu","I-Shou Chang","Yi-Hsin Connie Yang","Ti-Hao Wang","Ko-Jiunn Liu","Li-Tzong Chen","Sheau-Fang Yang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cancer Registry Information Extraction via Transfer Learning","tldr":"A cancer registry is a critical and massive database for which various types of domain knowledge are needed and whose maintenance requires labor-intensive data curation. In order to facilitate the curation process for building a high-quality and inte...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.31","presentation_id":"38939830","rocketchat_channel":"paper-clinicalnlp-31","speakers":"Yan-Jie Lin|Hong-Jie Dai|You-Chen Zhang|Chung-Yang Wu|Yu-Cheng Chang|Pin-Jou Lu|Chih-Jen Huang|Yu-Tsang Wang|Hui-Min Hsieh|Kun-San Chao|Tsang-Wu Liu|I-Shou Chang|Yi-Hsin Connie Yang|Ti-Hao Wang|Ko-Jiunn Liu|Li-Tzong Chen|Sheau-Fang Yang","title":"Cancer Registry Information Extraction via Transfer Learning"},{"content":{"abstract":"Recent studies have shown that adversarial examples can be generated by applying small perturbations to the inputs such that the well- trained deep learning models will misclassify. With the increasing number of safety and security-sensitive applications of deep learn- ing models, the robustness of deep learning models has become a crucial topic. The robustness of deep learning models for health- care applications is especially critical because the unique characteristics and the high financial interests of the medical domain make it more sensitive to adversarial attacks. Among the modalities of medical data, the clinical summaries have higher risks to be attacked because they are generated by third-party companies. As few works studied adversarial threats on clinical summaries, in this work we first apply adversarial attack to clinical summaries of electronic health records (EHR) to show the text-based deep learning systems are vulnerable to adversarial examples. Secondly, benefiting from the multi-modality of the EHR dataset, we propose a novel defense method, MATCH (Multimodal feATure Consistency cHeck), which leverages the consistency between multiple modalities in the data to defend against adversarial examples on a single modality. Our experiments demonstrate the effectiveness of MATCH on a hospital readmission prediction task comparing with baseline methods.","authors":["Wenjie Wang","Youngja Park","Taesung Lee","Ian Molloy","Pengfei Tang","Li Xiong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Utilizing Multimodal Feature Consistency to Detect Adversarial Examples on Clinical Summaries","tldr":"Recent studies have shown that adversarial examples can be generated by applying small perturbations to the inputs such that the well- trained deep learning models will misclassify. With the increasing number of safety and security-sensitive applicat...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.33","presentation_id":"38939831","rocketchat_channel":"paper-clinicalnlp-33","speakers":"Wenjie Wang|Youngja Park|Taesung Lee|Ian Molloy|Pengfei Tang|Li Xiong","title":"Utilizing Multimodal Feature Consistency to Detect Adversarial Examples on Clinical Summaries"},{"content":{"abstract":"De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 de-identification challenge datasets show that PHICON can help three selected de-identification models boost F1-score (by at most 8.6%) on cross-dataset test setting. We also discuss how much augmentation to use and how each augmentation method influences the performance.","authors":["Xiang Yue","Shuang Zhou"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation","tldr":"De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICO...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.37","presentation_id":"38939832","rocketchat_channel":"paper-clinicalnlp-37","speakers":"Xiang Yue|Shuang Zhou","title":"PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation"},{"content":{"abstract":"In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via \u201cexpert-review\u201d, where clinicians can have a dialogue with a domain expert (reviewers) and ask them questions about data entry rules. Automatically identifying \u201creal questions\u201d in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting. In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect an answer (information or help) about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence (e.g., can you clarify this?), which we will refer as \u201cc-questions\u201d. We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.","authors":["George Michalopoulos","Helen Chen","Alexander Wong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Where\u2019s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data","tldr":"In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via \u201cexpert-review...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.38","presentation_id":"38939833","rocketchat_channel":"paper-clinicalnlp-38","speakers":"George Michalopoulos|Helen Chen|Alexander Wong","title":"Where\u2019s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data"},{"content":{"abstract":"We address the problem of model generalization for sequence to sequence (seq2seq) architectures. We propose going beyond data augmentation via paraphrase-optimized multi-task learning and observe that it is useful in correctly handling unseen sentential paraphrases as inputs. Our models greatly outperform SOTA seq2seq models for semantic parsing on diverse domains (Overnight - up to 3.2% and emrQA - 7%) and Nematus, the winning solution for WMT 2017, for Czech to English translation (CzENG 1.6 - 1.5 BLEU).","authors":["So Yeon Min","Preethi Raghavan","Peter Szolovits"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Advancing Seq2seq with Joint Paraphrase Learning","tldr":"We address the problem of model generalization for sequence to sequence (seq2seq) architectures. We propose going beyond data augmentation via paraphrase-optimized multi-task learning and observe that it is useful in correctly handling unseen sentent...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.39","presentation_id":"38939834","rocketchat_channel":"paper-clinicalnlp-39","speakers":"So Yeon Min|Preethi Raghavan|Peter Szolovits","title":"Advancing Seq2seq with Joint Paraphrase Learning"},{"content":{"abstract":"In this paper, we evaluate several machine learning methods for multi-label classification of text questions. Every nursing student in the United States must pass the National Council Licensure Examination (NCLEX) to begin professional practice. NCLEX defines a number of competencies on which students are evaluated. By labeling test questions with NCLEX competencies, we can score students according to their performance in each competency. This information helps instructors measure how prepared students are for the NCLEX, as well as which competencies they may need help with. A key challenge is that questions may be related to more than one competency. Labeling questions with NCLEX competencies, therefore, equates to a multi-label, text classification problem where each competency is a label. Here we present an evaluation of several methods to support this use case along with a proposed approach. While our work is grounded in the nursing education domain, the methods described here can be used for any multi-label, text classification use case.","authors":["John Langton","Krishna Srihasam","Junlin Jiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions","tldr":"In this paper, we evaluate several machine learning methods for multi-label classification of text questions. Every nursing student in the United States must pass the National Council Licensure Examination (NCLEX) to begin professional practice. NCLE...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.4","presentation_id":"38939809","rocketchat_channel":"paper-clinicalnlp-4","speakers":"John Langton|Krishna Srihasam|Junlin Jiang","title":"Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions"},{"content":{"abstract":"Domain pretraining followed by task fine-tuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise domain unlabelled data by assigning pseudo labels from a general model. We evaluate the approach on two clinical STS datasets, and achieve r= 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r= 0.90, a new SOTA.","authors":["Yuxia Wang","Karin Verspoor","Timothy Baldwin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity","tldr":"Domain pretraining followed by task fine-tuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise domain unlabelled data by assigning pseudo labels from ...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.40","presentation_id":"38939835","rocketchat_channel":"paper-clinicalnlp-40","speakers":"Yuxia Wang|Karin Verspoor|Timothy Baldwin","title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity"},{"content":{"abstract":"ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient\u2019s visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in free text medical notes.In this paper, we propose a machine learningmodel, BERT-XML, for large scale automatedICD coding of EHR notes, utilizing recentlydeveloped unsupervised pretraining that haveachieved state of the art performance on a va-riety of NLP tasks. We train a BERT modelfrom scratch on EHR notes, learning with vo-cabulary better suited for EHR tasks and thusoutperform off-the-shelf models. We furtheradapt the BERT architecture for ICD codingwith multi-label attention. We demonstratethe effectiveness of BERT-based models on thelarge scale ICD code classification task usingmillions of EHR notes to predict thousands ofunique codes.","authors":["Zachariah Zhang","Jingshu Liu","Narges Razavian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining","tldr":"ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient\u2019s visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.43","presentation_id":"38939836","rocketchat_channel":"paper-clinicalnlp-43","speakers":"Zachariah Zhang|Jingshu Liu|Narges Razavian","title":"BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining"},{"content":{"abstract":"In drug development, protocols define how clinical trials are conducted, and are therefore of paramount importance. They contain key patient-, investigator-, medication-, and study-related information, often elaborated in different sections in the protocol texts. Granular-level parsing on large quantity of existing protocols can accelerate clinical trial design and provide actionable insights into trial optimization. Here, we report our progresses in using deep learning NLP algorithms to enable automated protocol analytics. In particular, we combined a pre-trained BERT transformer model with joint-learning strategies to simultaneously identify clinically relevant entities (i.e. Named Entity Recognition) and extract the syntactic relations between these entities (i.e. Relation Extraction) from the eligibility criteria section in protocol texts. When comparing to standalone NER and RE models, our joint-learning strategy can effectively improve the performance of RE task while retaining similarly high NER performance, likely due to the synergy of optimizing toward both tasks\u2019 objectives via shared parameters. The derived NLP model provides an end-to-end solution to convert unstructured protocol texts into structured data source, which will be embedded into a comprehensive clinical analytics workflow for downstream trial design missions such like patient population extraction, patient enrollment rate estimation, and protocol amendment prediction.","authors":["Miao Chen","Ganhui Lan","Fang Du","Victor Lobanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics","tldr":"In drug development, protocols define how clinical trials are conducted, and are therefore of paramount importance. They contain key patient-, investigator-, medication-, and study-related information, often elaborated in different sections in the pr...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.44","presentation_id":"38939837","rocketchat_channel":"paper-clinicalnlp-44","speakers":"Miao Chen|Ganhui Lan|Fang Du|Victor Lobanov","title":"Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics"},{"content":{"abstract":"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.","authors":["John Chen","Ian Berlot-Attwell","Xindi Wang","Safwan Hossain","Frank Rudzicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Text Specific vs Blackbox Fairness Algorithms in Multimodal Clinical NLP","tldr":"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for t...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.48","presentation_id":"38939838","rocketchat_channel":"paper-clinicalnlp-48","speakers":"John Chen|Ian Berlot-Attwell|Xindi Wang|Safwan Hossain|Frank Rudzicz","title":"Analyzing Text Specific vs Blackbox Fairness Algorithms in Multimodal Clinical NLP"},{"content":{"abstract":"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consuming, automatic structuring of the eligibility criteria into various semantic categories or aspects is the need of the hour. Existing methods use hand-crafted rules and feature-based statistical machine learning methods to dynamically induce semantic aspects. However, in order to deal with paucity of aspect-annotated clinical trials data, we propose a novel weakly-supervised co-training based method which can exploit a large pool of unlabeled criteria sentences to augment the limited supervised training data, and consequently enhance the performance. Experiments with 0.2M criteria sentences show that the proposed approach outperforms the competitive supervised baselines by 12% in terms of micro-averaged F1 score for all the aspects. Probing deeper into analysis, we observe domain-specific information boosts up the performance by a significant margin.","authors":["Tirthankar Dasgupta","Ishani Mondal","Abir Naskar","Lipika Dey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria","tldr":"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consumi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.49","presentation_id":"38939839","rocketchat_channel":"paper-clinicalnlp-49","speakers":"Tirthankar Dasgupta|Ishani Mondal|Abir Naskar|Lipika Dey","title":"Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria"},{"content":{"abstract":"Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the sequence of the notes. We evaluated our models on prolonged mechanical ventilation prediction problem and our experiments demonstrated that Clinical XLNet outperforms the best baselines consistently. The models and scripts are made publicly available.","authors":["Kexin Huang","Abhishek Singh","Sitong Chen","Edward Moseley","Chih-Ying Deng","Naomi George","Charolotta Lindvall"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation","tldr":"Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the s...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.6","presentation_id":"38939810","rocketchat_channel":"paper-clinicalnlp-6","speakers":"Kexin Huang|Abhishek Singh|Sitong Chen|Edward Moseley|Chih-Ying Deng|Naomi George|Charolotta Lindvall","title":"Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation"},{"content":{"abstract":"Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdominal lymph nodes with a hierarchical relationship. We then introduce an end-to-end approach based on the combination of rules and transformer-based methods to detect these abdominal lymph node mentions and classify their types from the MRI radiology reports. We demonstrate the superior performance of a model fine-tuned on MRI reports using BlueBERT, called MriBERT. We find that MriBERT outperforms the rule-based labeler (0.957 vs 0.644 in micro weighted F1-score) as well as other BERT-based variations (0.913 - 0.928). We make the code and MriBERT publicly available at https://github.com/ncbi-nlp/bluebert, with the hope that this method can facilitate the development of medical report annotators to produce labels from scratch at scale.","authors":["Yifan Peng","Sungwon Lee","Daniel C. Elton","Thomas Shen","Yu-xing Tang","Qingyu Chen","Shuai Wang","Yingying Zhu","Ronald Summers","Zhiyong Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatic recognition of abdominal lymph nodes from clinical text","tldr":"Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdomi...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.7","presentation_id":"38939811","rocketchat_channel":"paper-clinicalnlp-7","speakers":"Yifan Peng|Sungwon Lee|Daniel C. Elton|Thomas Shen|Yu-xing Tang|Qingyu Chen|Shuai Wang|Yingying Zhu|Ronald Summers|Zhiyong Lu","title":"Automatic recognition of abdominal lymph nodes from clinical text"},{"content":{"abstract":"Ample evidence suggests that better machine learning models may be steadily obtained by training on increasingly larger datasets on natural language processing (NLP) problems from non-medical domains. Whether the same holds true for medical NLP has by far not been thoroughly investigated. This work shows that this is indeed not always the case. We reveal the somehow counter-intuitive observation that performant medical NLP models may be obtained with small amount of labeled data, quite the opposite to the common belief, most likely due to the domain specificity of the problem. We show quantitatively the effect of training data size on a fixed test set composed of two of the largest public chest x-ray radiology report datasets on the task of abnormality classification. The trained models not only make use of the training data efficiently, but also outperform the current state-of-the-art rule-based systems by a significant margin.","authors":["Jean-Baptiste Lamare","Oloruntobiloba Olatunji","Li Yao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the diminishing return of labeling clinical reports","tldr":"Ample evidence suggests that better machine learning models may be steadily obtained by training on increasingly larger datasets on natural language processing (NLP) problems from non-medical domains. Whether the same holds true for medical NLP has b...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.8","presentation_id":"38939812","rocketchat_channel":"paper-clinicalnlp-8","speakers":"Jean-Baptiste Lamare|Oloruntobiloba Olatunji|Li Yao","title":"On the diminishing return of labeling clinical reports"},{"content":{"abstract":"While Dementia with Lewy Bodies (DLB) is the second most common type of neurodegenerative dementia following Alzheimer\u2019s Disease (AD), it is difficult to distinguish from AD. We propose a method for DLB detection by using mental health record (MHR) documents from a (3-month) period before a patient has been diagnosed with DLB or AD. Our objective is to develop a model that could be clinically useful to differentiate between DLB and AD across datasets from different healthcare institutions. We cast this as a classification task using Convolutional Neural Network (CNN), an efficient neural model for text classification. We experiment with different representation models, and explore the features that contribute to model performances. In addition, we apply temperature scaling, a simple but efficient model calibration method, to produce more reliable predictions. We believe the proposed method has important potential for clinical applications using routine healthcare records, and for generalising to other relevant clinical record datasets. To the best of our knowledge, this is the first attempt to distinguish DLB from AD using mental health records, and to improve the reliability of DLB predictions.","authors":["Zixu Wang","Julia Ive","Sinead Moylett","Christoph Mueller","Rudolf Cardinal","Sumithra Velupillai","John O\u2019Brien","Robert Stewart"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.clinicalnlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer\u2019s Disease (AD) using Mental Health Records: a Classification Approach","tldr":"While Dementia with Lewy Bodies (DLB) is the second most common type of neurodegenerative dementia following Alzheimer\u2019s Disease (AD), it is difficult to distinguish from AD. We propose a method for DLB detection by using mental health record (MHR) d...","track":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)"},"id":"WS-12.2020.clinicalnlp-1.19","presentation_id":"","rocketchat_channel":"paper-clinicalnlp-19","speakers":"Zixu Wang|Julia Ive|Sinead Moylett|Christoph Mueller|Rudolf Cardinal|Sumithra Velupillai|John O\u2019Brien|Robert Stewart","title":"Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer\u2019s Disease (AD) using Mental Health Records: a Classification Approach"}],"prerecorded_talks":[{"presentation_id":"38939840","speakers":"Hong Yu","title":"Keynote 3rd Clinical Natural Language Processing Workshop"}],"rocketchat_channel":"workshop-clinicalnlp","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 14:00:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"Opening remarks </br> <a href=\"https://zoom.us\">Zoom</a>","start_time":"Thu, 19 Nov 2020 13:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:45:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"Keynote Talk: Hong Yu  </br><i>Deep Learning is Conquering Human Tasks. How Far Can it Go in Medicine? Advances, Challenges, and Future Directions</i> </br> <a href=\"https://slideslive.com/38939840\">Video</a>","start_time":"Thu, 19 Nov 2020 14:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:00:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"Keynote Q&A <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 14:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:25:00 GMT","hosts":"Steven Bethard","link":"","session_name":"Oral Session 1\n\n- [Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records](https://virtual.2020.emnlp.org/paper_WS-12.15.html)\n- [Multiple Sclerosis Severity Classification From Clinical Text](https://virtual.2020.emnlp.org/paper_WS-12.12.html)\n- [BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining](https://virtual.2020.emnlp.org/paper_WS-12.43.html)","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:40:00 GMT","hosts":"Steven Bethard","link":"","session_name":"Oral Session 1 Q&A <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 15:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:50:00 GMT","hosts":"Unhosted","link":"","session_name":"Coffee Break (Gather.Town room L)","start_time":"Thu, 19 Nov 2020 15:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:25:00 GMT","hosts":"Kirk Roberts","link":"","session_name":"Oral Session 2\n\n- [Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries](https://virtual.2020.emnlp.org/paper_WS-12.21.html)\n- [Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder](https://virtual.2020.emnlp.org/paper_WS-12.10.html)\n- [Evaluation of Transfer Learning for Adverse Drug Event (ADE) and Medication Entity Extraction](https://virtual.2020.emnlp.org/paper_WS-12.18.html)\n- [BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition](https://virtual.2020.emnlp.org/paper_WS-12.30.html)","start_time":"Thu, 19 Nov 2020 15:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:40:00 GMT","hosts":"Kirk Roberts","link":"","session_name":"Oral Session 2 Q&A  <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 16:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:00:00 GMT","hosts":"Kirk Roberts","link":"","session_name":"Poster Teasers\n\n- [Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text](https://virtual.2020.emnlp.org/paper_WS-12.1.html)\n- [Classification of Syncope Cases in Norwegian Medical Records](https://virtual.2020.emnlp.org/paper_WS-12.3.html)\n- [Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions](https://virtual.2020.emnlp.org/paper_WS-12.4.html)\n- [Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation](https://virtual.2020.emnlp.org/paper_WS-12.6.html)\n- [Automatic recognition of abdominal lymph nodes from clinical text](https://virtual.2020.emnlp.org/paper_WS-12.7.html)\n- [How You Ask Matters: The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset](https://virtual.2020.emnlp.org/paper_WS-12.11.html)\n- [Relative and Incomplete Time Expression Anchoring for Clinical Text](https://virtual.2020.emnlp.org/paper_WS-12.16.html)\n- [MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining](https://virtual.2020.emnlp.org/paper_WS-12.17.html)\n- [Knowledge Grounded Conversational Symptom Detection with Graph Memory Networks](https://virtual.2020.emnlp.org/paper_WS-12.19.html)\n- [Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art](https://virtual.2020.emnlp.org/paper_WS-12.20.html)\n- [Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts](https://virtual.2020.emnlp.org/paper_WS-12.23.html)\n- [Distinguishing between Dementia with Lewy bodies (DLB) and Alzheimer\u2019s Disease (AD) using Mental Health Records: a Classification Approach](https://virtual.2020.emnlp.org/paper_WS-12.2020.clinicalnlp-1.19.html)\n- [Weakly Supervised Medication Regimen Extraction from Medical Conversations](https://virtual.2020.emnlp.org/paper_WS-12.26.html)\n- [Extracting Relations between Radiotherapy Treatment Details](https://virtual.2020.emnlp.org/paper_WS-12.27.html)\n- [Cancer Registry Information Extraction via Transfer Learning](https://virtual.2020.emnlp.org/paper_WS-12.31.html)\n- [PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation](https://virtual.2020.emnlp.org/paper_WS-12.37.html)\n- [Where\u2019s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data](https://virtual.2020.emnlp.org/paper_WS-12.38.html)\n- [Learning from Unlabelled Data for Clinical Semantic Textual Similarity](https://virtual.2020.emnlp.org/paper_WS-12.40.html)\n- [Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics](https://virtual.2020.emnlp.org/paper_WS-12.44.html)","start_time":"Thu, 19 Nov 2020 16:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:20:00 GMT","hosts":"Unhosted","link":"","session_name":"Poster Session & Lunch (Gather.Town rooms L and M)","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:55:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"Oral Session 3\n\n- [Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria](https://virtual.2020.emnlp.org/paper_WS-12.49.html)\n- [An Ensemble Approach to Automatic Structuring of Radiology Reports](https://virtual.2020.emnlp.org/paper_WS-12.13.html)\n- [Utilizing Multimodal Feature Consistency to Detect Adversarial Examples on Clinical Summaries](https://virtual.2020.emnlp.org/paper_WS-12.33.html)\n- [Advancing Seq2seq Models with Joint Paraphrase Learning](https://virtual.2020.emnlp.org/paper_WS-12.39.html)","start_time":"Thu, 19 Nov 2020 18:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:10:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"Oral Session 3 Q&A  <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 18:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:40:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"EMNLP Findings Session 1\n\n- [Learning to Generate Clinically Coherent Chest X-Ray Reports](https://virtual.2020.emnlp.org/paper_WS-12.1041.html)\n- [Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays](https://virtual.2020.emnlp.org/paper_WS-12.1640.html)\n- [Characterizing the Valuable Information in Medical Notes](https://virtual.2020.emnlp.org/paper_WS-12.1713.html)\n- [PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions](https://virtual.2020.emnlp.org/paper_WS-12.2127.html)","start_time":"Thu, 19 Nov 2020 19:10:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:55:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"EMNLP Findings Session 1 Q&A  <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 19:40:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:05:00 GMT","hosts":"Unhosted","link":"","session_name":"Coffee Break (Gather.Town room L)","start_time":"Thu, 19 Nov 2020 19:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:35:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"EMNLP Findings Session 2\n\n- [A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events](https://virtual.2020.emnlp.org/paper_WS-12.2509.html)\n- [Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures](https://virtual.2020.emnlp.org/paper_WS-12.2801.html)\n- [Generating Accurate Electronic Health Assessment from Medical Graph](https://virtual.2020.emnlp.org/paper_WS-12.2804.html)","start_time":"Thu, 19 Nov 2020 20:05:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:50:00 GMT","hosts":"Tristan Naumann","link":"","session_name":"EMNLP Findings Session 2 Q&A  <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 20:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:20:00 GMT","hosts":"Steven Bethard","link":"","session_name":"Best Paper Session\n\n- [On the diminishing return of labeling clinical reports](https://virtual.2020.emnlp.org/paper_WS-12.8.html)\n- [The Chilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in Spanish](https://virtual.2020.emnlp.org/paper_WS-12.29.html)\n- [Analyzing Text Specific vs Blackbox Fairness Algorithms in Multimodal Clinical NLP](https://virtual.2020.emnlp.org/paper_WS-12.48.html)","start_time":"Thu, 19 Nov 2020 20:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:35:00 GMT","hosts":"Steven Bethard","link":"","session_name":"Best Paper Session Q&A  <br/> <a href=\"https://zoom.us\">Zoom</a>, <a href=\"https://emnlp2020.rocket.chat/channel/workshop-clinicalnlp\">Rocket Chat</a>","start_time":"Thu, 19 Nov 2020 21:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:50:00 GMT","hosts":"Anna Rumshisky","link":"","session_name":"Concluding Session","start_time":"Thu, 19 Nov 2020 21:35:00 GMT"}],"title":"3rd Clinical Natural Language Processing Workshop (Clinical NLP 2020)","website":"https://clinical-nlp.github.io/2020/","zoom_links":["https://zoom.us"]},{"abstract":"Deep Learning Inside Out: Knowledge Extraction and Integration for Deep Learning Architectures with special focus on low-data regimes","blocks":[{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 02:40:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Fri, 20 Nov 2020 02:00:00 GMT"}],"id":"WS-13","livestream":null,"organizers":"Eneko Agirre, Marianna Apidianaki and Ivan Vuli\u0107","papers":[{"content":{"abstract":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common misuse of Chinese idioms leads to error in corpus and causes error in the learned semantic representation of Chinese idioms. In this paper, we introduce the definition written by Chinese experts to correct the misuse. We propose a model for the Chinese idiom cloze test integrating various information effectively. We propose an attention mechanism called Attribute Attention to balance the weight of different attributes among different descriptions of the Chinese idiom. Besides the given candidates of every blank, we also try to choose the answer from all Chinese idioms that appear in the dataset as the extra loss due to the uniqueness and specificity of Chinese idioms. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Hongsheng Zhao","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test","tldr":"The cloze test for Chinese idioms is a new challenge in machine reading comprehension: given a sentence with a blank, choosing a candidate Chinese idiom which matches the context. Chinese idiom is a type of Chinese idiomatic expression. The common mi...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1","presentation_id":"38939724","rocketchat_channel":"paper-deelio-1","speakers":"Xinyu Wang|Hongsheng Zhao|Tan Yang|Hongbo Wang","title":"Correcting the Misuse: A Method for the Chinese Idiom Cloze Test"},{"content":{"abstract":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.","authors":["Guanglin Niu","Bo Li","Yongfei Zhang","Shiliang Pu","Jingyang Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.105","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding","tldr":"Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1008","presentation_id":"38940167","rocketchat_channel":"paper-deelio-1008","speakers":"Guanglin Niu|Bo Li|Yongfei Zhang|Shiliang Pu|Jingyang Li","title":"AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding"},{"content":{"abstract":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available.","authors":["Hoang Nguyen","Chenwei Zhang","Congying Xia","Philip Yu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.108","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection","tldr":"Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1039","presentation_id":"38940168","rocketchat_channel":"paper-deelio-1039","speakers":"Hoang Nguyen|Chenwei Zhang|Congying Xia|Philip Yu","title":"Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection"},{"content":{"abstract":"","authors":["Kung-Hsiang Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs","tldr":null,"track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1059","presentation_id":"38940169","rocketchat_channel":"paper-deelio-1059","speakers":"Kung-Hsiang Huang","title":"Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs"},{"content":{"abstract":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attributes, even for common entities. To improve the precision of model-based entity-attribute extraction, we propose attribute-aware embeddings, which embeds entities and attributes in the same space by the similarity of their attributes. Our model, EANET, learns these embeddings by representing entities as a weighted sum of their attributes and concatenates these embeddings to mention level features. EANET achieves up to 91% classification accuracy, outperforming strong baselines and achieves 83% precision on manually labeled high confidence extractions, outperforming Biperpedia (Gupta et al., 2014), a previous state-of-the-art for large scale entity-attribute extraction.","authors":["Dan Iter","Xiao Yu","Fangtao Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings","tldr":"Entity-attribute relations are a fundamental component for building large-scale knowledge bases, which are widely employed in modern search engines. However, most such knowledge bases are manually curated, covering only a small fraction of all attrib...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.12","presentation_id":"38939729","rocketchat_channel":"paper-deelio-12","speakers":"Dan Iter|Xiao Yu|Fangtao Li","title":"Entity Attribute Relation Extraction with Attribute-Aware Embeddings"},{"content":{"abstract":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task\u2019s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.","authors":["Xin Guo","Yu Tian","Qinghan Xue","Panos Lampropoulos","Steven Eliuk","Kenneth Barner","Xiaolong Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.164","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Continual Learning Long Short Term Memory","tldr":"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell i...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.1524","presentation_id":"38940170","rocketchat_channel":"paper-deelio-1524","speakers":"Xin Guo|Yu Tian|Qinghan Xue|Panos Lampropoulos|Steven Eliuk|Kenneth Barner|Xiaolong Wang","title":"Continual Learning Long Short Term Memory"},{"content":{"abstract":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we present OSCR (Ontology-based Semantic Composition Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pre-training. We evaluated the performance of BERT pre-trained on Wikipedia with and without OSCR by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33.3%, 18.6%, and 4% improved accuracy compared to pre-training BERT without OSCR.","authors":["Travis Goodwin","Dina Demner-Fushman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization","tldr":"Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we prese...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.16","presentation_id":"38939730","rocketchat_channel":"paper-deelio-16","speakers":"Travis Goodwin|Dina Demner-Fushman","title":"Enhancing Question Answering by Injecting Ontological Knowledge through Regularization"},{"content":{"abstract":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) is learning target concept vector representations from scratch which requires more number of training instances. Our model is based on RoBERTa and target concept embeddings. In our model, we integrate a) target concept information in the form of target concept vectors generated by encoding target concept descriptions using SRoBERTa, state-of-the-art RoBERTa based sentence embedding model and b) domain lexicon knowledge by enriching target concept vectors with synonym relationship knowledge using retrofitting algorithm. It is the first attempt in MCN to exploit both target concept information as well as domain lexicon knowledge in the form of retrofitted target concept vectors. Our model outperforms all the existing models with an accuracy improvement up to 1.36% on three standard datasets. Further, our model when trained only on mapping lexicon synonyms achieves up to 4.87% improvement in accuracy.","authors":["Katikapalli Subramanyam Kalyan","Sivanesan Sangeetha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts","tldr":"Medical concept normalization (MCN) i.e., mapping of colloquial medical phrases to standard concepts is an essential step in analysis of medical social media text. The main drawback in existing state-of-the-art approach (Kalyan and Sangeetha, 2020b) ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.17","presentation_id":"38939731","rocketchat_channel":"paper-deelio-17","speakers":"Katikapalli Subramanyam Kalyan|Sivanesan Sangeetha","title":"Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts"},{"content":{"abstract":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes.","authors":["Ting-Yun Chang","Yang Liu","Karthik Gopalakrishnan","Behnam Hedayatnia","Pei Zhou","Dilek Hakkani-Tur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks","tldr":"Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to pe...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.18","presentation_id":"38939732","rocketchat_channel":"paper-deelio-18","speakers":"Ting-Yun Chang|Yang Liu|Karthik Gopalakrishnan|Behnam Hedayatnia|Pei Zhou|Dilek Hakkani-Tur","title":"Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks"},{"content":{"abstract":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for our work is the BERT assumption according to which a large number of NLP tasks can be solved with pre-trained Transformers with no substantial task-specific changes of the architecture. However, our experiments show that the encoding strategy can have a great impact on the quality of the fine-tuning. The combination between cross-encoding and multi-input models worked better than one cross-encoder and allowed us to achieve comparable results with the state-of-the-art without the use of any external data.","authors":["Sonia Cibu","Anca Marginean"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Commonsense Statements Identification and Explanation with Transformer based Encoders","tldr":"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE competition. The starting point for ou...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.20","presentation_id":"38939733","rocketchat_channel":"paper-deelio-20","speakers":"Sonia Cibu|Anca Marginean","title":"Commonsense Statements Identification and Explanation with Transformer based Encoders"},{"content":{"abstract":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.","authors":["Marjan Albooyeh","Rishab Goel","Seyed Mehran Kazemi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.241","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Out-of-Sample Representation Learning for Knowledge Graphs","tldr":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2047","presentation_id":"38940171","rocketchat_channel":"paper-deelio-2047","speakers":"Marjan Albooyeh|Rishab Goel|Seyed Mehran Kazemi","title":"Out-of-Sample Representation Learning for Knowledge Graphs"},{"content":{"abstract":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH assumes that a context surrounding a hyponym is more informative than that of a hypernym, it has never been tested on visual objects. Since our perception is tightly associated with language, it is meaningful to explore whether the DIH holds on visual objects. To this end, we consider visual objects as the context of a word and represent a word as a bag of visual objects found in images associated with the word. This allows us to test the feasibility of the visual DIH. To better distinguish word pairs in a hypernym relation from other relations such as co-hypernyms, we also propose a new measurable function that takes into account both the difference in the generality of meaning and similarity of meaning between words. Our experimental results show that the DIH holds on visual objects and that the proposed method combined with the proposed function outperforms existing unsupervised representation methods.","authors":["Masayasu Muraoka","Tetsuya Nasukawa","Bishwaranjan Bhattacharjee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.246","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment","tldr":"We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH as...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2085","presentation_id":"38940172","rocketchat_channel":"paper-deelio-2085","speakers":"Masayasu Muraoka|Tetsuya Nasukawa|Bishwaranjan Bhattacharjee","title":"Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment"},{"content":{"abstract":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge graph embeddings and fine-grain entity type representations. Our work also shows that jointly modeling both structured knowledge tuples and language improves both.","authors":["Rajat Patel","Francis Ferraro"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling","tldr":"We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge gr...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.22","presentation_id":"38939734","rocketchat_channel":"paper-deelio-22","speakers":"Rajat Patel|Francis Ferraro","title":"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling"},{"content":{"abstract":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional information retrieval step (IR) and a kNN search over a large datastore of an embedded text collection. Our contributions are as follows: i) BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training. ii) We show that BERT often identifies the correct response category (e.g., US city), but only kNN recovers the factually correct answer (e.g.,\u201cMiami\u201d). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN can easily handle facts not covered by BERT\u2019s training set, e.g., recent events.","authors":["Nora Kassner","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.307","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA","tldr":"Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we comb...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2513","presentation_id":"38940173","rocketchat_channel":"paper-deelio-2513","speakers":"Nora Kassner|Hinrich Sch\u00fctze","title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA"},{"content":{"abstract":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train classifiers without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates \u201cweak\u201d supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages: by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier: leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of word translations.","authors":["Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.323","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher","tldr":"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2666","presentation_id":"38940174","rocketchat_channel":"paper-deelio-2666","speakers":"Giannis Karamanolakis|Daniel Hsu|Luis Gravano","title":"Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher"},{"content":{"abstract":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner.","authors":["Xiaoyu Chen","Rohan Badlani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Relation Extraction with Contextualized Relation Embedding","tldr":"This submission is a paper that proposes an architecture for the relation extraction task which integrates semantic information with knowledge base modeling in a novel manner....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.4","presentation_id":"38939725","rocketchat_channel":"paper-deelio-4","speakers":"Xiaoyu Chen|Rohan Badlani","title":"Relation Extraction with Contextualized Relation Embedding"},{"content":{"abstract":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In attacks of this variety, the adversary substitutes words with synonyms. Since synonym substitution perturbations aim to satisfy all lexical, grammatical, and semantic constraints, they are difficult to detect with automatic syntax check as well as by humans. In this paper, we propose a structure-free defensive method that is capable of improving the performance of DNN-based models with both clean and adversarial data. Our findings show that replacing the embeddings of the important words in the input samples with the average of their synonyms\u2019 embeddings can significantly improve the generalization of DNN-based classifiers. By doing so, we reduce model sensitivity to particular words in the input samples. Our results indicate that the proposed defense is not only capable of defending against adversarial attacks, but is also capable of improving the performance of DNN-based models when tested on benign data. On average, the proposed defense improved the classification accuracy of the CNN and Bi-LSTM models by 41.30% and 55.66%, respectively, when tested under adversarial attacks. Extended investigation shows that our defensive method can improve the robustness of nonneural models, achieving an average of 17.62% and 22.93% classification accuracy increase on the SVM and XGBoost models, respectively. The proposed defensive method has also shown an average of 26.60% classification accuracy improvement when tested with the infamous BERT model. Our algorithm is generic enough to be applied in any NLP domain and to any model trained on any natural language.","authors":["Basemah Alshemali","Jugal Kalita"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generalization to Mitigate Synonym Substitution Attacks","tldr":"Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples \u2013 perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In at...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.6","presentation_id":"38939726","rocketchat_channel":"paper-deelio-6","speakers":"Basemah Alshemali|Jugal Kalita","title":"Generalization to Mitigate Synonym Substitution Attacks"},{"content":{"abstract":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors. The resulting entity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no expensive further pre-training of the BERT encoder. We evaluate E-BERT on unsupervised question answering (QA), supervised relation classification (RC) and entity linking (EL). On all three tasks, E-BERT outperforms BERT and other baselines. We also show quantitatively that the original BERT model is overly reliant on the surface form of entity names (e.g., guessing that someone with an Italian-sounding name speaks Italian), and that E-BERT mitigates this problem.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.71","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT","tldr":"We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT\u2019s native wordpiece vector space and use the aligned entit...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.696","presentation_id":"38940166","rocketchat_channel":"paper-deelio-696","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"content":{"abstract":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.","authors":["Steven Y. Feng","Varun Gangal","Dongyeop Kang","Teruko Mitamura","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GenAug: Data Augmentation for Finetuning Text Generators","tldr":"In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose a...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.7","presentation_id":"38939727","rocketchat_channel":"paper-deelio-7","speakers":"Steven Y. Feng|Varun Gangal|Dongyeop Kang|Teruko Mitamura|Eduard Hovy","title":"GenAug: Data Augmentation for Finetuning Text Generators"},{"content":{"abstract":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, joint pre-training (i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective) may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/wluper/retrograph.","authors":["Anne Lauscher","Olga Majewska","Leonardo F. R. Ribeiro","Iryna Gurevych","Nikolai Rozanov","Goran Glava\u0161"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.deelio-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers","tldr":"Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, ...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.9","presentation_id":"38939728","rocketchat_channel":"paper-deelio-9","speakers":"Anne Lauscher|Olga Majewska|Leonardo F. R. Ribeiro|Iryna Gurevych|Nikolai Rozanov|Goran Glava\u0161","title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"},{"content":{"abstract":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.","authors":["Adyasha Maharana","Mohit Bansal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.333","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension","tldr":"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2797","presentation_id":"38940111","rocketchat_channel":"paper-deelio-2797","speakers":"Adyasha Maharana|Mohit Bansal","title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension"},{"content":{"abstract":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at https://github.com/weakrules/Denoise-multi-weak-sources.","authors":["Wendi Ren","Yinghao Li","Hanting Su","David Kartchner","Cassie Mitchell","Chao Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.334","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Denoising Multi-Source Weak Supervision for Neural Text Classification","tldr":"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete....","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.2800","presentation_id":"38940139","rocketchat_channel":"paper-deelio-2800","speakers":"Wendi Ren|Yinghao Li|Hanting Su|David Kartchner|Cassie Mitchell|Chao Zhang","title":"Denoising Multi-Source Weak Supervision for Neural Text Classification"},{"content":{"abstract":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG\u02c6C, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG\u02c6C consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG\u02c6C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.","authors":["Yiben Yang","Chaitanya Malaviya","Jared Fernandez","Swabha Swayamdipta","Ronan Le Bras","Ji-Ping Wang","Chandra Bhagavatula","Yejin Choi","Doug Downey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.90","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generative Data Augmentation for Commonsense Reasoning","tldr":"Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models c...","track":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures"},"id":"WS-13.884","presentation_id":"38940138","rocketchat_channel":"paper-deelio-884","speakers":"Yiben Yang|Chaitanya Malaviya|Jared Fernandez|Swabha Swayamdipta|Ronan Le Bras|Ji-Ping Wang|Chandra Bhagavatula|Yejin Choi|Doug Downey","title":"Generative Data Augmentation for Commonsense Reasoning"}],"prerecorded_talks":[{"presentation_id":"38940164","speakers":"Eduard Hovy","title":"Keynote talk by Eduard Hovy"},{"presentation_id":"38940165","speakers":"Ellie Pavlick","title":"You can lead a horse to water...: Representing vs. Using Features in Neural NLP"}],"rocketchat_channel":"workshop-deelio","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 13:35:00 GMT","hosts":"Eneko Agirre (Zoom link 1)","link":"","session_name":"Opening Remarks","start_time":"Thu, 19 Nov 2020 13:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:35:00 GMT","hosts":"Marianna Apidianaki (Zoom link 1)","link":"","session_name":"Keynote Talk 1: Ellie Pavlick","start_time":"Thu, 19 Nov 2020 13:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 14:45:00 GMT","hosts":"-","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 14:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:00:00 GMT","hosts":"Ivan Vuli\u0107 (Zoom link 1)","link":"","session_name":"Session 1: Oral Presentations & QA<br><br>\n\u2022 Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers (Anne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov and Goran Glava\u0161) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.9.html\">paper+video</a><br><br>\n\u2022 Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Task(Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia, Pei Zhou and Dilek Hakkani-Tur) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.18.html\">paper+video</a><br><br>\n\u2022 Entity Attribute Relation Extraction with Attribute-Aware Embeddings (Dan Iter and Xiao Yu) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.12.html\">paper+video</a><br><br>\n\u2022 On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling (Rajat Patel and Francis Ferraro) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.22.html\">paper+video</a><br><br>\n\u2022 Commonsense Statements Identification and Explanation with Transformer based Encoders (Sonia Cibu and Anca Marginean) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.20.html\">paper+video</a><br><br>\n\u2022 Target Concept Guided Medical Concept Normalization in Noisy User-Generated Texts (Katikapalli Subramanyam Kalyan and Sivanesan Sangeetha) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.17.html\">paper+video</a><br><br>","start_time":"Thu, 19 Nov 2020 14:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:30:00 GMT","hosts":"-","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 16:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:30:00 GMT","hosts":"Eneko Agirre (Zoom link 1)","link":"","session_name":"Keynote Talk 2: Eduard Hovy","start_time":"Thu, 19 Nov 2020 16:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:45:00 GMT","hosts":"Marianna Apidianaki (Zoom link 1)","link":"","session_name":"Session 2: Oral Presentations & QA<br><br>\n\u2022 Relation Extraction with Contextualized Relation Embedding (Xiaoyu Chen and Rohan Badlani) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.4.html\">paper+video</a><br><br>\n\u2022 GenAug: Data Augmentation for Finetuning Text Generators (Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura and Eduard Hovy) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.7.html\">paper+video</a><br><br>\n\u2022 Generalization to Mitigate Synonym Substitution Attacks (Basemah Alshemali and Jugal Kalita) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.6.html\">paper+video</a><br><br>\n\u2022 Correcting the Misuse: A Method for the Chinese Idiom Cloze Test (Xinyu Wang, Hongsheng Zhao, Tan Yang and Hongbo Wang) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1.html\">paper+video</a><br><br>\n\u2022 Enhancing Question Answering by Injecting Ontological Knowledge through Regularization (Travis Goodwin and Dina Demner-Fushman) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.16.html\">paper+video</a><br><br>","start_time":"Thu, 19 Nov 2020 17:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 19:15:00 GMT","hosts":"-","link":"","session_name":"Break","start_time":"Thu, 19 Nov 2020 18:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:40:00 GMT","hosts":"Session 3A: Ivan Vuli\u0107 (Zoom link 1), Session 3B: Eneko Agirre (Zoom link 2)","link":"","session_name":"Session 3: Findings papers presentations & QA\n(Two Parallel sessions)","start_time":"Thu, 19 Nov 2020 19:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:40:00 GMT","hosts":"Ivan Vuli\u0107 (Zoom link 1)","link":"","session_name":"Session 3A<br><br>\n\u2022 E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT (Nina Poerner, Ulli Waltinger and Hinrich Sch\u00fctze) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.696.html\">paper+video</a><br><br>\n\u2022 Generative Data Augmentation for Commonsense Reasoning (Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi and Doug Downey) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.884.html\">paper+video</a><br><br>\n\u2022 AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding (Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu and Jingyang Li) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1008.html\">paper+video</a><br><br>\n\u2022 Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection (Hoang Nguyen, Chenwei Zhang, Congying Xia and Philip Yu) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1039.html\">paper+video</a><br><br>\n\u2022 Biomedical Event Extraction with Hierarchical Knowledge Graphs (Kung-Hsiang Huang, Mu Yang and Nanyun Peng) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1059.html\">paper+video</a><br><br>\n\u2022 BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA (Nora Kassner and Hinrich Sch\u00fctze) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2513.html\">paper+video</a><br><br>","start_time":"Thu, 19 Nov 2020 19:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 20:40:00 GMT","hosts":"Eneko Agirre (Zoom link 2)","link":"","session_name":"Session 3B<br><br>\n\u2022 Out-of-Sample Representation Learning for Knowledge Graphs (Marjan Albooyeh, Rishab Goel and Seyed Mehran Kazemi) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2047.html\">paper+video</a><br><br>\n\u2022 Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment (Masayasu Muraoka, Tetsuya Nasukawa and Bishwaranjan Bhattacharjee) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2085.html\">paper+video</a><br><br>\n\u2022 Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher (Giannis Karamanolakis, Daniel Hsu and Luis Gravano) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2666.html\">paper+video</a><br><br>\n\u2022 Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension (Adyasha Maharana and Mohit Bansal)  <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2797.html\">paper+video</a><br><br>\n\u2022 Continual Learning Long Short Term Memory (Xin Guo, Yu Tian, Qinghan Xue, Panos Lampropoulos, Steven Eliuk, Kenneth Barner and Xiaolong Wang)  <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1524.html\">paper+video</a><br><br>\n\u2022 Denoising Multi-Source Weak Supervision for Neural Text Classification (Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell and Chao Zhang)  <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2800.html\">paper+video</a><br><br>","start_time":"Thu, 19 Nov 2020 19:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 21:00:00 GMT","hosts":"E. Agirre, M. Apidianaki and I. Vuli\u0107 (Zoom link 1)","link":"","session_name":"Closing remarks","start_time":"Thu, 19 Nov 2020 20:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 02:40:00 GMT","hosts":"Please use separate Zoom links","link":"","session_name":"Session 4: Additional QA Sessions<br><br>\n\u2022 GenAug: Data Augmentation for Finetuning Text Generators (Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura and Eduard Hovy) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.7.html\">paper+bi</a> (<a href=\"https://cmu.zoom.us/j/91205554634?pwd=UkFWbDhHb2paN0RhZzI2c1ZjaHdDdz09\">zoom channel</a>)<br><br>\n\u2022 Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers (Anne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov and Goran Glava\u0161) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.9.html\">paper+video</a> (<a href=\"https://us04web.zoom.us/j/77688037366?pwd=S3NmeWhncmJGekh0VE5ZMnFvSS9oUT09\">zoom channel</a>)<br><br>\n\u2022 Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Task(Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia, Pei Zhou and Dilek Hakkani-Tur) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.18.html\">paper+video</a> (<a href=\"https://us05web.zoom.us/j/87035883818?pwd=MFVMU2hzM2tKL2ZyaDBaek5BY3FZUT09\">zoom channel</a>)<br><br>\n\u2022 Commonsense Statements Identification and Explanation with Transformer based Encoders (Sonia Cibu and Anca Marginean) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.20.html\">paper+video</a> (<a href=\"https://us04web.zoom.us/j/72704160575?pwd=SDk0Vzk3djRrUDYyZ0RwS2NidzBYdz09\">zoom channel</a>)<br><br>\n\u2022 On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling (Rajat Patel and Francis Ferraro) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.22.html\">paper+video</a> (<a href=\"https://zoom.us/j/99574609777?pwd=bm5qMDNpZVkwRXRPd2Jpd3NNd0daUT09\">zoom channel</a>)<br><br>\n\u2022 E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT (Nina Poerner, Ulli Waltinger and Hinrich Sch\u00fctze) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.696.html\">paper+video</a> (<a href=\"https://zoom.us?pwd=SGdMaUZXa1VyeG9Ea21EZllLWm1vdz09\">zoom channel</a>)<br><br>\n\u2022 AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding (Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu and Jingyang Li) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1008.html\">paper+video</a> (<a href=\"https://zoom.us/j/97149632460?pwd=N0RRWXVYcnltU0dTaXpOZEFERCtnUT09\">zoom channel</a>)<br><br>\n\u2022 Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection (Hoang Nguyen, Chenwei Zhang, Congying Xia and Philip Yu) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1039.html\">paper+video</a> (<a href=\"https://zoom.us/j/91263575056?pwd=Sjc2WllYd3lzTnVUN1Q5TjRjak13dz09\">zoom channel</a>)<br><br>\n\u2022 Continual Learning Long Short Term Memory (Xin Guo, Yu Tian, Qinghan Xue, Panos Lampropoulos, Steven Eliuk, Kenneth Barner and Xiaolong Wang)  <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.1524.html\">paper+video</a> (<a href=\"https://zoom.us?pwd=bGNMVTBkU3Vka2NiY3FoeUswRzVjZz09\">zoom channel</a>)<br><br>\n\u2022 Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment (Masayasu Muraoka, Tetsuya Nasukawa and Bishwaranjan Bhattacharjee) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2085.html\">paper+video</a> (<a href=\"https://us04web.zoom.us/j/71033235132?pwd=REhqRWNxQ1BEZW5MS3pLYlRjVVdrQT09\">zoom channel</a>)<br><br>\n\u2022 Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher (Giannis Karamanolakis, Daniel Hsu and Luis Gravano) <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2666.html\">paper+video</a> (<a href=\"https://us04web.zoom.us/j/75571792938?pwd=cnc5eHFVRUZmaTFkTUFON0JJc3ZrZz09\">zoom channel</a>)<br><br>\n\u2022 Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension (Adyasha Maharana and Mohit Bansal)  <a href=\"https://virtual.2020.emnlp.org/paper_WS-13.2797.html\">paper+video</a> (<a href=\"https://unc.zoom.us/j/97249841910\">zoom channel</a><br><br>","start_time":"Fri, 20 Nov 2020 02:00:00 GMT"}],"title":"Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures","website":"https://sites.google.com/view/deelio-ws/","zoom_links":["https://zoom.us","https://zoom.us"]},{"abstract":"Social media, crowdsourcing, medical texts, language learner essays, and more! 2 keynotes, 130 PC members, talks &amp; 1-min madness for posters","blocks":[{"end_time":"Thu, 19 Nov 2020 07:35:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Thu, 19 Nov 2020 04:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:35:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"}],"id":"WS-14","livestream":null,"organizers":"Wei Xu, Alan Ritter, Timothy Baldwin, Afshin Rahimi and Leon Derczynski","papers":[{"content":{"abstract":"We investigate using Named Entity Recognition on a new type of user-generated text: a call center conversation. These conversations combine problems from spontaneous speech with problems novel to conversational Automated Speech Recognition, including incorrect recognition, alongside other common problems from noisy user-generated text. Using our own corpus with new annotations, training custom contextual string embeddings, and applying a BiLSTM-CRF, we match state-of- the-art results on our novel task.","authors":["Micaela Kaplan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"May I Ask Who\u2019s Calling? Named Entity Recognition on Call Center Transcripts for Privacy Law Compliance","tldr":"We investigate using Named Entity Recognition on a new type of user-generated text: a call center conversation. These conversations combine problems from spontaneous speech with problems novel to conversational Automated Speech Recognition, including...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.1","presentation_id":"","rocketchat_channel":"paper-wnut2020-1","speakers":"Micaela Kaplan","title":"May I Ask Who\u2019s Calling? Named Entity Recognition on Call Center Transcripts for Privacy Law Compliance"},{"content":{"abstract":"With the increased use of social media platforms by people across the world, many new interesting NLP problems have come into existence. One such being the detection of sarcasm in the social media texts. We present a corpus of tweets for training custom word embeddings and a Hinglish dataset labelled for sarcasm detection. We propose a deep learning based approach to address the issue of sarcasm detection in Hindi-English code mixed tweets using bilingual word embeddings derived from FastText and Word2Vec approaches. We experimented with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention). We were able to outperform all state-of-the-art performances with our deep learning models, with attention based Bi-directional LSTMs giving the best performance exhibiting an accuracy of 78.49%.","authors":["Akshita Aggarwal","Anshul Wadhawan","Anshima Chaudhary","Kavita Maurya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cDid you really mean what you said?\u201d : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings","tldr":"With the increased use of social media platforms by people across the world, many new interesting NLP problems have come into existence. One such being the detection of sarcasm in the social media texts. We present a corpus of tweets for training cus...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.2","presentation_id":"","rocketchat_channel":"paper-wnut2020-2","speakers":"Akshita Aggarwal|Anshul Wadhawan|Anshima Chaudhary|Kavita Maurya","title":"\u201cDid you really mean what you said?\u201d : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings"},{"content":{"abstract":"Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by practitioners to build industrial NLP applications, it is hard to guarantee absence of any noise in the data. While BERT has performed exceedingly well for transferring the learnings from one use case to another, it remains unclear how BERT performs when fine-tuned on noisy text. In this work, we explore the sensitivity of BERT to noise in the data. We work with most commonly occurring noise (spelling mistakes, typos) and show that this results in significant degradation in the performance of BERT. We present experimental results to show that BERT\u2019s performance on fundamental NLP tasks like sentiment analysis and textual similarity drops significantly in the presence of (simulated) noise on benchmark datasets viz. IMDB Movie Review, STS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline that are responsible for this drop in performance. Our findings suggest that practitioners need to be vary of presence of noise in their datasets while fine-tuning BERT to solve industry use cases.","authors":["Ankit Kumar","Piyush Makhija","Anuj Gupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Noisy Text Data: Achilles\u2019 Heel of BERT","tldr":"Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.3","presentation_id":"","rocketchat_channel":"paper-wnut2020-3","speakers":"Ankit Kumar|Piyush Makhija|Anuj Gupta","title":"Noisy Text Data: Achilles\u2019 Heel of BERT"},{"content":{"abstract":"Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer (QA) plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response. We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers (Question Plausibility AUROC=0.75, Response Plausibility AUROC=0.78, Answer Extraction F1=0.665).","authors":["Rachel Gardner","Maya Varma","Clare Zhu","Ranjay Krishna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning","tldr":"Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.4","presentation_id":"","rocketchat_channel":"paper-wnut2020-4","speakers":"Rachel Gardner|Maya Varma|Clare Zhu|Ranjay Krishna","title":"Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning"},{"content":{"abstract":"Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the social media genre, despite the fact that social media often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the Arabic language, which is successful in categorizing social media posts in Arabic dialects, despite only having been trained on Modern Standard Arabic. Our hypothesis in this paper is that the performance of LMs for social media can nonetheless be improved by incorporating static word vectors that have been specifically trained on social media. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).","authors":["Israa Alghanmi","Luis Espinosa Anke","Steven Schockaert"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Combining BERT with Static Word Embeddings for Categorizing Social Media","tldr":"Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the social media genre, despite the fact that social media often has ver...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.5","presentation_id":"","rocketchat_channel":"paper-wnut2020-5","speakers":"Israa Alghanmi|Luis Espinosa Anke|Steven Schockaert","title":"Combining BERT with Static Word Embeddings for Categorizing Social Media"},{"content":{"abstract":"Cross-sentence attention has been widely applied in text matching, in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. However, commonly the intermediate representations are generated solely based on the preceding layers and the models may suffer from error propagation and unstable matching, especially when multiple attention layers are used. In this paper, we pro-pose an enhanced sentence alignment network with simple gated feature augmentation, where the model is able to flexibly integrate both original word and contextual features to improve the cross-sentence attention. Moreover, our model is less complex with fewer parameters compared to many state-of-the-art structures.Experiments on three benchmark datasets validate our model capacity for text matching.","authors":["Zhe Hu","Zuohui Fu","Cheng Peng","Weiwei Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhanced Sentence Alignment Network for Efficient Short Text Matching","tldr":"Cross-sentence attention has been widely applied in text matching, in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. However, commonly the intermediate representati...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.6","presentation_id":"","rocketchat_channel":"paper-wnut2020-6","speakers":"Zhe Hu|Zuohui Fu|Cheng Peng|Weiwei Wang","title":"Enhanced Sentence Alignment Network for Efficient Short Text Matching"},{"content":{"abstract":"Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding natural language to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as Google Translate fail at times to translate code-mixed text effectively. To address this challenge, we present a parallel corpus of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in English. In addition, we also propose a translation pipeline build on top of Google Translate. The evaluation of the proposed pipeline on PHINC demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.","authors":["Vivek Srivastava","Mayank Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation","tldr":"Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text messa...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.7","presentation_id":"","rocketchat_channel":"paper-wnut2020-7","speakers":"Vivek Srivastava|Mayank Singh","title":"PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation"},{"content":{"abstract":"Sentiment analysis research in low-resource languages such as Bengali is still unexplored due to the scarcity of annotated data and the lack of text processing tools. Therefore, in this work, we focus on generating resources and showing the applicability of the cross-lingual sentiment analysis approach in Bengali. For benchmarking, we created and annotated a comprehensive corpus of around 12000 Bengali reviews. To address the lack of standard text-processing tools in Bengali, we leverage resources from English utilizing machine translation. We determine the performance of supervised machine learning (ML) classifiers in machine-translated English corpus and compare it with the original Bengali corpus. Besides, we examine sentiment preservation in the machine-translated corpus utilizing Cohen\u2019s Kappa and Gwet\u2019s AC1. To circumvent the laborious data labeling process, we explore lexicon-based methods and study the applicability of utilizing cross-domain labeled data from the resource-rich language. We find that supervised ML classifiers show comparable performances in Bengali and machine-translated English corpus. By utilizing labeled data, they achieve 15%-20% higher F1 scores compared to both lexicon-based and transfer learning-based methods. Besides, we observe that machine translation does not alter the sentiment polarity of the review for most of the cases. Our experimental results demonstrate that the machine translation based cross-lingual approach can be an effective way for sentiment classification in Bengali.","authors":["Salim Sazzed"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cross-lingual sentiment classification in low-resource Bengali language","tldr":"Sentiment analysis research in low-resource languages such as Bengali is still unexplored due to the scarcity of annotated data and the lack of text processing tools. Therefore, in this work, we focus on generating resources and showing the applicabi...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.8","presentation_id":"","rocketchat_channel":"paper-wnut2020-8","speakers":"Salim Sazzed","title":"Cross-lingual sentiment classification in low-resource Bengali language"},{"content":{"abstract":"As the largest institutionalized second language variety of English, Indian English has received a sustained focus from linguists for decades. However, to the best of our knowledge, no prior study has contrasted web-expressions of Indian English in noisy social media with English generated by a social media user base that are predominantly native speakers. In this paper, we address this gap in the literature through conducting a comprehensive analysis considering multiple structural and semantic aspects. In addition, we propose a novel application of language models to perform automatic linguistic quality assessment.","authors":["Rupak Sarkar","Sayantan Mahinder","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Non-native Speaker Aspect: Indian English in Social Media","tldr":"As the largest institutionalized second language variety of English, Indian English has received a sustained focus from linguists for decades. However, to the best of our knowledge, no prior study has contrasted web-expressions of Indian English in n...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.9","presentation_id":"","rocketchat_channel":"paper-wnut2020-9","speakers":"Rupak Sarkar|Sayantan Mahinder|Ashiqur KhudaBukhsh","title":"The Non-native Speaker Aspect: Indian English in Social Media"},{"content":{"abstract":"For NLP, sentence boundary detection (SBD) is an essential task to decompose a text into sentences. Most of the previous studies have used a simple rule that uses only typical characters as sentence boundaries. However, some characters may or may not be sentence boundaries depending on the context. We focused on line breaks in them. We newly constructed annotated corpora, implemented sentence boundary detectors, and analyzed performance of SBD in several settings.","authors":["Yuta Hayashibe","Kensuke Mitsuzawa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Sentence Boundary Detection on Line Breaks in Japanese","tldr":"For NLP, sentence boundary detection (SBD) is an essential task to decompose a text into sentences. Most of the previous studies have used a simple rule that uses only typical characters as sentence boundaries. However, some characters may or may not...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.10","presentation_id":"","rocketchat_channel":"paper-wnut2020-10","speakers":"Yuta Hayashibe|Kensuke Mitsuzawa","title":"Sentence Boundary Detection on Line Breaks in Japanese"},{"content":{"abstract":"Recently, the number of user-generated recipes on the Internet has increased. In such recipes, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a user-generated recipe are not actually edible ingredients. For example, headings, comments, and kitchenware sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use recipes for a variety of tasks, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed method achieved a 93.3 F1 score.","authors":["Yasuhiro Yamaguchi","Shintaro Inuzuka","Makoto Hiramatsu","Jun Harashima"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach","tldr":"Recently, the number of user-generated recipes on the Internet has increased. In such recipes, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a user-generate...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.11","presentation_id":"","rocketchat_channel":"paper-wnut2020-11","speakers":"Yasuhiro Yamaguchi|Shintaro Inuzuka|Makoto Hiramatsu|Jun Harashima","title":"Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach"},{"content":{"abstract":"We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenting a diversified set of sentences from the documents that explain its decision on the correctness of the textual claim. Prior approaches to address the problem of fact checking and evidence extraction have relied on simple concatenation of claim and document word embeddings as an input to claim driven attention weight computation. This is done so as to extract salient words and sentences from the documents that help establish the correctness of the claim. However this design of claim-driven attention fails to capture the contextual information in documents properly. We improve on the prior art by using improved claim and title guided hierarchical attention to model effective contextual cues. We show the efficacy of our approach on political, healthcare, and environmental datasets.","authors":["Rahul Mishra","Dhruv Gupta","Markus Leippold"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Fact Checking Summaries for Web Claims","tldr":"We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenti...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.12","presentation_id":"","rocketchat_channel":"paper-wnut2020-12","speakers":"Rahul Mishra|Dhruv Gupta|Markus Leippold","title":"Generating Fact Checking Summaries for Web Claims"},{"content":{"abstract":"This paper explores how Dutch diary fragments, written by family coaches in the social sector, can be analysed automatically using machine learning techniques to quantitatively measure the impact of social coaching. The focus lays on two tasks: determining which sentiment a fragment contains (sentiment analysis) and investigating which fundamental social rights (education, employment, legal aid, etc.) are addressed in the fragment. To train and test the new algorithms, a dataset consisting of 1715 Dutch diary fragments is used. These fragments are manually labelled on sentiment and on the applicable fundamental social rights. The sentiment analysis models were trained to classify the fragments into three classes: negative, neutral or positive. Fine-tuning the Dutch pre-trained Bidirectional Encoder Representations from Transformers (BERTje) (de Vries et al., 2019) language model surpassed the more classic algorithms by correctly classifying 79.6% of the fragments on the sentiment analysis, which is considered as a good result. This technique also achieved the best results in the identification of the fundamental rights, where for every fragment the three most likely fundamental rights were given as output. In this way, 93% of the present fundamental rights were correctly recognised. To our knowledge, we are the first to try to extract social rights from written text with the help of Natural Language Processing techniques.","authors":["Koen Kicken","Tessa De Maesschalck","Bart Vanrumste","Tom De Keyser","Hee Reen Shim"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Intelligent Analyses on Storytelling for Impact Measurement","tldr":"This paper explores how Dutch diary fragments, written by family coaches in the social sector, can be analysed automatically using machine learning techniques to quantitatively measure the impact of social coaching. The focus lays on two tasks: deter...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.13","presentation_id":"","rocketchat_channel":"paper-wnut2020-13","speakers":"Koen Kicken|Tessa De Maesschalck|Bart Vanrumste|Tom De Keyser|Hee Reen Shim","title":"Intelligent Analyses on Storytelling for Impact Measurement"},{"content":{"abstract":"Automated agents (\u201cbots\u201d) have emerged as an ubiquitous and influential presence on social media. Bots engage on social media platforms by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single bot on Reddit. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on Reddit. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on social media with relatively simple bots is important for preparing for more advanced bots in the future.","authors":["Ming-Cheng Ma","John P. Lalor"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Analysis of Human-Bot Interaction on Reddit","tldr":"Automated agents (\u201cbots\u201d) have emerged as an ubiquitous and influential presence on social media. Bots engage on social media platforms by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.14","presentation_id":"","rocketchat_channel":"paper-wnut2020-14","speakers":"Ming-Cheng Ma|John P. Lalor","title":"An Empirical Analysis of Human-Bot Interaction on Reddit"},{"content":{"abstract":"We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, orthographic variation, acronyms, and slang. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a topic model for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on information retrieval.","authors":["Jack Hughes","Seth Aycock","Andrew Caines","Paula Buttery","Alice Hutchings"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Trending Terms in Cybersecurity Forum Discussions","tldr":"We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.15","presentation_id":"","rocketchat_channel":"paper-wnut2020-15","speakers":"Jack Hughes|Seth Aycock|Andrew Caines|Paula Buttery|Alice Hutchings","title":"Detecting Trending Terms in Cybersecurity Forum Discussions"},{"content":{"abstract":"Crowdsourcing is the go-to solution for data collection and annotation in the context of NLP tasks. Nevertheless, crowdsourced data is noisy by nature; the source is often unknown and additional validation work is performed to guarantee the dataset\u2019s quality. In this article, we compare two crowdsourcing sources on a dialogue paraphrasing task revolving around a chatbot service. We observe that workers hired on crowdsourcing platforms produce lexically poorer and less diverse rewrites than service users engaged voluntarily. Notably enough, on dialogue clarity and optimality, the two paraphrase sources\u2019 human-perceived quality does not differ significantly. Furthermore, for the chatbot service, the combined crowdsourced data is enough to train a transformer-based Natural Language Generation (NLG) system. To enable similar services, we also release tools for collecting data and training the dialogue-act-based transformer-based NLG module.","authors":["Luca Molteni","Mittul Singh","Juho Leinonen","Katri Leino","Mikko Kurimo","Emanuele Della Valle"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Service registration chatbot: collecting and comparing dialogues from AMT workers and service\u2019s users","tldr":"Crowdsourcing is the go-to solution for data collection and annotation in the context of NLP tasks. Nevertheless, crowdsourced data is noisy by nature; the source is often unknown and additional validation work is performed to guarantee the dataset\u2019s...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.16","presentation_id":"","rocketchat_channel":"paper-wnut2020-16","speakers":"Luca Molteni|Mittul Singh|Juho Leinonen|Katri Leino|Mikko Kurimo|Emanuele Della Valle","title":"Service registration chatbot: collecting and comparing dialogues from AMT workers and service\u2019s users"},{"content":{"abstract":"The requirement of performing assessments continually on a larger scale necessitates the implementation of automated systems for evaluation of the learners\u2019 responses to free-text questions. We target children of age group 8-14 years and use an ASR integrated assessment app to crowdsource learners\u2019 responses to free text questions in Hindi. The app helped collect 39641 user answers to 35 different questions of Science topics. Since the users are young children from rural India and may not be well-equipped with technology, it brings in various noise types in the answers. We describe these noise types and propose a preprocessing pipeline to denoise user\u2019s answers. We showcase the performance of different similarity metrics on the noisy and denoised versions of user and model answers. Our findings have large-scale applications for automated answer assessment for school children in India in low resource settings.","authors":["Dolly Agarwal","Somya Gupta","Nishant Baghel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automated Assessment of Noisy Crowdsourced Free-text Answers for Hindi in Low Resource Setting","tldr":"The requirement of performing assessments continually on a larger scale necessitates the implementation of automated systems for evaluation of the learners\u2019 responses to free-text questions. We target children of age group 8-14 years and use an ASR i...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.17","presentation_id":"","rocketchat_channel":"paper-wnut2020-17","speakers":"Dolly Agarwal|Somya Gupta|Nishant Baghel","title":"Automated Assessment of Noisy Crowdsourced Free-text Answers for Hindi in Low Resource Setting"},{"content":{"abstract":"Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.","authors":["Tanvirul Alam","Akib Khan","Firoj Alam"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Punctuation Restoration using Transformer Models for Resource-Rich and -Poor Languages","tldr":"Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.18","presentation_id":"","rocketchat_channel":"paper-wnut2020-18","speakers":"Tanvirul Alam|Akib Khan|Firoj Alam","title":"Punctuation Restoration using Transformer Models for Resource-Rich and -Poor Languages"},{"content":{"abstract":"True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user queries to a voice-controlled dia- log system. In particular, we compare a rule- based, an n-gram language model (LM) and a recurrent neural network (RNN) approaches, evaluating the results on a German Q&A cor- pus and reporting accuracy for different case categories. We show that while RNNs reach higher accuracy especially on large datasets, character n-gram models with interpolation are still competitive, in particular on mixed- case words where their fall-back mechanisms come into play.","authors":["Yulia Grishina","Thomas Gueudre","Ralf Winkler"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Truecasing German user-generated conversational text","tldr":"True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user que...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.19","presentation_id":"","rocketchat_channel":"paper-wnut2020-19","speakers":"Yulia Grishina|Thomas Gueudre|Ralf Winkler","title":"Truecasing German user-generated conversational text"},{"content":{"abstract":"The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that fine-tuning using naturally occurring noise along with pseudo-references (i.e. \u201ccorrected\u201d non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from English to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here: https://github.com/mahfuzibnalam/finetuning_for_robustness .","authors":["Md Mahfuz Ibn Alam","Antonios Anastasopoulos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations","tldr":"The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate th...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.20","presentation_id":"","rocketchat_channel":"paper-wnut2020-20","speakers":"Md Mahfuz Ibn Alam|Antonios Anastasopoulos","title":"Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations"},{"content":{"abstract":"Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.","authors":["Aparna Balagopalan","Ksenia Shkaruta","Jekaterina Novikova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Impact of ASR on Alzheimer\u2019s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others","tldr":"Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.21","presentation_id":"","rocketchat_channel":"paper-wnut2020-21","speakers":"Aparna Balagopalan|Ksenia Shkaruta|Jekaterina Novikova","title":"Impact of ASR on Alzheimer\u2019s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others"},{"content":{"abstract":"The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among polyglots. Given the rising importance of dialogue agents, it is imperative that they understand code-mixing, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The dataset by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of language modeling, data augmentation, translation, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this dataset. We obtain an 8.09% increase in test set accuracy over the current state of the art.","authors":["Sharanya Chakravarthy","Anjana Umapathy","Alan W Black"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Entailment in Code-Mixed Hindi-English Conversations","tldr":"The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages,...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.22","presentation_id":"","rocketchat_channel":"paper-wnut2020-22","speakers":"Sharanya Chakravarthy|Anjana Umapathy|Alan W Black","title":"Detecting Entailment in Code-Mixed Hindi-English Conversations"},{"content":{"abstract":"Student reviews often make reference to professors\u2019 physical appearances. Until recently RateMyProfessors.com, the website of this study\u2019s focus, used a design feature to encourage a \u201chot or not\u201d rating of college professors. In the wake of recent #MeToo and #TimesUp movements, social awareness of the inappropriateness of these reviews has grown; however, objectifying comments remain and continue to be posted in this online context. We describe two supervised text classifiers for detecting objectifying commentary in professor reviews. We then ensemble these classifiers and use the resulting model to track objectifying commentary at scale. We measure correlations between objectifying commentary, changes to the review website interface, and teacher gender across a ten-year period.","authors":["Angie Waller","Kyle Gorman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Objectifying Language in Online Professor Reviews","tldr":"Student reviews often make reference to professors\u2019 physical appearances. Until recently RateMyProfessors.com, the website of this study\u2019s focus, used a design feature to encourage a \u201chot or not\u201d rating of college professors. In the wake of recent #M...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.23","presentation_id":"","rocketchat_channel":"paper-wnut2020-23","speakers":"Angie Waller|Kyle Gorman","title":"Detecting Objectifying Language in Online Professor Reviews"},{"content":{"abstract":"India is home to several languages with more than 30m speakers. These languages exhibit significant presence on social media platforms. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these languages is also typically authored in the Roman script as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our models to facilitate downstream analyses in these low-resource languages. Experiments across multiple social media corpora demonstrate the model\u2019s robustness and provide several interesting insights on Indian language usage patterns on social media. We release an annotated data set of 1,000 comments in ten Romanized languages as a social media evaluation benchmark.","authors":["Shriphani Palakodety","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Annotation Efficient Language Identification from Weak Labels","tldr":"India is home to several languages with more than 30m speakers. These languages exhibit significant presence on social media platforms. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) m...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.24","presentation_id":"","rocketchat_channel":"paper-wnut2020-24","speakers":"Shriphani Palakodety|Ashiqur KhudaBukhsh","title":"Annotation Efficient Language Identification from Weak Labels"},{"content":{"abstract":"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an example of how this method can be used to assist classification in fields where interpretability is important, such as health care.","authors":["Ben Eyre","Aparna Balagopalan","Jekaterina Novikova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach","tldr":"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manuall...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.25","presentation_id":"","rocketchat_channel":"paper-wnut2020-25","speakers":"Ben Eyre|Aparna Balagopalan|Jekaterina Novikova","title":"Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach"},{"content":{"abstract":"Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to use may significantly impact the success of the training. In this work, we propose a metric for evaluating augmentation heuristics; specifically, we quantify the extent to which an example is \u201chard to distinguish\u201d by considering the difference between the distribution of the augmented samples of different classes. Experimenting with multiple heuristics in two prediction tasks (positive/negative sentiment and verbosity/conciseness) validates our claims by revealing the connection between the distribution difference of different classes and the classification accuracy.","authors":["Omid Kashefi","Rebecca Hwa"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation","tldr":"Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.26","presentation_id":"","rocketchat_channel":"paper-wnut2020-26","speakers":"Omid Kashefi|Rebecca Hwa","title":"Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation"},{"content":{"abstract":"The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, however, unclear whether these improvements translate to noisy user-generated text, such as tweets. In this paper, we present an experimental survey of a wide range of well-known text representation techniques for the task of text clustering on noisy Twitter data. Our results indicate that the more advanced models do not necessarily work best on tweets and that more exploration in this area is needed.","authors":["Lili Wang","Chongyang Gao","Jason Wei","Weicheng Ma","Ruibo Liu","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data","tldr":"The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, how...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.27","presentation_id":"","rocketchat_channel":"paper-wnut2020-27","speakers":"Lili Wang|Chongyang Gao|Jason Wei|Weicheng Ma|Ruibo Liu|Soroush Vosoughi","title":"An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data"},{"content":{"abstract":"We present CUT, a dataset for studying Civil Unrest on Twitter. Our dataset includes 4,381 tweets related to civil unrest, hand-annotated with information related to the study of civil unrest discussion and events. Our dataset is drawn from 42 countries from 2014 to 2019. We present baseline systems trained on this data for the identification of tweets related to civil unrest. We include a discussion of ethical issues related to research on this topic.","authors":["Justin Sech","Alexandra DeLucia","Anna L. Buczak","Mark Dredze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest","tldr":"We present CUT, a dataset for studying Civil Unrest on Twitter. Our dataset includes 4,381 tweets related to civil unrest, hand-annotated with information related to the study of civil unrest discussion and events. Our dataset is drawn from 42 countr...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.28","presentation_id":"","rocketchat_channel":"paper-wnut2020-28","speakers":"Justin Sech|Alexandra DeLucia|Anna L. Buczak|Mark Dredze","title":"Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest"},{"content":{"abstract":"To identify what entities are being talked about in tweets, we need to automatically link named entities that appear in tweets to structured KBs like WikiData. Existing approaches often struggle with such short, noisy texts, or their complex design and reliance on supervision make them brittle, difficult to use and maintain, and lose significance over time. Further, there is a lack of a large, linked corpus of tweets to aid researchers, along with lack of gold dataset to evaluate the accuracy of entity linking. In this paper, we introduce (1) Tweeki, an unsupervised, modular entity linking system for Twitter, (2) TweekiData, a large, automatically-annotated corpus of Tweets linked to entities in WikiData, and (3) TweekiGold, a gold dataset for entity linking evaluation. Through comprehensive analysis, we show that Tweeki is comparable to the performance of recent state-of-the-art entity linkers models, the dataset is of high quality, and a use case of how the dataset can be used to improve downstream tasks in social media analysis (geolocation prediction).","authors":["Bahareh Harandizadeh","Sameer Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tweeki: Linking Named Entities on Twitter to a Knowledge Graph","tldr":"To identify what entities are being talked about in tweets, we need to automatically link named entities that appear in tweets to structured KBs like WikiData. Existing approaches often struggle with such short, noisy texts, or their complex design a...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.29","presentation_id":"","rocketchat_channel":"paper-wnut2020-29","speakers":"Bahareh Harandizadeh|Sameer Singh","title":"Tweeki: Linking Named Entities on Twitter to a Knowledge Graph"},{"content":{"abstract":"In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document styles and to enhance model performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of writing style. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but it relies only on the metadata of the articles which are available in huge amounts. We evaluate the model on multiple datasets, on both the authorship clustering and the authorship attribution tasks.","authors":["Julien Hay","Bich-Lien Doan","Fabrice Popineau","Ouassim Ait Elhara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Representation learning of writing style","tldr":"In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document sty...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.30","presentation_id":"","rocketchat_channel":"paper-wnut2020-30","speakers":"Julien Hay|Bich-Lien Doan|Fabrice Popineau|Ouassim Ait Elhara","title":"Representation learning of writing style"},{"content":{"abstract":"The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a social media post. Unlike previous works, we impose inductive biases that capture platform specific user behavior. These biases, coupled with social media fine-tuning of BERT allow for better language understanding, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.","authors":["Karthik Radhakrishnan","Tushar Kanakagiri","Sharanya Chakravarthy","Vidhisha Balachandran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"\u201cA Little Birdie Told Me ... \" - Social Media Rumor Detection","tldr":"The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.31","presentation_id":"","rocketchat_channel":"paper-wnut2020-31","speakers":"Karthik Radhakrishnan|Tushar Kanakagiri|Sharanya Chakravarthy|Vidhisha Balachandran","title":"\u201cA Little Birdie Told Me ... \" - Social Media Rumor Detection"},{"content":{"abstract":"Paraphrase generation is an important problem in Natural Language Processing that has been addressed with neural network-based approaches recently. This paper presents an adversarial framework to address the paraphrase generation problem in English. Unlike previous methods, we employ the discriminator output as penalization instead of using policy gradients, and we propose a global discriminator to avoid the Monte-Carlo search. In addition, this work use and compare different settings of input representation. We compare our methods to some baselines in the Quora question pairs dataset. The results show that our framework is competitive against the previous benchmarks.","authors":["Gerson Vizcarra","Jose Ochoa-Luna"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrase Generation via Adversarial Penalizations","tldr":"Paraphrase generation is an important problem in Natural Language Processing that has been addressed with neural network-based approaches recently. This paper presents an adversarial framework to address the paraphrase generation problem in English. ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.32","presentation_id":"","rocketchat_channel":"paper-wnut2020-32","speakers":"Gerson Vizcarra|Jose Ochoa-Luna","title":"Paraphrase Generation via Adversarial Penalizations"},{"content":{"abstract":"This paper presents the results of the wet labinformation extraction task at WNUT 2020.This task consisted of two sub tasks- (1) anamed entity recognition task with 13 partic-ipants; and (2) a relation extraction task with2 participants. We outline the task, data an-notation process, corpus statistics, and providea high-level overview of the participating sys-tems for each sub task.","authors":["Jeniya Tabassum","Wei Xu","Alan Ritter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols","tldr":"This paper presents the results of the wet labinformation extraction task at WNUT 2020.This task consisted of two sub tasks- (1) anamed entity recognition task with 13 partic-ipants; and (2) a relation extraction task with2 participants. We outline t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.33","presentation_id":"","rocketchat_channel":"paper-wnut2020-33","speakers":"Jeniya Tabassum|Wei Xu|Alan Ritter","title":"WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols"},{"content":{"abstract":"Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks.For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the medical field contains a lot of different tokens used only in the medical industry such as the name of different diseases, devices, organisms,medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the System for Named Entity Tagging based on Bio-Bert. Experimental results show that our model gives substantial improvements over the baseline and stood the fourth runner up in terms of F1 score, and first runner up in terms of Recall with just 2.21 F1 score behind the best one.","authors":["Tejas Vaidhya","Ayush Kaushal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IITKGP at W-NUT 2020 Shared Task-1: Domain specific BERT representation for Named Entity Recognition of lab protocol","tldr":"Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks.For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.34","presentation_id":"","rocketchat_channel":"paper-wnut2020-34","speakers":"Tejas Vaidhya|Ayush Kaushal","title":"IITKGP at W-NUT 2020 Shared Task-1: Domain specific BERT representation for Named Entity Recognition of lab protocol"},{"content":{"abstract":"In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings (like Flair, BERT-based) and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling (SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.","authors":["Janvijay Singh","Anshul Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings","tldr":"In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with vari...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.35","presentation_id":"","rocketchat_channel":"paper-wnut2020-35","speakers":"Janvijay Singh|Anshul Wadhawan","title":"PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings"},{"content":{"abstract":"Relation and event extraction is an important task in natural language processing. We introduce a system which uses contextualized knowledge graph completion to classify relations and events between known entities in a noisy text environment. We report results which show that our system is able to effectively extract relations and events from a dataset of wet lab protocols.","authors":["Chris Miller","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Big Green at WNUT 2020 Shared Task-1: Relation Extraction as Contextualized Sequence Classification","tldr":"Relation and event extraction is an important task in natural language processing. We introduce a system which uses contextualized knowledge graph completion to classify relations and events between known entities in a noisy text environment. We repo...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.36","presentation_id":"","rocketchat_channel":"paper-wnut2020-36","speakers":"Chris Miller|Soroush Vosoughi","title":"Big Green at WNUT 2020 Shared Task-1: Relation Extraction as Contextualized Sequence Classification"},{"content":{"abstract":"The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols.","authors":["Kaushik Acharya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT 2020 Shared Task-1: Conditional Random Field(CRF) based Named Entity Recognition(NER) for Wet Lab Protocols","tldr":"The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols....","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.37","presentation_id":"","rocketchat_channel":"paper-wnut2020-37","speakers":"Kaushik Acharya","title":"WNUT 2020 Shared Task-1: Conditional Random Field(CRF) based Named Entity Recognition(NER) for Wet Lab Protocols"},{"content":{"abstract":"We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60% in terms of F-score as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46% in terms of F-score as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.","authors":["Mohammad Golam Sohrab","Anh-Khoa Duong Nguyen","Makoto Miwa","Hiroya Takamura"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.38","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"mgsohrab at WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols","tldr":"We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.38","presentation_id":"","rocketchat_channel":"paper-wnut2020-38","speakers":"Mohammad Golam Sohrab|Anh-Khoa Duong Nguyen|Makoto Miwa|Hiroya Takamura","title":"mgsohrab at WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols"},{"content":{"abstract":"Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more interests with the development of deep learning. This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity extract, that we conducted studies in several models, including a BiLSTM CRF model and a Bert case model which can be used to complete wet lab entity extraction. And we mainly discussed the performance differences of Bert case under different situations such as transformers versions, case sensitivity that may don\u2019t get enough attention before.","authors":["Qingcheng Zeng","Xiaoyang Fang","Zhexin Liang","Haoding Meng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.39","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fancy Man Launches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction","tldr":"Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more i...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.39","presentation_id":"","rocketchat_channel":"paper-wnut2020-39","speakers":"Qingcheng Zeng|Xiaoyang Fang|Zhexin Liang|Haoding Meng","title":"Fancy Man Launches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction"},{"content":{"abstract":"Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper, we present a 3-step method based on deep neural language models that reported the best overall exact match F1-score (77.99%) of the competition. By fine-tuning 10 times, 10 different pretrained language models, this work shows the advantage of having more models in an ensemble based on a majority of votes strategy. On top of that, having 100 different models allowed us to analyse the combinations of ensemble that demonstrated the impact of having multiple pretrained models versus fine-tuning a pretrained model multiple times.","authors":["Julien Knafou","Nona Naderi","Jenny Copara","Douglas Teodoro","Patrick Ruch"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models","tldr":"Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper,...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.40","presentation_id":"","rocketchat_channel":"paper-wnut2020-40","speakers":"Julien Knafou|Nona Naderi|Jenny Copara|Douglas Teodoro|Patrick Ruch","title":"BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models"},{"content":{"abstract":"In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.","authors":["Dat Quoc Nguyen","Thanh Vu","Afshin Rahimi","Mai Hoang Dao","Linh The Nguyen","Long Doan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.41","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets","tldr":"In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.41","presentation_id":"","rocketchat_channel":"paper-wnut2020-41","speakers":"Dat Quoc Nguyen|Thanh Vu|Afshin Rahimi|Mai Hoang Dao|Linh The Nguyen|Long Doan","title":"WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.","authors":["Anh Tuan Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TATL at WNUT-2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets","tldr":"As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the m...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.42","presentation_id":"","rocketchat_channel":"paper-wnut2020-42","speakers":"Anh Tuan Nguyen","title":"TATL at WNUT-2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"The outbreak of COVID-19 has greatly impacted our daily lives. In these circumstances, it is important to grasp the latest information to avoid causing too much fear and panic. To help grasp new information, extracting information from social networking sites is one of the effective ways. In this paper, we describe a method to identify whether a tweet related to COVID-19 is informative or not, which can help to grasp new information. The key features of our method are its use of graph attention networks to encode syntactic dependencies and word positions in the sentence, and a loss function based on connectionist temporal classification (CTC) that can learn a label for each token without reference data for each token. Experimental results show that the proposed method achieved an F1 score of 0.9175, out- performing baseline methods.","authors":["Yuki Yasuda","Taichi Ishiwatari","Taro Miyazaki","Jun Goto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.43","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NHK_STRL at WNUT-2020 Task 2: GATs with Syntactic Dependencies as Edges and CTC-based Loss for Text Classification","tldr":"The outbreak of COVID-19 has greatly impacted our daily lives. In these circumstances, it is important to grasp the latest information to avoid causing too much fear and panic. To help grasp new information, extracting information from social network...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.43","presentation_id":"","rocketchat_channel":"paper-wnut2020-43","speakers":"Yuki Yasuda|Taichi Ishiwatari|Taro Miyazaki|Jun Goto","title":"NHK_STRL at WNUT-2020 Task 2: GATs with Syntactic Dependencies as Edges and CTC-based Loss for Text Classification"},{"content":{"abstract":"With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.","authors":["Anders Giovanni M\u00f8ller","Rob van der Goot","Barbara Plank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.44","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets","tldr":"With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Iden...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.44","presentation_id":"","rocketchat_channel":"paper-wnut2020-44","speakers":"Anders Giovanni M\u00f8ller|Rob van der Goot|Barbara Plank","title":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets"},{"content":{"abstract":"Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as \u201cinfodemic.\u201d The ill-effects of such misinformation are multifarious. Thus, identifying and eliminating the sources of misinformation becomes very crucial, especially when mass panic can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on social media. WNUT-2020 Task 2 aims at building systems for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3% in the final evaluation.","authors":["Siva Sai"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.45","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Siva at WNUT-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets","tldr":"Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as \u201cinfodemic.\u201d The ill-effects of such misinformation are multifarious. Thus...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.45","presentation_id":"","rocketchat_channel":"paper-wnut2020-45","speakers":"Siva Sai","title":"Siva at WNUT-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets"},{"content":{"abstract":"In this paper, we present IIITBH team\u2019s effort to solve the second shared task of the 6th Workshop on Noisy User-generated Text (W-NUT)i.e Identification of informative COVID-19 English Tweets. The central theme of the task is to develop a system that automatically identify whether an English Tweet related to the novel coronavirus (COVID-19) is Informative or not. Our approach is based on exploiting semantic information from both max pooling and average pooling, to this end we propose two models.","authors":["Saichethan Reddy","Pradeep Biswal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.46","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IIITBH at WNUT-2020 Task 2: Exploiting the best of both worlds","tldr":"In this paper, we present IIITBH team\u2019s effort to solve the second shared task of the 6th Workshop on Noisy User-generated Text (W-NUT)i.e Identification of informative COVID-19 English Tweets. The central theme of the task is to develop a system tha...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.46","presentation_id":"","rocketchat_channel":"paper-wnut2020-46","speakers":"Saichethan Reddy|Pradeep Biswal","title":"IIITBH at WNUT-2020 Task 2: Exploiting the best of both worlds"},{"content":{"abstract":"This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to the novel coronavirus (COVID-19) is informative or not. We solve the task in three stages. The first stage involves pre-processing the dataset by filtering only relevant information. This is followed by experimenting with multiple deep learning models like CNNs, RNNs and Transformer based models. In the last stage, we propose an ensemble of the best model trained on different subsets of the provided dataset. Our final approach achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score as the evaluation criteria.","authors":["Anshul Wadhawan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.47","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting","tldr":"This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.47","presentation_id":"","rocketchat_channel":"paper-wnut2020-47","speakers":"Anshul Wadhawan","title":"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting"},{"content":{"abstract":"This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned features in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional features can improve classification results and achieve a score within 2 points of the top performing team.","authors":["Calum Perrio","Harish Tayyar Madabushi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.48","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets - RoBERTa Ensembles and The Continued Relevance of Handcrafted Features","tldr":"This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation th...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.48","presentation_id":"","rocketchat_channel":"paper-wnut2020-48","speakers":"Calum Perrio|Harish Tayyar Madabushi","title":"CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets - RoBERTa Ensembles and The Continued Relevance of Handcrafted Features"},{"content":{"abstract":"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10th place in the final rankings scoring 0.9004 F1 score for the test set.","authors":["Hansi Hettiarachchi","Tharindu Ranasinghe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction","tldr":"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.49","presentation_id":"","rocketchat_channel":"paper-wnut2020-49","speakers":"Hansi Hettiarachchi|Tharindu Ranasinghe","title":"InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction"},{"content":{"abstract":"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81% on the test set.","authors":["Tin Huynh","Luan Thanh Luan","Son T. Luu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.50","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models","tldr":"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction s...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.50","presentation_id":"","rocketchat_channel":"paper-wnut2020-50","speakers":"Tin Huynh|Luan Thanh Luan|Son T. Luu","title":"BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models"},{"content":{"abstract":"This document describes the system description developed by team datamafia at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. This paper contains a thorough study of pre-trained language models on downstream binary classification task over noisy user generated Twitter data. The solution submitted to final test leaderboard is a fine tuned RoBERTa model which achieves F1 score of 90.8% and 89.4% on the dev and test data respectively. In the later part, we explore several techniques for injecting regularization explicitly into language models to generalize predictions over noisy data. Our experiments show that adding regularizations to RoBERTa pre-trained model can be very robust to data and annotation noises and can improve overall performance by more than 1.2%.","authors":["Ayan Sengupta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.51","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DATAMAFIA at WNUT-2020 Task 2: A Study of Pre-trained Language Models along with Regularization Techniques for Downstream Tasks","tldr":"This document describes the system description developed by team datamafia at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. This paper contains a thorough study of pre-trained language models on downstream binary classifica...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.51","presentation_id":"","rocketchat_channel":"paper-wnut2020-51","speakers":"Ayan Sengupta","title":"DATAMAFIA at WNUT-2020 Task 2: A Study of Pre-trained Language Models along with Regularization Techniques for Downstream Tasks"},{"content":{"abstract":"Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic of infectious diseases that are informative in nature also span various topics such as news, politics and humor which makes the data mining challenging. We present a system to identify tweets about the COVID19 disease outbreak that are deemed to be informative on Twitter for use in downstream applications. The system scored a F1-score of 0.8941, Precision of 0.9028, Recall of 0.8856 and Accuracy of 0.9010. In the shared task organized as part of the 6th Workshop of Noisy User-generated Text (WNUT), the system was ranked 18th by F1-score and 13th by Accuracy.","authors":["Arjun Magge","Varad Pimpalkhute","Divya Rallapalli","David Siguenza","Graciela Gonzalez-Hernandez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.52","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UPennHLP at WNUT-2020 Task 2 : Transformer models for classification of COVID19 posts on Twitter","tldr":"Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.52","presentation_id":"","rocketchat_channel":"paper-wnut2020-52","speakers":"Arjun Magge|Varad Pimpalkhute|Divya Rallapalli|David Siguenza|Graciela Gonzalez-Hernandez","title":"UPennHLP at WNUT-2020 Task 2 : Transformer models for classification of COVID19 posts on Twitter"},{"content":{"abstract":"Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94% with the third place on the leaderboard of this task which attracted 56 submitted teams in total.","authors":["Khiem Tran","Hao Phan","Kiet Nguyen","Ngan Luu Thuy Nguyen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19 Information on the Twitter Social Network","tldr":"Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.53","presentation_id":"","rocketchat_channel":"paper-wnut2020-53","speakers":"Khiem Tran|Hao Phan|Kiet Nguyen|Ngan Luu Thuy Nguyen","title":"UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19 Information on the Twitter Social Network"},{"content":{"abstract":"This paper describes the system developed by the Emory team for the WNUT-2020 Task 2: \u201cIdentifi- cation of Informative COVID-19 English Tweet\u201d. Our system explores three recent Transformer- based deep learning models pretrained on large- scale data to encode documents. Moreover, we developed two feature enrichment methods to en- hance document embeddings by integrating emoji embeddings and syntactic features into deep learn- ing models. Our system achieved F1-score of 0.897 and accuracy of 90.1% on the test set, and ranked in the top-third of all 55 teams.","authors":["Yuting Guo","Mohammed Ali Al-Garadi","Abeed Sarker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.54","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emory at WNUT-2020 Task 2: Combining Pretrained Deep Learning Models and Feature Enrichment for Informative Tweet Identification","tldr":"This paper describes the system developed by the Emory team for the WNUT-2020 Task 2: \u201cIdentifi- cation of Informative COVID-19 English Tweet\u201d. Our system explores three recent Transformer- based deep learning models pretrained on large- scale data t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.54","presentation_id":"","rocketchat_channel":"paper-wnut2020-54","speakers":"Yuting Guo|Mohammed Ali Al-Garadi|Abeed Sarker","title":"Emory at WNUT-2020 Task 2: Combining Pretrained Deep Learning Models and Feature Enrichment for Informative Tweet Identification"},{"content":{"abstract":"COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of twitter makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this task. We propose a neural model that adopts the strength of transfer learning and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7% (approx.).","authors":["Fareen Tasneem","Jannatun Naim","Radiathun Tasnia","Tashin Hossain","Abu Nowshed Chy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.55","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CSECU-DSG at WNUT-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets","tldr":"COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.55","presentation_id":"","rocketchat_channel":"paper-wnut2020-55","speakers":"Fareen Tasneem|Jannatun Naim|Radiathun Tasnia|Tashin Hossain|Abu Nowshed Chy","title":"CSECU-DSG at WNUT-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks: DistilBERT and FastText. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.","authors":["Supriya Chanda","Eshita Nandy","Sukomal Pal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.56","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IRLab@IITBHU at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets using BERT","tldr":"This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification t...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.56","presentation_id":"","rocketchat_channel":"paper-wnut2020-56","speakers":"Supriya Chanda|Eshita Nandy|Sukomal Pal","title":"IRLab@IITBHU at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets using BERT"},{"content":{"abstract":"We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task 2 and ranks 1st on the leaderboard. The ensemble of the models trained using adversarial training also produces similar result.","authors":["Priyanshu Kumar","Aadarsh Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.57","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training","tldr":"We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.57","presentation_id":"","rocketchat_channel":"paper-wnut2020-57","speakers":"Priyanshu Kumar|Aadarsh Singh","title":"NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training"},{"content":{"abstract":"Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our model on a public dataset with an F1-score of 0.89 on the validation dataset and 0.87 on the leaderboard.","authors":["Sirigireddy Dhana Laxmi","Rohit Agarwal","Aman Sinha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.58","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa","tldr":"Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we presen...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.58","presentation_id":"","rocketchat_channel":"paper-wnut2020-58","speakers":"Sirigireddy Dhana Laxmi|Rohit Agarwal|Aman Sinha","title":"DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa"},{"content":{"abstract":"Since the outbreak of COVID-19, there has been a surge of digital content on social media. The content ranges from news articles, academic reports, tweets, videos, and even memes. Among such an overabundance of data, it is crucial to distinguish which information is actually informative or merely sensational, redundant or false. This work focuses on developing such a language system that can differentiate between Informative or Uninformative tweets associated with COVID-19 for WNUT-2020 Shared Task 2. For this purpose, we employ deep transfer learning models such as BERT along other techniques such as Noisy Data Augmentation and Progress Training. The approach achieves a competitive F1-score of 0.8715 on the final testing dataset.","authors":["Vasudev Awatramani","Anupam Kumar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.59","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguist Geeks on WNUT-2020 Task 2: COVID-19 Informative Tweet Identification using Progressive Trained Language Models and Data Augmentation","tldr":"Since the outbreak of COVID-19, there has been a surge of digital content on social media. The content ranges from news articles, academic reports, tweets, videos, and even memes. Among such an overabundance of data, it is crucial to distinguish whic...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.59","presentation_id":"","rocketchat_channel":"paper-wnut2020-59","speakers":"Vasudev Awatramani|Anupam Kumar","title":"Linguist Geeks on WNUT-2020 Task 2: COVID-19 Informative Tweet Identification using Progressive Trained Language Models and Data Augmentation"},{"content":{"abstract":"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85% and 78.54% as F1-score on the provided test dataset. The experimental code is available online.","authors":["Rajesh Kumar Mundotiya","Rupjyoti Baruah","Bhavana Srivastava","Anil Kumar Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.60","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLPRL at WNUT-2020 Task 2: ELMo-based System for Identification of COVID-19 Tweets","tldr":"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.60","presentation_id":"","rocketchat_channel":"paper-wnut2020-60","speakers":"Rajesh Kumar Mundotiya|Rupjyoti Baruah|Bhavana Srivastava|Anil Kumar Singh","title":"NLPRL at WNUT-2020 Task 2: ELMo-based System for Identification of COVID-19 Tweets"},{"content":{"abstract":"In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classification performance of classification models such as BERT and CNN. We show that ensembling can reduce the variance in performance, specifically for BERT base models.","authors":["Kenan Fayoumi","Reyyan Yeniterzi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.61","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SU-NLP at WNUT-2020 Task 2: The Ensemble Models","tldr":"In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classif...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.61","presentation_id":"","rocketchat_channel":"paper-wnut2020-61","speakers":"Kenan Fayoumi|Reyyan Yeniterzi","title":"SU-NLP at WNUT-2020 Task 2: The Ensemble Models"},{"content":{"abstract":"We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.","authors":["Sora Ohashi","Tomoyuki Kajiwara","Chenhui Chu","Noriko Takemura","Yuta Nakashima","Hajime Nagahara"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.62","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"IDSOU at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets","tldr":"We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score....","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.62","presentation_id":"","rocketchat_channel":"paper-wnut2020-62","speakers":"Sora Ohashi|Tomoyuki Kajiwara|Chenhui Chu|Noriko Takemura|Yuta Nakashima|Hajime Nagahara","title":"IDSOU at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as graph classification, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.","authors":["Kellin Pelrine","Jacob Danovitch","Albert Orozco Camacho","Reihaneh Rabbany"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.63","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ComplexDataLab at W-NUT 2020 Task 2: Detecting Informative COVID-19 Tweets by Attending over Linked Documents","tldr":"Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as graph cl...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.63","presentation_id":"","rocketchat_channel":"paper-wnut2020-63","speakers":"Kellin Pelrine|Jacob Danovitch|Albert Orozco Camacho|Reihaneh Rabbany","title":"ComplexDataLab at W-NUT 2020 Task 2: Detecting Informative COVID-19 Tweets by Attending over Linked Documents"},{"content":{"abstract":"Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in finding relevant information. In this paper, we present a BERT classifier system for W-NUT2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Further, we show that BERT exploits some easy signals to identify informative tweets, and adding simple patterns to uninformative tweets drastically degrades BERT performance. In particular, simply adding \u201c10 deaths\u201d to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also propose a simple data augmentation technique that helps in improving the robustness and generalization ability of the BERT classifier.","authors":["Kumud Chauhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.64","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative","tldr":"Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in find...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.64","presentation_id":"","rocketchat_channel":"paper-wnut2020-64","speakers":"Kumud Chauhan","title":"NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative"},{"content":{"abstract":"In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.","authors":["Abhilasha Sancheti","Kushal Chawla","Gaurav Verma"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.65","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets","tldr":"In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.65","presentation_id":"","rocketchat_channel":"paper-wnut2020-65","speakers":"Abhilasha Sancheti|Kushal Chawla|Gaurav Verma","title":"LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (novel coronavirus) or not. These informative tweets provide information about recovered, confirmed, suspected, and death cases as well as location or travel history of the cases. The proposed approach includes pre-processing techniques and pre-trained RoBERTa with suitable hyperparameters for English coronavirus tweet classification. The performance achieved by the proposed model for shared task WNUT 2020 Task2 is 89.14% in the F1-score metric.","authors":["Jagadeesh M S","Alphonse P J A"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.66","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets","tldr":"This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (n...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.66","presentation_id":"","rocketchat_channel":"paper-wnut2020-66","speakers":"Jagadeesh M S|Alphonse P J A","title":"NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets"},{"content":{"abstract":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.","authors":["Nickil Maveli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.67","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets","tldr":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically mo...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.67","presentation_id":"","rocketchat_channel":"paper-wnut2020-67","speakers":"Nickil Maveli","title":"EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets"},{"content":{"abstract":"In this system paper, we present a transformer-based approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recovery, suspected and confirmed cases and COVID-19 related deaths, from uninformative tweets. We present two transformer-based approaches as well as a Naive Bayes classifier and a support vector machine as baseline systems. The transformer models outperform the baselines by more than 0.1 in F1-score, with F1-scores of 0.9091 and 0.9036. Our models were submitted to the shared task Identification of informative COVID-19 English tweets WNUT-2020 Task 2.","authors":["Hanna Varachkina","Stefan Ziehe","Tillmann D\u00f6nicke","Franziska Pannach"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.68","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"#GCDH at WNUT-2020 Task 2: BERT-Based Models for the Detection of Informativeness in English COVID-19 Related Tweets","tldr":"In this system paper, we present a transformer-based approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recover...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.68","presentation_id":"","rocketchat_channel":"paper-wnut2020-68","speakers":"Hanna Varachkina|Stefan Ziehe|Tillmann D\u00f6nicke|Franziska Pannach","title":"#GCDH at WNUT-2020 Task 2: BERT-Based Models for the Detection of Informativeness in English COVID-19 Related Tweets"},{"content":{"abstract":"As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people\u2019s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overshadows the more informative ones. In response to such, we proposed a model that, given an English tweet, automatically identifies whether that tweet bears informative content regarding COVID-19 or not. By ensembling different BERTweet model configurations, we have achieved competitive results that are only shy of those by top performing teams by roughly 1% in terms of F1 score on the informative class. In the post-competition period, we have also experimented with various other approaches that potentially boost generalization to a new dataset.","authors":["Thai Hoang","Phuong Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.69","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Not-NUTs at WNUT-2020 Task 2: A BERT-based System in Identifying Informative COVID-19 English Tweets","tldr":"As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people\u2019s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overs...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.69","presentation_id":"","rocketchat_channel":"paper-wnut2020-69","speakers":"Thai Hoang|Phuong Vu","title":"Not-NUTs at WNUT-2020 Task 2: A BERT-based System in Identifying Informative COVID-19 English Tweets"},{"content":{"abstract":"This paper presents our models for WNUT2020 shared task2. The shared task2 involves identification of COVID-19 related informative tweets. We treat this as binary text clas-sification problem and experiment with pre-trained language models. Our first model which is based on CT-BERT achieves F1-scoreof 88.7% and second model which is an ensemble of CT-BERT, RoBERTa and SVM achieves F1-score of 88.52%.","authors":["Yandrapati Prakash Babu","Rajagopal Eswari"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.70","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using Pre-trained Language Models","tldr":"This paper presents our models for WNUT2020 shared task2. The shared task2 involves identification of COVID-19 related informative tweets. We treat this as binary text clas-sification problem and experiment with pre-trained language models. Our first...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.70","presentation_id":"","rocketchat_channel":"paper-wnut2020-70","speakers":"Yandrapati Prakash Babu|Rajagopal Eswari","title":"CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using Pre-trained Language Models"},{"content":{"abstract":"This paper reports our approach and the results of our experiments for W-NUT task 2: Identification of Informative COVID-19 English Tweets. In this paper, we test out the effectiveness of transfer learning method with state of the art language models as RoBERTa on this text classification task. Moreover, we examine the benefit of applying additional fine-tuning and training techniques including fine-tuning discrimination, gradual unfreezing as well as our custom head for the classifier. Our best model results in a high F1-score of 89.89 on the task\u2019s private test dataset and that of 90.96 on public test set without ensembling multiple models and additional data.","authors":["Huy Dao Quang","Tam Nguyen Minh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.71","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UET at WNUT-2020 Task 2: A Study of Combining Transfer Learning Methods for Text Classification with RoBERTa","tldr":"This paper reports our approach and the results of our experiments for W-NUT task 2: Identification of Informative COVID-19 English Tweets. In this paper, we test out the effectiveness of transfer learning method with state of the art language models...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.71","presentation_id":"","rocketchat_channel":"paper-wnut2020-71","speakers":"Huy Dao Quang|Tam Nguyen Minh","title":"UET at WNUT-2020 Task 2: A Study of Combining Transfer Learning Methods for Text Classification with RoBERTa"},{"content":{"abstract":"We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT\u2019s performance in this classification task by fine-tuning BERT and concatenating its embeddings with Tweet-specific features and training a Support Vector Machine (SVM) for classification (henceforth called BERT+). We compared its performance to a suite of machine learning models. We used a Twitter specific data cleaning pipeline and word-level TF-IDF to extract features for the non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.","authors":["Dylan Whang","Soroush Vosoughi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.72","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dartmouth CS at WNUT-2020 Task 2: Fine tuning BERT for Tweet classification","tldr":"We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT\u2019s performance in this classification ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.72","presentation_id":"","rocketchat_channel":"paper-wnut2020-72","speakers":"Dylan Whang|Soroush Vosoughi","title":"Dartmouth CS at WNUT-2020 Task 2: Fine tuning BERT for Tweet classification"},{"content":{"abstract":"This paper proposes an improved custom model for WNUT task 2: Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model RoBERTa. We make a preliminary instantiation of this formal model for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 F1-score on public validation set and the ensemble version settles at top 9 F1-score (0.9005) and top 2 Recall (0.9301) on private test set.","authors":["Linh Doan Bao","Viet Anh Nguyen","Quang Pham Huu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.73","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SunBear at WNUT-2020 Task 2: Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain","tldr":"This paper proposes an improved custom model for WNUT task 2: Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model RoBERTa. We make a preli...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.73","presentation_id":"","rocketchat_channel":"paper-wnut2020-73","speakers":"Linh Doan Bao|Viet Anh Nguyen|Quang Pham Huu","title":"SunBear at WNUT-2020 Task 2: Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain"},{"content":{"abstract":"This paper presents Iswara\u2019s participation in the WNUT-2020 Task 2 \u201cIdentification of Informative COVID-19 English Tweets using BERT and FastText Embeddings\u201d,which tries to classify whether a certain tweet is considered informative or not. We proposed a method that utilizes word embeddings and using word occurrence related to the topic for this task. We compare several models to get the best performance. Results show that pairing BERT with word occurrences outperforms fastText with F1-Score, precision, recall, and accuracy on test data of 76%, 81%, 72%, and 79%, respectively","authors":["Wava Carissa Putri","Rani Aulia Hidayat","Isnaini Nurul Khasanah","Rahmad Mahendra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.74","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ISWARA at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings","tldr":"This paper presents Iswara\u2019s participation in the WNUT-2020 Task 2 \u201cIdentification of Informative COVID-19 English Tweets using BERT and FastText Embeddings\u201d,which tries to classify whether a certain tweet is considered informative or not. We propose...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.74","presentation_id":"","rocketchat_channel":"paper-wnut2020-74","speakers":"Wava Carissa Putri|Rani Aulia Hidayat|Isnaini Nurul Khasanah|Rahmad Mahendra","title":"ISWARA at WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings"},{"content":{"abstract":"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.","authors":["Ali H\u00fcrriyeto\u011flu","Ali Safaya","Osman Mutlu","Nelleke Oostdijk","Erdem Y\u00f6r\u00fck"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.75","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules","tldr":"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically infor...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.75","presentation_id":"","rocketchat_channel":"paper-wnut2020-75","speakers":"Ali H\u00fcrriyeto\u011flu|Ali Safaya|Osman Mutlu|Nelleke Oostdijk|Erdem Y\u00f6r\u00fck","title":"COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules"},{"content":{"abstract":"The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he/she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro F1.","authors":["Chacha Chen","Chieh-Yang Huang","Yaqi Hou","Yang Shi","Enyan Dai","Jiaqi Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.76","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TEST_POSITIVE at W-NUT 2020 Shared Task-3: Cross-task modeling","tldr":"The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important ques...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.76","presentation_id":"","rocketchat_channel":"paper-wnut2020-76","speakers":"Chacha Chen|Chieh-Yang Huang|Yaqi Hou|Yang Shi|Enyan Dai|Jiaqi Wang","title":"TEST_POSITIVE at W-NUT 2020 Shared Task-3: Cross-task modeling"},{"content":{"abstract":"In this paper, we present our system designed to address the W-NUT 2020 shared task for COVID-19 Event Extraction from Twitter. To mitigate the noisy nature of the Twitter stream, our system makes use of the COVID-Twitter-BERT (CT-BERT), which is a language model pre-trained on a large corpus of COVID-19 related Twitter messages. Our system is trained on the COVID-19 Twitter Event Corpus and is able to identify relevant text spans that answer pre-defined questions (i.e., slot types) for five COVID-19 related events (i.e., TESTED POSITIVE, TESTED NEGATIVE, CAN-NOT-TEST, DEATH and CURE & PREVENTION). We have experimented with different architectures; our best performing model relies on a multilabel classifier on top of the CT-BERT model that jointly trains all the slot types for a single event. Our experimental results indicate that our Multilabel-CT-BERT system outperforms the baseline methods by 7 percentage points in terms of micro average F1 score. Our model ranked as 4th in the shared task leaderboard.","authors":["Xiangyu Yang","Giannis Bekoulis","Nikos Deligiannis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.77","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"imec-ETRO-VUB at W-NUT 2020 Shared Task-3: A multilabel BERT-based system for predicting COVID-19 events","tldr":"In this paper, we present our system designed to address the W-NUT 2020 shared task for COVID-19 Event Extraction from Twitter. To mitigate the noisy nature of the Twitter stream, our system makes use of the COVID-Twitter-BERT (CT-BERT), which is a l...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.77","presentation_id":"","rocketchat_channel":"paper-wnut2020-77","speakers":"Xiangyu Yang|Giannis Bekoulis|Nikos Deligiannis","title":"imec-ETRO-VUB at W-NUT 2020 Shared Task-3: A multilabel BERT-based system for predicting COVID-19 events"},{"content":{"abstract":"In this paper, we describe our approach in the shared task: COVID-19 event extraction from Twitter. The objective of this task is to extract answers from COVID-related tweets to a set of predefined slot-filling questions. Our approach treats the event extraction task as a question answering task by leveraging the transformer-based T5 text-to-text model. According to the official evaluation scores returned, namely F1, our submitted run achieves competitive performance compared to other participating runs (Top 3). However, we argue that this evaluation may underestimate the actual performance of runs based on text-generation. Although some such runs may answer the slot questions well, they may not be an exact string match for the gold standard answers. To measure the extent of this underestimation, we adopt a simple exact-answer transformation method aiming at converting the well-answered predictions to exactly-matched predictions. The results show that after this transformation our run overall reaches the same level of performance as the best participating run and state-of-the-art F1 scores in three of five COVID-related events. Our code is publicly available to aid reproducibility","authors":["Congcong Wang","David Lillis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.78","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19 Event Extraction on Social Media","tldr":"In this paper, we describe our approach in the shared task: COVID-19 event extraction from Twitter. The objective of this task is to extract answers from COVID-related tweets to a set of predefined slot-filling questions. Our approach treats the even...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.78","presentation_id":"","rocketchat_channel":"paper-wnut2020-78","speakers":"Congcong Wang|David Lillis","title":"UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19 Event Extraction on Social Media"},{"content":{"abstract":"Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks, while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-BERT with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leaderboard with F1 of 0.6598, without using any ensembles or additional datasets.","authors":["Ayush Kaushal","Tejas Vaidhya"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.79","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Winners at W-NUT 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting COVID Entities from Tweets","tldr":"Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction o...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.79","presentation_id":"","rocketchat_channel":"paper-wnut2020-79","speakers":"Ayush Kaushal|Tejas Vaidhya","title":"Winners at W-NUT 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting COVID Entities from Tweets"},{"content":{"abstract":"Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from Twitter has the potential to inform surveillance systems that play a critical role in public health. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This architecture uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of Hopfield Networks, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while it remains competitive when training on smaller datasets.","authors":["Maxwell Weinzierl","Sanda Harabagiu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.wnut-1.80","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HLTRI at W-NUT 2020 Shared Task-3: COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling","tldr":"Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from Twitter has the potential to inform surveillance systems that play a critical role in public health. The event extraction challenge presented by the ...","track":"6th Workshop on Noisy User-generated Text (W-NUT 2020)"},"id":"WS-14.2020.wnut-1.80","presentation_id":"","rocketchat_channel":"paper-wnut2020-80","speakers":"Maxwell Weinzierl|Sanda Harabagiu","title":"HLTRI at W-NUT 2020 Shared Task-3: COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-wnut2020","schedule":null,"sessions":[{"end_time":"Thu, 19 Nov 2020 04:15:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Opening","start_time":"Thu, 19 Nov 2020 04:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 04:45:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Invited Talk by Rob Munro & QA","start_time":"Thu, 19 Nov 2020 04:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 05:15:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Oral Presentations (research papers) 1a","start_time":"Thu, 19 Nov 2020 04:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 05:35:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Shared Task #2 - Identification of informative COVID-19 English Tweets","start_time":"Thu, 19 Nov 2020 05:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 05:50:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Breakout Rooms (social event)","start_time":"Thu, 19 Nov 2020 05:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 06:20:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Invited Talk by Irwin King & QA","start_time":"Thu, 19 Nov 2020 05:50:00 GMT"},{"end_time":"Thu, 19 Nov 2020 06:35:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"Oral Presentations (research papers) 1b","start_time":"Thu, 19 Nov 2020 06:20:00 GMT"},{"end_time":"Thu, 19 Nov 2020 07:35:00 GMT","hosts":"Tim Baldwin & Afshin Rahimi ","link":"","session_name":"GatherTown Poster Session","start_time":"Thu, 19 Nov 2020 06:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:35:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Invited Talk by Manaal Faruqui & QA","start_time":"Thu, 19 Nov 2020 15:00:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:45:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Lightning Talks  (research papers) 2a","start_time":"Thu, 19 Nov 2020 15:35:00 GMT"},{"end_time":"Thu, 19 Nov 2020 15:55:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Shared Task #3 - COVID-19 Event Extraction from Twitter","start_time":"Thu, 19 Nov 2020 15:45:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:15:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Oral Presentations (research papers) 2a","start_time":"Thu, 19 Nov 2020 15:55:00 GMT"},{"end_time":"Thu, 19 Nov 2020 16:30:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Breakout Rooms (social event)","start_time":"Thu, 19 Nov 2020 16:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:05:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Invited Talk by Eduardo Blanco & QA","start_time":"Thu, 19 Nov 2020 16:30:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:15:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Oral Presentations (research papers) 2b","start_time":"Thu, 19 Nov 2020 17:05:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:25:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Shared Task #1 - Entity and Relation Recognition over Wet-lab Protocols","start_time":"Thu, 19 Nov 2020 17:15:00 GMT"},{"end_time":"Thu, 19 Nov 2020 17:35:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"Lightning Talks  (research papers) 2b","start_time":"Thu, 19 Nov 2020 17:25:00 GMT"},{"end_time":"Thu, 19 Nov 2020 18:35:00 GMT","hosts":"Wei Xu & Alan Ritter","link":"","session_name":"GatherTown Poster Session","start_time":"Thu, 19 Nov 2020 17:35:00 GMT"}],"title":"6th Workshop on Noisy User-generated Text (W-NUT 2020)","website":"http://noisy-text.github.io","zoom_links":["https://zoom.us","https://zoom.us"]},{"abstract":"Promoting computationally efficient NLP and justified model complexity. Encouraging conceptual NLP novelty, discouraging \"more compute\".","blocks":[{"end_time":"Fri, 20 Nov 2020 10:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 02:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 3","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"}],"id":"WS-15","livestream":null,"organizers":"Angela Fan, Goran Glava\u0161, Shafiq Joty, Nafise Sadat Moosavi, Vered Shwartz, Alex Wang and Thomas Wolf","papers":[{"content":{"abstract":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.","authors":["Ali Akbar Septiandri","Yosef Ardhito Winatmoko","Ilham Firdausi Putra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?","tldr":"We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent develo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1","presentation_id":"38939419","rocketchat_channel":"paper-sustainlp2020-1","speakers":"Ali Akbar Septiandri|Yosef Ardhito Winatmoko|Ilham Firdausi Putra","title":"Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?"},{"content":{"abstract":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but they are seldom available and expensive to hire. This has lead to the thriving of crowdsourcing platforms such as Amazon Mechanical Turk (AMT). However, the annotations provided by one worker cannot be used directly to train the model due to the lack of expertise. Existing literature in annotation aggregation focuses on binary and multi-choice problems. In contrast, little work has been done on complex tasks such as sequence labeling with imbalanced classes, a ubiquitous task in Natural Language Processing (NLP), and Bio-Informatics. We propose OptSLA, an Optimization-based Sequential Label Aggregation method, that jointly considers the characteristics of sequential labeling tasks, workers reliabilities, and advanced deep learning techniques to conquer the challenge. We evaluate our model on crowdsourced data for named entity recognition task. Our results show that the proposed OptSLA outperforms the state-of-the-art aggregation methods, and the results are easier to interpret.","authors":["Nasim Sabetpour","Adithya Kulkarni","Qi Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.119","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation","tldr":"The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but th...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1098","presentation_id":"38940107","rocketchat_channel":"paper-sustainlp2020-1098","speakers":"Nasim Sabetpour|Adithya Kulkarni|Qi Li","title":"OptSLA: an Optimization-Based Approach for Sequential Label Aggregation"},{"content":{"abstract":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.","authors":["Moshe Wasserblat","Oren Pereg","Peter Izsak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring the Boundaries of Low-Resource BERT Distillation","tldr":"In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models\u2019 large computational consumption...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.12","presentation_id":"38939426","rocketchat_channel":"paper-sustainlp2020-12","speakers":"Moshe Wasserblat|Oren Pereg|Peter Izsak","title":"Exploring the Boundaries of Low-Resource BERT Distillation"},{"content":{"abstract":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT - BERT F1 delta, at 5% of BioBERT\u2019s CO 2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.134","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","tldr":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1286","presentation_id":"38940121","rocketchat_channel":"paper-sustainlp2020-1286","speakers":"Nina Poerner|Ulli Waltinger|Hinrich Sch\u00fctze","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"content":{"abstract":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.","authors":["Sosuke Kobayashi","Sho Yokoi","Jun Suzuki","Kentaro Inui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Estimation of Influence of a Training Instance","tldr":"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.13","presentation_id":"38939427","rocketchat_channel":"paper-sustainlp2020-13","speakers":"Sosuke Kobayashi|Sho Yokoi|Jun Suzuki|Kentaro Inui","title":"Efficient Estimation of Influence of a Training Instance"},{"content":{"abstract":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.","authors":["Yi-Te Hsu","Sarthak Garg","Yi-Hsiu Liao","Ilya Chatsviorkin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Efficient Inference For Neural Machine Translation","tldr":"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.14","presentation_id":"38939429","rocketchat_channel":"paper-sustainlp2020-14","speakers":"Yi-Te Hsu|Sarthak Garg|Yi-Hsiu Liao|Ilya Chatsviorkin","title":"Efficient Inference For Neural Machine Translation"},{"content":{"abstract":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations \u2013 a main performance bottleneck. Consequently, our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets.","authors":["Yatin Chaudhary","Pankaj Gupta","Khushbu Saxena","Vivek Kulkarni","Thomas Runkler","Hinrich Sch\u00fctze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.152","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TopicBERT for Energy Efficient Document Classification","tldr":"Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1418","presentation_id":"38940122","rocketchat_channel":"paper-sustainlp2020-1418","speakers":"Yatin Chaudhary|Pankaj Gupta|Khushbu Saxena|Vivek Kulkarni|Thomas Runkler|Hinrich Sch\u00fctze","title":"TopicBERT for Energy Efficient Document Classification"},{"content":{"abstract":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in F1 that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both PyTorch and TensorFlow.","authors":["Brian Lester","Daniel Pressel","Amy Hemmeter","Sagnik Ray Choudhury","Srinivas Bangalore"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.166","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers","tldr":"Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. C...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1537","presentation_id":"38940105","rocketchat_channel":"paper-sustainlp2020-1537","speakers":"Brian Lester|Daniel Pressel|Amy Hemmeter|Sagnik Ray Choudhury|Srinivas Bangalore","title":"Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers"},{"content":{"abstract":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.","authors":["Alicia Tsai","Laurent El Ghaoui"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm","tldr":"We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrai...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.17","presentation_id":"38939430","rocketchat_channel":"paper-sustainlp2020-17","speakers":"Alicia Tsai|Laurent El Ghaoui","title":"Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm"},{"content":{"abstract":"","authors":["Kunal Chawla"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.1887","presentation_id":"38940140","rocketchat_channel":"paper-sustainlp2020-1887","speakers":"Kunal Chawla","title":"Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization"},{"content":{"abstract":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization (HMF) that achieves this dual objective. HMF improves low-rank matrix factorization (LMF) techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-timethan pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks (Translation, Intent Detection,Language Modeling) and show that for similar accuracy values and compression factors, HMF can achieve more than 2.32x faster inference run-time than pruning and 16.77% better accuracy than LMF.","authors":["Urmish Thakker","Jesse Beu","Dibakar Gope","Ganesh Dasika","Matthew Mattina"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Rank and run-time aware compression of NLP Applications","tldr":"Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2","presentation_id":"38939420","rocketchat_channel":"paper-sustainlp2020-2","speakers":"Urmish Thakker|Jesse Beu|Dibakar Gope|Ganesh Dasika|Matthew Mattina","title":"Rank and run-time aware compression of NLP Applications"},{"content":{"abstract":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.","authors":["Jiezhong Qiu","Hao Ma","Omer Levy","Wen-tau Yih","Sinong Wang","Jie Tang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.232","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Blockwise Self-Attention for Long Document Understanding","tldr":"We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/infere...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2015","presentation_id":"38940119","rocketchat_channel":"paper-sustainlp2020-2015","speakers":"Jiezhong Qiu|Hao Ma|Omer Levy|Wen-tau Yih|Sinong Wang|Jie Tang","title":"Blockwise Self-Attention for Long Document Understanding"},{"content":{"abstract":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of severe information loss. In this paper, we present a simple but effective unsupervised neural generative semantic hashing method with a focus on few-bits hashing. Our model is built upon variational autoencoder and represents each hash bit as a Bernoulli variable, which allows the model to be end-to-end trainable. To address the issue of information loss, we introduce a set of auxiliary implicit topic vectors. With the aid of these topic vectors, the generated hash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves significant improvements over state-of-the-art semantic hashing methods in few-bits hashing.","authors":["Fanghua Ye","Jarana Manotumruksa","Emine Yilmaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.233","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling","tldr":"Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guarante...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2017","presentation_id":"38940106","rocketchat_channel":"paper-sustainlp2020-2017","speakers":"Fanghua Ye|Jarana Manotumruksa|Emine Yilmaz","title":"Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling"},{"content":{"abstract":"","authors":["Jiecao Chen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2182","presentation_id":"38940104","rocketchat_channel":"paper-sustainlp2020-2182","speakers":"Jiecao Chen","title":"DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling"},{"content":{"abstract":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","authors":["Yuxiang Wu","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering","tldr":"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have show...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.22","presentation_id":"38939431","rocketchat_channel":"paper-sustainlp2020-22","speakers":"Yuxiang Wu|Pasquale Minervini|Pontus Stenetorp|Sebastian Riedel","title":"Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering"},{"content":{"abstract":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational cost during inference can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an encoder and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this encoder, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher accuracy and lower computational cost once the system is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the computational cost when multiple tasks are performed on the same text.","authors":["Jingfei Du","Myle Ott","Haoran Li","Xing Zhou","Veselin Stoyanov"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.271","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference","tldr":"The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a sin...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2230","presentation_id":"38940109","rocketchat_channel":"paper-sustainlp2020-2230","speakers":"Jingfei Du|Myle Ott|Haoran Li|Xing Zhou|Veselin Stoyanov","title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference"},{"content":{"abstract":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.","authors":["Giorgos Vernikos","Katerina Margatina","Alexandra Chronopoulou","Ion Androutsopoulos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.278","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Domain Adversarial Fine-Tuning as an Effective Regularizer","tldr":"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2288","presentation_id":"38940129","rocketchat_channel":"paper-sustainlp2020-2288","speakers":"Giorgos Vernikos|Katerina Margatina|Alexandra Chronopoulou|Ion Androutsopoulos","title":"Domain Adversarial Fine-Tuning as an Effective Regularizer"},{"content":{"abstract":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.","authors":["Zhiheng Huang","Davis Liang","Peng Xu","Bing Xiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.298","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improve Transformer Models with Better Relative Position Embeddings","tldr":"The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2453","presentation_id":"38940108","rocketchat_channel":"paper-sustainlp2020-2453","speakers":"Zhiheng Huang|Davis Liang|Peng Xu|Bing Xiang","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"content":{"abstract":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from \u201cgenuine\u201d ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.","authors":["Zhao Wang","Aron Culotta"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.308","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Spurious Correlations for Robust Text Classification","tldr":"The predictions of text classifiers are often driven by spurious correlations \u2013 e.g., the term \u201cSpielberg\u201d correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we pr...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2516","presentation_id":"38940117","rocketchat_channel":"paper-sustainlp2020-2516","speakers":"Zhao Wang|Aron Culotta","title":"Identifying Spurious Correlations for Robust Text Classification"},{"content":{"abstract":"","authors":["Urmish Thakker"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Doped Structured Matrices for Extreme Compression of LSTM Models","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.27","presentation_id":"38940744","rocketchat_channel":"paper-sustainlp2020-27","speakers":"Urmish Thakker","title":"Doped Structured Matrices for Extreme Compression of LSTM Models"},{"content":{"abstract":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.","authors":["Cennet Oguz","Ngoc Thang Vu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings","tldr":"Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.28","presentation_id":"38939432","rocketchat_channel":"paper-sustainlp2020-28","speakers":"Cennet Oguz|Ngoc Thang Vu","title":"A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings"},{"content":{"abstract":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.","authors":["Ji Xin","Rodrigo Nogueira","Yaoliang Yu","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Early Exiting BERT for Efficient Document Ranking","tldr":"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for d...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.29","presentation_id":"38939433","rocketchat_channel":"paper-sustainlp2020-29","speakers":"Ji Xin|Rodrigo Nogueira|Yaoliang Yu|Jimmy Lin","title":"Early Exiting BERT for Efficient Document Ranking"},{"content":{"abstract":"","authors":["Patrick Xia","Jo\u00e3o Sedoc","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Incremental Neural Coreference Resolution in Constant Memory","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3","presentation_id":"38939421","rocketchat_channel":"paper-sustainlp2020-3","speakers":"Patrick Xia|Jo\u00e3o Sedoc|Benjamin Van Durme","title":"Incremental Neural Coreference Resolution in Constant Memory"},{"content":{"abstract":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of these approaches still need to be trained on very large datasets. In this paper, we introduce BeGanKP, a new conditional GAN model to address the problem of Keyphrase Generation in a low-resource scenario. Our main contribution relies in the Discriminator\u2019s architecture: a new BERT-based module which is able to distinguish between the generated and humancurated KPs reliably. Its characteristics allow us to use it in a low-resource scenario, where only a small amount of training data are available, obtaining an efficient Generator. The resulting architecture achieves, on five public datasets, competitive results with respect to the state-of-the-art approaches, using less than 1% of the training data.","authors":["Giuseppe Lancioni","Saida S.Mohamed","Beatrice Portelli","Giuseppe Serra","Carlo Tasso"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Keyphrase Generation with GANs in Low-Resources Scenarios","tldr":"Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.30","presentation_id":"38939434","rocketchat_channel":"paper-sustainlp2020-30","speakers":"Giuseppe Lancioni|Saida S.Mohamed|Beatrice Portelli|Giuseppe Serra|Carlo Tasso","title":"Keyphrase Generation with GANs in Low-Resources Scenarios"},{"content":{"abstract":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.","authors":["Umanga Bista","Alexander Mathews","Aditya Menon","Lexing Xie"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.367","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy","tldr":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information pres...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3078","presentation_id":"38940131","rocketchat_channel":"paper-sustainlp2020-3078","speakers":"Umanga Bista|Alexander Mathews|Aditya Menon|Lexing Xie","title":"SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy"},{"content":{"abstract":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we can get benefits similar to an ensemble of classifiers with a fraction of the resources required.We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique that encourages the model to develop an internal representation of the problem at hand which is beneficial to multiple output units of the classifier at the same time. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.","authors":["Norbert Kis-Szab\u00f3","G\u00e1bor Berend"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles","tldr":"We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we c...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.32","presentation_id":"38939435","rocketchat_channel":"paper-sustainlp2020-32","speakers":"Norbert Kis-Szab\u00f3|G\u00e1bor Berend","title":"Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles"},{"content":{"abstract":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that \u201csome\u201d labeled in-domain data can be worse than none at all.","authors":["Xinyu Zhang","Andrew Yates","Jimmy Lin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data","tldr":"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.34","presentation_id":"38939436","rocketchat_channel":"paper-sustainlp2020-34","speakers":"Xinyu Zhang|Andrew Yates|Jimmy Lin","title":"A Little Bit Is Worse Than None: Ranking with Limited Training Data"},{"content":{"abstract":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed model is able to generate fluent questions with high completeness and outperforms the strongest baseline by 20.8% in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU","authors":["Dan Su","Yan Xu","Wenliang Dai","Ziwei Ji","Tiezheng Yu","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.416","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi-hop Question Generation with Graph Convolutional Network","tldr":"Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3444","presentation_id":"38940120","rocketchat_channel":"paper-sustainlp2020-3444","speakers":"Dan Su|Yan Xu|Wenliang Dai|Ziwei Ji|Tiezheng Yu|Pascale Fung","title":"Multi-hop Question Generation with Graph Convolutional Network"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3459","presentation_id":"38940124","rocketchat_channel":"paper-sustainlp2020-3459","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"","authors":["Rajarshi Das"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3526","presentation_id":"38940133","rocketchat_channel":"paper-sustainlp2020-3526","speakers":"Rajarshi Das","title":"Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion"},{"content":{"abstract":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8\u00d7 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3\u00d7 reduction in run-time memory footprints and 3.5\u00d7 speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.","authors":["Insoo Chung","Byeongwook Kim","Yoonjung Choi","Se Jung Kwon","Yongkweon Jeon","Baeseong Park","Sangha Kim","Dongsoo Lee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.433","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation","tldr":"The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quan...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.3562","presentation_id":"38940118","rocketchat_channel":"paper-sustainlp2020-3562","speakers":"Insoo Chung|Byeongwook Kim|Yoonjung Choi|Se Jung Kwon|Yongkweon Jeon|Baeseong Park|Sangha Kim|Dongsoo Lee","title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"},{"content":{"abstract":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that determines which existing source model would minimize error for transfer learning to a given target. This technique does not require learning for prediction, and avoids computational costs of trail-and-error. We have evaluated this technique on nine datasets across diverse domains, including newswire, user forums, air flight booking, cybersecurity news, etc. We show that it per-forms better than existing techniques such as fine-tuning over vanilla BERT, or curriculum learning over the largest dataset on top of BERT, resulting in average F1 score gains in excess of 3%. Moreover, our technique consistently selects the best model using fewer tries.","authors":["Parul Awasthy","Bishwaranjan Bhattacharjee","John Kender","Radu Florian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks","tldr":"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that dete...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.36","presentation_id":"38939437","rocketchat_channel":"paper-sustainlp2020-36","speakers":"Parul Awasthy|Bishwaranjan Bhattacharjee|John Kender|Radu Florian","title":"Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks"},{"content":{"abstract":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.","authors":["Julian Eisenschlos","Syrine Krichene","Thomas M\u00fcller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding tables with intermediate pre-training","tldr":"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on t...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.361","presentation_id":"38940134","rocketchat_channel":"paper-sustainlp2020-361","speakers":"Julian Eisenschlos|Syrine Krichene|Thomas M\u00fcller","title":"Understanding tables with intermediate pre-training"},{"content":{"abstract":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.","authors":["Amine Abdaoui","Camille Pradel","Gr\u00e9goire Sigel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Load What You Need: Smaller Versions of Mutlilingual BERT","tldr":"Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.37","presentation_id":"38939438","rocketchat_channel":"paper-sustainlp2020-37","speakers":"Amine Abdaoui|Camille Pradel|Gr\u00e9goire Sigel","title":"Load What You Need: Smaller Versions of Mutlilingual BERT"},{"content":{"abstract":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert","authors":["Forrest Iandola","Albert Shaw","Ravi Krishna","Kurt Keutzer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?","tldr":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.38","presentation_id":"38939439","rocketchat_channel":"paper-sustainlp2020-38","speakers":"Forrest Iandola|Albert Shaw|Ravi Krishna|Kurt Keutzer","title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"},{"content":{"abstract":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification. However, it lacks sufficient contextual information enhancement and thus is less useful for sequence labelling tasks such as chunking and named entity recognition (NER). To address this limitation, we propose masked adversarial training (MAT) to improve robustness from contextual information in sequence labelling. MAT masks or replaces some words in the sentence when computing adversarial loss from perturbed inputs and consequently enhances model robustness using more context-level information. In our experiments, our method shows significant improvements on accuracy and robustness of sequence labelling. By further incorporating with ELMo embeddings, our model achieves better or comparable results to state-of-the-art on CoNLL 2000 and 2003 benchmarks using much less parameters.","authors":["Luoxin Chen","Xinyue Liu","Weitong Ruan","Jianhua Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training","tldr":"Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input n...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.381","presentation_id":"38940127","rocketchat_channel":"paper-sustainlp2020-381","speakers":"Luoxin Chen|Xinyue Liu|Weitong Ruan|Jianhua Lu","title":"Enhance Robustness of Sequence Labelling with Masked Adversarial Training"},{"content":{"abstract":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource constraint devices. These models try to minimize resource requirements like RAM and storage without hurting the accuracy much. We utilized these models on multiple benchmark natural language processing tasks, which were sentimental analysis, spam message detection, emotion analysis and fake news classification. The experiment results shows that the tree-based algorithm, Bonsai, surpassed the rest of the machine learning algorithms by achieve higher accuracy scores while having significantly lower memory usage.","authors":["Raj Pranesh","Ambesh Shekhar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing","tldr":"In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource cons...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.39","presentation_id":"38939440","rocketchat_channel":"paper-sustainlp2020-39","speakers":"Raj Pranesh|Ambesh Shekhar","title":"Analysis of Resource-efficient Predictive Models for Natural Language Processing"},{"content":{"abstract":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.","authors":["Qingqing Cao","Aruna Balasubramanian","Niranjan Balasubramanian"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards Accurate and Reliable Energy Measurement of NLP Models","tldr":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate beca...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.42","presentation_id":"38939441","rocketchat_channel":"paper-sustainlp2020-42","speakers":"Qingqing Cao|Aruna Balasubramanian|Niranjan Balasubramanian","title":"Towards Accurate and Reliable Energy Measurement of NLP Models"},{"content":{"abstract":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.","authors":["Young Jin Kim","Hany Hassan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding","tldr":"Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficien...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.43","presentation_id":"38939442","rocketchat_channel":"paper-sustainlp2020-43","speakers":"Young Jin Kim|Hany Hassan","title":"FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"},{"content":{"abstract":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.","authors":["Ariadna Quattoni","Xavier Carreras"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A comparison between CNNs and WFAs for Sequence Classification","tldr":"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.45","presentation_id":"38939443","rocketchat_channel":"paper-sustainlp2020-45","speakers":"Ariadna Quattoni|Xavier Carreras","title":"A comparison between CNNs and WFAs for Sequence Classification"},{"content":{"abstract":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of pseudo-positive labels. We validate that the effectiveness of augmented labels are comparable to positives, such that ours outperform state-of-the-arts without augmentation.","authors":["Seungtaek Choi","Myeongho Jeong","Jinyoung Yeo","Seung-won Hwang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Counterfactual Augmentation for Training Next Response Selection","tldr":"This paper studies label augmentation for training dialogue response selection. The existing model is trained by \u201cobservational\u201d annotation, where one observed response is annotated as gold. In this paper, we propose \u201ccounterfactual augmentation\u201d of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.46","presentation_id":"38939444","rocketchat_channel":"paper-sustainlp2020-46","speakers":"Seungtaek Choi|Myeongho Jeong|Jinyoung Yeo|Seung-won Hwang","title":"Counterfactual Augmentation for Training Next Response Selection"},{"content":{"abstract":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as \u2018Entailment\u2019, \u2018Neutral\u2019, and \u2018Contradiction\u2019 for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2% data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.","authors":["Swaroop Mishra","Bhavdeep Singh Sachdeva"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Need to Create Big Datasets to Learn a Task?","tldr":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.47","presentation_id":"38939445","rocketchat_channel":"paper-sustainlp2020-47","speakers":"Swaroop Mishra|Bhavdeep Singh Sachdeva","title":"Do We Need to Create Big Datasets to Learn a Task?"},{"content":{"abstract":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.","authors":["Ameet Deshpande","Karthik Narasimhan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.419","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Guiding Attention for Self-Supervised Learning with Transformers","tldr":"In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models cont...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.49","presentation_id":"38939446","rocketchat_channel":"paper-sustainlp2020-49","speakers":"Ameet Deshpande|Karthik Narasimhan","title":"Guiding Attention for Self-Supervised Learning with Transformers"},{"content":{"abstract":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.","authors":["Harshil Shah","Julien Fauqueur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models","tldr":"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.5","presentation_id":"38939422","rocketchat_channel":"paper-sustainlp2020-5","speakers":"Harshil Shah|Julien Fauqueur","title":"Learning Informative Representations of Biomedical Relations with Latent Variable Models"},{"content":{"abstract":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed \u2013 non-learnable \u2013 attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.","authors":["Alessandro Raganato","Yves Scherrer","J\u00f6rg Tiedemann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.49","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation","tldr":"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of ...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.512","presentation_id":"38940110","rocketchat_channel":"paper-sustainlp2020-512","speakers":"Alessandro Raganato|Yves Scherrer|J\u00f6rg Tiedemann","title":"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"},{"content":{"abstract":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.","authors":["Zhao Jinman","Shawn Zhong","Xiaomin Zhang","Yingyu Liang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.53","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding","tldr":"We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.547","presentation_id":"38940115","rocketchat_channel":"paper-sustainlp2020-547","speakers":"Zhao Jinman|Shawn Zhong|Xiaomin Zhang|Yingyu Liang","title":"PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding"},{"content":{"abstract":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.","authors":["Kumar Shridhar","Harshil Jain","Akshat Agarwal","Denis Kleyko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End to End Binarized Neural Networks for Text Classification","tldr":"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.6","presentation_id":"38939423","rocketchat_channel":"paper-sustainlp2020-6","speakers":"Kumar Shridhar|Harshil Jain|Akshat Agarwal|Denis Kleyko","title":"End to End Binarized Neural Networks for Text Classification"},{"content":{"abstract":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.","authors":["Zi Lin","Jeremiah Liu","Zi Yang","Nan Hua","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.64","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior","tldr":"Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which p...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.651","presentation_id":"38940112","rocketchat_channel":"paper-sustainlp2020-651","speakers":"Zi Lin|Jeremiah Liu|Zi Yang|Nan Hua|Dan Roth","title":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"},{"content":{"abstract":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.","authors":["Mingzhu Wu","Nafise Sadat Moosavi","Andreas R\u00fcckl\u00e9","Iryna Gurevych"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.74","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases","tldr":"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge abo...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.724","presentation_id":"38940113","rocketchat_channel":"paper-sustainlp2020-724","speakers":"Mingzhu Wu|Nafise Sadat Moosavi|Andreas R\u00fcckl\u00e9|Iryna Gurevych","title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases"},{"content":{"abstract":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of pretrained language models (PLMs), we investigate how to incorporate large PKM into PLMs that can be finetuned for a wide variety of downstream NLP tasks. We define a new memory usage metric, and careful observation using this metric reveals that most memory slots remain outdated during the training of PKM-augmented models. To train better PLMs by tackling this issue, we propose simple but effective solutions: (1) initialization from the model weights pretrained without memory and (2) augmenting PKM by addition rather than replacing a feed-forward network. We verify that both of them are crucial for the pretraining of PKM-augmented PLMs, enhancing memory utilization and downstream performance. Code and pretrained weights are available at https://github.com/clovaai/pkm-transformers.","authors":["Gyuwan Kim","Tae Hwan Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.362","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Product Key Memory for Pretrained Language Models","tldr":"Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal langua...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.8","presentation_id":"38939424","rocketchat_channel":"paper-sustainlp2020-8","speakers":"Gyuwan Kim|Tae Hwan Jung","title":"Large Product Key Memory for Pretrained Language Models"},{"content":{"abstract":"","authors":["Vivek Gupta","Ankit Saw","Pegah Nokhiz","Praneeth Netrapalli","Piyush Rai","Partha Talukdar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"P-SIF: Document Embeddings using Partition Averaging","tldr":null,"track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.9","presentation_id":"38939425","rocketchat_channel":"paper-sustainlp2020-9","speakers":"Vivek Gupta|Ankit Saw|Pegah Nokhiz|Praneeth Netrapalli|Piyush Rai|Partha Talukdar","title":"P-SIF: Document Embeddings using Partition Averaging"},{"content":{"abstract":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on domain-specific, labeled data. In this paper, we propose ESTeR , an unsupervised model for identifying emotions using a novel similarity function based on random walks on graphs. Our model combines large-scale word co-occurrence information with word-associations from lexicons avoiding not only the dependence on labeled datasets, but also an explicit mapping of words to latent spaces used in emotion-enriched word embeddings. Our similarity function can also be computed efficiently. We study a range of datasets including recent tweets related to COVID-19 to illustrate the superior performance of our model and report insights on public emotions during the on-going pandemic.","authors":["Sujatha Das Gollapalli","Polina Rozenshtein","See-Kiong Ng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.93","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection","tldr":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using comp...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.929","presentation_id":"38940135","rocketchat_channel":"paper-sustainlp2020-929","speakers":"Sujatha Das Gollapalli|Polina Rozenshtein|See-Kiong Ng","title":"ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection"},{"content":{"abstract":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20\u00d7 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.","authors":["Alex Wang","Thomas Wolf"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.sustainlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Overview of the SustaiNLP 2020 Shared Task","tldr":"We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We des...","track":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"},"id":"WS-15.2020.sustainlp-1.24","presentation_id":"","rocketchat_channel":"paper-sustainlp2020-24","speakers":"Alex Wang|Thomas Wolf","title":"Overview of the SustaiNLP 2020 Shared Task"}],"prerecorded_talks":[{"presentation_id":"38940099","speakers":"Mona Diab","title":"Invited Talk"},{"presentation_id":"38940100","speakers":"Heng Ji","title":"Invited Talk"},{"presentation_id":"38940101","speakers":"Graham Neubig","title":"Invited Talk"},{"presentation_id":"38940102","speakers":"Alexander Rush","title":"Invited Talk"},{"presentation_id":"38940103","speakers":"Armand Joulin","title":"Invited Talk"}],"rocketchat_channel":"workshop-sustainlp2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 09:00:00 GMT","hosts":"Shafiq Joty","link":"","session_name":"QA session1: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.8.html\">Large Product Key Memory for Pre-trained Language Models</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.12.html\">Exploring the Boundaries of Low-Resource BERT Distillation</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.30.html\">Keyphrase Generation with GANs in Low-Resources Scenarios</a><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.2288.html\">Domain Adversarial Fine-Tuning as an Effective Regularizer</a>","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:00:00 GMT","hosts":"Shafiq Joty","link":"","session_name":"QA session2: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.22.html\">Don\u2019t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering</a><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.45.html\">A comparison between CNNs and WFAs for Sequence Classification</a><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.46.html\">Counterfactual Augmentation for Training Next Response Selection </a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.651.html\"> Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior </a>","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:00:00 GMT","hosts":" Goran Glava\u0161","link":"","session_name":"<b>Invited Talk</b><br> <a href=\"https://zoom.us\">Zoom link 1</a> <br> <b>Mona Diab</b>: Non-parameterized sentence encoders: an efficient alternative","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"Thomas Wolf","link":"","session_name":"QA session3-A: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.1.html\">Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in Bahasa Indonesia?</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.5.html\">Learning Informative Representations of Biomedical Relations with Latent Variable Models</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.6.html\">End to End Binarized Neural Networks for Text Classification</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.36.html\">Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks</a>","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"Marzieh Fadaee","link":"","session_name":"QA session3-B: <br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.547.html\">PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.2017.html\">Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.512.html\"> Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation</a>","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":" Goran Glava\u0161","link":"","session_name":"<b>Invited speakers QA</b> <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://slideslive.com/38940100\"><b>Heng Ji</b>: Efficient Language Acquisition through Multimedia Curriculum Learning</a> <br>\n\u2022 <a href=\"https://slideslive.com/38940101\"> <b>Graham Neubig</b>: More is Less? Non-parametric Language Models and Efficiency</a> <br>\n\u2022 <a href=\"https://slideslive.com/38940102\"><b>Alexander Rush</b>: Advances in Sequence Knowledge Distillation </a> <br>\n\u2022 <a href=\"https://slideslive.com/38940103\"> <b>Armand Joulin</b>: Making Pre-trained Models More Sustainable </a>","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:00:00 GMT","hosts":"Vered Shwartz & Angela Fan","link":"","session_name":"<b>Panel Discussion</b> <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n<b>Panelists</b>: Kyunghyun Cho, Yejin Choi, Mona Diab, Yoav Goldberg, Iryna Gurevych, Heng Ji, Graham Neubig, Alexander Rush, Armand Joulin, Luke Zettlemoyer","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"Nafise Sadat Moosavi","link":"","session_name":"QA session4: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.29.html\">Early Exiting BERT for Efficient Document Ranking</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.32.html\">Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.2516.html\">Identifying Spurious Correlations for Robust Text Classification</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.724.html\"> Improving QA Generalization by Concurrent Modeling of Multiple Biases</a>","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":"Nafise Sadat Moosavi","link":"","session_name":"QA session5: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.37.html\">Load What You Need: Smaller Versions of Mutlilingual BERT</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.42.html\">Towards Accurate and Reliable Energy Measurement of NLP Models</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.47.html\">Do We Need to Create Big Datasets to Learn a Task?</a>","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 00:00:00 GMT","hosts":"Alex Wang","link":"","session_name":"QA Session6-A: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.2.html\">Rank and run-time aware compression of NLP Applications</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.3.html\">Incremental Neural Coreference Resolution in Constant Memory</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.13.html\">Efficient Estimation of Influence of a Training Instance</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.14.html\">Efficient Inference For Neural Machine Translation</a>","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 00:00:00 GMT","hosts":"Ameet Deshpande","link":"","session_name":"QA Session6-B: <br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.3459.html\">Guiding Attention for Self-Supervised Learning with Transformers</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.1887.html\">Semi-supervised Formality Style Transfer using LanguageModel Discriminator and Mutual Information Maximization</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.3078.html\">SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.3526.html\"> Probabilstic Case-based Reasoning for Open-World Knowledge Graph Completion</a>","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 01:00:00 GMT","hosts":"Vered Shwartz","link":"","session_name":"QA Session7: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.9.html\">P-SIF: Document Embeddings using Partition Averaging</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.17.html\">Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.39.html\"> Analysis of Resource-efficient Predictive Models for Natural Language Processing</a><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.34.html\">A Little Bit Is Worse Than None: Ranking with Limited Training Data</a>","start_time":"Sat, 21 Nov 2020 00:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 02:00:00 GMT","hosts":"Alex Wang","link":"","session_name":"QA Session8: <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.27.html\"> Doped Structured Matrices for Extreme Compression of LSTM Models</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.28.html\">A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.38.html\">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.43.html\">FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</a> <br>\n\u2022 <a href=\"https://virtual.2020.emnlp.org/paper_WS-15.1098.html\">  OptSLA: an Optimization-Based Approach for Sequential Label Aggregation  </a>","start_time":"Sat, 21 Nov 2020 01:00:00 GMT"}],"title":"SustaiNLP: Workshop on Simple and Efficient Natural Language Processing","website":"https://sites.google.com/view/sustainlp2020","zoom_links":["https://zoom.us","https://zoom.us"]},{"abstract":"The Workshop on Computational Approaches to Discourse (CODI) covers all aspects of discourse and its computational modeling.","blocks":[{"end_time":"Fri, 20 Nov 2020 20:30:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"}],"id":"WS-16","livestream":null,"organizers":"Chlo\u00e9 Braud, Christian Hardmeier, Junyi Jessy Li, Annie Louis and Michael Strube","papers":[{"content":{"abstract":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an action, and propose a composed variational natural language generator (CLANG), a transformer-based conditional variational autoencoder. CLANG utilizes two latent variables to represent the utterances corresponding to two different independent parts (domain and action) in the intent, and the latent variables are composed together to generate natural examples. Additionally, to improve the generator learning, we adopt the contrastive regularization loss that contrasts the in-class with the out-of-class utterance generation given the intent. To evaluate the quality of the generated utterances, experiments are conducted on the generalized few-shot intent detection task. Empirical results show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets.","authors":["Congying Xia","Caiming Xiong","Philip Yu","Richard Socher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.303","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Composed Variational Natural Language Generation for Few-shot Intents","tldr":"In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2487","presentation_id":"38940699","rocketchat_channel":"paper-codi2020-2487","speakers":"Congying Xia|Caiming Xiong|Philip Yu|Richard Socher","title":"Composed Variational Natural Language Generation for Few-shot Intents"},{"content":{"abstract":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model. Our results show that the span representation is able to encode a significant amount of coreference information. In addition, we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. Last, our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.","authors":["Patrick Kahardipraja","Olena Vyshnevska","Sharid Lo\u00e1iciga"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Span Representations in Neural Coreference Resolution","tldr":"In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a prob...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.10","presentation_id":"38939689","rocketchat_channel":"paper-codi2020-10","speakers":"Patrick Kahardipraja|Olena Vyshnevska|Sharid Lo\u00e1iciga","title":"Exploring Span Representations in Neural Coreference Resolution"},{"content":{"abstract":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience\u2019s response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a) text classification labels, indicating which actor\u2019s lines made the studio audience laugh; b) information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles.","authors":["Maolin Li"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts","tldr":"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people\u2019s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.11","presentation_id":"38939690","rocketchat_channel":"paper-codi2020-11","speakers":"Maolin Li","title":"Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts"},{"content":{"abstract":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to abstract entities and events. These features are inspired through several sources \u2013 cognitive parameters, pragmatic factors and typological status. We pay attention to the distributions of these features in a dataset containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and feature selection to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for contrastive linguistics, translation studies and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.","authors":["Ekaterina Lapshinova-Koltunski","Kerstin Kunz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Coreference Features in Heterogeneous Data with Text Classification","tldr":"The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communica...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.13","presentation_id":"38939691","rocketchat_channel":"paper-codi2020-13","speakers":"Ekaterina Lapshinova-Koltunski|Kerstin Kunz","title":"Exploring Coreference Features in Heterogeneous Data with Text Classification"},{"content":{"abstract":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a discourse relation or not. We study the influence of those context-specific embeddings. Further, we show the benefit of training the tasks of connective disambiguation and sense classification together at the same time. The success of our approach is supported by state-of-the-art results.","authors":["Ren\u00e9 Knaebel","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing","tldr":"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a disc...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.14","presentation_id":"38939692","rocketchat_channel":"paper-codi2020-14","speakers":"Ren\u00e9 Knaebel|Manfred Stede","title":"Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing"},{"content":{"abstract":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in DSNDM is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.","authors":["Alexander Chernyavskiy","Dmitry Ilvovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking","tldr":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political stateme...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.15","presentation_id":"38939693","rocketchat_channel":"paper-codi2020-15","speakers":"Alexander Chernyavskiy|Dmitry Ilvovsky","title":"DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking"},{"content":{"abstract":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.","authors":["Laurine Huber","Chaker Memmadi","Mathilde Dargnat","Yannick Toussaint"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?","tldr":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.17","presentation_id":"38939694","rocketchat_channel":"paper-codi2020-17","speakers":"Laurine Huber|Chaker Memmadi|Mathilde Dargnat|Yannick Toussaint","title":"Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.18","presentation_id":"38939695","rocketchat_channel":"paper-codi2020-18","speakers":"Patrick Huber|Giuseppe Carenini","title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder"},{"content":{"abstract":"","authors":["Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Large Discourse Treebanks from Scalable Distant Supervision","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.19","presentation_id":"38939696","rocketchat_channel":"paper-codi2020-19","speakers":"Patrick Huber|Giuseppe Carenini","title":"Large Discourse Treebanks from Scalable Distant Supervision"},{"content":{"abstract":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining different portions of OntoNotes with Twitter data show that selecting text genres for the training data can beat the mere maximization of training data amount. In addition, we inspect several phenomena such as the role of deictic pronouns in conversational data, and present additional results for variant settings. Our best configuration improves the performance of the\u201dout of the box\u201d system by 21.6%.","authors":["Berfin Akta\u015f","Veronika Solopova","Annalena Kohnert","Manfred Stede"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.222","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Adapting Coreference Resolution to Twitter Conversations","tldr":"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter con...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.1951","presentation_id":"38940697","rocketchat_channel":"paper-codi2020-1951","speakers":"Berfin Akta\u015f|Veronika Solopova|Annalena Kohnert|Manfred Stede","title":"Adapting Coreference Resolution to Twitter Conversations"},{"content":{"abstract":"","authors":["Diane Litman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discourse for Argument Mining, and Argument Mining as Discourse","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.20","presentation_id":"38939697","rocketchat_channel":"paper-codi2020-20","speakers":"Diane Litman","title":"Discourse for Argument Mining, and Argument Mining as Discourse"},{"content":{"abstract":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI love you.\u201d We designed a system to allow virtual assistants to take a voice message from one user, convert the point of view of the message, and then deliver the result to its target user. We developed a rule-based model, which integrates a linear text classification model, part-of-speech tagging, and constituency parsing with rule-based transformation methods. We also investigated Neural Machine Translation (NMT) approaches, including LSTMs, CopyNet, and T5. We explored 5 metrics to gauge both naturalness and faithfulness automatically, and we chose to use BLEU plus METEOR for faithfulness and relative perplexity using a separately trained language model (GPT) for naturalness. Transformer-Copynet and T5 performed similarly on faithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a METEOR score of 83.0. CopyNet was the most natural, with a relative perplexity of 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly released our dataset, which is composed of 46,565 crowd-sourced samples.","authors":["Gunhee Lee","Vera Zu","Sai Srujana Buddi","Dennis Liang","Purva Kulkarni","Jack FitzGerald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Converting the Point of View of Messages Spoken to Virtual Assistants","tldr":"Virtual Assistants can be quite literal at times. If the user says \u201ctell Bob I love him,\u201d most virtual assistants will extract the message \u201cI love him\u201d and send it to the user\u2019s contact named Bob, rather than properly converting the message to \u201cI lov...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.208","presentation_id":"38940694","rocketchat_channel":"paper-codi2020-208","speakers":"Gunhee Lee|Vera Zu|Sai Srujana Buddi|Dennis Liang|Purva Kulkarni|Jack FitzGerald","title":"Converting the Point of View of Messages Spoken to Virtual Assistants"},{"content":{"abstract":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.","authors":["Yunmo Chen","Tongfei Chen","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Joint Modeling of Arguments for Event Understanding","tldr":"We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach all...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.21","presentation_id":"38939698","rocketchat_channel":"paper-codi2020-21","speakers":"Yunmo Chen|Tongfei Chen|Benjamin Van Durme","title":"Joint Modeling of Arguments for Event Understanding"},{"content":{"abstract":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes semi-structured commonsense inference rules and learns to incorporate them in a transformer-based reasoning cell.We assess the model\u2019s performance on two tasks that require different reasoning skills: Abductive Natural Language Inference and Counterfactual Invariance Prediction as a new task. We show that our proposed model improves performance over strong state-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we are, to the best of our knowledge, the first to demonstrate that a model that learns to perform counterfactual reasoning helps predicting the best explanation in an abductive reasoning task. We validate the robustness of the model\u2019s reasoning capabilities by perturbing the knowledge and provide qualitative analysis on the model\u2019s knowledge incorporation capabilities.","authors":["Debjit Paul","Anette Frank"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.267","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention","tldr":"Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes se...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.2195","presentation_id":"38940698","rocketchat_channel":"paper-codi2020-2195","speakers":"Debjit Paul|Anette Frank","title":"Social Commonsense Reasoning with Multi-Head Knowledge Attention"},{"content":{"abstract":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.","authors":["Youmna Farag","Josef Valvoda","Helen Yannakoudakis","Ted Briscoe"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Neural Discourse Coherence Models","tldr":"In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensit...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.22","presentation_id":"38939699","rocketchat_channel":"paper-codi2020-22","speakers":"Youmna Farag|Josef Valvoda|Helen Yannakoudakis|Ted Briscoe","title":"Analyzing Neural Discourse Coherence Models"},{"content":{"abstract":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on a Multi-Layer Perceptron study and a Sequential Forward Search experiment, followed by Bayes Factor analysis of the outcomes. The results suggest that recency metrics counting paragraphs and sentences contribute to referential choice prediction more than other recency-related metrics. Based on the results of our analysis, we argue that, sensitivity to discourse structure is important for recency metrics used in determining referring expression forms.","authors":["Fahime Same","Kees van Deemter"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse","tldr":"First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on ...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.23","presentation_id":"38939700","rocketchat_channel":"paper-codi2020-23","speakers":"Fahime Same|Kees van Deemter","title":"Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse"},{"content":{"abstract":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \u201cSynthesizer\u201d framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.","authors":["Wen Xiao","Patrick Huber","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !","tldr":"The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechani...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.24","presentation_id":"38939701","rocketchat_channel":"paper-codi2020-24","speakers":"Wen Xiao|Patrick Huber|Giuseppe Carenini","title":"Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !"},{"content":{"abstract":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.","authors":["Li Liang","Zheng Zhao","Bonnie Webber"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Extending Implicit Discourse Relation Recognition to the PDTB-3","tldr":"The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse r...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.26","presentation_id":"38939702","rocketchat_channel":"paper-codi2020-26","speakers":"Li Liang|Zheng Zhao|Bonnie Webber","title":"Extending Implicit Discourse Relation Recognition to the PDTB-3"},{"content":{"abstract":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.","authors":["Chenguang Zhu","Ruochen Xu","Michael Zeng","Xuedong Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining","tldr":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization in...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.263","presentation_id":"38940695","rocketchat_channel":"paper-codi2020-263","speakers":"Chenguang Zhu|Ruochen Xu|Michael Zeng|Xuedong Huang","title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining"},{"content":{"abstract":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En lexicon includes 95 entries, whereas the Tr-En lexicon contains 133 entries. The lexicons constitute the first step of a larger project of developing a multilingual discourse connective lexicon.","authors":["Murathan Kurfal\u0131","Sibel Ozer","Deniz Zeyrek","Am\u00e1lia Mendes"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex","tldr":"In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.27","presentation_id":"38939703","rocketchat_channel":"paper-codi2020-27","speakers":"Murathan Kurfal\u0131|Sibel Ozer|Deniz Zeyrek|Am\u00e1lia Mendes","title":"TED-MDB Lexicons: TrEnConnLex, PtEnConnLex"},{"content":{"abstract":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects\u2014lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements.","authors":["Haixia Chai","Wei Zhao","Steffen Eger","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks","tldr":"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.28","presentation_id":"38939704","rocketchat_channel":"paper-codi2020-28","speakers":"Haixia Chai|Wei Zhao|Steffen Eger|Michael Strube","title":"Evaluation of Coreference Resolution Systems Under Adversarial Attacks"},{"content":{"abstract":"","authors":["Belen Saldias","Deb Roy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.29","presentation_id":"38939705","rocketchat_channel":"paper-codi2020-29","speakers":"Belen Saldias|Deb Roy","title":"Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types"},{"content":{"abstract":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, starting with a strong baseline neural parser unaware of any coreference information, we compare a parser which utilizes only the output of a neural coreference resolver, with a more sophisticated model, where discourse parsing and coreference resolution are jointly learned in a neural multitask fashion. Results indicate that these initial attempts to incorporate coreference information do not boost the performance of discourse parsing in a statistically significant way.","authors":["Grigorii Guz","Giuseppe Carenini"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Coreference for Discourse Parsing: A Neural Approach","tldr":"We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, startin...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.31","presentation_id":"38939706","rocketchat_channel":"paper-codi2020-31","speakers":"Grigorii Guz|Giuseppe Carenini","title":"Coreference for Discourse Parsing: A Neural Approach"},{"content":{"abstract":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \\delta-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.","authors":["Rachel Rudinger","Vered Shwartz","Jena D. Hwang","Chandra Bhagavatula","Maxwell Forbes","Ronan Le Bras","Noah A. Smith","Yejin Choi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.418","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language","tldr":"Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference ha...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3452","presentation_id":"38940700","rocketchat_channel":"paper-codi2020-3452","speakers":"Rachel Rudinger|Vered Shwartz|Jena D. Hwang|Chandra Bhagavatula|Maxwell Forbes|Ronan Le Bras|Noah A. Smith|Yejin Choi","title":"Thinking Like a Skeptic: Defeasible Inference in Natural Language"},{"content":{"abstract":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model\u2019s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other.","authors":["Yehudit Meged","Avi Caciularu","Vered Shwartz","Ido Dagan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.440","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin","tldr":"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as dista...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.3598","presentation_id":"38940701","rocketchat_channel":"paper-codi2020-3598","speakers":"Yehudit Meged|Avi Caciularu|Vered Shwartz|Ido Dagan","title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin"},{"content":{"abstract":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A comparative study for the language pair can discover the discourse differences between Spanish and Chinese, and can benefit the Spanish-Chinese translation. In this work, based on a Spanish-Chinese parallel corpus annotated with discourse information, we compare the annotation results between the language pair and analyze how discourse affects Spanish-Chinese translation. The research results in our study can help human translators who work with the language pair.","authors":["Shuyuan Cao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus","tldr":"With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A compa...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.4","presentation_id":"38939686","rocketchat_channel":"paper-codi2020-4","speakers":"Shuyuan Cao","title":"How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus"},{"content":{"abstract":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.","authors":["Yifan Gao","Piji Li","Wei Bi","Xiaojiang Liu","Michael Lyu","Irwin King"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.40","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning","tldr":"Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.475","presentation_id":"38940696","rocketchat_channel":"paper-codi2020-475","speakers":"Yifan Gao|Piji Li|Wei Bi|Xiaojiang Liu|Michael Lyu|Irwin King","title":"Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning"},{"content":{"abstract":"","authors":["Juntao Yu","Nafise Sadat Moosavi","Silviu Paun","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.6","presentation_id":"38940702","rocketchat_channel":"paper-codi2020-6","speakers":"Juntao Yu|Nafise Sadat Moosavi|Silviu Paun|Massimo Poesio","title":"Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution"},{"content":{"abstract":"","authors":["Juntao Yu","Massimo Poesio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multitask Learning-Based Neural Bridging Reference Resolution","tldr":null,"track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.7","presentation_id":"38940703","rocketchat_channel":"paper-codi2020-7","speakers":"Juntao Yu|Massimo Poesio","title":"Multitask Learning-Based Neural Bridging Reference Resolution"},{"content":{"abstract":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled. These features were transformed using tf-idf and word embedding; feature selection was done using Principal Component Analysis (PCA). We ran experiments on six combinations of features to predict sequences with Hierarchical Agglomerative Clustering. An analysis of the clustering results indicate that using word embeddings and syntactic features, significantly improved the results.","authors":["Maitreyee Maitreyee"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues","tldr":"This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled....","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.8","presentation_id":"38939687","rocketchat_channel":"paper-codi2020-8","speakers":"Maitreyee Maitreyee","title":"Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues"},{"content":{"abstract":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.","authors":["Sopan Khosla","Carolyn Rose"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.codi-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Type Information to Improve Entity Coreference Resolution","tldr":"Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic know...","track":"1st Workshop on Computational Approaches to Discourse"},"id":"WS-16.9","presentation_id":"38939688","rocketchat_channel":"paper-codi2020-9","speakers":"Sopan Khosla|Carolyn Rose","title":"Using Type Information to Improve Entity Coreference Resolution"}],"prerecorded_talks":[{"link":"https://vimeo.com/486317957","speakers":"Eduard Hovy","title":"Discourse processing in the time of DNNs"},{"link":"https://vimeo.com/486319650","speakers":"Eunsol Choi","title":"Learning to Understand Language in Context"}],"rocketchat_channel":"workshop-codi2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 14:10:00 GMT","hosts":"TBD","link":"","session_name":"<b>Opening remarks</b> <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:00:00 GMT","hosts":"TBD","link":"","session_name":"<b>Discussion on next year workshop and SIG</b> <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 14:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"TBD","link":"","session_name":"<b>Keynote: </b><a href=\"https://codi-workshop.github.io/invited-speakers/\"><b>Eunsol Choi</b> - <i>Learning to Understand Language in Context</i></a><br><a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:15:00 GMT","hosts":"TBD","link":"","session_name":"<b> break - Gather.town (Room K) </b>","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:45:00 GMT","hosts":"Michael Strube","link":"","session_name":"<b>Session 1: QA - Anaphora and coreference - Part 1</b> <br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br> \u2022 Exploring Span Representations in Neural Coreference Resolution, Patrick Kahardipraja, Olena Vyshnevska and Sharid Lo\u00e1iciga <br>\n\u2022 Using Type Information to Improve Entity Coreference Resolution, Sopan Khosla and Carolyn Rose <br>\n\u2022 Eliminating Mention Overlaps: Evaluation of Coreference Resolution Systems Under Adversarial Attacks, Haixia Chai, Wei Zhao, Steffen Eger and Michael Strube <br>\n\u2022 COLING Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution, Juntao Yu, Nafise Sadat Moosavi, Silviu Paun and Massimo Poesio","start_time":"Fri, 20 Nov 2020 16:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:45:00 GMT","hosts":"Chlo\u00e9 Braud and Annie Louis","link":"","session_name":"<b>Session 1: QA - Coherence relations, discourse parsing and corpora - Part 1</b> <br> <a href=\"https://zoom.us\">Zoom link 3</a> <br><br>\n\u2022 Coreference for Discourse Parsing: A Neural Approach, Grigorii Guz and Giuseppe Carenini <br>\n\u2022 Analyzing Neural Discourse Coherence Models, Youmna Farag, Josef Valvoda, Helen Yannakoudakis and Ted Briscoe  <br>\n\u2022 DSNDM: Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking, Alexander Chernyavskiy and Dmitry Ilvovsky <br>\n\u2022 Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder, Patrick Huber and Giuseppe Carenini","start_time":"Fri, 20 Nov 2020 16:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:45:00 GMT","hosts":"Christian Hardmeier and Jessy Li","link":"","session_name":"<b>Session 1: QA - Discourse and dialog - Part 1 </b> <br> <a href=\"https://zoom.us\">Zoom link 4</a> <br><br>\n\u2022 Joint Modeling of Arguments for Event Understanding, Yunmo Chen, Tongfei Chen and Benjamin Van Durme <br>\n\u2022 Beyond Adjacency Pairs: Extracting Longer Regularities in Human-Machine Dialogues, Maitreyee Maitreyee   <br>\n\u2022 (FINDINGS) Converting the Point of View of Messages Spoken to Virtual Assistants, Isabelle G. Lee, Vera Zu, Sai Srujana Buddi, Dennis Liang, Purva Kulkarni, Jack G.M. Fitzgerald   <br>\n\u2022 (FINDINGS) Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning, Yifan Gao, Piji Li, Wei Bi, Xiaojiang Liu, Michael R. Lyu, Irwin King  <br>\n\u2022 (FINDINGS) Composed Variational Natural Language Generation for Few-shot Intents, Congying Xia, Caiming Xiong, Philip Yu, Richard Socher <br>\n\u2022 (FINDINGS) A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining, Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang","start_time":"Fri, 20 Nov 2020 16:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:15:00 GMT","hosts":"TBD","link":"","session_name":"<b>Breakout Sessions + Lunch</b> <br><a href=\"https://zoom.us\">Zoom link 1</a><br><b>Then meet in: Gather.town (Room K) </b><br>","start_time":"Fri, 20 Nov 2020 16:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:15:00 GMT","hosts":"TBD","link":"","session_name":"<b>Keynote: </b><a href=\"https://codi-workshop.github.io/invited-speakers/\"><b>Eduard Hovy </b> - <i>- Discourse processing in the time of DNNs</i></a> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 18:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"Michael Strube","link":"","session_name":"<b>Session 2: QA - Anaphora and coreference - Part 2 </b> <br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br>\n\u2022 Adapting Coreference Resolution to Twitter ConversationExploring Coreference Features in Heterogeneous Data with Text Classification, Ekaterina Lapshinova-Koltunski and Kerstin Kunz <br>\n\u2022 (COLING) Multitask Learning-Based Neural Bridging Reference Resolution, Juntao Yu and Massimo Poesio  <br>\n\u2022 (FINDINGS) Paraphrasing vs Coreferring: Two Sides of the Same Coin Yehudit Meged   <br>\n\u2022 (FINDINGS) Adapting Coreference Resolution to Twitter Conversations Berfin Aktas","start_time":"Fri, 20 Nov 2020 19:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"Chlo\u00e9 Braud  ","link":"","session_name":"<b>Session 2: QA - Coherence relations, discourse parsing and corpora - Part 2 </b> <br> <a href=\"https://zoom.us\">Zoom link 3</a> <br><br>\n\u2022 TED-MDB Lexicons: TrEnConnLex, PtEnConnLex, Murathan Kurfal\u0131, Sibel Ozer, Deniz Zeyrek and Am\u00e1lia Mendes <br>\n\u2022 Large Discourse Treebanks from Scalable Distant Supervision, Patrick Huber and Giuseppe Carenini  <br>\n\u2022 Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing, Ren\u00e9 Knaebel and Manfred Stede <br>\n\u2022 Extending Implicit Discourse Relation Recognition to the PDTB-3, Li Liang, Zheng Zhao and Bonnie Webber","start_time":"Fri, 20 Nov 2020 19:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"Christian Hardmeier and Annie Louis","link":"","session_name":"<b>Session 2: QA - Discourse and language understanding </b>   <br> <a href=\"https://zoom.us\">Zoom link 4</a> <br><br>\n\u2022 Discourse for Argument Mining, and Argument Mining as Discourse, Diane Litman  <br>\n\u2022 Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts?, Laurine Huber, Chaker Memmadi, Mathilde Dargnat and Yannick Toussaint  <br>\n\u2022 How does discourse affect Spanish-Chinese Translation? A case study based on a Spanish-Chinese parallel corpus, Shuyuan Cao  <br>\n\u2022 (FINDINGS) Social Commonsense Reasoning with Multi-Head Knowledge Attention, Debjit Paul, Anette Frank <br>\n\u2022 (FINDINGS) Thinking Like a Skeptic: Defeasible Inference in Natural Language, Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, Yejin Choi","start_time":"Fri, 20 Nov 2020 19:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"Jessy Li","link":"","session_name":"<b>Session 2: QA - Generation and applications </b>  <br> <a href=\"https://zoom.us\">Zoom link 5</a> <br><br>\n\u2022 Computational Interpretation of Recency for the Choice of Referring Expressions in Discourse, Fahime Same and Kees van Deemter <br>\n\u2022 Do We Really Need Thrs In Transformer For Extractive Summarization? Discourse Can Help!, Wen Xiao, Patrick Huber and Giuseppe Carenini  <br>\n\u2022 Supporting Comedy Writers: Predicting Audience\u2019s Response from Sketch Comedy and Crosstalk Scripts, Maolin Li  <br>\n\u2022  Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types, Belen Saldias and Deb Roy","start_time":"Fri, 20 Nov 2020 19:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:30:00 GMT","hosts":"TBD","link":"","session_name":"<b>Discussion and closing remarks</b> <br><a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 19:45:00 GMT"}],"title":"1st Workshop on Computational Approaches to Discourse","website":"https://cad-workshop.github.io/","zoom_links":["https://zoom.us","https://zoom.us","https://zoom.us","https://zoom.us","https://zoom.us"]},{"abstract":"The workshop focuses on online abuse through a multi-disciplinary lens, drawing on social sciences, law, media studies, and more.","blocks":[{"end_time":"Fri, 20 Nov 2020 19:30:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"}],"id":"WS-17","livestream":null,"organizers":"Zeerak Waseem, Vinodkumar Prabhakaran, Seyi Akiwowo and Bertie Vidgen","papers":[{"content":{"abstract":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classification has not been fully explored. We present the first systematic study on how data augmentation techniques impact performance across toxic language classifiers, ranging from shallow logistic regression architectures to BERT \u2013 a state-of-the-art pretrained Transformer network. We compare the performance of eight techniques on very scarce seed datasets. We show that while BERT performed the best, shallow classifiers performed comparably when trained on data augmented with a combination of three techniques, including GPT-2-generated sentences. We discuss the interplay of performance and computational overhead, which can inform the choice of techniques under different constraints.","authors":["Mika Juuti","Tommi Gr\u00f6ndahl","Adrian Flanagan","N. Asokan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.269","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A little goes a long way: Improving toxic language classification despite data scarcity","tldr":"Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation \u2013 generating new synthetic data from a labeled seed dataset \u2013 can help. The efficacy of data augmentation on toxic language classifi...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2217","presentation_id":"38940137","rocketchat_channel":"paper-woah4-2217","speakers":"Mika Juuti|Tommi Gr\u00f6ndahl|Adrian Flanagan|N. Asokan","title":"A little goes a long way: Improving toxic language classification despite data scarcity"},{"content":{"abstract":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators \u2013 not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists\u2019 experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.","authors":["Ishaan Arora","Julia Guo","Sarah Ita Levitan","Susan McGregor","Julia Hirschberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter","tldr":"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annot...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.10","presentation_id":"38939517","rocketchat_channel":"paper-woah4-10","speakers":"Ishaan Arora|Julia Guo|Sarah Ita Levitan|Susan McGregor|Julia Hirschberg","title":"A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter"},{"content":{"abstract":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. However, its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. Here we use a unique situation in Germany where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our pipeline achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97\u2014accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.","authors":["Joshua Garland","Keyan Ghazi-Zahedi","Jean-Gabriel Young","Laurent H\u00e9bert-Dufresne","Mirta Galesic"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Countering hate on social media: Large scale classification of hate and counter speech","tldr":"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engag...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.13","presentation_id":"38939518","rocketchat_channel":"paper-woah4-13","speakers":"Joshua Garland|Keyan Ghazi-Zahedi|Jean-Gabriel Young|Laurent H\u00e9bert-Dufresne|Mirta Galesic","title":"Countering hate on social media: Large scale classification of hate and counter speech"},{"content":{"abstract":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politically-biased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.","authors":["Maximilian Wich","Jan Bauer","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Impact of politically biased data on hate speech classification","tldr":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their v...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.15","presentation_id":"38939519","rocketchat_channel":"paper-woah4-15","speakers":"Maximilian Wich|Jan Bauer|Georg Groh","title":"Impact of politically biased data on hate speech classification"},{"content":{"abstract":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.","authors":["Michele Banko","Brendon MacKeen","Laurie Ray"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Unified Taxonomy of Harmful Content","tldr":"The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.16","presentation_id":"38939520","rocketchat_channel":"paper-woah4-16","speakers":"Michele Banko|Brendon MacKeen|Laurie Ray","title":"A Unified Taxonomy of Harmful Content"},{"content":{"abstract":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.","authors":["Kosisochukwu Madukwe","Xiaoying Gao","Bing Xue"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets","tldr":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining ...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.19","presentation_id":"38939521","rocketchat_channel":"paper-woah4-19","speakers":"Kosisochukwu Madukwe|Xiaoying Gao|Bing Xue","title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets"},{"content":{"abstract":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.","authors":["Claire Pershan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach","tldr":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and inter...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2","presentation_id":"38939516","rocketchat_channel":"paper-woah4-2","speakers":"Claire Pershan","title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach"},{"content":{"abstract":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.","authors":["Kadir Bulut Ozler","Kate Kenski","Steve Rains","Yotam Shmargad","Kevin Coe","Steven Bethard"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection","tldr":"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.24","presentation_id":"38939522","rocketchat_channel":"paper-woah4-24","speakers":"Kadir Bulut Ozler|Kate Kenski|Steve Rains|Yotam Shmargad|Kevin Coe|Steven Bethard","title":"Fine-tuning BERT for multi-domain and multi-label incivil language detection"},{"content":{"abstract":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of \u2018unhealthy online conversation\u2019. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.","authors":["Ilan Price","Jordan Gifford-Moore","Jory Flemming","Saul Musker","Maayan Roichman","Guillaume Sylvain","Nithum Thain","Lucas Dixon","Jeffrey Sorensen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Six Attributes of Unhealthy Conversations","tldr":"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either \u2018healthy\u2019 or \u2018unhealthy\u2019, in addition to binary labels for the presence of six potentially \u2018unhealthy\u2019 sub-attributes: (1) hostile; (...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.25","presentation_id":"38939523","rocketchat_channel":"paper-woah4-25","speakers":"Ilan Price|Jordan Gifford-Moore|Jory Flemming|Saul Musker|Maayan Roichman|Guillaume Sylvain|Nithum Thain|Lucas Dixon|Jeffrey Sorensen","title":"Six Attributes of Unhealthy Conversations"},{"content":{"abstract":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now. A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or \u0436\u0435\u043d\u0449\u0438\u043d\u0430, \u0447\u0435\u0440\u043d\u044b\u0439, \u0435\u0432\u0440\u0435\u0439) that are not toxic, but serve as triggers for the classifier due to model caveats. In this paper, we describe our efforts towards classifying hate speech in Russian, and propose simple techniques of reducing unintended bias, such as generating training data with language models using terms and words related to protected identities as context and applying word dropout to such words.","authors":["Nadezhda Zueva","Madina Kabirova","Pavel Kalaidin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection","tldr":"Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both res...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.31","presentation_id":"38939524","rocketchat_channel":"paper-woah4-31","speakers":"Nadezhda Zueva|Madina Kabirova|Pavel Kalaidin","title":"Reducing Unintended Identity Bias in Russian Hate Speech Detection"},{"content":{"abstract":"","authors":["Rosalie Gillett","Nicolas Suzor","Jean Burgess","Bridget Harris","Molly Dragiewicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating takedowns of abuse on Twitter","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.32","presentation_id":"38939525","rocketchat_channel":"paper-woah4-32","speakers":"Rosalie Gillett|Nicolas Suzor|Jean Burgess|Bridget Harris|Molly Dragiewicz","title":"Investigating takedowns of abuse on Twitter"},{"content":{"abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","authors":["Bertie Vidgen","Scott Hale","Ella Guest","Helen Margetts","David Broniatowski","Zeerak Waseem","Austin Botelho","Matthew Hall","Rebekah Tromble"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting East Asian Prejudice on Social Media","tldr":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier t...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.37","presentation_id":"38939526","rocketchat_channel":"paper-woah4-37","speakers":"Bertie Vidgen|Scott Hale|Ella Guest|Helen Margetts|David Broniatowski|Zeerak Waseem|Austin Botelho|Matthew Hall|Rebekah Tromble","title":"Detecting East Asian Prejudice on Social Media"},{"content":{"abstract":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.","authors":["Dante Razo","Sandra K\u00fcbler"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Sampling Bias in Abusive Language Detection","tldr":"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.39","presentation_id":"38939527","rocketchat_channel":"paper-woah4-39","speakers":"Dante Razo|Sandra K\u00fcbler","title":"Investigating Sampling Bias in Abusive Language Detection"},{"content":{"abstract":"","authors":["Ian Kivlichan","Olivia Redfield","Rachel Rosen","Raquel Saxe","Nitesh Goyal","Lucy Vasserman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.42","presentation_id":"38939528","rocketchat_channel":"paper-woah4-42","speakers":"Ian Kivlichan|Olivia Redfield|Rachel Rosen|Raquel Saxe|Nitesh Goyal|Lucy Vasserman","title":"Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity"},{"content":{"abstract":"","authors":["Viktorya Vilk","Elodie Vialle","Matt Bailey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse","tldr":null,"track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.43","presentation_id":"38939529","rocketchat_channel":"paper-woah4-43","speakers":"Viktorya Vilk|Elodie Vialle|Matt Bailey","title":"Free Expression by Design: Improving in-platform mechanisms and third-party tools to tackle online abuse"},{"content":{"abstract":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.","authors":["Anna Koufakou","Endang Wahyu Pamungkas","Valerio Basile","Viviana Patti"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language","tldr":"The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features deriv...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.44","presentation_id":"38939530","rocketchat_channel":"paper-woah4-44","speakers":"Anna Koufakou|Endang Wahyu Pamungkas|Valerio Basile|Viviana Patti","title":"HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language"},{"content":{"abstract":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in human-based incivility detection, it is urgent to develop validated and versatile automatic approaches to identifying uncivil posts and comments. This project advances both a neural, BERT-based classifier as well as a logistic regression classifier to identify uncivil comments. The classifier is trained on a dataset of Reddit posts, which are annotated for incivility, and further expanded using a combination of labeled data from Reddit and Twitter. Our best performing model achieves an F1 of 0.802 on our Reddit test set. The final model is not only applicable across social media platforms and their distinct data structures, but also computationally versatile, and - as such - ready to be used on vast volumes of online data. All trained models and annotated data are made available to the research community.","authors":["Sam Davidson","Qiusi Sun","Magdalena Wojcieszak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Developing a New Classifier for Automated Identification of Incivility in Social Media","tldr":"Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in h...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.47","presentation_id":"38939531","rocketchat_channel":"paper-woah4-47","speakers":"Sam Davidson|Qiusi Sun|Magdalena Wojcieszak","title":"Developing a New Classifier for Automated Identification of Incivility in Social Media"},{"content":{"abstract":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that classifiers based on syntactic structure of the text, dependency graphical convolutional networks (DepGCNs) can achieve state-of-the-art performance on abusive language datasets. The overall performance is at par with of strong baselines such as fine-tuned BERT. Further, our GCN-based approach is much more efficient than BERT at inference time making it suitable for real-time detection.","authors":["Kanika Narang","Chris Brew"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Abusive Language Detection using Syntactic Dependency Graphs","tldr":"Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that cla...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.48","presentation_id":"38939532","rocketchat_channel":"paper-woah4-48","speakers":"Kanika Narang|Chris Brew","title":"Abusive Language Detection using Syntactic Dependency Graphs"},{"content":{"abstract":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comments written by victims of abuse are frequently labelled as hateful, even if they discuss or reclaim slurs. Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. We make two main contributions to this end. First, we provide an annotation guide that outlines 4 main categories of online slur usage, which we further divide into a total of 12 sub-categories. Second, we present a publicly available corpus based on our taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, our taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.","authors":["Jana Kurrek","Haji Mohammad Saleem","Derek Ruths"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage","tldr":"Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comme...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.49","presentation_id":"38939533","rocketchat_channel":"paper-woah4-49","speakers":"Jana Kurrek|Haji Mohammad Saleem|Derek Ruths","title":"Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage"},{"content":{"abstract":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop computational models to incorporate emotions into textual cues to improve aggression identification. We evaluate our proposed methods on a set of corpora related to the task and show promising results with respect to abusive language detection.","authors":["Niloofar Safi Samghabadi","Afsheen Hatami","Mahsa Shafaei","Sudipta Kar","Thamar Solorio"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attending the Emotions to Detect Online Abusive Language","tldr":"In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.50","presentation_id":"38939534","rocketchat_channel":"paper-woah4-50","speakers":"Niloofar Safi Samghabadi|Afsheen Hatami|Mahsa Shafaei|Sudipta Kar|Thamar Solorio","title":"Attending the Emotions to Detect Online Abusive Language"},{"content":{"abstract":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widely researched problem, with current research having a strong focus on a binary classification of bullying versus non-bullying. This paper proposes a novel approach to enhancing cyberbullying detection through role modeling. We utilise a dataset from ASKfm to perform multi-class classification to detect participant roles (e.g. victim, harasser). Our preliminary results demonstrate promising performance including 0.83 and 0.76 of F1-score for cyberbullying and role classification respectively, outperforming baselines.","authors":["Gathika Rathnayake","Thushari Atapattu","Mahen Herath","Georgia Zhang","Katrina Falkner"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Enhancing the Identification of Cyberbullying through Participant Roles","tldr":"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widel...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.51","presentation_id":"38939535","rocketchat_channel":"paper-woah4-51","speakers":"Gathika Rathnayake|Thushari Atapattu|Mahen Herath|Georgia Zhang|Katrina Falkner","title":"Enhancing the Identification of Cyberbullying through Participant Roles"},{"content":{"abstract":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either \u2018Hateful\u2019, \u2018Normal\u2019 or \u2018Offensive\u2019. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.","authors":["Vebj\u00f8rn Isaksen","Bj\u00f6rn Gamb\u00e4ck"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online","tldr":"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.52","presentation_id":"38939536","rocketchat_channel":"paper-woah4-52","speakers":"Vebj\u00f8rn Isaksen|Bj\u00f6rn Gamb\u00e4ck","title":"Using Transfer-based Language Models to Detect Hateful and Offensive Language Online"},{"content":{"abstract":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics\u2019 keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.","authors":["Isar Nejadgholi","Svetlana Kiritchenko"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","tldr":"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applie...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.56","presentation_id":"38939537","rocketchat_channel":"paper-woah4-56","speakers":"Isar Nejadgholi|Svetlana Kiritchenko","title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse"},{"content":{"abstract":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.","authors":["Hala Al Kuwatly","Maximilian Wich","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics","tldr":"Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. O...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.57","presentation_id":"38939538","rocketchat_channel":"paper-woah4-57","speakers":"Hala Al Kuwatly|Maximilian Wich|Georg Groh","title":"Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics"},{"content":{"abstract":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately, machine learning is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in classification performance or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias \u2014 a form of bias that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the annotation behavior from annotators. To do so, we build a graph based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a data set. The proposed method and collected insights can contribute to developing fairer and more reliable hate speech classification models.","authors":["Maximilian Wich","Hala Al Kuwatly","Georg Groh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Annotator Bias with a Graph-Based Approach","tldr":"A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.58","presentation_id":"38939539","rocketchat_channel":"paper-woah4-58","speakers":"Maximilian Wich|Hala Al Kuwatly|Georg Groh","title":"Investigating Annotator Bias with a Graph-Based Approach"},{"content":{"abstract":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research communities in tackling online abuse. We report on the discussions that took place, and present an analysis of four key issues which emerged: Problems in tackling online abuse, Solutions, Meta concerns and the Ecosystem of content moderation and research. We argue there is a pressing need for NLP research communities to engage with human rights perspectives, and identify four key ways in which NLP research into online abuse could immediately be enhanced to create better and more ethical solutions.","authors":["Vinodkumar Prabhakaran","Zeerak Waseem","Seyi Akiwowo","Bertie Vidgen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.alw-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020","tldr":"In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research c...","track":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW"},"id":"WS-17.2020.alw-1.1","presentation_id":"","rocketchat_channel":"paper-woah4-1","speakers":"Vinodkumar Prabhakaran|Zeerak Waseem|Seyi Akiwowo|Bertie Vidgen","title":"Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020"}],"prerecorded_talks":[{"presentation_id":"38939540","speakers":"Andr\u00e9 Brock","title":"Keynote1"},{"presentation_id":"38939541","speakers":"Alex Hanna & Maliha Ahmed","title":"\"100,000 Lines in the Sand\": Agency and Policy Feedback in Content Moderation"},{"presentation_id":"38939542","speakers":"Maria Y. Rodriguez","title":"The Root Of Algorithmic Bias And How To Deal With It"}],"rocketchat_channel":"workshop-woah4","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 13:10:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Opening Remarks","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:55:00 GMT","hosts":"Bertie Vidgen","link":"","session_name":"<b>Keynote</b><br> TBD <i>- Andr\u00e9 Brock</i> <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 13:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:40:00 GMT","hosts":"Bertie Vidgen","link":"","session_name":"<b>Keynote</b><br>\"100,000 Lines in the Sand\": Agency and Policy Feedback in Content Moderation <i>- Alex Hanna and Maliha Ahmed</i> <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 13:55:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:45:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 14:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:30:00 GMT","hosts":"Bertie Vidgen","link":"","session_name":"<b>Keynote</b><br>\"The Root Of Algorithmic Bias And How To Deal With It\" <i>- Maria Rodriguez</i> <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 14:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:40:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 15:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:40:00 GMT","hosts":"Bertie Vidgen","link":"","session_name":"Keynote Panel: Maliha Ahmed, Andr\u00e9 Brock, Alex Hanna and Maria Rodriguez <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 15:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 16:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Paper Q&A Panels I (3 parallel sessions)","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Session 1, Panel I: Methods for classifying online abuse <br> <a href=\"https://zoom.us\">Zoom link 1</a> <br><br>\n\u2022 A Novel Methodology for Developing Automatic Harassment Classifiers for Twitter (Ishaan Arora, Julia Guo, Sarah Ita Levitan, Susan McGregor and Julia Hirschberg)<br>\n\u2022 Using Transfer-based Language Models to Detect Hateful and Offensive Language Online (Vebj\u00f8rn Isaksen and Bj\u00f6rn Gamb\u00e4ck)<br>\n\u2022 Fine-tuning BERT for multi-domain and multi-label incivil language detection (Kadir Bulut Ozler, Kate Kenski, Steve Rains, Yotam Shmargad, Kevin Coe and Steven Bethard)<br>\n\u2022 HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language (Anna Koufakou, Endang Wahyu Pamungkas, Valerio Basile and Viviana Patti)<br>\n\u2022 Abusive Language Detection using Syntactic Dependency Graphs (Kanika Narang and Chris Brew)","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Session 1, Panel II: Biases in datasets for abuse<br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br>\n\u2022 Impact of politically biased data on hate speech classification (Maximilian Wich, Jan Bauer and Georg Groh)<br>\n\u2022 Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics (Hala Al Kuwatly, Maximilian Wich and Georg Groh)<br>\n\u2022 Investigating Annotator Bias with a Graph-Based Approach (Maximilian Wich, Hala Al Kuwatly and Georg Groh)<br>\n\u2022 Reducing Unintended Identity Bias in Russian Hate Speech Detection (Nadezhda Zueva, Madina Kabirova and Pavel Kalaidin)<br>\n\u2022 Investigating Sampling Bias in Abusive Language Detection (Dante Razo and Sandra K\u00fcbler)<br>\n\u2022 Is your toxicity my toxicity? Understanding the influence of rater identity on perceptions of toxicity (Ian Kivlichan, Olivia Redfield, Rachel Rosen, Raquel Saxe, Nitesh Goyal and Lucy Vasserman)","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Session 1, Panel III: Technical challenges in classifying online abuse<br> <a href=\"https://zoom.us\">Zoom link 3</a> <br><br>\n\u2022 Attending the Emotions to Detect Online Abusive Language (Niloofar Safi Samghabadi, Afsheen Hatami, Mahsa Shafaei, Sudipta Kar and Thamar Solorio)<br>\n\u2022 Enhancing the Identification of Cyberbullying through Participant Roles (Gathika Rathnayake, Thushari Atapattu, Mahen Herath, Georgia Zhang and Katrina Falkner)<br>\n\u2022 Developing a New Classifier for Automated Identification of Incivility in Social Media (Sam Davidson, Qiusi Sun and Magdalena Wojcieszak)<br>\n\u2022 [Findings] Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection (Michele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli and Serena Villata)<br>\n\u2022 Countering hate on social media: Large scale classification of hate and counter speech (Joshua Garland, Keyan Ghazi-Zahedi, Jean-Gabriel Young, Laurent H\u00e9bertDufresne and Mirta Galesic)","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 17:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Paper Q&A Panels II (3 parallel sessions)","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Session 2, Panel IV: Ways of tackling online abuse<br> <a href=\"https://zoom.us\">Zoom link 1</a><br><br>\n\u2022 Moderating Our (Dis)Content: Renewing the Regulatory Approach (Claire Pershan)<br>\n\u2022 Investigating takedowns of misogynistic abuse on Twitter: a story of iterative failures and partial successes (Nicolas Suzor and Rosalie Gillett)<br>\n\u2022 Six Attributes of Unhealthy Conversations (Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon and Jeffrey Sorensen)<br>\n\u2022 Free Expression By Design: Improving In-Platform Features & Third-Party Tools to Tackle Online Abuse (Viktorya Vilk, Elodie Vialle and Matt Bailey)<br>\n\u2022 A Unified Taxonomy of Harmful Content (Michele Banko, Brendon MacKeen and Laurie Ray)<br>","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:45:00 GMT","hosts":"Zeerak Waseem","link":"","session_name":"Session 2, Panel V: New datasets for abuse<br> <a href=\"https://zoom.us\">Zoom link 2</a> <br><br>\n\u2022 Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage (Jana Kurrek, Haji Mohammad Saleem and Derek Ruths)<br>\n\u2022 In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets (Kosisochukwu Madukwe, Xiaoying Gao and Bing Xue)<br>\n\u2022 Detecting East Asian Prejudice on Social Media (Bertie Vidgen, Scott Hale, Ella Guest, Helen Margetts, David Broniatowski, Zeerak Waseem, Austin Botelho, Matthew Hall and Rebekah Tromble)<br>\n\u2022 On Cross-Dataset Generalization in Automatic Detection of Online Abuse Isar Nejadgholi and Svetlana Kiritchenko<br>\n\u2022 [Findings] A little goes a long way: Improving toxic language classification despite data scarcity Mika Juuti, Tommi Gr\u00f6ndahl, Adrian Flanagan and N. Asokan","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 18:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:20:00 GMT","hosts":"Vinodkumar Prabhakaran","link":"","session_name":"Online Abuse and Human Rights: Report on WOAH RightsCon Satellite Session<br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:30:00 GMT","hosts":"Vinodkumar Prabhakaran","link":"","session_name":"Closing Remarks <br> <a href=\"https://zoom.us\">Zoom link 1</a>","start_time":"Fri, 20 Nov 2020 19:20:00 GMT"}],"title":"The Fourth Workshop on Online Abuse and Harms (WOAH) a.k.a. ALW","website":"https://www.workshopononlineabuse.com/home","zoom_links":["https://zoom.us","https://zoom.us","https://zoom.us"]},{"abstract":"This workshop focuses on NLP for Social Sciences, interdisciplinary work in CSS, and integrating CSS with current trends in NLP.","blocks":[{"end_time":"Fri, 20 Nov 2020 22:35:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"}],"id":"WS-18","livestream":null,"organizers":"Svitlana Volkova, David Jurgens, David Bamman, Dirk Hovy and Brendan T. O'Connor","papers":[{"content":{"abstract":"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.","authors":["Jonathan Dunn","Tom Coupe","Benjamin Adams"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Measuring Linguistic Diversity During COVID-19","tldr":"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulti...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.13","presentation_id":"38940618","rocketchat_channel":"paper-nlpcss-13","speakers":"Jonathan Dunn|Tom Coupe|Benjamin Adams","title":"Measuring Linguistic Diversity During COVID-19"},{"content":{"abstract":"","authors":["David DeFranza","Arul Mishra","Himanshu Mishra"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How Language Influences Attitudes Toward Brands","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.15","presentation_id":"38940628","rocketchat_channel":"paper-nlpcss-15","speakers":"David DeFranza|Arul Mishra|Himanshu Mishra","title":"How Language Influences Attitudes Toward Brands"},{"content":{"abstract":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning-based text classification models for automatic coding in the area of psycho-social online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning-based classifiers against human coders.","authors":["Philipp Grandeit","Carolyn Haberkern","Maximiliane Lang","Jens Albrecht","Robert Lehmann"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling","tldr":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need t...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.17","presentation_id":"38940609","rocketchat_channel":"paper-nlpcss-17","speakers":"Philipp Grandeit|Carolyn Haberkern|Maximiliane Lang|Jens Albrecht|Robert Lehmann","title":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling"},{"content":{"abstract":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.","authors":["Nico Blokker","Erenay Dayanik","Gabriella Lapesa","Sebastian Pad\u00f3"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Swimming with the Tide? Positional Claim Detection across Political Text Types","tldr":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party posit...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.19","presentation_id":"38940616","rocketchat_channel":"paper-nlpcss-19","speakers":"Nico Blokker|Erenay Dayanik|Gabriella Lapesa|Sebastian Pad\u00f3","title":"Swimming with the Tide? Positional Claim Detection across Political Text Types"},{"content":{"abstract":"Individuals recovering from substance use often seek social support (emotional and informational) on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums is that certain posts (some of which may be support seeking) receive no comments. In this work, we use data from two Reddit substance recovery forums: /r/Leaves and /r/OpiatesRecovery, to determine the relationship between the social supports expressed in the titles of posts and the number of comments they receive. We show that the types of social support expressed in post titles that elicit comments vary from one substance use recovery forum to the other.","authors":["Anietie Andy","Sharath Chandra Guntuku"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Does Social Support (Expressed in Post Titles) Elicit Comments in Online Substance Use Recovery Forums?","tldr":"Individuals recovering from substance use often seek social support (emotional and informational) on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.20","presentation_id":"38940623","rocketchat_channel":"paper-nlpcss-20","speakers":"Anietie Andy|Sharath Chandra Guntuku","title":"Does Social Support (Expressed in Post Titles) Elicit Comments in Online Substance Use Recovery Forums?"},{"content":{"abstract":"With the world on a lockdown due to the COVID-19 pandemic, this paper studies emotions expressed on Twitter. Using a combined strategy of time series analysis of emotions augmented by tweet topics, this study provides an insight into emotion transitions during the pandemic. After tweets are annotated with dominant emotions and topics, a time-series emotion analysis is used to identify disgust and anger as the most commonly identified emotions. Through longitudinal analysis of each user, we construct emotion transition graphs, observing key transitions between disgust and anger, and self-transitions within anger and disgust emotional states. Observing user patterns through clustering of user longitudinal analyses reveals emotional transitions fall into four main clusters: (1) erratic motion over short period of time, (2) disgust -> anger, (3) optimism -> joy. (4) erratic motion over a prolonged period. Finally, we propose a method for predicting users subsequent topic, and by consequence their emotions, through constructing an Emotion Topic Hidden Markov Model, augmenting emotion transition states with topic information. Results suggests that the predictions fare better than baselines, spurring directions of predicting emotional states based on Twitter posts.","authors":["Hui Xian Lynnette Ng","Roy Ka-Wei Lee","Md Rabiul Awal"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"I miss you babe: Analyzing Emotion Dynamics During COVID-19 Pandemic","tldr":"With the world on a lockdown due to the COVID-19 pandemic, this paper studies emotions expressed on Twitter. Using a combined strategy of time series analysis of emotions augmented by tweet topics, this study provides an insight into emotion transiti...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.21","presentation_id":"38940603","rocketchat_channel":"paper-nlpcss-21","speakers":"Hui Xian Lynnette Ng|Roy Ka-Wei Lee|Md Rabiul Awal","title":"I miss you babe: Analyzing Emotion Dynamics During COVID-19 Pandemic"},{"content":{"abstract":"Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easily population-level mental health data can be integrated into health and policy decision-making. Here, we demonstrate that natural language processing applied to publicly-available social media data can provide real-time estimates of psychological distress in the population (specifically, English-speaking Twitter users in the US). We examine population-level changes in linguistic correlates of mental health symptoms in response to the COVID-19 pandemic and to the killing of George Floyd. As a case study, we focus on social media data from healthcare providers, compared to a control sample. Our results provide a concrete demonstration of how the tools of computational social science can be applied to provide real-time or near-real-time insight into the impact of public events on mental health.","authors":["Alex Fine","Patrick Crutchley","Jenny Blase","Joshua Carroll","Glen Coppersmith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using NLP applied to social media data","tldr":"Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easi...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.22","presentation_id":"38940624","rocketchat_channel":"paper-nlpcss-22","speakers":"Alex Fine|Patrick Crutchley|Jenny Blase|Joshua Carroll|Glen Coppersmith","title":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using NLP applied to social media data"},{"content":{"abstract":"","authors":["Michael Yeomans","Alison Wood Brooks"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Topic preference detection: A novel approach to understand perspective taking in conversation","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.23","presentation_id":"38940626","rocketchat_channel":"paper-nlpcss-23","speakers":"Michael Yeomans|Alison Wood Brooks","title":"Topic preference detection: A novel approach to understand perspective taking in conversation"},{"content":{"abstract":"Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already being used to create fake news. They can also be exploited to generate biased news, which can then be used to attack news aggregators to change their reader\u2019s behavior and influence their bias. In this paper, we use a threat model to demonstrate that the publicly available language models can reliably generate biased news content based on an input original news. We also show that a large number of high-quality biased news articles can be generated using controllable text generation. A subjective evaluation with 80 participants demonstrated that the generated biased news is generally fluent, and a bias evaluation with 24 participants demonstrated that the bias (left or right) is usually evident in the generated articles and can be easily identified.","authors":["Saurabh Gupta","Hong Huy Nguyen","Junichi Yamagishi","Isao Echizen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Viable Threat on News Reading: Generating Biased News Using Natural Language Models","tldr":"Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.26","presentation_id":"38940610","rocketchat_channel":"paper-nlpcss-26","speakers":"Saurabh Gupta|Hong Huy Nguyen|Junichi Yamagishi|Isao Echizen","title":"Viable Threat on News Reading: Generating Biased News Using Natural Language Models"},{"content":{"abstract":"","authors":["Sandeep Soni","Lauren Klein","Jacob Eisenstein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Lexical Semantic Leadership Network of Nineteenth CenturyAbolitionist Newspapers","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.28","presentation_id":"38940625","rocketchat_channel":"paper-nlpcss-28","speakers":"Sandeep Soni|Lauren Klein|Jacob Eisenstein","title":"A Lexical Semantic Leadership Network of Nineteenth CenturyAbolitionist Newspapers"},{"content":{"abstract":"Each year, thousands of roughly 150-page parole hearing transcripts in California go unread because legal experts lack the time to review them. Yet, reviewing transcripts is the only means of public oversight in the parole process. To assist reviewers, we present a simple unsupervised technique for using language models (LMs) to identify procedural anomalies in long-form legal text. Our technique highlights unusual passages that suggest further review could be necessary. We utilize a contrastive perplexity score to identify passages, defined as the scaled difference between its perplexities from two LMs, one fine-tuned on the target (parole) domain, and another pre-trained on out-of-domain text to normalize for grammatical or syntactic anomalies. We present quantitative analysis of the results and note that our method has identified some important cases for review. We are also excited about potential applications in unsupervised anomaly detection, and present a brief analysis of results for detecting fake TripAdvisor reviews.","authors":["Graham Todd","Catalin Voss","Jenny Hong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Anomaly Detection in Parole Hearings using Language Models","tldr":"Each year, thousands of roughly 150-page parole hearing transcripts in California go unread because legal experts lack the time to review them. Yet, reviewing transcripts is the only means of public oversight in the parole process. To assist reviewer...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.29","presentation_id":"38940611","rocketchat_channel":"paper-nlpcss-29","speakers":"Graham Todd|Catalin Voss|Jenny Hong","title":"Unsupervised Anomaly Detection in Parole Hearings using Language Models"},{"content":{"abstract":"Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of machine learning models to predict worry on Twitter. While this task has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate task given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry \u2013 normative and pathological \u2013 as stated in psychology literature. In addition, we show that existing emotion detection techniques underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.","authors":["Reyha Verma","Christian von der Weth","Jithin Vachery","Mohan Kankanhalli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Worry in Twitter: Beyond Emotion Analysis","tldr":"Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e....","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.32","presentation_id":"38940602","rocketchat_channel":"paper-nlpcss-32","speakers":"Reyha Verma|Christian von der Weth|Jithin Vachery|Mohan Kankanhalli","title":"Identifying Worry in Twitter: Beyond Emotion Analysis"},{"content":{"abstract":"We present experiments to structure job ads into text zones and classify them into pro- fessions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings on the benefits of contextualized embeddings and the potential of multi-task models for this purpose. With contextualized in-domain embeddings in BiLSTM-CRF models, we reach an accuracy of 91% for token-level text zoning and outperform previous approaches. A multi-tasking BERT model performs well for our classification tasks. We further compare transfer approaches for our multilingual data.","authors":["Ann-Sophie Gnehm","Simon Clematide"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Text Zoning and Classification for Job Advertisements in German, French and English","tldr":"We present experiments to structure job ads into text zones and classify them into pro- fessions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings o...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.33","presentation_id":"38940604","rocketchat_channel":"paper-nlpcss-33","speakers":"Ann-Sophie Gnehm|Simon Clematide","title":"Text Zoning and Classification for Job Advertisements in German, French and English"},{"content":{"abstract":"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.","authors":["Katja Geertruida Schmahl","Tom Julian Viering","Stavros Makrodimitris","Arman Naseri Jahfari","David Tax","Marco Loog"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings","tldr":"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.34","presentation_id":"38940605","rocketchat_channel":"paper-nlpcss-34","speakers":"Katja Geertruida Schmahl|Tom Julian Viering|Stavros Makrodimitris|Arman Naseri Jahfari|David Tax|Marco Loog","title":"Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings"},{"content":{"abstract":"It has been shown that anonymity affects various aspects of online communications such as message credibility, the trust among communicators, and the participants\u2019 accountability and reputation. Anonymity influences social interactions in online communities in these many ways, which can lead to influences on opinion change and the persuasiveness of a message. Prior studies also suggest that the effect of anonymity can vary in different online communication contexts and online communities. In this study, we focus on Wikipedia Articles for Deletion (AfD) discussions as an example of online collaborative communities to study the relationship between anonymity and persuasiveness in this context. We find that in Wikipedia AfD discussions, more identifiable users tend to be more persuasive. The higher persuasiveness can be related to multiple aspects, including linguistic features of the comments, the user\u2019s motivation to participate, persuasive skills the user learns over time, and the user\u2019s identity and credibility established in the community through participation.","authors":["Yimin Xiao","Lu Xiao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Effects of Anonymity on Comment Persuasiveness in Wikipedia Articles for Deletion Discussions","tldr":"It has been shown that anonymity affects various aspects of online communications such as message credibility, the trust among communicators, and the participants\u2019 accountability and reputation. Anonymity influences social interactions in online comm...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.36","presentation_id":"38940619","rocketchat_channel":"paper-nlpcss-36","speakers":"Yimin Xiao|Lu Xiao","title":"Effects of Anonymity on Comment Persuasiveness in Wikipedia Articles for Deletion Discussions"},{"content":{"abstract":"Methods and applications are inextricably linked in science, and in particular in the domain of text-as-data. In this paper, we examine one such text-as-data application, an established economic index that measures economic policy uncertainty from keyword occurrences in news. This index, which is shown to correlate with firm investment, employment, and excess market returns, has had substantive impact in both the private sector and academia. Yet, as we revisit and extend the original authors\u2019 annotations and text measurements we find interesting text-as-data methodological research questions: (1) Are annotator disagreements a reflection of ambiguity in language? (2) Do alternative text measurements correlate with one another and with measures of external predictive validity? We find for this application (1) some annotator disagreements of economic policy uncertainty can be attributed to ambiguity in language, and (2) switching measurements from keyword-matching to supervised machine learning classifiers results in low correlation, a concerning implication for the validity of the index.","authors":["Katherine Keith","Christoph Teichmann","Brendan O\u2019Connor","Edgar Meij"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Uncertainty over Uncertainty: Investigating the Assumptions, Annotations, and Text Measurements of Economic Policy Uncertainty","tldr":"Methods and applications are inextricably linked in science, and in particular in the domain of text-as-data. In this paper, we examine one such text-as-data application, an established economic index that measures economic policy uncertainty from ke...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.37","presentation_id":"38940620","rocketchat_channel":"paper-nlpcss-37","speakers":"Katherine Keith|Christoph Teichmann|Brendan O\u2019Connor|Edgar Meij","title":"Uncertainty over Uncertainty: Investigating the Assumptions, Annotations, and Text Measurements of Economic Policy Uncertainty"},{"content":{"abstract":"We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the \u2018raw\u2019 scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of online abuse. We examine two widely used classifiers (created by Perspective and Davidson et al.) on a dataset of tweets directed against candidates in the UK\u2019s 2017 general election. A Bayesian approach is presented to recalibrate the raw scores from the classifiers, using probabilistic programming and newly annotated data. We argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.","authors":["Bertie Vidgen","Scott Hale","Sam Staton","Tom Melham","Helen Margetts","Ohad Kammar","Marcin Szymczak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recalibrating classifiers for interpretable abusive content detection","tldr":"We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the \u2018raw\u2019 scores are used) align poorly with human evaluations. This limits their use for under...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.38","presentation_id":"38940621","rocketchat_channel":"paper-nlpcss-38","speakers":"Bertie Vidgen|Scott Hale|Sam Staton|Tom Melham|Helen Margetts|Ohad Kammar|Marcin Szymczak","title":"Recalibrating classifiers for interpretable abusive content detection"},{"content":{"abstract":"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients\u2019 life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss\u2019 Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures (macro) of 95% averaged across topics, showing that this task can be automatically solved.","authors":["Angelika Maier","Philipp Cimiano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Predicting independent living outcomes from written reports of social workers","tldr":"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients\u2019 life. To support this tas...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.43","presentation_id":"38940617","rocketchat_channel":"paper-nlpcss-43","speakers":"Angelika Maier|Philipp Cimiano","title":"Predicting independent living outcomes from written reports of social workers"},{"content":{"abstract":"Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such a form of unfair news coverage can be exposed. This paper addresses the automatic detection of bias, but it goes one step further in that it explores how political bias and unfairness are manifested linguistically. We utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com to develop a neural model for bias assessment. Analyzing the model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.","authors":["Wei-Fan Chen","Khalid Al Khatib","Henning Wachsmuth","Benno Stein"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity","tldr":"Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.44","presentation_id":"38940612","rocketchat_channel":"paper-nlpcss-44","speakers":"Wei-Fan Chen|Khalid Al Khatib|Henning Wachsmuth|Benno Stein","title":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity"},{"content":{"abstract":"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.","authors":["Sarang Gupta","Kumari Nishu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model","tldr":"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-pol...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.45","presentation_id":"38940613","rocketchat_channel":"paper-nlpcss-45","speakers":"Sarang Gupta|Kumari Nishu","title":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model"},{"content":{"abstract":"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from adult learners to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages (English, French, Italian, Spanish), extracting data from an internet forum where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker (but with different addressees), which makes it more reliable.","authors":["Aleksandrs Berdicevskis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Foreigner-directed speech is simpler than native-directed: Evidence from social media","tldr":"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addre...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.47","presentation_id":"38940614","rocketchat_channel":"paper-nlpcss-47","speakers":"Aleksandrs Berdicevskis","title":"Foreigner-directed speech is simpler than native-directed: Evidence from social media"},{"content":{"abstract":"Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity (resulting in one embedding representing several distinct and unrelated people) and unlinked references (leading to several distinct embeddings which represent the same person). In this paper, we show that using named entity recognition and heuristic name linking steps before training a diachronic embedding model leads to more accurate representations of references to people, as compared to the token-only baseline. In large news corpus of articles from The Guardian, we provide examples of several types of analysis that can be performed using these new embeddings. Further, we show that real world events and context changes can be detected using our proposed model.","authors":["Felix Hennig","Steven Wilson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Diachronic Embeddings for People in the News","tldr":"Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity (resulting in one embedding representing several ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.49","presentation_id":"38940606","rocketchat_channel":"paper-nlpcss-49","speakers":"Felix Hennig|Steven Wilson","title":"Diachronic Embeddings for People in the News"},{"content":{"abstract":"In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. We apply natural language processing (NLP) techniques to more than 3 million Tweets collected from 20,000 Twitter users. We find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans\u2019 issues significantly more frequently than their male counterparts. By contrast, male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. Our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population, and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.","authors":["Kacie Kelly","Alex Fine","Glen Coppersmith"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Social media data as a lens onto care-seeking behavior among women veterans of the US armed forces","tldr":"In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.50","presentation_id":"38940615","rocketchat_channel":"paper-nlpcss-50","speakers":"Kacie Kelly|Alex Fine|Glen Coppersmith","title":"Social media data as a lens onto care-seeking behavior among women veterans of the US armed forces"},{"content":{"abstract":"The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on social media.We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including social mobility and unemployment rate.","authors":["Mohammadzaman Zamani","H. Andrew Schwartz","Johannes Eichstaedt","Sharath Chandra Guntuku","Adithya Virinchipuram Ganesan","Sean Clouston","Salvatore Giorgi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling","tldr":"The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.51","presentation_id":"38940607","rocketchat_channel":"paper-nlpcss-51","speakers":"Mohammadzaman Zamani|H. Andrew Schwartz|Johannes Eichstaedt|Sharath Chandra Guntuku|Adithya Virinchipuram Ganesan|Sean Clouston|Salvatore Giorgi","title":"Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling"},{"content":{"abstract":"Emoji are widely used to express emotions and concepts on social media, and prior work has shown that users\u2019 choice of emoji reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts made by users, and this view has provided important insights into phenomena such as emotional expression and self-representation. In addition to making posts, however, social media platforms like Twitter allow for users to provide a short bio, which is an opportunity to briefly describe their account as a whole. In this work, we focus on the use of emoji in these bio statements. We explore the ways in which users include emoji in these self-descriptions, finding different patterns than those observed around emoji usage in tweets. We examine the relationships between emoji used in bios and the content of users\u2019 tweets, showing that the topics and even the average sentiment of tweets varies for users with different emoji in their bios. Lastly, we confirm that homophily effects exist with respect to the types of emoji that are included in bios of users and their followers.","authors":["Jinhang Li","Giorgos Longinos","Steven Wilson","Walid Magdy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emoji and Self-Identity in Twitter Bios","tldr":"Emoji are widely used to express emotions and concepts on social media, and prior work has shown that users\u2019 choice of emoji reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts ...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.52","presentation_id":"38940622","rocketchat_channel":"paper-nlpcss-52","speakers":"Jinhang Li|Giorgos Longinos|Steven Wilson|Walid Magdy","title":"Emoji and Self-Identity in Twitter Bios"},{"content":{"abstract":"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the \u201cgenderedness\u201d of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work\u2019s creator correlates with the types of tropes that they use.","authors":["Dhruvil Gala","Mohammad Omar Khursheed","Hannah Lerner","Brendan O\u2019Connor","Mohit Iyyer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcss-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Analyzing Gender Bias within Narrative Tropes","tldr":"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias wit...","track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.53","presentation_id":"38940608","rocketchat_channel":"paper-nlpcss-53","speakers":"Dhruvil Gala|Mohammad Omar Khursheed|Hannah Lerner|Brendan O\u2019Connor|Mohit Iyyer","title":"Analyzing Gender Bias within Narrative Tropes"},{"content":{"abstract":"","authors":["Kunal Khadilkar","Ashiqur KhudaBukhsh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Unfair Affinity Toward Fairness: Characterizing 70 Years of Social Biases in B^Hollywood","tldr":null,"track":"NLP and Computational Social Science (NLP+CSS)"},"id":"WS-18.57","presentation_id":"38940627","rocketchat_channel":"paper-nlpcss-57","speakers":"Kunal Khadilkar|Ashiqur KhudaBukhsh","title":"An Unfair Affinity Toward Fairness: Characterizing 70 Years of Social Biases in B^Hollywood"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-nlpcss","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 15:15:00 GMT","hosts":"David Jurgens","link":"","session_name":"Opening Remarks","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"David Jurgens","link":"","session_name":"Invited Speaker: Dong Nguyen, Assistant Professor, Information and Computing Sciences, Utrecht University \"When NLP Meets Language Variation\"","start_time":"Fri, 20 Nov 2020 15:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"Dirk Hovy","link":"","session_name":"Posters/papers session 1","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:15:00 GMT","hosts":"Dirk Hovy","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:00:00 GMT","hosts":"David Bamman","link":"","session_name":"Invited Speaker: Elizabeth E. Bruch, Associate Professor in Sociology and Complex Systems, University of Michigan \"How (and Why) Online Dating Experiences Differ across American Cities\"","start_time":"Fri, 20 Nov 2020 17:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"David Bamman","link":"","session_name":"Lunch + Birds of a feather","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"Brendan O'Connor","link":"","session_name":"Invited Speaker: Jesse Shapiro, Eastman Professor of Political Economy, Brown University \"Measuring Group Differences in High-Dimensional Choices: Method and Application to Congressional Speech\"","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:00:00 GMT","hosts":"Brendan O'Connor","link":"","session_name":"Posters/papers session 2","start_time":"Fri, 20 Nov 2020 19:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:45:00 GMT","hosts":"Svitlana Volkova","link":"","session_name":"Invited Speaker: Diyi Yang, Assistant Professor, Interactive Computing, Georgia Institute of Technology\n\"Persuasion, Bias, and Choice? Building Socially-aware Language Technologies\"","start_time":"Fri, 20 Nov 2020 21:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 22:30:00 GMT","hosts":"Svitlana Volkova","link":"","session_name":"Panel \"CSS Research: An Industry Perspective\"","start_time":"Fri, 20 Nov 2020 21:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 22:35:00 GMT","hosts":"David Jurgens","link":"","session_name":"Closing Remarks","start_time":"Fri, 20 Nov 2020 22:30:00 GMT"}],"title":"NLP and Computational Social Science (NLP+CSS)","website":"https://sites.google.com/site/nlpandcss/nlp-css-at-emnlp-2020","zoom_links":["https://zoom.us"]},{"abstract":"an interdisciplinary forum for researchers interested in automated processing of health documents","blocks":[{"end_time":"Fri, 20 Nov 2020 17:30:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"}],"id":"WS-19","livestream":null,"organizers":"Alberto Lavelli, James Pustejovsky, Eben Holderness, Antonio Jimeno Yepes, Anne-Lyse Minard and Fabio Rinaldi","papers":[{"content":{"abstract":"Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services\u2019 interoperability within healthcare national information networks. Health and medical science are constantly evolving causing requirements to advance the terminologies editions. In this paper, we present our evaluation work of the latest machine translation techniques addressing medical terminologies. Experiments have been conducted leveraging selected statistical and neural machine translation methods. The devised procedure is tested on a validated sample of ICD-11 and ICF terminologies from English to French with promising results.","authors":["Konstantinos Skianis","Yann Briand","Florent Desgrippes"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluation of Machine Translation Methods applied to Medical Terminologies","tldr":"Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services\u2019 interoperability within healthcare national information networks. Health and medical science are constantly evolving causi...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.12","presentation_id":"38940042","rocketchat_channel":"paper-louhi2020-12","speakers":"Konstantinos Skianis|Yann Briand|Florent Desgrippes","title":"Evaluation of Machine Translation Methods applied to Medical Terminologies"},{"content":{"abstract":"We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIE-R, a bespoke rule-based system, and two neural network models, EdIE-BiLSTM and EdIE-BERT, both multi-task learning models with a BiLSTM and BERT encoder respectively. We compare our models both on an in-sample and an out-of-sample dataset containing mentions of stroke findings and draw on our error analysis to suggest improvements for effective annotation when building clinical NLP models for a new domain. Our analysis finds that our rule-based system outperforms the neural models on both datasets and seems to generalise to the out-of-sample dataset. On the other hand, the neural models do not generalise negation to the out-of-sample dataset, despite metrics on the in-sample dataset suggesting otherwise.","authors":["Andreas Grivas","Beatrice Alex","Claire Grover","Richard Tobin","William Whiteley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports","tldr":"We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIE-R, a bespoke rule-based system, and two neural network models, EdIE-B...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.13","presentation_id":"38940043","rocketchat_channel":"paper-louhi2020-13","speakers":"Andreas Grivas|Beatrice Alex|Claire Grover|Richard Tobin|William Whiteley","title":"Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports"},{"content":{"abstract":"Biomedical event extraction from natural text is a challenging task as it searches for complex and often nested structures describing specific relationships between multiple molecular entities, such as genes, proteins, or cellular components. It usually is implemented by a complex pipeline of individual tools to solve the different relation extraction subtasks. We present an alternative approach where the detection of relationships between entities is described uniformly as questions, which are iteratively answered by a question answering (QA) system based on the domain-specific language model SciBERT. This model outperforms two strong baselines in two biomedical event extraction corpora in a Knowledge Base Population setting, and also achieves competitive performance in BioNLP challenge evaluation settings.","authors":["Xing David Wang","Leon Weber","Ulf Leser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Biomedical Event Extraction as Multi-turn Question Answering","tldr":"Biomedical event extraction from natural text is a challenging task as it searches for complex and often nested structures describing specific relationships between multiple molecular entities, such as genes, proteins, or cellular components. It usua...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.14","presentation_id":"38940044","rocketchat_channel":"paper-louhi2020-14","speakers":"Xing David Wang|Leon Weber|Ulf Leser","title":"Biomedical Event Extraction as Multi-turn Question Answering"},{"content":{"abstract":"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely dis tributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.","authors":["Florian Borchert","Christina Lohr","Luise Modersohn","Thomas Langer","Markus Follmann","Jan Philipp Sachs","Udo Hahn","Matthieu-P. Schapranow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines","tldr":"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (Ger...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.15","presentation_id":"38940045","rocketchat_channel":"paper-louhi2020-15","speakers":"Florian Borchert|Christina Lohr|Luise Modersohn|Thomas Langer|Markus Follmann|Jan Philipp Sachs|Udo Hahn|Matthieu-P. Schapranow","title":"GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines"},{"content":{"abstract":"Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a clinical knowledge base. It is much beyond simple string matching and requires a deep semantic understanding of concept mentions. Recent research approach concept normalization as either text classification or text similarity. The main drawback in existing a) text classification approach is ignoring valuable target concepts information in learning input concept mention representation b) text similarity approach is the need to separately generate target concept embeddings which is time and resource consuming. Our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. First, we learn input concept mention representation using RoBERTa. Second, we find cosine similarity between embeddings of input concept mention and all the target concepts. Here, embeddings of target concepts are randomly initialized and then updated during training. Finally, the target concept with maximum cosine similarity is assigned to the input concept mention. Our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31%.","authors":["Katikapalli Subramanyam Kalyan","Sivanesan Sangeetha"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings","tldr":"Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a clinical knowledge base. It is much beyond simple string matching and requires a deep semantic underst...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.17","presentation_id":"38940046","rocketchat_channel":"paper-louhi2020-17","speakers":"Katikapalli Subramanyam Kalyan|Sivanesan Sangeetha","title":"Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings"},{"content":{"abstract":"The impact of de-identification on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-identification has a limited impact on models for downstream tasks, it remains unclear what the impact is with various levels and forms of de-identification, in particular concerning the trade-off between precision and recall. In this paper, the impact of de-identification is studied on downstream named entity recognition in Swedish clinical text. The results indicate that de-identification models with moderate to high precision lead to similar downstream performance, while low precision has a substantial negative impact. Furthermore, different strategies for concealing sensitive information affect performance to different degrees, ranging from pseudonymisation having a low impact to the removal of entire sentences with sensitive information having a high impact. This study indicates that it is possible to increase the recall of models for identifying sensitive information without negatively affecting the use of de-identified text data for training models for clinical named entity recognition; however, there is ultimately a trade-off between the level of de-identification and the subsequent utility of the data.","authors":["Hanna Berg","Aron Henriksson","Hercules Dalianis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text","tldr":"The impact of de-identification on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-ident...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.2","presentation_id":"38940038","rocketchat_channel":"paper-louhi2020-2","speakers":"Hanna Berg|Aron Henriksson|Hercules Dalianis","title":"The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text"},{"content":{"abstract":"We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relations. The main contributions are in re-defining CONTAINS and OVERLAP relations into CONTAINS, CONTAINS-SUBEVENT, OVERLAP and NOTED-ON. We demonstrate that these refinements lead to substantial gains in learnability for state-of-the-art transformer models as compared to previously reported results on the original THYME corpus. We thus establish a baseline for the automatic extraction of these refined temporal relations. Although our study is done on clinical narrative, we believe it addresses far-reaching challenges that are corpus- and domain- agnostic.","authors":["Kristin Wright-Bettner","Chen Lin","Timothy Miller","Steven Bethard","Dmitriy Dligach","Martha Palmer","James H. Martin","Guergana Savova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Defining and Learning Refined Temporal Relations in the Clinical Narrative","tldr":"We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relati...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.24","presentation_id":"38940047","rocketchat_channel":"paper-louhi2020-24","speakers":"Kristin Wright-Bettner|Chen Lin|Timothy Miller|Steven Bethard|Dmitriy Dligach|Martha Palmer|James H. Martin|Guergana Savova","title":"Defining and Learning Refined Temporal Relations in the Clinical Narrative"},{"content":{"abstract":"Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of entities and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The system performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR.","authors":["Maciej Wiatrak","Juha Iso-Sipila"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text","tldr":"Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.26","presentation_id":"38940048","rocketchat_channel":"paper-louhi2020-26","speakers":"Maciej Wiatrak|Juha Iso-Sipila","title":"Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text"},{"content":{"abstract":"We address the problem of automatic detection of psychiatric disorders from the linguistic content of social media posts. We build a large scale dataset of Reddit posts from users with eight disorders and a control user group. We extract and analyze linguistic characteristics of posts and identify differences between diagnostic groups. We build strong classification models based on deep contextualized word representations and show that they outperform previously applied statistical models with simple linguistic features by large margins. We compare user-level and post-level classification performance, as well as an ensembled multiclass model.","authors":["Zhengping Jiang","Sarah Ita Levitan","Jonathan Zomick","Julia Hirschberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detection of Mental Health from Reddit via Deep Contextualized Representations","tldr":"We address the problem of automatic detection of psychiatric disorders from the linguistic content of social media posts. We build a large scale dataset of Reddit posts from users with eight disorders and a control user group. We extract and analyze ...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.27","presentation_id":"38940049","rocketchat_channel":"paper-louhi2020-27","speakers":"Zhengping Jiang|Sarah Ita Levitan|Jonathan Zomick|Julia Hirschberg","title":"Detection of Mental Health from Reddit via Deep Contextualized Representations"},{"content":{"abstract":"","authors":["Sarah Valentin","Renaud Lancelot","Mathieu Roche"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information retrieval for animal disease surveillance: a pattern-based approach","tldr":null,"track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.28","presentation_id":"38940050","rocketchat_channel":"paper-louhi2020-28","speakers":"Sarah Valentin|Renaud Lancelot|Mathieu Roche","title":"Information retrieval for animal disease surveillance: a pattern-based approach"},{"content":{"abstract":"Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how the human body reacts to different medications. Twitter, a popular social media service, is being considered as an important alternative data source for collecting personal experience information with medications. Identifying personal experience tweets is a challenging classification task in natural language processing. In this study, we utilized three methods based on Facebook\u2019s Robustly Optimized BERT Pretraining Approach (RoBERTa) to predict personal experience tweets related to medication use: the first one combines the pre-trained RoBERTa model with a classifier, the second combines the updated pre-trained RoBERTa model using a corpus of unlabeled tweets with a classifier, and the third combines the RoBERTa model that was trained with our unlabeled tweets from scratch with the classifier too. Our results show that all of these approaches outperform the published methods (Word Embedding + LSTM) in classification performance (p < 0.05), and updating the pre-trained language model with tweets related to medications could even improve the performance further.","authors":["Minghao Zhu","Youzhe Song","Ge Jin","Keyuan Jiang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating","tldr":"Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how t...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.32","presentation_id":"38940051","rocketchat_channel":"paper-louhi2020-32","speakers":"Minghao Zhu|Youzhe Song|Ge Jin|Keyuan Jiang","title":"Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating"},{"content":{"abstract":"Health departments have been deploying text classification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in English and, as a result, a promising direction is to increase coverage and recall by considering documents in additional languages, such as Spanish or Chinese. Training previous systems for more languages, however, would be expensive, as it would require the manual annotation of many documents for each new target language. To address this challenge, we consider cross-lingual learning and train multilingual classifiers using only the annotations for English-language reviews. Recent zero-shot approaches based on pre-trained multi-lingual BERT (mBERT) have been shown to effectively align languages for aspects such as sentiment. Interestingly, we show that those approaches are less effective for capturing the nuances of foodborne illness, our public health application of interest. To improve performance without extra annotations, we create artificial training documents in the target language through machine translation and train mBERT jointly for the source (English) and target language. Furthermore, we show that translating labeled documents to multiple languages leads to additional performance improvements for some target languages. We demonstrate the benefits of our approach through extensive experiments with Yelp restaurant reviews in seven languages. Our classifiers identify foodborne illness complaints in multilingual reviews from the Yelp Challenge dataset, which highlights the potential of our general approach for deployment in health departments.","authors":["Ziyi Liu","Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only","tldr":"Health departments have been deploying text classification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in Engl...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.35","presentation_id":"38940052","rocketchat_channel":"paper-louhi2020-35","speakers":"Ziyi Liu|Giannis Karamanolakis|Daniel Hsu|Luis Gravano","title":"Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only"},{"content":{"abstract":"The automatic mapping of Adverse Drug Reaction (ADR) reports from user-generated content to concepts in a controlled medical vocabulary provides valuable insights for monitoring public health. While state-of-the-art deep learning-based sequence classification techniques achieve impressive performance for medical concepts with large amounts of training data, they show their limit with long-tail concepts that have a low number of training samples. The above hinders their adaptability to the changes of layman\u2019s terminology and the constant emergence of new informal medical terms. Our objective in this paper is to tackle the problem of normalizing long-tail ADR mentions in user-generated content. In this paper, we exploit the implicit semantics of rare ADRs for which we have few training samples, in order to detect the most similar class for the given ADR. The evaluation results demonstrate that our proposed approach addresses the limitations of the existing techniques when the amount of training data is limited.","authors":["Emmanouil Manousogiannis","Sepideh Mesbah","Alessandro Bozzon","Robert-Jan Sips","Zoltan Szlanik","Selene Baez"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Normalization of Long-tail Adverse Drug Reactions in Social Media","tldr":"The automatic mapping of Adverse Drug Reaction (ADR) reports from user-generated content to concepts in a controlled medical vocabulary provides valuable insights for monitoring public health. While state-of-the-art deep learning-based sequence class...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.4","presentation_id":"38940039","rocketchat_channel":"paper-louhi2020-4","speakers":"Emmanouil Manousogiannis|Sepideh Mesbah|Alessandro Bozzon|Robert-Jan Sips|Zoltan Szlanik|Selene Baez","title":"Normalization of Long-tail Adverse Drug Reactions in Social Media"},{"content":{"abstract":"Healthcare systems have increased patients\u2019 exposure to their own health materials to enhance patients\u2019 health levels, but this has been impeded by patients\u2019 lack of understanding of their health material. We address potential barriers to their comprehension by developing a context-aware text simplification system for health material. Given the scarcity of annotated parallel corpora in healthcare domains, we design our system to be independent of a parallel corpus, complementing the availability of data-driven neural methods when such corpora are available. Our system compensates for the lack of direct supervision using a biomedical lexical database: Unified Medical Language System (UMLS). Compared to a competitive prior approach that uses a tool for identifying biomedical concepts and a consumer-directed vocabulary list, we empirically show the enhanced accuracy of our system due to improved handling of ambiguous terms. We also show the enhanced accuracy of our system over directly-supervised neural methods in this low-resource setting. Finally, we show the direct impact of our system on laypeople\u2019s comprehension of health material via a human subjects\u2019 study (n=160).","authors":["Tarek Sakakini","Jong Yoon Lee","Aditya Duri","Renato F.L. Azevedo","Victor Sadauskas","Kuangxiao Gu","Suma Bhat","Dan Morrow","James Graumlich","Saqib Walayat","Mark Hasegawa-Johnson","Thomas Huang","Ann Willemsen-Dunlap","Donald Halpin"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains","tldr":"Healthcare systems have increased patients\u2019 exposure to their own health materials to enhance patients\u2019 health levels, but this has been impeded by patients\u2019 lack of understanding of their health material. We address potential barriers to their compr...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.44","presentation_id":"38940053","rocketchat_channel":"paper-louhi2020-44","speakers":"Tarek Sakakini|Jong Yoon Lee|Aditya Duri|Renato F.L. Azevedo|Victor Sadauskas|Kuangxiao Gu|Suma Bhat|Dan Morrow|James Graumlich|Saqib Walayat|Mark Hasegawa-Johnson|Thomas Huang|Ann Willemsen-Dunlap|Donald Halpin","title":"Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains"},{"content":{"abstract":"Detecting negation and speculation in language has been a task of considerable interest to the biomedical community, as it is a key component of Information Extraction systems from Biomedical documents. Prior work has individually addressed Negation Detection and Speculation Detection, and both have been addressed in the same way, using 2 stage pipelined approach: Cue Detection followed by Scope Resolution. In this paper, we propose Multitask learning approaches over 2 sets of tasks: Negation Cue Detection & Speculation Cue Detection, and Negation Scope Resolution & Speculation Scope Resolution. We utilise transformer-based architectures like BERT, XLNet and RoBERTa as our core model architecture, and finetune these using the Multitask learning approaches. We show that this Multitask Learning approach outperforms the single task learning approach, and report new state-of-the-art results on Negation and Speculation Scope Resolution on the BioScope Corpus and the SFU Review Corpus.","authors":["Aditya Khandelwal","Benita Kathleen Britto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multitask Learning of Negation and Speculation using Transformers","tldr":"Detecting negation and speculation in language has been a task of considerable interest to the biomedical community, as it is a key component of Information Extraction systems from Biomedical documents. Prior work has individually addressed Negation ...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.5","presentation_id":"38940040","rocketchat_channel":"paper-louhi2020-5","speakers":"Aditya Khandelwal|Benita Kathleen Britto","title":"Multitask Learning of Negation and Speculation using Transformers"},{"content":{"abstract":"In this work we addressed the problem of capturing sequential information contained in longitudinal electronic health records (EHRs). Clinical notes, which is a particular type of EHR data, are a rich source of information and practitioners often develop clever solutions how to maximise the sequential information contained in free-texts. We proposed a systematic methodology for learning from chronological events available in clinical notes. The proposed methodological path signature framework creates a non-parametric hierarchical representation of sequential events of any type and can be used as features for downstream statistical learning tasks. The methodology was developed and externally validated using the largest in the UK secondary care mental health EHR data on a specific task of predicting survival risk of patients diagnosed with Alzheimer\u2019s disease. The signature-based model was compared to a common survival random forest model. Our results showed a 15.4% increase of risk prediction AUC at the time point of 20 months after the first admission to a specialist memory clinic and the signature method outperformed the baseline mixed-effects model by 13.2 %.","authors":["Andrey Kormilitzin","Nemanja Vaci","Qiang Liu","Hao Ni","Goran Nenadic","Alejo Nevado-Holgado"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An efficient representation of chronological events in medical texts","tldr":"In this work we addressed the problem of capturing sequential information contained in longitudinal electronic health records (EHRs). Clinical notes, which is a particular type of EHR data, are a rich source of information and practitioners often dev...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.8","presentation_id":"38940041","rocketchat_channel":"paper-louhi2020-8","speakers":"Andrey Kormilitzin|Nemanja Vaci|Qiang Liu|Hao Ni|Goran Nenadic|Alejo Nevado-Holgado","title":"An efficient representation of chronological events in medical texts"},{"content":{"abstract":"Animal diseases-related news articles are richin information useful for risk assessment. In this paper, we explore a method to automatically retrieve sentence-level epidemiological information. Our method is an incremental approach to create and expand patterns at both lexical and syntactic levels. Expert knowledge input are used at different steps of the approach. Distributed vector representations (word embedding) were used to expand the patterns at the lexical level, thus alleviating manual curation. We showed that expert validation was crucial to improve the precision of automatically generated patterns.","authors":["Sarah Valentin","Mathieu Roche","Renaud Lancelot"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.louhi-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Information retrieval for animal disease surveillance: a pattern-based approach.","tldr":"Animal diseases-related news articles are richin information useful for risk assessment. In this paper, we explore a method to automatically retrieve sentence-level epidemiological information. Our method is an incremental approach to create and expa...","track":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis"},"id":"WS-19.2020.louhi-1.8","presentation_id":"","rocketchat_channel":"paper-louhi2020-8","speakers":"Sarah Valentin|Mathieu Roche|Renaud Lancelot","title":"Information retrieval for animal disease surveillance: a pattern-based approach."}],"prerecorded_talks":[],"rocketchat_channel":"workshop-louhi2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 09:15:00 GMT","hosts":"TBD","link":"","session_name":"Introduction","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:45:00 GMT","hosts":"Antonio Jimeno","link":"","session_name":"<b>Session 1</b>: oral presentations & QA <br>\nThe Impact of De-identification on Downstream Named Entity Recognition in Clinical Text (Hanna Berg, Aron Henriksson and Hercules Dalianis) <br>\nSimple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text (Maciej Wiatrak and Juha Iso-Sipila) <br>\nMedical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings (Katikapalli Subramanyam Kalyan and Sivanesan Sangeetha) <br>\nNot a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports (Andreas Grivas, Beatrice Alex, Claire Grover, Richard Tobin and William Whiteley) <br>\nGGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines (Florian Borchert, Christina Lohr, Luise Modersohn, Thomas Langer, Markus Follmann, Jan Philipp Sachs, Udo Hahn and Matthieu-P. Schapranow)","start_time":"Fri, 20 Nov 2020 09:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 10:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 12:20:00 GMT","hosts":"TBD","link":"","session_name":"<b>Session 2</b>: oral presentations & QA <br>\nNormalization of Long-tail Adverse Drug Reactions in Social Media (Emmanouil Manousogiannis, Sepideh Mesbah, Alessandro Bozzon, Robert-Jan Sips, Zoltan Szlanik and Selene Baez) <br>\nEvaluation of Machine Translation Methods applied to Medical Term (Konstantinos Skianis, Yann Briand and Florent Desgrippes) <br>\nInformation retrieval for animal disease surveillance: a pattern-based approach (Sarah Valentin, Mathieu Roche and Renaud Lancelot) <br>\nMultitask Learning of Negation and Speculation using Transformers (Aditya Khandelwal and Benita Kathleen Britto)","start_time":"Fri, 20 Nov 2020 11:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 12:20:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:45:00 GMT","hosts":"Eben Holderness","link":"","session_name":"<b>Keynote speaker: Guergana Savova </b>","start_time":"Fri, 20 Nov 2020 14:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:00:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 14:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:15:00 GMT","hosts":"Fabio Rinaldi","link":"","session_name":"<b>Session 3</b>: oral presentations & QA <br>\nBiomedical Event Extraction as Multi-turn Question Answering (Xing David Wang, Leon Weber and Ulf Leser) <br>\nAn efficient representation of chronological events in medical texts (Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Hao Ni, Goran Nenadic and Alejo Nevado-Holgado) <br>\nDefining and Learning Refined Temporal Relations in the Clinical Narrative (Kristin Wright-Bettner, Chen Lin, Timothy Miller, Steven Bethard, Dmitriy Dligach, Martha Palmer, James H. Martin and Guergana Savova) <br>\nContext-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains (Tarek Sakakini, Jong Yoon Lee, Aditya Duri, Renato F.L. Azevedo, Victor Sadauskas, Kuangxiao Gu, Suma Bhat, Dan Morrow, James Graumlich, Saqib Walayat, Mark Hasegawa-Johnson, Thomas Huang, Ann Willemsen-Dunlap and Donald Halpin)","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:30:00 GMT","hosts":"TBD","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 16:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:30:00 GMT","hosts":"TBD","link":"","session_name":"<b>Session 4</b>: oral presentations & QA <br>\nIdentifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating (Minghao Zhu, Youzhe Song, Ge Jin and Keyuan Jiang) <br>\nDetecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only (Ziyi Liu, Giannis Karamanolakis, Daniel Hsu and Luis Gravano) <br>\nDetection of Mental Health from Reddit via Deep Contextualized Representations (Zhengping Jiang, Sarah Ita Levitan, Jonathan Zomick and Julia Hirschberg)","start_time":"Fri, 20 Nov 2020 16:30:00 GMT"}],"title":"LOUHI 2020 - 11th International Workshop on Health Text Mining and Information Analysis","website":"https://louhi2020.fbk.eu/","zoom_links":["https://zoom.us"]},{"abstract":"The workshop focuses on designing evaluation metrics, reporting trustworthy results and creating adequate and correct evaluation data.","blocks":[{"end_time":"Fri, 20 Nov 2020 15:35:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"}],"id":"WS-20","livestream":null,"organizers":"Steffen Eger, Yang Gao, Maxime Peyrard, Wei Zhao,\u00a0Eduard Hovy,\u00a0Mohit Bansal, Robert West,\u00a0Ani Nenkova, and Ido Dagan","papers":[{"content":{"abstract":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete picture of the model\u2019s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed metrics address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model\u2019s confidence score assignments have an impact on the system\u2019s behavior. We describe four key benefits of our proposed metrics as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics\u2019 definitions; the remaining benefits, generalization, and functional consistency are demonstrated empirically.","authors":["Reda Yacouby","Dustin Axman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models","tldr":"In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete p...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.13","presentation_id":"38939710","rocketchat_channel":"paper-eval4nlp-13","speakers":"Reda Yacouby|Dustin Axman","title":"Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models"},{"content":{"abstract":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need for better summary evaluation methods that go beyond conventional overlap-based metrics. Our typology is structured into two dimensions. First, the Mapping Dimension describes surface-level errors and provides insight into word-sequence transformation issues. Second, the Meaning Dimension describes issues related to interpretation and provides insight into breakdowns in truth, i.e., factual faithfulness to the original text. Comparative analysis revealed that two neural summarization systems leveraging pre-trained models have an advantage in decreasing grammaticality errors, but not necessarily factual errors. We also discuss the importance of ensuring that summary length and abstractiveness do not interfere with evaluating summary quality.","authors":["Klaus-Michael Lux","Maya Sappelli","Martha Larson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries","tldr":"This paper presents a typology of errors produced by automatic summarization systems. The typology was created by manually analyzing the output of four recent neural summarization systems. Our work is motivated by the growing awareness of the need fo...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.15","presentation_id":"38939711","rocketchat_channel":"paper-eval4nlp-15","speakers":"Klaus-Michael Lux|Maya Sappelli|Martha Larson","title":"Truth or Error? Towards systematic analysis of factual errors in abstractive summaries"},{"content":{"abstract":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons: (1) it requires that word embeddings be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for model selection in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.","authors":["Nathan Stringham","Mike Izbicki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Word Embeddings on Low-Resource Languages","tldr":"The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reaso...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.16","presentation_id":"38939712","rocketchat_channel":"paper-eval4nlp-16","speakers":"Nathan Stringham|Mike Izbicki","title":"Evaluating Word Embeddings on Low-Resource Languages"},{"content":{"abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","authors":["Hila Gonen","Kellie Webster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.180","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","tldr":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Whil...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.1663","presentation_id":"38940033","rocketchat_channel":"paper-eval4nlp-1663","speakers":"Hila Gonen|Kellie Webster","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations"},{"content":{"abstract":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with human quality ratings. As a solution, we propose crowdsourcing as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of summarization by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, BLEU, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.","authors":["Neslihan Iskender","Tim Polzehl","Sebastian M\u00f6ller"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation","tldr":"One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.18","presentation_id":"38939713","rocketchat_channel":"paper-eval4nlp-18","speakers":"Neslihan Iskender|Tim Polzehl|Sebastian M\u00f6ller","title":"Best Practices for Crowd-based Evaluation of German Summarization: Comparing Crowd, Expert and Automatic Evaluation"},{"content":{"abstract":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.","authors":["Wanzheng Zhu","Suma Bhat"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"GRUEN for Evaluating Linguistic Quality of Generated Text","tldr":"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge th...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.183","presentation_id":"38940645","rocketchat_channel":"paper-eval4nlp-183","speakers":"Wanzheng Zhu|Suma Bhat","title":"GRUEN for Evaluating Linguistic Quality of Generated Text"},{"content":{"abstract":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document\u2019s text. We present evidence that BLANC scores have as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.","authors":["Oleg Vasilyev","Vedant Dharnidharka","John Bohannon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Fill in the BLANC: Human-free quality estimation of document summaries","tldr":"We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measur...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.21","presentation_id":"38939714","rocketchat_channel":"paper-eval4nlp-21","speakers":"Oleg Vasilyev|Vedant Dharnidharka|John Bohannon","title":"Fill in the BLANC: Human-free quality estimation of document summaries"},{"content":{"abstract":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model\u2019s behavior, and ignores linguistic properties of words that may allow some mis-predicted tokens to be useful in practice. Furthermore, statistics directly tied to prediction accuracy (including perplexity) may be confounded by the Zipfian nature of written language, as the majority of the prediction attempts will occur with frequently-occurring types. A model\u2019s performance may vary greatly between high- and low-frequency words, which in practice could lead to failure modes such as repetitive and dull generated text being produced by a downstream consumer of a language model. To address this, we propose two new intrinsic evaluation measures within the framework of a simple word prediction task that are designed to give a more holistic picture of a language model\u2019s performance. We evaluate several commonly-used large English language models using our proposed metrics, and demonstrate that our approach reveals functional differences in performance between the models that are obscured by more traditional metrics.","authors":["Shiran Dudy","Steven Bedrick"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Are Some Words Worth More than Others?","tldr":"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.22","presentation_id":"38939715","rocketchat_channel":"paper-eval4nlp-22","speakers":"Shiran Dudy|Steven Bedrick","title":"Are Some Words Worth More than Others?"},{"content":{"abstract":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.","authors":["Adam Poliak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A survey on Recognizing Textual Entailment as an NLP Evaluation","tldr":"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the ...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.23","presentation_id":"38939716","rocketchat_channel":"paper-eval4nlp-23","speakers":"Adam Poliak","title":"A survey on Recognizing Textual Entailment as an NLP Evaluation"},{"content":{"abstract":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedical data, and a range of evaluation measures suited to the task. The approach is applied to two recent DWSI systems, thus demonstrating its relevance and providing an in-depth analysis of the models.","authors":["Ashjan Alsulaimani","Erwan Moreau","Carl Vogel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.284","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Evaluation Method for Diachronic Word Sense Induction","tldr":"The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2311","presentation_id":"38940034","rocketchat_channel":"paper-eval4nlp-2311","speakers":"Ashjan Alsulaimani|Erwan Moreau|Carl Vogel","title":"An Evaluation Method for Diachronic Word Sense Induction"},{"content":{"abstract":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.","authors":["Zorik Gekhman","Roee Aharoni","Genady Beryozkin","Markus Freitag","Wolfgang Macherey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.287","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"KoBE: Knowledge-Based Machine Translation Evaluation","tldr":"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a la...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2378","presentation_id":"38940035","rocketchat_channel":"paper-eval4nlp-2378","speakers":"Zorik Gekhman|Roee Aharoni|Genady Beryozkin|Markus Freitag|Wolfgang Macherey","title":"KoBE: Knowledge-Based Machine Translation Evaluation"},{"content":{"abstract":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that still achieve good performance. We present several reproduction studies of intrinsic evaluation tasks that evaluate non-contextual word representations in multiple languages. Furthermore, we present 50-8-8, a new data set for the outlier identification task, which avoids limitations of the original data set, such as ambiguous words, infrequent words, and multi-word tokens, while increasing the number of test cases. The data set is expanded to contain semantic and syntactic tests and is multilingual (English, German, and Italian). We provide an in-depth analysis of word embedding models with a range of hyper-parameters. Our analysis shows the suitability of different models and hyper-parameters for different tasks and the greater difficulty of representing German and Italian languages.","authors":["Jesper Brink Andersen","Mikkel Bak Bertelsen","Mikkel H\u00f8rby Schou","Manuel R. Ciosici","Ira Assent"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations","tldr":"Word embeddings are an active topic in the NLP research community. State-of-the-art neural models achieve high performance on downstream tasks, albeit at the cost of computationally expensive training. Cost aware solutions require cheaper models that...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.25","presentation_id":"38939717","rocketchat_channel":"paper-eval4nlp-25","speakers":"Jesper Brink Andersen|Mikkel Bak Bertelsen|Mikkel H\u00f8rby Schou|Manuel R. Ciosici|Ira Assent","title":"One of these words is not like the other: a reproduction of outlier identification using non-contextual word representations"},{"content":{"abstract":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways (i.e. abstractive and extractive) on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in https://github.com/zide05/CDEvalSumm.","authors":["Yiran Chen","Pengfei Liu","Ming Zhong","Zi-Yi Dou","Danqing Wang","Xipeng Qiu","Xuanjing Huang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.329","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems","tldr":"Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.2740","presentation_id":"38940036","rocketchat_channel":"paper-eval4nlp-2740","speakers":"Yiran Chen|Pengfei Liu|Ming Zhong|Zi-Yi Dou|Danqing Wang|Xipeng Qiu|Xuanjing Huang","title":"CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems"},{"content":{"abstract":"","authors":["Guy Tevet","Jonathan Berant"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating the Evaluation of Diversity in Natural Language Generation","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.28","presentation_id":"38940785","rocketchat_channel":"paper-eval4nlp-28","speakers":"Guy Tevet|Jonathan Berant","title":"Evaluating the Evaluation of Diversity in Natural Language Generation"},{"content":{"abstract":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since it allows the assessment of both models and the prompts used to evaluate them. We use IRT to efficiently assess chatbots, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.","authors":["Jo\u00e3o Sedoc","Lyle Ungar"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Item Response Theory for Efficient Human Evaluation of Chatbots","tldr":"Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, usin...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.29","presentation_id":"38939718","rocketchat_channel":"paper-eval4nlp-29","speakers":"Jo\u00e3o Sedoc|Lyle Ungar","title":"Item Response Theory for Efficient Human Evaluation of Chatbots"},{"content":{"abstract":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges don\u2019t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.","authors":["Rahul Jha","Keping Bi","Yang Li","Mahdi Pakdaman","Asli Celikyilmaz","Ivan Zhiboedov","Kieran McDonald"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization","tldr":"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarizatio...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3","presentation_id":"38939707","rocketchat_channel":"paper-eval4nlp-3","speakers":"Rahul Jha|Keping Bi|Yang Li|Mahdi Pakdaman|Asli Celikyilmaz|Ivan Zhiboedov|Kieran McDonald","title":"Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization"},{"content":{"abstract":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the similarity score. Experimental results on three benchmark datasets show that our method correlates significantly better with human judgments than all existing metrics.","authors":["Hwanhee Lee","Seunghyun Yoon","Franck Dernoncourt","Doo Soon Kim","Trung Bui","Kyomin Jung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT","tldr":"In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic represent...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.30","presentation_id":"38939719","rocketchat_channel":"paper-eval4nlp-30","speakers":"Hwanhee Lee|Seunghyun Yoon|Franck Dernoncourt|Doo Soon Kim|Trung Bui|Kyomin Jung","title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT"},{"content":{"abstract":"","authors":["Asiye Tuba K\u00f6ksal","\u00d6zge Bozal","Emre Y\u00fcrekli","Gizem Gezici"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction","tldr":null,"track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3117","presentation_id":"38940037","rocketchat_channel":"paper-eval4nlp-3117","speakers":"Asiye Tuba K\u00f6ksal|\u00d6zge Bozal|Emre Y\u00fcrekli|Gizem Gezici","title":"#TurkihTweets: A Benchmark Dataset for Turkish Text Correction"},{"content":{"abstract":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.","authors":["Harris Chan","Jamie Kiros","William Chan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.376","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels","tldr":"A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work,...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.3148","presentation_id":"38940114","rocketchat_channel":"paper-eval4nlp-3148","speakers":"Harris Chan|Jamie Kiros|William Chan","title":"Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels"},{"content":{"abstract":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and DBpedia facts having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in DBpedia. We in- vestigate whether\u2014and, if so, how\u2014a given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.","authors":["Kiril Gashteovski","Rainer Gemulla","Bhushan Kotnis","Sven Hertling","Christian Meilicke"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study","tldr":"Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and auto...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.34","presentation_id":"38939720","rocketchat_channel":"paper-eval4nlp-34","speakers":"Kiril Gashteovski|Rainer Gemulla|Bhushan Kotnis|Sven Hertling|Christian Meilicke","title":"On Aligning OpenIE Extractions with Knowledge Bases: A Case Study"},{"content":{"abstract":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time, it has been argued that contextualized word representations exhibit sub-optimal statistical properties for encoding the true similarity between words or sentences. In this paper, we present two techniques for improving encoding representations for similarity metrics: a batch-mean centering strategy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERT-backbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks.","authors":["Xi Chen","Nan Ding","Tomer Levinboim","Radu Soricut"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance","tldr":"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.35","presentation_id":"38939721","rocketchat_channel":"paper-eval4nlp-35","speakers":"Xi Chen|Nan Ding|Tomer Levinboim|Radu Soricut","title":"Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance"},{"content":{"abstract":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper establishes a framework for addressing n-best evaluation by outlining three different questions one could consider when determining how one would define a \u2018good\u2019 n-best list and proposing evaluation measures for each question. The first and principal contribution is an evaluation measure that characterizes the translation quality of an entire n-best list by asking whether many of the valid translations are placed near the top of the list. The second is a measure that uses gold translations with preference annotations to ask to what degree systems can produce ranked lists in preference order. The third is a measure that rewards partial matches, evaluating the closeness of the many items in an n-best list to a set of many valid references. These three perspectives make clear that having access to many references can be useful when n-best evaluation is the goal.","authors":["Jacob Bremerman","Huda Khayrallah","Douglas Oard","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Evaluation of Machine Translation n-best Lists","tldr":"The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper es...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.36","presentation_id":"38939722","rocketchat_channel":"paper-eval4nlp-36","speakers":"Jacob Bremerman|Huda Khayrallah|Douglas Oard|Matt Post","title":"On the Evaluation of Machine Translation n-best Lists"},{"content":{"abstract":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as \u201cpsycholinguistic subjects\u201d and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA\u2019s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018a), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC.","authors":["Jingcheng Niu","Gerald Penn"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Grammaticality and Language Modelling","tldr":"Ever since Pereira (2000) provided evidence against Chomsky\u2019s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regard...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.37","presentation_id":"38939723","rocketchat_channel":"paper-eval4nlp-37","speakers":"Jingcheng Niu|Gerald Penn","title":"Grammaticality and Language Modelling"},{"content":{"abstract":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a Python-based tool for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a sentiment analysis and a patent classification task.","authors":["Hanna Wecker","Annemarie Friedrich","Heike Adel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation","tldr":"This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clu...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.5","presentation_id":"38939708","rocketchat_channel":"paper-eval4nlp-5","speakers":"Hanna Wecker|Annemarie Friedrich|Heike Adel","title":"ClusterDataSplit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation"},{"content":{"abstract":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversity can be estimated using statistical measures such as perplexity, measuring language quality requires human evaluation. However, because human evaluation at scale is slow and expensive, it is used sparingly; it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for machine translation. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a kernel function. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that \u2013 on average \u2013 the quality estimation from a BLEU Neighbors model has a lower mean squared error and higher Spearman correlation with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art models on automatically grading essays, including models that have access to a gold-standard reference essay.","authors":["Kawin Ethayarajh","Dorsa Sadigh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.eval4nlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation","tldr":"Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversi...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.7","presentation_id":"38939709","rocketchat_channel":"paper-eval4nlp-7","speakers":"Kawin Ethayarajh|Dorsa Sadigh","title":"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation"},{"content":{"abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","authors":["Daniel Deutsch","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlposs-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","tldr":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the off...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.8","presentation_id":"38940784","rocketchat_channel":"paper-eval4nlp-8","speakers":"Daniel Deutsch|Dan Roth","title":"SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics"},{"content":{"abstract":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better coverage of the space of valid translations and thereby improve its correlation with human judgments. Our experiments on the into-English language directions of the WMT19 metrics task (at both the system and sentence level) show that using paraphrased references does generally improve BLEU, and when it does, the more diverse the better. However, we also show that better results could be achieved if those paraphrases were to specifically target the parts of the space most relevant to the MT outputs being evaluated. Moreover, the gains remain slight even when human paraphrases are used, suggesting inherent limitations to BLEU\u2019s capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentence-level BLEU.","authors":["Rachel Bawden","Biao Zhang","Lisa Yankovskaya","Andre T\u00e4ttar","Matt Post"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.82","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing","tldr":"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional *diverse* references can provide better cove...","track":"Evaluation and Comparison of NLP Systems"},"id":"WS-20.815","presentation_id":"38940032","rocketchat_channel":"paper-eval4nlp-815","speakers":"Rachel Bawden|Biao Zhang|Lisa Yankovskaya|Andre T\u00e4ttar|Matt Post","title":"A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-eval4nlp","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 09:15:00 GMT","hosts":"Eval4NLP","link":"","session_name":"Opening Remarks","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:15:00 GMT","hosts":"Wei Zhao","link":"","session_name":"Keynote 1: Goran Glavas","start_time":"Fri, 20 Nov 2020 09:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:30:00 GMT","hosts":" ","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 10:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:30:00 GMT","hosts":"Maxime Peyrard","link":"","session_name":"Keynote 2: Ido Dagan","start_time":"Fri, 20 Nov 2020 10:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 12:30:00 GMT","hosts":" ","link":"","session_name":"Lunch Break","start_time":"Fri, 20 Nov 2020 11:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:30:00 GMT","hosts":"Steffen Eger","link":"","session_name":"Keynote 3: Asli Celikyilmaz","start_time":"Fri, 20 Nov 2020 12:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:45:00 GMT","hosts":" ","link":"","session_name":"Break","start_time":"Fri, 20 Nov 2020 13:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 14:30:00 GMT","hosts":"Steffen Eger","link":"","session_name":"Best paper awards incl. short presentations","start_time":"Fri, 20 Nov 2020 13:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:30:00 GMT","hosts":"Yang Gao","link":"","session_name":"Keynote 4: William Wang","start_time":"Fri, 20 Nov 2020 14:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:35:00 GMT","hosts":"Yang Gao","link":"","session_name":"Concluding Remarks","start_time":"Fri, 20 Nov 2020 15:30:00 GMT"}],"title":"Evaluation and Comparison of NLP Systems","website":"https://nlpevaluation2020.github.io/","zoom_links":["https://zoom.us"]},{"abstract":"Structured prediction with continuous representations, task-level supervision and latent linguistic structure.","blocks":[{"end_time":"Fri, 20 Nov 2020 17:30:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"}],"id":"WS-21","livestream":null,"organizers":"Andr\u00e9 F. T. Martins, Andreas Vlachos, Zornitsa Kozareva, Sujith Ravi, Gerasimos Lampouras, Priyanka Agrawal and Julia Kreutzer","papers":[{"content":{"abstract":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.","authors":["Federico L\u00f3pez","Michael Strube"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.42","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification","tldr":"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.490","presentation_id":"38940646","rocketchat_channel":"paper-spnlp20-490","speakers":"Federico L\u00f3pez|Michael Strube","title":"A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification"},{"content":{"abstract":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structured prediction problem, consisting in inferring the most probable instance of the semantic model given the text. In this work, we focus on the challenging sub-problem of cardinality prediction that consists in predicting the number of distinct individuals of each class in the semantic model. We show that cardinality prediction can successfully be approached by modeling the overall task as a joint inference problem, predicting the number of individuals of certain classes while at the same time extracting their properties. We approach this task with probabilistic graphical models computing the maximum-a-posteriori instance of the semantic model. Our main contribution lies on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury.","authors":["Hendrik ter Horst","Philipp Cimiano"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension","tldr":"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structure...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.10","presentation_id":"38940161","rocketchat_channel":"paper-spnlp20-10","speakers":"Hendrik ter Horst|Philipp Cimiano","title":"Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension"},{"content":{"abstract":"","authors":["Yuntian Deng","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Cascaded Text Generation with Markov Transformers","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.11","presentation_id":"38940151","rocketchat_channel":"paper-spnlp20-11","speakers":"Yuntian Deng|Alexander Rush","title":"Cascaded Text Generation with Markov Transformers"},{"content":{"abstract":"","authors":["Justin Chiu","Alexander Rush"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Scaling Hidden Markov Language Models","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.12","presentation_id":"38940160","rocketchat_channel":"paper-spnlp20-12","speakers":"Justin Chiu|Alexander Rush","title":"Scaling Hidden Markov Language Models"},{"content":{"abstract":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.","authors":["Anh Duong Trinh","Robert J. Ross","John D. Kelleher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking","tldr":"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, su...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.14","presentation_id":"38940154","rocketchat_channel":"paper-spnlp20-14","speakers":"Anh Duong Trinh|Robert J. Ross|John D. Kelleher","title":"Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking"},{"content":{"abstract":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.","authors":["Ning Shi","Ziheng Zeng","Haotian Zhang","Yichen Gong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.159","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Recurrent Inference in Text Editing","tldr":"In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying dec...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1463","presentation_id":"38940648","rocketchat_channel":"paper-spnlp20-1463","speakers":"Ning Shi|Ziheng Zeng|Haotian Zhang|Yichen Gong","title":"Recurrent Inference in Text Editing"},{"content":{"abstract":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper, we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. The ability of our end-to-end method to retrieve structured information is assessed on a large set of business documents. We show that it performs competitively with a standard word classifier without requiring costly word level supervision.","authors":["Cl\u00e9ment Sage","Alex Aussem","V\u00e9ronique Eglin","Haytham Elghazel","J\u00e9r\u00e9my Espinas"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks","tldr":"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.16","presentation_id":"38940153","rocketchat_channel":"paper-spnlp20-16","speakers":"Cl\u00e9ment Sage|Alex Aussem|V\u00e9ronique Eglin|Haytham Elghazel|J\u00e9r\u00e9my Espinas","title":"End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks"},{"content":{"abstract":"","authors":["Zihao Deng","Sijia Wang","Brendan Juba"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntactically restricted self-attention for Semantic Role Labeling","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.17","presentation_id":"38940162","rocketchat_channel":"paper-spnlp20-17","speakers":"Zihao Deng|Sijia Wang|Brendan Juba","title":"Syntactically restricted self-attention for Semantic Role Labeling"},{"content":{"abstract":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an end-to-end deep learning framework of the quality estimation and automatic post-editing of the machine translation output. Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model. To imitate the behavior of human translators, we design three efficient delegation modules \u2013 quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them. We examine this approach with the English\u2013German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance. We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation.","authors":["Ke Wang","Jiayi Wang","Niyu Ge","Yangbin Shi","Yu Zhao","Kai Fan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.197","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing","tldr":"With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by p...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.1774","presentation_id":"38940649","rocketchat_channel":"paper-spnlp20-1774","speakers":"Ke Wang|Jiayi Wang|Niyu Ge|Yangbin Shi|Yu Zhao|Kai Fan","title":"Computer Assisted Translation with Neural Quality Estimation and Automatic Post-Editing"},{"content":{"abstract":"","authors":["Manuel Widmoser","Maria Pacheco","Jean Honorio","Dan Goldwasser"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Randomized Deep Structured Prediction for Argumentation Mining","tldr":null,"track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.19","presentation_id":"38940158","rocketchat_channel":"paper-spnlp20-19","speakers":"Manuel Widmoser|Maria Pacheco|Jean Honorio|Dan Goldwasser","title":"Randomized Deep Structured Prediction for Argumentation Mining"},{"content":{"abstract":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.","authors":["Noe Casas","Jos\u00e9 A. R. Fonollosa","Marta R. Costa-juss\u00e0"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation","tldr":"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradig...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2","presentation_id":"38940163","rocketchat_channel":"paper-spnlp20-2","speakers":"Noe Casas|Jos\u00e9 A. R. Fonollosa|Marta R. Costa-juss\u00e0","title":"Syntax-driven Iterative Expansion Language Models for Controllable Text Generation"},{"content":{"abstract":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.","authors":["Nikolaos Manginas","Ilias Chalkidis","Prodromos Malakasiotis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations","tldr":"Although BERT is widely used by the NLP community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT\u2019s over-...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.20","presentation_id":"38940156","rocketchat_channel":"paper-spnlp20-20","speakers":"Nikolaos Manginas|Ilias Chalkidis|Prodromos Malakasiotis","title":"Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations"},{"content":{"abstract":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both cost-augmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.","authors":["Lifu Tu","Richard Yuanzhe Pang","Kevin Gimpel"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks","tldr":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \u201cinference networks\u201d to approximate structu...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.21","presentation_id":"38940143","rocketchat_channel":"paper-spnlp20-21","speakers":"Lifu Tu|Richard Yuanzhe Pang|Kevin Gimpel","title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks"},{"content":{"abstract":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.","authors":["Shucheng Li","Lingfei Wu","Shiwei Feng","Fangli Xu","Fengyuan Xu","Sheng Zhong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.255","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem","tldr":"The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as se...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2146","presentation_id":"38940650","rocketchat_channel":"paper-spnlp20-2146","speakers":"Shucheng Li|Lingfei Wu|Shiwei Feng|Fangli Xu|Fengyuan Xu|Sheng Zhong","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"content":{"abstract":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, \u201cSome person was born in some location at some time.\u201d We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.","authors":["Yunmo Chen","Tongfei Chen","Seth Ebner","Aaron Steven White","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reading the Manual: Event Extraction as Definition Comprehension","tldr":"We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.22","presentation_id":"38940159","rocketchat_channel":"paper-spnlp20-22","speakers":"Yunmo Chen|Tongfei Chen|Seth Ebner|Aaron Steven White|Benjamin Van Durme","title":"Reading the Manual: Event Extraction as Definition Comprehension"},{"content":{"abstract":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.","authors":["Daivik Swarup","Ahsaas Bajaj","Sheshera Mysore","Tim O\u2019Gorman","Rajarshi Das","Andrew McCallum"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.270","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text","tldr":"In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different way...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2220","presentation_id":"38940651","rocketchat_channel":"paper-spnlp20-2220","speakers":"Daivik Swarup|Ahsaas Bajaj|Sheshera Mysore|Tim O\u2019Gorman|Rajarshi Das|Andrew McCallum","title":"An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text"},{"content":{"abstract":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.","authors":["Vikas Raunak","Siddharth Dalmia","Vivek Gupta","Florian Metze"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.276","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Long-Tailed Phenomena in Neural Machine Translation","tldr":"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2284","presentation_id":"38940652","rocketchat_channel":"paper-spnlp20-2284","speakers":"Vikas Raunak|Siddharth Dalmia|Vivek Gupta|Florian Metze","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"content":{"abstract":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y\u02c6 given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y\u02c6, y* | x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets. Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.","authors":["Jason Lee","Dustin Tran","Orhan Firat","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Discrepancy between Density Estimation and Sequence Generation","tldr":"Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.23","presentation_id":"38940144","rocketchat_channel":"paper-spnlp20-23","speakers":"Jason Lee|Dustin Tran|Orhan Firat|Kyunghyun Cho","title":"On the Discrepancy between Density Estimation and Sequence Generation"},{"content":{"abstract":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a target-to-source translation model and a language model. By applying Bayes\u2019 rule strategically, we reformulate this approach as a log-linear combination of translation, sentence-level and document-level language model probabilities. In addition to using static coefficients for each term, this formulation alternatively allows for the learning of dynamic per-token weights to more finely control the impact of the language models. Using both static or dynamic coefficients leads to improvements over a context-agnostic baseline and a context-aware concatenation model.","authors":["S\u00e9bastien Jean","Kyunghyun Cho"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation","tldr":"We seek to maximally use various data sources, such as parallel and monolingual data, to build an effective and efficient document-level translation system. In particular, we start by considering a noisy channel approach (CITATION) that combines a ta...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.24","presentation_id":"38940157","rocketchat_channel":"paper-spnlp20-24","speakers":"S\u00e9bastien Jean|Kyunghyun Cho","title":"Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation"},{"content":{"abstract":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.","authors":["Alireza Mohammadshahi","James Henderson"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.294","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing","tldr":"We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dep...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.2417","presentation_id":"38940653","rocketchat_channel":"paper-spnlp20-2417","speakers":"Alireza Mohammadshahi|James Henderson","title":"Graph-to-Graph Transformer for Transition-based Dependency Parsing"},{"content":{"abstract":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for Natural Language Understanding without relying on a parser. Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our method is interpretable due to the bias introduced by the KR approach.","authors":["Arindam Mitra","Sanjay Narayana","Chitta Baral"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective","tldr":"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launc...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.26","presentation_id":"38940152","rocketchat_channel":"paper-spnlp20-26","speakers":"Arindam Mitra|Sanjay Narayana|Chitta Baral","title":"Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner\u2019s Perspective"},{"content":{"abstract":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2018) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness.","authors":["Renato Negrinho","Matthew R. Gormley","Geoff Gordon"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.406","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"An Empirical Investigation of Beam-Aware Training in Supertagging","tldr":"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and ...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.3373","presentation_id":"38940654","rocketchat_channel":"paper-spnlp20-3373","speakers":"Renato Negrinho|Matthew R. Gormley|Geoff Gordon","title":"An Empirical Investigation of Beam-Aware Training in Supertagging"},{"content":{"abstract":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.","authors":["Abhinav Singh","Patrick Xia","Guanghui Qin","Mahsa Yarmohammadi","Benjamin Van Durme"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models","tldr":"Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where eac...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.4","presentation_id":"38940142","rocketchat_channel":"paper-spnlp20-4","speakers":"Abhinav Singh|Patrick Xia|Guanghui Qin|Mahsa Yarmohammadi|Benjamin Van Durme","title":"CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models"},{"content":{"abstract":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset for navigation domain.","authors":["Ke Tran","Ming Tan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.spnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations","tldr":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabiliti...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.7","presentation_id":"38940155","rocketchat_channel":"paper-spnlp20-7","speakers":"Ke Tran|Ming Tan","title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations"},{"content":{"abstract":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.","authors":["Youngbin Ro","Yukyung Lee","Pilsung Kang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.99","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT","tldr":"In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query...","track":"4th Workshop on Structured Prediction for NLP"},"id":"WS-21.957","presentation_id":"38940647","rocketchat_channel":"paper-spnlp20-957","speakers":"Youngbin Ro|Yukyung Lee|Pilsung Kang","title":"Multi\u02c62OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT"}],"prerecorded_talks":[{"presentation_id":"38940145","speakers":"Isabelle Augenstein","title":"Invited talk by Isabelle Augenstein"},{"presentation_id":"38940146","speakers":"Jonathan Berant","title":"Invited talk by Jonathan Berant"},{"presentation_id":"38940147","speakers":"Mark Johnson","title":"Invited talk by Mark Johnson"},{"presentation_id":"38940148","speakers":"Alexander Rush","title":"Invited talk by Alexander Rush"},{"presentation_id":"38940149","speakers":"Sunita Sarawagi","title":"Invited talk by Sunita Sarawagi"},{"presentation_id":"38940150","speakers":"Ivan Titov","title":"Invited talk by Ivan Titov"}],"rocketchat_channel":"workshop-spnlp20","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 09:10:00 GMT","hosts":"Priyanka Agrawal, Andr\u00e9 Martins\nZoom Link 1","link":"","session_name":"Welcome Speech","start_time":"Fri, 20 Nov 2020 09:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 09:50:00 GMT","hosts":"Priyanka Agrawal, Andr\u00e9 Martins\nZoom Link 1","link":"","session_name":"Invited Talk: Introduction to Structured Prediction in NLP\nMark Johnson (Macquarie University)","start_time":"Fri, 20 Nov 2020 09:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:30:00 GMT","hosts":"Priyanka Agrawal, Andr\u00e9 Martins\nZoom Link 1","link":"","session_name":"Invited Talk: The Role and Representation of Alignments in Modern Seq2Seq Architectures\nSunita Sarawagi (IIT Bombay)","start_time":"Fri, 20 Nov 2020 09:50:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:00:00 GMT","hosts":"Gather Room E","link":"","session_name":"Coffee Break","start_time":"Fri, 20 Nov 2020 10:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:40:00 GMT","hosts":"Priyanka Agrawal, Andreas Vlachos\nZoom Link 1","link":"","session_name":"Invited Talk: Explainable Fact Checking as Structured Prediction\nIsabelle Augenstein (University of Copenhagen)","start_time":"Fri, 20 Nov 2020 11:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:55:00 GMT","hosts":"Priyanka Agrawal, Andreas Vlachos\nZoom Link 1","link":"","session_name":"Contributed Talk: On the Discrepancy between Density Estimation and Sequence Generation\n(Jason Lee, Dustin Tran, Orhan Firat and Kyunghyun Cho)","start_time":"Fri, 20 Nov 2020 11:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 12:10:00 GMT","hosts":"Priyanka Agrawal, Andreas Vlachos\nZoom Link 1","link":"","session_name":"Contributed Talk: Improving Joint Training of Inference Networks and Structured Prediction Energy Networks\n(Lifu Tu, Richard Yuanzhe Pang and Kevin Gimpel)","start_time":"Fri, 20 Nov 2020 11:55:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:00:00 GMT","hosts":"Gather Room E","link":"","session_name":"Lunch Break","start_time":"Fri, 20 Nov 2020 12:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:40:00 GMT","hosts":"Andreas Vlachos,  Gerasimos L\nZoom Link 1","link":"","session_name":"Invited Talk: Latent Tree Models for Compositional Generalization\n Jonathan Berant (Tel-Aviv University)","start_time":"Fri, 20 Nov 2020 13:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:55:00 GMT","hosts":"Andreas Vlachos,  Gerasimos L\nZoom Link 1","link":"","session_name":"Contributed Talk: CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models\n(Abhinav Singh, Patrick Xia, Guanghui Qin, Mahsa Yarmohammadi and Benjamin Van Durme)","start_time":"Fri, 20 Nov 2020 13:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:30:00 GMT","hosts":"Andr\u00e9 Martins, Andreas Vlachos\nGather Rooms E & F","link":"","session_name":"Poster Session","start_time":"Fri, 20 Nov 2020 13:55:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"Andr\u00e9 Martins, Andreas Vlachos\nGather Rooms E & F","link":"","session_name":"Coffee Break + Poster Session","start_time":"Fri, 20 Nov 2020 15:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:40:00 GMT","hosts":"Julia Kreutzer,  Gerasimos L\nZoom Link 1","link":"","session_name":"Invited Talk: Exploring Deep Structured NLP\nAlexander Rush (Cornell Tech)","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:20:00 GMT","hosts":"Julia Kreutzer,  Gerasimos L\nZoom Link 1","link":"","session_name":"Invited Talk: Interpretability with Differentiable Subset Selection\nIvan Titov (University of Edinburgh)","start_time":"Fri, 20 Nov 2020 16:40:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:30:00 GMT","hosts":"Julia Kreutzer,  Andr\u00e9 Martins\nZoom Link 1","link":"","session_name":"Closing Remarks","start_time":"Fri, 20 Nov 2020 17:20:00 GMT"}],"title":"4th Workshop on Structured Prediction for NLP","website":"http://structuredprediction.github.io/SPNLP20","zoom_links":["https://zoom.us","https://zoom.us"]},{"abstract":"Can you really know if someone is sarcastic by only reading his texts? If the answer is no, the NLPBT workshop is what you're looking for.","blocks":[{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"}],"id":"WS-23","livestream":null,"organizers":"Erik Cambria, Giuseppe Castellucci, Simone Filice, Soujanya Poria, Lucia Specia","papers":[{"content":{"abstract":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.","authors":["Elman Mansimov","Mitchell Stern","Mia Chen","Orhan Firat","Jakob Uszkoreit","Puneet Jain"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Towards End-to-End In-Image Neural Machine Translation","tldr":"In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model...","track":"NLP Beyond Text"},"id":"WS-23.106","presentation_id":"38939782","rocketchat_channel":"paper-nlpbt2020-106","speakers":"Elman Mansimov|Mitchell Stern|Mia Chen|Orhan Firat|Jakob Uszkoreit|Puneet Jain","title":"Towards End-to-End In-Image Neural Machine Translation"},{"content":{"abstract":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic inputs from a wide range of datasets to challenge, and sometimes surpass, the state-of-the-art in the field. To demonstrate the efficiency of our models, we carefully evaluate their performances on the IEMOCAP, MOSI, MOSEI and MELD dataset. The experiments can be directly replicated and the code is fully open for future researches.","authors":["Jean-Benoit Delbrouck","No\u00e9 Tits","St\u00e9phane Dupont"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition","tldr":"This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic ...","track":"NLP Beyond Text"},"id":"WS-23.110","presentation_id":"38939779","rocketchat_channel":"paper-nlpbt2020-110","speakers":"Jean-Benoit Delbrouck|No\u00e9 Tits|St\u00e9phane Dupont","title":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition"},{"content":{"abstract":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.","authors":["Tejas Srinivasan","Ramon Sanabria","Florian Metze","Desmond Elliott"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Multimodal Speech Recognition with Unstructured Audio Masking","tldr":"Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fix...","track":"NLP Beyond Text"},"id":"WS-23.114","presentation_id":"38939780","rocketchat_channel":"paper-nlpbt2020-114","speakers":"Tejas Srinivasan|Ramon Sanabria|Florian Metze|Desmond Elliott","title":"Multimodal Speech Recognition with Unstructured Audio Masking"},{"content":{"abstract":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.","authors":["Aman Khullar","Udit Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention","tldr":"This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities \u2013 text, audio and video \u2013 in a multimodal video. Prior work on multimodal abstractive text summarization only util...","track":"NLP Beyond Text"},"id":"WS-23.119","presentation_id":"38939781","rocketchat_channel":"paper-nlpbt2020-119","speakers":"Aman Khullar|Udit Arora","title":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention"},{"content":{"abstract":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model\u2019s performance particularly improved on questions that required coreference resolution.","authors":["Muhammad Shah","Shikib Mehri","Tejas Srinivasan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Reasoning Over History: Context Aware Visual Dialog","tldr":"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existin...","track":"NLP Beyond Text"},"id":"WS-23.122","presentation_id":"38939783","rocketchat_channel":"paper-nlpbt2020-122","speakers":"Muhammad Shah|Shikib Mehri|Tejas Srinivasan","title":"Reasoning Over History: Context Aware Visual Dialog"},{"content":{"abstract":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle","authors":["Chaitanya Ahuja","Dong Won Lee","Ryo Ishii","Louis-Philippe Morency"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.170","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures","tldr":"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made a...","track":"NLP Beyond Text"},"id":"WS-23.1589","presentation_id":"38940175","rocketchat_channel":"paper-nlpbt2020-1589","speakers":"Chaitanya Ahuja|Dong Won Lee|Ryo Ishii|Louis-Philippe Morency","title":"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures"},{"content":{"abstract":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process.","authors":["Wanqing Cui","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.392","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Beyond Language: Learning Commonsense from Images for Reasoning","tldr":"This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thous...","track":"NLP Beyond Text"},"id":"WS-23.3273","presentation_id":"38940176","rocketchat_channel":"paper-nlpbt2020-3273","speakers":"Wanqing Cui|Yanyan Lan|Liang Pang|Jiafeng Guo|Xueqi Cheng","title":"Beyond Language: Learning Commonsense from Images for Reasoning"},{"content":{"abstract":"","authors":["Loic Barrault"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Vision on (Simultaneous) Multimodal Machine Translation","tldr":null,"track":"NLP Beyond Text"},"id":"WS-23.Loic","presentation_id":"38939784","rocketchat_channel":"paper-nlpbt2020-Loic","speakers":"Loic Barrault","title":"A Vision on (Simultaneous) Multimodal Machine Translation"},{"content":{"abstract":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get the summaries, and fuses the summaries. Recently, some multi-modal models based on the architecture of BERT are proposed such as ViLBERT. However, they can only be pretrained on the image-text data. In this paper, we propose an image-text model for sarcasm detection using the pretrained BERT and ResNet without any further pretraining. BERT and ResNet have been pretrained on much larger text or image data than image-text data. We connect the vector spaces of BERT and ResNet to utilize more data. We use the pretrained Multi-Head Attention of BERT to model the text and image. Besides, we propose a 2D-Intra-Attention to extract the relationships between words and images. In experiments, our model outperforms the state-of-the-art model.","authors":["Xinyu Wang","Xiaowen Sun","Tan Yang","Hongbo Wang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data","tldr":"Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get t...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.3","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-3","speakers":"Xinyu Wang|Xiaowen Sun|Tan Yang|Hongbo Wang","title":"Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data"},{"content":{"abstract":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.","authors":["Frank F. Xu","Lei Ji","Botian Shi","Junyi Du","Graham Neubig","Yonatan Bisk","Nan Duan"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos","tldr":"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quant...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.4","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-4","speakers":"Frank F. Xu|Lei Ji|Botian Shi|Junyi Du|Graham Neubig|Yonatan Bisk|Nan Duan","title":"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos"},{"content":{"abstract":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped English LibriVox audiobooks and their corresponding English Gutenberg Project e-books to Italian e-books with a set of three complementary methods. Then we aligned the English and the Italian texts using both traditional Gale-Church based alignment methods and a recently proposed tool to perform bilingual sentences alignment computing the cosine similarity of multilingual sentence embeddings. Finally, we forced the alignment between the English audiobooks and the English side of our textual parallel corpus with a text-to-speech and dynamic time warping based forced alignment tool. For each step, we provide the reader with a critical discussion based on detailed evaluation and comparison of the results of the different methods.","authors":["Giuseppe Della Corte","Sara Stymne"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation","tldr":"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped En...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.5","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-5","speakers":"Giuseppe Della Corte|Sara Stymne","title":"A Multi-Modal English-Italian Parallel Corpus for End-to-End Speech-to-Text Machine Translation"},{"content":{"abstract":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situations, where the answers are more likely to be sentences rather than single words. To bridge the gap between this natural VQA and existing VQA approaches, a novel unsupervised keyword extraction method is proposed. The method is based on the principle that the full-sentence answers can be decomposed into two parts: one that contains new information answering the question (i.e. keywords), and one that contains information already included in the question. Discriminative decoders were designed to achieve such decomposition, and the method was experimentally implemented on VQA datasets containing full-sentence answers. The results show that the proposed model can accurately extract the keywords without being given explicit annotations describing them.","authors":["Kohei Uehara","Tatsuya Harada"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpbt-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Keyword Extraction for Full-Sentence VQA","tldr":"In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situation...","track":"NLP Beyond Text"},"id":"WS-23.2020.nlpbt-1.6","presentation_id":"","rocketchat_channel":"paper-nlpbt2020-6","speakers":"Kohei Uehara|Tatsuya Harada","title":"Unsupervised Keyword Extraction for Full-Sentence VQA"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-nlpbt2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 15:10:00 GMT","hosts":"Simone Filice","link":"","session_name":"Intro to NLPBT","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:30:00 GMT","hosts":"Simone Filice","link":"","session_name":"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition","start_time":"Fri, 20 Nov 2020 15:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:50:00 GMT","hosts":"Simone Filice","link":"","session_name":"Multimodal Speech Recognition with Unstructured Audio Masking","start_time":"Fri, 20 Nov 2020 15:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:10:00 GMT","hosts":"Simone Filice","link":"","session_name":"Findings Paper: No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures","start_time":"Fri, 20 Nov 2020 15:50:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"Simone Filice","link":"","session_name":"Keynote Talk:  A Vision on (Simultaneous) Multimodal Machine Translation Loic Barrault","start_time":"Fri, 20 Nov 2020 16:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:30:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"Poster Session on gather.town","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:50:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention","start_time":"Fri, 20 Nov 2020 18:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:10:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"Towards End-to-End In-Image Neural Machine Translation","start_time":"Fri, 20 Nov 2020 18:50:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:30:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"Findings Paper: Beyond Language: Learning Commonsense from Images for Reasoning","start_time":"Fri, 20 Nov 2020 19:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:50:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"Reasoning Over History: Context Aware Visual Dialog","start_time":"Fri, 20 Nov 2020 19:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":"Giuseppe Castellucci","link":"","session_name":"Closing NLPBT","start_time":"Fri, 20 Nov 2020 19:50:00 GMT"}],"title":"NLP Beyond Text","website":"https://sites.google.com/view/nlpbt-2020","zoom_links":["https://zoom.us"]},{"abstract":"PrivateNLP focuses on Privacy Preserving systems: differential privacy, anonymized datasets and models, GDPR compliance, and shared tasks!","blocks":[{"end_time":"Sat, 21 Nov 2020 01:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 16:45:00 GMT"}],"id":"WS-24","livestream":null,"organizers":"Oluwaseyi Feyisetan, Sepideh Ghanavati, Patricia Thaine and Shervin Malmasi","papers":[{"content":{"abstract":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution defined by the ground truth labels). In this paper, we show that a malicious modeler, upon obtaining access to the Log-Loss scores on its predictions, can exploit this information to infer all the ground truth labels of arbitrary test datasets with full accuracy. We provide an efficient algorithm to perform this inference. A particularly interesting application where this attack can be exploited is to breach privacy in the setting of Membership Inference Attacks. These attacks exploit the vulnerabilities of exposing models trained on customer data to queries made by an adversary. Privacy auditing tools for measuring leakage from sensitive datasets assess the total privacy leakage based on the adversary\u2019s predictions for datapoint membership. An instance of the proposed attack can hence, cause complete membership privacy breach, obviating any attack model training or access to side knowledge with the adversary. Moreover, our algorithm is agnostic to the model under attack and hence, enables perfect membership inference even for models that do not memorize or overfit. In particular, our observations provide insight into the extent of information leakage from statistical aggregates and how they can be exploited.","authors":["Abhinav Aggarwal","Zekun Xu","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On Log-Loss Scores and (No) Privacy","tldr":"A common metric for assessing the performance of binary classifiers is the Log-Loss score, which is a real number indicating the cross entropy distance between the predicted distribution over the labels and the true distribution (a point distribution...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.1","presentation_id":"38939769","rocketchat_channel":"paper-privatenlp2020-1","speakers":"Abhinav Aggarwal|Zekun Xu|Oluwaseyi Feyisetan|Nathanael Teissier","title":"On Log-Loss Scores and (No) Privacy"},{"content":{"abstract":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-centric encoders and leverage them to create an interface that allows users to search for privacy settings using free-form queries. In order to achieve this goal, we create a custom Semantic Similarity dataset, which consists of real user queries covering various privacy settings. We then use this dataset to fine-tune a state of the art encoder. Using this fine-tuned encoder, we perform semantic matching between the user queries and the privacy settings to retrieve the most relevant setting. Finally, we also use the encoder to generate embeddings of privacy settings from the top 100 websites and perform unsupervised clustering to learn about the online privacy settings types. We find that the most common type of privacy settings are \u2018Personalization\u2019 and \u2018Notifications\u2019, with coverage of 35.8% and 34.4%, respectively, in our dataset.","authors":["Rishabh Khandelwal","Asmit Nayak","Yao Yao","Kassem Fawaz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Surfacing Privacy Settings Using Semantic Matching","tldr":"Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-cent...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.11","presentation_id":"38939773","rocketchat_channel":"paper-privatenlp2020-11","speakers":"Rishabh Khandelwal|Asmit Nayak|Yao Yao|Kassem Fawaz","title":"Surfacing Privacy Settings Using Semantic Matching"},{"content":{"abstract":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.","authors":["Gavin Kerrigan","Dylan Slack","Jens Tuyls"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Differentially Private Language Models Benefit from Public Pre-training","tldr":"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorit...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.12","presentation_id":"38939774","rocketchat_channel":"paper-privatenlp2020-12","speakers":"Gavin Kerrigan|Dylan Slack|Jens Tuyls","title":"Differentially Private Language Models Benefit from Public Pre-training"},{"content":{"abstract":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is first mapped into a continuous embedding space, perturbed by sampling a spherical noise from an appropriate distribution, and then projected back to the discrete vocabulary space. While this allows the perturbation to admit the required metric differential privacy, often the utility of downstream tasks modeled on this perturbed data is low because the spherical noise does not account for the variability in the density around different words in the embedding space. In particular, words in a sparse region are likely unchanged even when the noise scale is large. In this paper, we propose a text perturbation mechanism based on a carefully designed regularized variant of the Mahalanobis metric to overcome this problem. For any given noise scale, this metric adds an elliptical noise to account for the covariance structure in the embedding space. This heterogeneity in the noise scale along different directions helps ensure that the words in the sparse region have sufficient likelihood of replacement without sacrificing the overall utility. We provide a text-perturbation algorithm based on this metric and formally prove its privacy guarantees. Additionally, we empirically show that our mechanism improves the privacy statistics to achieve the same level of utility as compared to the state-of-the-art Laplace mechanism.","authors":["Zekun Xu","Abhinav Aggarwal","Oluwaseyi Feyisetan","Nathanael Teissier"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric","tldr":"Balancing the privacy-utility tradeoff is a crucial requirement of many practical machine learning systems that deal with sensitive customer data. A popular approach for privacy- preserving text analysis is noise injection, in which text data is firs...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2","presentation_id":"38939770","rocketchat_channel":"paper-privatenlp2020-2","speakers":"Zekun Xu|Abhinav Aggarwal|Oluwaseyi Feyisetan|Nathanael Teissier","title":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric"},{"content":{"abstract":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and social networks, as well as the power of modern AI to synthesize and gain insights from this data. This paper presents an approach to detect emotional and informational self-disclosure in natural language. We hypothesize that identifying frame semantics can meaningfully support this task. Specifically, we use Semantic Role Labeling to identify the lexical units and their semantic roles that signal self-disclosure. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.","authors":["Chandan Akiti","Anna Squicciarini","Sarah Rajtmajer"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.312","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content","tldr":"As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal infor...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.2534","presentation_id":"38940639","rocketchat_channel":"paper-privatenlp2020-2534","speakers":"Chandan Akiti|Anna Squicciarini|Sarah Rajtmajer","title":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content"},{"content":{"abstract":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.","authors":["Yangsibo Huang","Zhao Song","Danqi Chen","Kai Li","Sanjeev Arora"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.123","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TextHide: Tackling Data Privacy in Language Understanding Tasks","tldr":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language unders...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.3","presentation_id":"38939771","rocketchat_channel":"paper-privatenlp2020-3","speakers":"Yangsibo Huang|Zhao Song|Danqi Chen|Kai Li|Sanjeev Arora","title":"TextHide: Tackling Data Privacy in Language Understanding Tasks"},{"content":{"abstract":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations.","authors":["Mitra Bokaie Hosseini","Pragyan K C","Irwin Reyes","Serge Egelman"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.privatenlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies","tldr":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulation...","track":"PrivateNLP 2020: The First Workshop on Privacy in NLP"},"id":"WS-24.9","presentation_id":"38939772","rocketchat_channel":"paper-privatenlp2020-9","speakers":"Mitra Bokaie Hosseini|Pragyan K C|Irwin Reyes|Serge Egelman","title":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies"}],"prerecorded_talks":[{"presentation_id":"38939775","speakers":"Aaron Roth","title":"invited1"},{"presentation_id":"38939776","speakers":"Reza Shokri","title":"invited2"},{"presentation_id":"38939777","speakers":"Krishnaram Kenthapadi","title":"invited3"},{"presentation_id":"38939778","speakers":"Annabelle McIver, Mark Dras","title":"invited4"}],"rocketchat_channel":"workshop-privatenlp2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"TBD","link":"","session_name":"Opening remarks","start_time":"Fri, 20 Nov 2020 16:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:15:00 GMT","hosts":"TBD","link":"","session_name":"Session 1","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:45:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk by Aaron Roth","start_time":"Fri, 20 Nov 2020 17:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:15:00 GMT","hosts":"TBD","link":"","session_name":"On Log-Loss Scores and (No) Privacy","start_time":"Fri, 20 Nov 2020 17:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:15:00 GMT","hosts":"TBD","link":"","session_name":"Session 2","start_time":"Fri, 20 Nov 2020 18:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:15:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk by Reza Shokri","start_time":"Fri, 20 Nov 2020 18:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:45:00 GMT","hosts":"TBD","link":"","session_name":"A Differentially Private Text Perturbation Method Using Regularized Mahalanobis Metric","start_time":"Fri, 20 Nov 2020 19:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:15:00 GMT","hosts":"TBD","link":"","session_name":"TextHide: Tackling Data Privacy in Language Understanding Tasks","start_time":"Fri, 20 Nov 2020 19:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 22:45:00 GMT","hosts":"TBD","link":"","session_name":"Session 3","start_time":"Fri, 20 Nov 2020 21:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:45:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk by Mark Dras and Annabell McIver","start_time":"Fri, 20 Nov 2020 21:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 22:15:00 GMT","hosts":"TBD","link":"","session_name":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies","start_time":"Fri, 20 Nov 2020 21:45:00 GMT"},{"end_time":"Fri, 20 Nov 2020 22:45:00 GMT","hosts":"TBD","link":"","session_name":"Surfacing Privacy Settings Using Semantic Matching","start_time":"Fri, 20 Nov 2020 22:15:00 GMT"},{"end_time":"Sat, 21 Nov 2020 01:00:00 GMT","hosts":"Patricia","link":"","session_name":"Session 4","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 23:45:00 GMT","hosts":"TBD","link":"","session_name":"Invited talk by Krishnaram Kenthapadi","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 00:15:00 GMT","hosts":"TBD","link":"","session_name":"Differentially Private Language Models Benefit from Public Pre-training","start_time":"Fri, 20 Nov 2020 23:45:00 GMT"},{"end_time":"Sat, 21 Nov 2020 00:45:00 GMT","hosts":"TBD","link":"","session_name":"A Semantics-based Approach to Disclosure Classification in User-Generated Online Content","start_time":"Sat, 21 Nov 2020 00:15:00 GMT"},{"end_time":"Sat, 21 Nov 2020 01:00:00 GMT","hosts":"TBD","link":"","session_name":"Closing remarks","start_time":"Sat, 21 Nov 2020 00:45:00 GMT"}],"title":"PrivateNLP 2020: The First Workshop on Privacy in NLP","website":"https://sites.google.com/view/privatenlp/","zoom_links":["https://zoom.us"]},{"abstract":"Open the black-box of neural networks in NLP and make sense of the big pile of goo inside it","blocks":[{"end_time":"Fri, 20 Nov 2020 13:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 2","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 03:45:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 3","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"}],"id":"WS-25","livestream":null,"organizers":"Yonatan Belinkov, Afra Alishahi, Grzegorz Chrupa\u0142a, Dieuwke Hupkes, Yuval Pinter and Hassan Sajjad","papers":[{"content":{"abstract":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving millions of clients. They cannot afford traditional fine tuning for individual clients. Many clients cannot even afford significant fine tuning, and own little or no labeled data. Recognizing that word usage and salience diversity across clients leads to reduced accuracy, we initiate a study of practical and lightweight adaptation of centralized NLP services to clients. Each client uses an unsupervised, corpus-based sketch to register to the service. The server modifies its network mildly to accommodate client sketches, and occasionally trains the augmented network over existing clients. When a new client registers with its sketch, it gets immediate accuracy benefits. We demonstrate the proposed architecture using sentiment labeling, NER, and predictive language modeling.","authors":["Sahil Shah","Vihari Piratla","Soumen Chakrabarti","Sunita Sarawagi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.357","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"NLP Service APIs and Models for Efficient Registration of New Clients","tldr":"State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2976","presentation_id":"38940136","rocketchat_channel":"paper-blackboxnlp2020-2976","speakers":"Sahil Shah|Vihari Piratla|Soumen Chakrabarti|Sunita Sarawagi","title":"NLP Service APIs and Models for Efficient Registration of New Clients"},{"content":{"abstract":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages \u2013 developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance \u2013 a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.","authors":["Yilin Yang","Longyue Wang","Shuming Shi","Prasad Tadepalli","Stefan Lee","Zhaopeng Tu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.findings-emnlp.432","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Sub-layer Functionalities of Transformer Decoder","tldr":"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.3561","presentation_id":"38940141","rocketchat_channel":"paper-blackboxnlp2020-3561","speakers":"Yilin Yang|Longyue Wang|Shuming Shi|Prasad Tadepalli|Stefan Lee|Zhaopeng Tu","title":"On the Sub-layer Functionalities of Transformer Decoder"},{"content":{"abstract":"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.","authors":["Jasmijn Bastings","Katja Filippova"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?","tldr":"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.33","presentation_id":"38939764","rocketchat_channel":"paper-blackboxnlp2020-33","speakers":"Jasmijn Bastings|Katja Filippova","title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?"},{"content":{"abstract":"Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model\u2019s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.","authors":["Rajiv Movva","Jason Zhao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation","tldr":"Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model\u2019s learned representations. By probing Transformers with more and mo...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.43","presentation_id":"38939765","rocketchat_channel":"paper-blackboxnlp2020-43","speakers":"Rajiv Movva|Jason Zhao","title":"Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation"},{"content":{"abstract":"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that \u201cthe doctor visited the lawyer\u201d does not entail \u201cthe lawyer visited the doctor\u201d), accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.","authors":["R. Thomas McCoy","Junghyun Min","Tal Linzen"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance","tldr":"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Infere...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.45","presentation_id":"38939766","rocketchat_channel":"paper-blackboxnlp2020-45","speakers":"R. Thomas McCoy|Junghyun Min|Tal Linzen","title":"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance"},{"content":{"abstract":"Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.","authors":["Benjamin Newman","John Hewitt","Percy Liang","Christopher D. Manning"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The EOS Decision and Length Extrapolation","tldr":"Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process t...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.54","presentation_id":"38939767","rocketchat_channel":"paper-blackboxnlp2020-54","speakers":"Benjamin Newman|John Hewitt|Percy Liang|Christopher D. Manning","title":"The EOS Decision and Length Extrapolation"},{"content":{"abstract":"Interpretability methods for neural networks are difficult to evaluate because we do not understand the black-box models typically used to test them. This paper proposes a framework in which interpretability methods are evaluated using manually constructed networks, which we call white-box networks, whose behavior is understood a priori. We evaluate five methods for producing attribution heatmaps by applying them to white-box LSTM classifiers for tasks based on formal languages. Although our white-box classifiers solve their tasks perfectly and transparently, we find that all five attribution methods fail to produce the expected model explanations.","authors":["Yiding Hao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Evaluating Attribution Methods using White-Box LSTMs","tldr":"Interpretability methods for neural networks are difficult to evaluate because we do not understand the black-box models typically used to test them. This paper proposes a framework in which interpretability methods are evaluated using manually const...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.59","presentation_id":"38939768","rocketchat_channel":"paper-blackboxnlp2020-59","speakers":"Yiding Hao","title":"Evaluating Attribution Methods using White-Box LSTMs"},{"content":{"abstract":"In this paper we introduce diagNNose, an open source library for analysing the activations of deep neural networks. diagNNose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of diagNNose with a case study on subject-verb agreement within language models. diagNNose is available at https://github.com/i-machine-think/diagnnose.","authors":["Jaap Jumelet"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"diagNNose: A Library for Neural Activation Analysis","tldr":"In this paper we introduce diagNNose, an open source library for analysing the activations of deep neural networks. diagNNose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural net...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.70","presentation_id":"38940638","rocketchat_channel":"paper-blackboxnlp2020-70","speakers":"Jaap Jumelet","title":"diagNNose: A Library for Neural Activation Analysis"},{"content":{"abstract":"While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques\u2014supervised probing, unsupervised similarity analysis, and layer-based ablations\u2014we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.","authors":["Amil Merchant","Elahe Rahimtoroghi","Ellie Pavlick","Ian Tenney"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"What Happens To BERT Embeddings During Fine-tuning?","tldr":"While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis tech...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.8","presentation_id":"38939763","rocketchat_channel":"paper-blackboxnlp2020-8","speakers":"Amil Merchant|Elahe Rahimtoroghi|Ellie Pavlick|Ian Tenney","title":"What Happens To BERT Embeddings During Fine-tuning?"},{"content":{"abstract":"Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT\u2019s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, varying from a low of 17.77% for Place to a high of 51.61% for Artifact. Next, we find that linear combinations of these heads, estimated with approx. 11% of available total event argument detection supervision, can push performance well higher for some roles \u2014 highest two being Victim (68.29% Accuracy) and Artifact (58.82% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate \u201cbest heads\u201d for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in another sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated \u201cnonce\u201d words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.","authors":["Varun Gangal","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset","tldr":"Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT\u2019s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.1","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-1","speakers":"Varun Gangal|Eduard Hovy","title":"BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset"},{"content":{"abstract":"Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them.","authors":["Eugene Kharitonov","Marco Baroni"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Emergent Language Generalization and Acquisition Speed are not tied to Compositionality","tldr":"Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the ag...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.2","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-2","speakers":"Eugene Kharitonov|Marco Baroni","title":"Emergent Language Generalization and Acquisition Speed are not tied to Compositionality"},{"content":{"abstract":"Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sentential, rhetorical knowledge. In this paper, we propose a method that quantitatively evaluates the rhetorical capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method shows an avenue towards quantifying the rhetorical capacities of neural LMs.","authors":["Zining Zhu","Chuer Pan","Mohamed Abdalla","Frank Rudzicz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Examining the rhetorical capacities of neural language models","tldr":"Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sententia...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.3","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-3","speakers":"Zining Zhu|Chuer Pan|Mohamed Abdalla|Frank Rudzicz","title":"Examining the rhetorical capacities of neural language models"},{"content":{"abstract":"Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations.","authors":["Hila Gonen","Shauli Ravfogel","Yanai Elazar","Yoav Goldberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"It\u2019s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT","tldr":"Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that e...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.5","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-5","speakers":"Hila Gonen|Shauli Ravfogel|Yanai Elazar|Yoav Goldberg","title":"It\u2019s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT"},{"content":{"abstract":"We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY\u2014a white box attack\u2014performed on the approximate model by 25% F1, and the ADDSENT attack\u2014a black box attack\u2014by 11% F1.","authors":["Naveen Jafer Nizar","Ari Kobren"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Leveraging Extracted Model Adversaries for Improved Black Box Attacks","tldr":"We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we u...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.6","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-6","speakers":"Naveen Jafer Nizar|Ari Kobren","title":"Leveraging Extracted Model Adversaries for Improved Black Box Attacks"},{"content":{"abstract":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.","authors":["Marius Mosbach","Anna Khokhlova","Michael A. Hedderich","Dietrich Klakow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers","tldr":"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, u...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.7","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-7","speakers":"Marius Mosbach|Anna Khokhlova|Michael A. Hedderich|Dietrich Klakow","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"content":{"abstract":"It is challenging to automatically evaluate the answer of a QA model at inference time. Although many models provide confidence scores, and simple heuristics can go a long way towards indicating answer correctness, such measures are heavily dataset-dependent and are unlikely to generalise. In this work, we begin by investigating the hidden representations of questions, answers, and contexts in transformer-based QA architectures. We observe a consistent pattern in the answer representations, which we show can be used to automatically evaluate whether or not a predicted answer span is correct. Our method does not require any labelled data and outperforms strong heuristic baselines, across 2 datasets and 7 domains. We are able to predict whether or not a model\u2019s answer is correct with 91.37% accuracy on SQuAD, and 80.7% accuracy on SubjQA. We expect that this method will have broad applications, e.g., in semi-automatic development of QA datasets.","authors":["Lukas Muttenthaler","Isabelle Augenstein","Johannes Bjerva"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Evaluation for Question Answering with Transformers","tldr":"It is challenging to automatically evaluate the answer of a QA model at inference time. Although many models provide confidence scores, and simple heuristics can go a long way towards indicating answer correctness, such measures are heavily dataset-d...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.8","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-8","speakers":"Lukas Muttenthaler|Isabelle Augenstein|Johannes Bjerva","title":"Unsupervised Evaluation for Question Answering with Transformers"},{"content":{"abstract":"Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.","authors":["Shauli Ravfogel","Yanai Elazar","Jacob Goldberger","Yoav Goldberg"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Unsupervised Distillation of Syntactic Information from Contextualized Word Representations","tldr":"Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language represe...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.9","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-9","speakers":"Shauli Ravfogel|Yanai Elazar|Jacob Goldberger|Yoav Goldberg","title":"Unsupervised Distillation of Syntactic Information from Contextualized Word Representations"},{"content":{"abstract":"Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier\u2019s decision. We use this framework to compare several explainers, including gradient methods, erasure, and attention mechanisms, in terms of their communication success. In addition, we reinterpret these methods in the light of classical feature selection, and use this as inspiration for new embedded explainers, through the use of selective, sparse attention. Experiments in text classification and natural language inference, using different configurations of explainers and laypeople (including both machines and humans), reveal an advantage of attention-based explainers over gradient and erasure methods, and show that selective attention is a simpler alternative to stochastic rationalizers. Human experiments show strong results on text classification with post-hoc explainers trained to optimize communication success.","authors":["Marcos Treviso","Andr\u00e9 F. T. Martins"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"The Explanation Game: Towards Prediction Explainability through Sparse Communication","tldr":"Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier\u2019s decision. We use this framework to compare s...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.10","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-10","speakers":"Marcos Treviso|Andr\u00e9 F. T. Martins","title":"The Explanation Game: Towards Prediction Explainability through Sparse Communication"},{"content":{"abstract":"Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM (Shen et al., 2019), which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that (1) the model has reasonably consistent parsing behaviors across different restarts, (2) the model struggles with the internal structures of complex noun phrases, (3) the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling.","authors":["Yian Zhang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?","tldr":"Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM (Shen et al., 2019), which is trained on language modelling and has near-state-of-the-art performance...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.11","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-11","speakers":"Yian Zhang","title":"Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?"},{"content":{"abstract":"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models\u2019 performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.","authors":["Chuanrong Li","Lin Shengshuo","Zeyu Liu","Xinyi Wu","Xuhui Zhou","Shane Steinert-Threlkeld"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets","tldr":"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets ...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.12","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-12","speakers":"Chuanrong Li|Lin Shengshuo|Zeyu Liu|Xinyi Wu|Xuhui Zhou|Shane Steinert-Threlkeld","title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets"},{"content":{"abstract":"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets.","authors":["Hande Celikkanat","Sami Virpioja","J\u00f6rg Tiedemann","Marianna Apidianaki"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations","tldr":"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of em...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.13","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-13","speakers":"Hande Celikkanat|Sami Virpioja|J\u00f6rg Tiedemann|Marianna Apidianaki","title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations"},{"content":{"abstract":"The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.","authors":["David Yenicelik","Florian Schmidt","Yannic Kilcher"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"How does BERT capture semantics? A closer look at polysemous words","tldr":"The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polys...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.15","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-15","speakers":"David Yenicelik|Florian Schmidt|Yannic Kilcher","title":"How does BERT capture semantics? A closer look at polysemous words"},{"content":{"abstract":"We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on lexical entailment and negation. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.","authors":["Atticus Geiger","Kyle Richardson","Christopher Potts"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation","tldr":"We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systemati...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.16","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-16","speakers":"Atticus Geiger|Kyle Richardson|Christopher Potts","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"content":{"abstract":"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT\u2019s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.","authors":["Jaspreet Singh","Jonas Wallat","Avishek Anand"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT","tldr":"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it capture...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.17","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-17","speakers":"Jaspreet Singh|Jonas Wallat|Avishek Anand","title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT"},{"content":{"abstract":"Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to investigate for evidence of compositional reasoning over numerical data in various natural language number systems. By using both grammaticality judgment and value comparison classification tasks in English, Japanese, Danish, and French, we find evidence that the information encoded in these pretrained models\u2019 embeddings is sufficient for grammaticality judgments but generally not for value comparisons. We analyze possible reasons for this and discuss how our tasks could be extended in further studies.","authors":["Devin Johnson","Denise Mak","Andrew Barker","Lexi Loessberg-Zahl"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models","tldr":"Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual prob...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.18","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-18","speakers":"Devin Johnson|Denise Mak|Andrew Barker|Lexi Loessberg-Zahl","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"content":{"abstract":"Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task structure, while probing task evaluations often look at only a few attributes and models. We address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.","authors":["Andrew Runge","Eduard Hovy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploring Neural Entity Representations for Semantic Information","tldr":"Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task struc...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.20","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-20","speakers":"Andrew Runge|Eduard Hovy","title":"Exploring Neural Entity Representations for Semantic Information"},{"content":{"abstract":"Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.","authors":["John Morris"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples","tldr":"Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is dete...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.22","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-22","speakers":"John Morris","title":"Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples"},{"content":{"abstract":"How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model\u2019s output is changed in the way predicted by our analysis.","authors":["Paul Soulos","R. Thomas McCoy","Tal Linzen","Paul Smolensky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks","tldr":"How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.23","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-23","speakers":"Paul Soulos|R. Thomas McCoy|Tal Linzen|Paul Smolensky","title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks"},{"content":{"abstract":"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing (NLP) models. Very recent work suggests that the self-attention in the Transformer encodes syntactic information; Here, we show that self-attention scores encode semantics by considering sentiment analysis tasks. In contrast to gradient-based feature attribution methods, we propose a simple and effective Layer-wise Attention Tracing (LAT) method to analyze structured attention weights. We apply our method to Transformer models trained on two tasks that have surface dissimilarities, but share common semantics\u2014sentiment analysis of movie reviews and time-series valence prediction in life story narratives. Across both tasks, words with high aggregated attention weights were rich in emotional semantics, as quantitatively validated by an emotion lexicon labeled by human annotators. Our results show that structured attention weights encode rich semantics in sentiment analysis, and match human interpretations of semantics.","authors":["Zhengxuan Wu","Thanh-Son Nguyen","Desmond Ong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis","tldr":"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing (NLP) models. Very recent work suggests that the self-attention in the Transformer encodes synta...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.24","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-24","speakers":"Zhengxuan Wu|Thanh-Son Nguyen|Desmond Ong","title":"Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis"},{"content":{"abstract":"Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT\u2019s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.","authors":["Tristan Thrush","Ethan Wilcox","Roger Levy"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization","tldr":"Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We addr...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.25","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-25","speakers":"Tristan Thrush|Ethan Wilcox|Roger Levy","title":"Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization"},{"content":{"abstract":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results.","authors":["Xikun Zhang","Deepak Ramachandran","Ian Tenney","Yanai Elazar","Dan Roth"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Do Language Embeddings capture Scales?","tldr":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.27","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-27","speakers":"Xikun Zhang|Deepak Ramachandran|Ian Tenney|Yanai Elazar|Dan Roth","title":"Do Language Embeddings capture Scales?"},{"content":{"abstract":"With the increase in the use of AI systems, a need for explanation systems arises. Building an explanation system requires a definition of explanation. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences. We study multiple perspectives and aspects of explainability of recommendations or predictions made by AI systems, and provide a generic definition of explanation. The proposed definition is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.","authors":["Tejaswani Verma","Christoph Lingenfelder","Dietrich Klakow"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Defining Explanation in an AI Context","tldr":"With the increase in the use of AI systems, a need for explanation systems arises. Building an explanation system requires a definition of explanation. However, the natural language term explanation is difficult to define formally as it includes mult...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.29","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-29","speakers":"Tejaswani Verma|Christoph Lingenfelder|Dietrich Klakow","title":"Defining Explanation in an AI Context"},{"content":{"abstract":"We study the behavior of several black-box search algorithms used for generating adversarial examples for natural language processing (NLP) tasks. We perform a fine-grained analysis of three elements relevant to search: search algorithm, search space, and search budget. When new search algorithms are proposed in past work, the attack search space is often modified alongside the search algorithm. Without ablation studies benchmarking the search algorithm change with the search space held constant, one cannot tell if an increase in attack success rate is a result of an improved search algorithm or a less restrictive search space. Additionally, many previous studies fail to properly consider the search algorithms\u2019 run-time cost, which is essential for downstream tasks like adversarial training. Our experiments provide a reproducible benchmark of search algorithms across a variety of search spaces and query budgets to guide future research in adversarial NLP. Based on our experiments, we recommend greedy attacks with word importance ranking when under a time constraint or attacking long inputs, and either beam search or particle swarm optimization otherwise.","authors":["Jin Yong Yoo","John Morris","Eli Lifland","Yanjun Qi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples","tldr":"We study the behavior of several black-box search algorithms used for generating adversarial examples for natural language processing (NLP) tasks. We perform a fine-grained analysis of three elements relevant to search: search algorithm, search space...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.30","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-30","speakers":"Jin Yong Yoo|John Morris|Eli Lifland|Yanjun Qi","title":"Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples"},{"content":{"abstract":"Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the ability of these models to generalize these capacities to novel words. This type of generalization is exhibited by humans, and is intimately related to morphology\u2014humans are in many cases able to identify inflections of novel words in the appropriate context. This type of morphological capacity has not been previously tested in BERT models, and is important for morphologically-rich languages, which are under-studied in the literature regarding BERT\u2019s linguistic capacities. In this work, we investigate this by considering monolingual and multilingual BERT models\u2019 abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch. We find that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of BERT models.","authors":["Coleman Haley"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.blackboxnlp-1.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"This is a BERT. Now there are several of them. Can they generalize to novel words?","tldr":"Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the a...","track":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP"},"id":"WS-25.2020.blackboxnlp-1.31","presentation_id":"","rocketchat_channel":"paper-blackboxnlp2020-31","speakers":"Coleman Haley","title":"This is a BERT. Now there are several of them. Can they generalize to novel words?"}],"prerecorded_talks":[{"presentation_id":"38939760","speakers":"Roger Levy","title":"Evaluating and calibrating neural language models for human-like language processing"},{"presentation_id":"38939761","speakers":"Idan Blank","title":"Understanding NLP\u2019s blackbox with the brain\u2019s blackbox and vice versa"},{"presentation_id":"38939762","speakers":"Anna Rogers","title":"When BERT plays the lottery, all tickets are winning!"}],"rocketchat_channel":"workshop-blackboxnlp2020","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 08:15:00 GMT","hosts":"Grzegorz Chrupala","link":"","session_name":"Opening remarks","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 09:00:00 GMT","hosts":"Grzegorz Chrupala","link":"","session_name":"Keynote 1 - Anna Rogers","start_time":"Fri, 20 Nov 2020 08:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:00:00 GMT","hosts":"Yonatan Belinkov","link":"","session_name":"Oral session 1","start_time":"Fri, 20 Nov 2020 09:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:30:00 GMT","hosts":"Yonatan Belinkov","link":"","session_name":"System demonstration","start_time":"Fri, 20 Nov 2020 10:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 12:00:00 GMT","hosts":"gather.town","link":"","session_name":"Poster session A","start_time":"Fri, 20 Nov 2020 10:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 13:00:00 GMT","hosts":"Yonatan Belinkov","link":"","session_name":"Keynote 1 - Anna Rogers (repeat)","start_time":"Fri, 20 Nov 2020 12:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 15:45:00 GMT","hosts":"Dieuwke Hupkes","link":"","session_name":"Keynote 2 - Roger Levy","start_time":"Fri, 20 Nov 2020 15:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:00:00 GMT","hosts":"Dieuwke Hupkes","link":"","session_name":"Oral session 2","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:00:00 GMT","hosts":"Yonatan Belinkov","link":"","session_name":"Keynote 3 - Idan Blank","start_time":"Fri, 20 Nov 2020 17:15:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:20:00 GMT","hosts":"Yonatan Belinkov","link":"","session_name":"Awards","start_time":"Fri, 20 Nov 2020 18:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:00:00 GMT","hosts":"gather.town","link":"","session_name":"Poster session B","start_time":"Fri, 20 Nov 2020 18:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 23:45:00 GMT","hosts":"Yuval Pinter","link":"","session_name":"Keynote 2 - Roger Levy (repeat)","start_time":"Fri, 20 Nov 2020 23:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 01:30:00 GMT","hosts":"gather.town","link":"","session_name":"Poster session C","start_time":"Sat, 21 Nov 2020 00:00:00 GMT"},{"end_time":"Sat, 21 Nov 2020 02:45:00 GMT","hosts":"Yuval Pinter","link":"","session_name":"Oral session 3","start_time":"Sat, 21 Nov 2020 01:30:00 GMT"},{"end_time":"Sat, 21 Nov 2020 03:45:00 GMT","hosts":"Yuval Pinter","link":"","session_name":"Keynote 3 - Idan Blank (repeat)","start_time":"Sat, 21 Nov 2020 03:00:00 GMT"}],"title":"BlackboxNLP 2020: Analyzing and interpreting neural networks for NLP","website":"https://blackboxnlp.github.io","zoom_links":["https://zoom.us"]},{"abstract":null,"blocks":[{"end_time":"Fri, 20 Nov 2020 21:00:00 GMT","hosts":null,"link":"","session_name":"W-Live Session 1","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"}],"id":"WS-26","livestream":null,"organizers":"Karin Verspoor, Kevin Bretonnel Cohen, Mike Conway, Berry de Bruijn, Mark Dredze, Rada Mihalcea and Byron Wallace","papers":[{"content":{"abstract":"Public health surveillance and tracking virus via social media can be a useful digital tool for contact tracing and preventing the spread of the virus. Nowadays, large volumes of COVID-19 tweets can quickly be processed in real-time to offer information to researchers. Nonetheless, due to the absence of labeled data for COVID-19, the preliminary supervised classifier or semi-supervised self-labeled methods will not handle non-spherical data with adequate accuracy. With the seasonal influenza and novel Coronavirus having many similar symptoms, we propose using few shot learning to fine-tune a semi-supervised model built on unlabeled COVID-19 and previously labeled influenza dataset that can provide in- sights into COVID-19 that have not been investigated. The experimental results show the efficacy of the proposed model with an accuracy of 86%, identification of Covid-19 related discussion using recently collected tweets.","authors":["Brandon Lwowski","Peyman Najafirad"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.9","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19 Surveillance through Twitter using Self-Supervised and Few Shot Learning","tldr":"Public health surveillance and tracking virus via social media can be a useful digital tool for contact tracing and preventing the spread of the virus. Nowadays, large volumes of COVID-19 tweets can quickly be processed in real-time to offer informat...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.1","presentation_id":"38939841","rocketchat_channel":"paper-nlp-covid19-emnlp-1","speakers":"Brandon Lwowski|Peyman Najafirad","title":"COVID-19 Surveillance through Twitter using Self-Supervised and Few Shot Learning"},{"content":{"abstract":"The Covid-19 pandemic urged the scientific community to join efforts at an unprecedented scale, leading to faster than ever dissemination of data and results, which in turn motivated more research works. This paper presents and discusses information retrieval models aimed at addressing the challenge of searching the large number of publications that stem from these studies. The model presented, based on classical baselines followed by an interaction based neural ranking model, was evaluated and evolved within the TREC Covid challenge setting. Results on this dataset show that, when starting with a strong baseline, our light neural ranking model can achieve results that are comparable to other model architectures that use very large number of parameters.","authors":["Tiago Almeida","S\u00e9rgio Matos"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.3","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Frugal neural reranking: evaluation on the Covid-19 literature","tldr":"The Covid-19 pandemic urged the scientific community to join efforts at an unprecedented scale, leading to faster than ever dissemination of data and results, which in turn motivated more research works. This paper presents and discusses information ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.10","presentation_id":"38939845","rocketchat_channel":"paper-nlp-covid19-emnlp-10","speakers":"Tiago Almeida|S\u00e9rgio Matos","title":"Frugal neural reranking: evaluation on the Covid-19 literature"},{"content":{"abstract":"Ever since the COVID-19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioinformaticians, nurses, data scientists, and all of the affiliated relevant disciplines have been mobilized to help discover efficient treatments for the infected population, as well as a vaccine solution to prevent further the virus spread. In this combat against the virus responsible for the pandemic, key for any advancements is the timely, accurate, peer-reviewed, and efficient communication of any novel research findings. In this paper we present a novel framework to address the information need of filtering efficiently the scientific bibliography for relevant literature around COVID-19. The contributions of the paper are summarized in the following: we define and describe the information need that encompasses the major requirements for COVID-19 articles relevancy, we present and release an expert-curated benchmark set for the task, and we analyze the performance of several state-of-the-art machine learning classifiers that may distinguish the relevant from the non-relevant COVID-19 literature.","authors":["Zubair Afzal","Vikrant Yadav","Olga Fedorova","Vaishnavi Kandala","Janneke van de Loo","Saber A. Akhondi","Pascal Coupet","George Tsatsaronis"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.2","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles","tldr":"Ever since the COVID-19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioi...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.16","presentation_id":"38939846","rocketchat_channel":"paper-nlp-covid19-emnlp-16","speakers":"Zubair Afzal|Vikrant Yadav|Olga Fedorova|Vaishnavi Kandala|Janneke van de Loo|Saber A. Akhondi|Pascal Coupet|George Tsatsaronis","title":"CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles"},{"content":{"abstract":"We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.","authors":["Alexandre B\u00e9rard","Zae Myung Kim","Vassilina Nikoulina","Eunjeong Lucy Park","Matthias Gall\u00e9"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.16","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Multilingual Neural Machine Translation Model for Biomedical Data","tldr":"We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large am...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2","presentation_id":"38939842","rocketchat_channel":"paper-nlp-covid19-emnlp-2","speakers":"Alexandre B\u00e9rard|Zae Myung Kim|Vassilina Nikoulina|Eunjeong Lucy Park|Matthias Gall\u00e9","title":"A Multilingual Neural Machine Translation Model for Biomedical Data"},{"content":{"abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of pandemic-related discussion. Next, we examine the volume of activity in order to determine whether the number of people discussing mental health has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","authors":["Laura Biester","Katie Matton","Janarthanan Rajendran","Emily Mower Provost","Rada Mihalcea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.8","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","tldr":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to bette...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.20","presentation_id":"38939847","rocketchat_channel":"paper-nlp-covid19-emnlp-20","speakers":"Laura Biester|Katie Matton|Janarthanan Rajendran|Emily Mower Provost|Rada Mihalcea","title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums"},{"content":{"abstract":"Social media is a rich source where we can learn about people\u2019s reactions to social issues. As COVID-19 has significantly impacted on people\u2019s lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people\u2019s reactions and concerns about COVID-19 in North America, especially focusing on Canada. We analyze COVID-19 related tweets using topic modeling and aspect-based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID-19. We also examine people\u2019s sentiment about COVID-19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","authors":["Hyeju Jang","Emily Rempel","Giuseppe Carenini","Naveed Janjua"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.18","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Exploratory Analysis of COVID-19 Related Tweets in North America to Inform Public Health Institutes","tldr":"Social media is a rich source where we can learn about people\u2019s reactions to social issues. As COVID-19 has significantly impacted on people\u2019s lives, it is essential to capture how people react to public health interventions and understand their conc...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.28","presentation_id":"38939848","rocketchat_channel":"paper-nlp-covid19-emnlp-28","speakers":"Hyeju Jang|Emily Rempel|Giuseppe Carenini|Naveed Janjua","title":"Exploratory Analysis of COVID-19 Related Tweets in North America to Inform Public Health Institutes"},{"content":{"abstract":"Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on heterogeneous meanings in communities across the United States and that these disparate meanings shaped communities\u2019 response to the virus during the early, vital stages of the outbreak in the U.S. Using word embeddings, we demonstrate that counties where residents socially distanced less on average (as measured by residential mobility) more semantically associated the virus in their COVID discourse with concepts of fraud, the political left, and more benign illnesses like the flu. We also show that the different meanings the virus took on in different communities explains a substantial fraction of what we call the \u201c\u201dTrump Gap\u201d, or the empirical tendency for more Trump-supporting counties to socially distance less. This work demonstrates that community-level processes of meaning-making in part determined behavioral responses to the COVID-19 pandemic and that these processes can be measured unobtrusively using Twitter.","authors":["Austin Van Loon","Sheridan Stewart","Brandon Waldon","Shrinidhi K Lakshmikanth","Ishan Shah","Sharath Chandra Guntuku","Garrick Sherman","James Zou","Johannes Eichstaedt"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.10","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Explaining the Trump Gap in Social Distancing Using COVID Discourse","tldr":"Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on hetero...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.31","presentation_id":"38939849","rocketchat_channel":"paper-nlp-covid19-emnlp-31","speakers":"Austin Van Loon|Sheridan Stewart|Brandon Waldon|Shrinidhi K Lakshmikanth|Ishan Shah|Sharath Chandra Guntuku|Garrick Sherman|James Zou|Johannes Eichstaedt","title":"Explaining the Trump Gap in Social Distancing Using COVID Discourse"},{"content":{"abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce Expressive Interviewing \u2013 an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\u2019s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","authors":["Charles Welch","Allison Lahnala","Veronica Perez-Rosas","Siqi Shen","Sarah Seraj","Larry An","Kenneth Resnicow","James Pennebaker","Rada Mihalcea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.6","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","tldr":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolatio...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.35","presentation_id":"38939850","rocketchat_channel":"paper-nlp-covid19-emnlp-35","speakers":"Charles Welch|Allison Lahnala|Veronica Perez-Rosas|Siqi Shen|Sarah Seraj|Larry An|Kenneth Resnicow|James Pennebaker|Rada Mihalcea","title":"Expressive Interviewing: A Conversational System for Coping with COVID-19"},{"content":{"abstract":"The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misinformation detection datasets are not effective for evaluating systems designed to detect misinformation on this topic. Misinformation detection can be divided into two sub-tasks: (i) retrieval of misconceptions relevant to posts being checked for veracity, and (ii) stance detection to identify whether the posts Agree, Disagree, or express No Stance towards the retrieved misconceptions. To facilitate research on this task, we release COVIDLies (https://ucinlp.github.io/covid19 ), a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon.","authors":["Tamanna Hossain","Robert L. Logan IV","Arjuna Ugarte","Yoshitomo Matsubara","Sean Young","Sameer Singh"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.11","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVIDLies: Detecting COVID-19 Misinformation on Social Media","tldr":"The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misi...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.37","presentation_id":"38939851","rocketchat_channel":"paper-nlp-covid19-emnlp-37","speakers":"Tamanna Hossain|Robert L. Logan IV|Arjuna Ugarte|Yoshitomo Matsubara|Sean Young|Sameer Singh","title":"COVIDLies: Detecting COVID-19 Misinformation on Social Media"},{"content":{"abstract":"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories.","authors":["Akiko Aizawa","Frederic Bergeron","Junjie Chen","Fei Cheng","Katsuhiko Hayashi","Kentaro Inui","Hiroyoshi Ito","Daisuke Kawahara","Masaru Kitsuregawa","Hirokazu Kiyomaru","Masaki Kobayashi","Takashi Kodama","Sadao Kurohashi","Qianying Liu","Masaki Matsubara","Yusuke Miyao","Atsuyuki Morishima","Yugo Murawaki","Kazumasa Omura","Haiyue Song","Eiichiro Sumita","Shinji Suzuki","Ribeka Tanaka","Yu Tanaka","Masashi Toyoda","Nobuhiro Ueda","Honai Ueoka","Masao Utiyama","Ying Zhong"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.13","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A System for Worldwide COVID-19 Information Aggregation","tldr":"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g.,...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.45","presentation_id":"38939852","rocketchat_channel":"paper-nlp-covid19-emnlp-45","speakers":"Akiko Aizawa|Frederic Bergeron|Junjie Chen|Fei Cheng|Katsuhiko Hayashi|Kentaro Inui|Hiroyoshi Ito|Daisuke Kawahara|Masaru Kitsuregawa|Hirokazu Kiyomaru|Masaki Kobayashi|Takashi Kodama|Sadao Kurohashi|Qianying Liu|Masaki Matsubara|Yusuke Miyao|Atsuyuki Morishima|Yugo Murawaki|Kazumasa Omura|Haiyue Song|Eiichiro Sumita|Shinji Suzuki|Ribeka Tanaka|Yu Tanaka|Masashi Toyoda|Nobuhiro Ueda|Honai Ueoka|Masao Utiyama|Ying Zhong","title":"A System for Worldwide COVID-19 Information Aggregation"},{"content":{"abstract":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depres- sion, supported by the literature. We propose a methodology for providing insight into tem- poral mental health dynamics to be utilised for strategic decision-making.","authors":["Tom Tabak","Matthew Purver"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.7","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Temporal Mental Health Dynamics on Social Media","tldr":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global CO...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.47","presentation_id":"38939853","rocketchat_channel":"paper-nlp-covid19-emnlp-47","speakers":"Tom Tabak|Matthew Purver","title":"Temporal Mental Health Dynamics on Social Media"},{"content":{"abstract":"The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, \u201dpivot\u201d languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.","authors":["Antonios Anastasopoulos","Alessandro Cattelan","Zi-Yi Dou","Marcello Federico","Christian Federmann","Dmitriy Genzel","Franscisco Guzm\u00e1n","Junjie Hu","Macduff Hughes","Philipp Koehn","Rosie Lazar","Will Lewis","Graham Neubig","Mengmeng Niu","Alp \u00d6ktem","Eric Paquin","Grace Tang","Sylwia Tur"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.5","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"TICO-19: the Translation Initiative for COvid-19","tldr":"The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collab...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.50","presentation_id":"38939854","rocketchat_channel":"paper-nlp-covid19-emnlp-50","speakers":"Antonios Anastasopoulos|Alessandro Cattelan|Zi-Yi Dou|Marcello Federico|Christian Federmann|Dmitriy Genzel|Franscisco Guzm\u00e1n|Junjie Hu|Macduff Hughes|Philipp Koehn|Rosie Lazar|Will Lewis|Graham Neubig|Mengmeng Niu|Alp \u00d6ktem|Eric Paquin|Grace Tang|Sylwia Tur","title":"TICO-19: the Translation Initiative for COvid-19"},{"content":{"abstract":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. In this paper, we analyse Dutch public sentiment on governmental COVID-19 measures from text data collected across three online media sources (Twitter, Reddit and Nu.nl) from February to September 2020. We apply sentiment analysis methods to analyse polarity over time, as well as to identify stance towards two specific pandemic policies regarding social distancing and wearing face masks. The presented preliminary results provide valuable insights into the narratives shown in vast social media text data, which help understand the influence of COVID-19 measures on the general public.","authors":["Shihan Wang","Marijn Schraagen","Erik Tjong Kim Sang","Mehdi Dastani"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.17","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media","tldr":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-dat...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.53","presentation_id":"38939855","rocketchat_channel":"paper-nlp-covid19-emnlp-53","speakers":"Shihan Wang|Marijn Schraagen|Erik Tjong Kim Sang|Mehdi Dastani","title":"Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media"},{"content":{"abstract":"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well.","authors":["Jinhyuk Lee","Sean S. Yi","Minbyul Jeong","Mujeen Sung","WonJin Yoon","Yonghwa Choi","Miyoung Ko","Jaewoo Kang"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.1","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Answering Questions on COVID-19 in Real-Time","tldr":"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our e...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.6","presentation_id":"38939843","rocketchat_channel":"paper-nlp-covid19-emnlp-6","speakers":"Jinhyuk Lee|Sean S. Yi|Minbyul Jeong|Mujeen Sung|WonJin Yoon|Yonghwa Choi|Miyoung Ko|Jaewoo Kang","title":"Answering Questions on COVID-19 in Real-Time"},{"content":{"abstract":"A dataset of COVID-19-related scientific literature is compiled, combining the articles from several online libraries and selecting those with open access and full text available. Then, hierarchical nonnegative matrix factorization is used to organize literature related to the novel coronavirus into a tree structure that allows researchers to search for relevant literature based on detected topics. We discover eight major latent topics and 52 granular subtopics in the body of literature, related to vaccines, genetic structure and modeling of the disease and patient studies, as well as related diseases and virology. In order that our tool may help current researchers, an interactive website is created that organizes available literature using this hierarchical structure.","authors":["Rachel Grotheer","Longxiu Huang","Yihuan Huang","Alona Kryshchenko","Oleksandr Kryshchenko","Pengyu Li","Xia Li","Elizaveta Rebrova","Kyung Ha","Deanna Needell"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.4","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19 Literature Topic-Based Search via Hierarchical NMF","tldr":"A dataset of COVID-19-related scientific literature is compiled, combining the articles from several online libraries and selecting those with open access and full text available. Then, hierarchical nonnegative matrix factorization is used to organiz...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.60","presentation_id":"38939856","rocketchat_channel":"paper-nlp-covid19-emnlp-60","speakers":"Rachel Grotheer|Longxiu Huang|Yihuan Huang|Alona Kryshchenko|Oleksandr Kryshchenko|Pengyu Li|Xia Li|Elizaveta Rebrova|Kyung Ha|Deanna Needell","title":"COVID-19 Literature Topic-Based Search via Hierarchical NMF"},{"content":{"abstract":"Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topic-based methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compare traditional topic models based on word tokens with topic models based on medical concepts, and propose several ways to improve topic coherence and specificity.","authors":["Yulia Otmakhova","Karin Verspoor","Timothy Baldwin","Simon \u0160uster"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.12","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration","tldr":"Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topic-based methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compa...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.63","presentation_id":"38939857","rocketchat_channel":"paper-nlp-covid19-emnlp-63","speakers":"Yulia Otmakhova|Karin Verspoor|Timothy Baldwin|Simon \u0160uster","title":"Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration"},{"content":{"abstract":"We present a Question Answering (QA) system that won one of the tasks of the Kaggle CORD-19 Challenge, according to the qualitative evaluation of experts. The system is a combination of an Information Retrieval module and a reading comprehension module that finds the answers in the retrieved passages. In this paper we present a quantitative and qualitative analysis of the system. The quantitative evaluation using manually annotated datasets contradicted some of our design choices, e.g. the fact that using QuAC for fine-tuning provided better answers over just using SQuAD. We analyzed this mismatch with an additional A/B test which showed that the system using QuAC was indeed preferred by users, confirming our intuition. Our analysis puts in question the suitability of automatic metrics and its correlation to user preferences. We also show that automatic metrics are highly dependent on the characteristics of the gold standard, such as the average length of the answers.","authors":["Arantxa Otegi","Jon Ander Campos","Gorka Azkune","Aitor Soroa","Eneko Agirre"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.15","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Automatic Evaluation vs. User Preference in Neural Textual QuestionAnswering over COVID-19 Scientific Literature","tldr":"We present a Question Answering (QA) system that won one of the tasks of the Kaggle CORD-19 Challenge, according to the qualitative evaluation of experts. The system is a combination of an Information Retrieval module and a reading comprehension modu...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.64","presentation_id":"38939858","rocketchat_channel":"paper-nlp-covid19-emnlp-64","speakers":"Arantxa Otegi|Jon Ander Campos|Gorka Azkune|Aitor Soroa|Eneko Agirre","title":"Automatic Evaluation vs. User Preference in Neural Textual QuestionAnswering over COVID-19 Scientific Literature"},{"content":{"abstract":"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.","authors":["Lin Miao","Mark Last","Marina Litvak"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.19","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures","tldr":"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at contain...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.65","presentation_id":"38939859","rocketchat_channel":"paper-nlp-covid19-emnlp-65","speakers":"Lin Miao|Mark Last|Marina Litvak","title":"Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures"},{"content":{"abstract":"We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.","authors":["Dan Su","Yan Xu","Tiezheng Yu","Farhad Bin Siddique","Elham Barezi","Pascale Fung"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.14","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management","tldr":"We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.9","presentation_id":"38939844","rocketchat_channel":"paper-nlp-covid19-emnlp-9","speakers":"Dan Su|Yan Xu|Tiezheng Yu|Farhad Bin Siddique|Elham Barezi|Pascale Fung","title":"CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management"},{"content":{"abstract":"With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific researchers. To this end, we developed a pipeline that creates an implicit feedback matrix based on Named Entity Recognition (NER) on a corpus of documents, using multidisciplinary ontologies for recognizing and linking the entities. Our hypothesis is that by using ontologies from different fields in the NER phase, we can improve the results for state-of-the-art collaborative-filtering recommender systems applied to the dataset created. The tests performed using the COVID-19 Open Research Dataset (CORD-19) dataset show that when using four ontologies, the results for precision@k, for example, reach the 80%, whereas when using only one ontology, the results for precision@k drops to 20%, for the same users. Furthermore, the use of multi-fields entities may help in the discovery of new items, even if the researchers do not have items from that field in their set of preferences.","authors":["Marcia Afonso Barros","Andre Lamurias","Diana Sousa","Pedro Ruas","Francisco M. Couto"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.20","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"COVID-19: A Semantic-Based Pipeline for Recommending Biomedical Entities","tldr":"With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific rese...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.20","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-20","speakers":"Marcia Afonso Barros|Andre Lamurias|Diana Sousa|Pedro Ruas|Francisco M. Couto","title":"COVID-19: A Semantic-Based Pipeline for Recommending Biomedical Entities"},{"content":{"abstract":"Coronavirus Disease of 2019 (COVID-19) created dire consequences globally and triggered an intense scientific effort from different domains. The resulting publications created a huge text collection in which finding the studies related to a biomolecule of interest is challenging for general purpose search engines because the publications are rich in domain specific terminology. Here, we present Vapur: an online COVID-19 search engine specifically designed to find related protein - chemical pairs. Vapur is empowered with a relation-oriented inverted index that is able to retrieve and group studies for a query biomolecule with respect to its related entities. The inverted index of Vapur is automatically created with a BioNLP pipeline and integrated with an online user interface. The online interface is designed for the smooth traversal of the current literature by domain researchers and is publicly available at https://tabilab.cmpe.boun.edu.tr/vapur/.","authors":["Abdullatif K\u00f6ksal","Hilal D\u00f6nmez","R\u0131za \u00d6z\u00e7elik","Elif Ozkirimli","Arzucan \u00d6zg\u00fcr"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.21","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Vapur: A Search Engine to Find Related Protein - Compound Pairs in COVID-19 Literature","tldr":"Coronavirus Disease of 2019 (COVID-19) created dire consequences globally and triggered an intense scientific effort from different domains. The resulting publications created a huge text collection in which finding the studies related to a biomolecu...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.21","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-21","speakers":"Abdullatif K\u00f6ksal|Hilal D\u00f6nmez|R\u0131za \u00d6z\u00e7elik|Elif Ozkirimli|Arzucan \u00d6zg\u00fcr","title":"Vapur: A Search Engine to Find Related Protein - Compound Pairs in COVID-19 Literature"},{"content":{"abstract":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of $500$ sentences that were manually selected by the researchers from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of $100,959$ sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","authors":["Alejandro Piad-Morffis","Suilan Estevez-Velarde","Ernesto Luis Estevanell-Valladares","Yoan Guti\u00e9rrez","Andr\u00e9s Montoyo","Rafael Mu\u00f1oz","Yudivi\u00e1n Almeida-Cruz"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.22","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Knowledge Discovery in COVID-19 Research Literature","tldr":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of $...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.22","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-22","speakers":"Alejandro Piad-Morffis|Suilan Estevez-Velarde|Ernesto Luis Estevanell-Valladares|Yoan Guti\u00e9rrez|Andr\u00e9s Montoyo|Rafael Mu\u00f1oz|Yudivi\u00e1n Almeida-Cruz","title":"Knowledge Discovery in COVID-19 Research Literature"},{"content":{"abstract":"The COVID-19 pandemic has thrown natural life out of gear across the globe. Strict measures are deployed to curb the spread of the virus that is causing it, and the most effective of them have been social isolation. This has led to wide-spread gloom and depression across society but more so among the young and the elderly. There are currently more than 200 million college students in 186 countries worldwide, affected due to the pandemic. The mode of education has changed suddenly, with the rapid adaptation of e-learning, whereby teaching is undertaken remotely and on digital platforms. This study presents insights gathered from social media posts that were posted by students and young adults during the COVID times. Using statistical and NLP techniques, we analyzed the behavioural issues reported by users themselves in their posts in depression related communities on Reddit. We present methodologies to systematically analyze content using linguistic techniques to find out the stress-inducing factors. Online education, losing jobs, isolation from friends and abusive families emerge as key stress factors","authors":["Sachin Thukral","Suyash Sangwan","Arnab Chatterjee","Lipika Dey"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.23","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Identifying pandemic-related stress factors from social-media posts \u2013 Effects on students and young-adults","tldr":"The COVID-19 pandemic has thrown natural life out of gear across the globe. Strict measures are deployed to curb the spread of the virus that is causing it, and the most effective of them have been social isolation. This has led to wide-spread gloom ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.23","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-23","speakers":"Sachin Thukral|Suyash Sangwan|Arnab Chatterjee|Lipika Dey","title":"Identifying pandemic-related stress factors from social-media posts \u2013 Effects on students and young-adults"},{"content":{"abstract":"The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restrictions like lockdown and social distancing, it became important to understand the emotional response of the public towards the pandemic. In this paper, we study the reaction of Saudi Arabia citizens towards the pandemic. We utilize a collection of Arabic tweets that were sent during 2020, primarily through hashtags that were originated from Saudi Arabia. Our results showed that people had kept a positive reaction towards the pandemic. This positive reaction was at its highest at the beginning of the COVID-19 crisis and started to decline as time passes. Overall, the results showed that people were so supportive of each other through this pandemic. This research can help researchers and policymakers in understanding the emotional effect of a pandemic on societies.","authors":["Aseel Addawood","Alhanouf Alsuwailem","Ali Alohali","Dalal Alajaji","Mashail Alturki","Jaida Alsuhaibani","Fawziah Aljabli"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.24","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Tracking And Understanding Public Reaction During COVID-19: Saudi Arabia As A Use Case","tldr":"The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restric...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.24","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-24","speakers":"Aseel Addawood|Alhanouf Alsuwailem|Ali Alohali|Dalal Alajaji|Mashail Alturki|Jaida Alsuhaibani|Fawziah Aljabli","title":"Tracking And Understanding Public Reaction During COVID-19: Saudi Arabia As A Use Case"},{"content":{"abstract":"Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help with symptoms. In this work, we mined a large twitter dataset of 424 million tweets of COVID-19 chatter to identify discourse around drug mentions. While seemingly a straightforward task, due to the informal nature of language use in Twitter, we demonstrate the need of machine learning alongside traditional automated methods to aid in this task. By applying these complementary methods, we are able to recover almost 15% additional data, making misspelling handling a needed task as a pre-processing step when dealing with social media data.","authors":["Ramya Tekumalla","Juan M Banda"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.25","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Characterizing drug mentions in COVID-19 Twitter Chatter","tldr":"Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help wit...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.25","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-25","speakers":"Ramya Tekumalla|Juan M Banda","title":"Characterizing drug mentions in COVID-19 Twitter Chatter"},{"content":{"abstract":"Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, and feelings about a wide range of issues. In this study, using more than 530,000 original tweets in Persian/Farsi on COVID-19, we analyzed the topics discussed among users, who are mainly Iranians, to gauge and track the response to the pandemic and how it evolved over time. We applied a combination of manual annotation of a random sample of tweets and topic modeling tools to classify the contents and frequency of each category of topics. We identified the top 25 topics among which living experience under home quarantine emerged as a major talking point. We additionally categorized the broader content of tweets that shows satire, followed by news, is the dominant tweet type among Iranian users. While this framework and methodology can be used to track public response to ongoing developments related to COVID-19, a generalization of this framework can become a useful framework to gauge Iranian public reaction to ongoing policy measures or events locally and internationally.","authors":["Pedram Hosseini","Poorya Hosseini","David Broniatowski"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.26","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in Iran using NLP","tldr":"Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.26","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-26","speakers":"Pedram Hosseini|Poorya Hosseini|David Broniatowski","title":"Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in Iran using NLP"},{"content":{"abstract":"The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition and normalisation in parallel to help find relevant publications and to aid in downstream NLP tasks such as text summarisation. In our approach, we are using a dictionary-based system for its high recall in conjunction with two models based on BioBERT for their accuracy. Their outputs are combined according to different strategies depending on the entity type. In addition, we are using a manually crafted dictionary to increase performance for new concepts related to COVID-19. We have previously evaluated our work on the CRAFT corpus, and make the output of our pipeline available on two visualisation platforms.","authors":["Nico Colic","Lenz Furrer","Fabio Rinaldi"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.27","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Annotating the Pandemic: Named Entity Recognition and Normalisation in COVID-19 Literature","tldr":"The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.27","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-27","speakers":"Nico Colic|Lenz Furrer|Fabio Rinaldi","title":"Annotating the Pandemic: Named Entity Recognition and Normalisation in COVID-19 Literature"},{"content":{"abstract":"In a recent project, the Language Application Grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid \u201cAskMe\u201d query and retrieval engine. We describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications.","authors":["Keith Suderman","Nancy Ide","Verhagen Marc","Brent Cochran","James Pustejovsky"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.28","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"AskMe: A LAPPS Grid-based NLP Query and Retrieval System for Covid-19 Literature","tldr":"In a recent project, the Language Application Grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid \u201cAskMe\u201d...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.28","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-28","speakers":"Keith Suderman|Nancy Ide|Verhagen Marc|Brent Cochran|James Pustejovsky","title":"AskMe: A LAPPS Grid-based NLP Query and Retrieval System for Covid-19 Literature"},{"content":{"abstract":"Understanding scientific articles related to COVID-19 requires broad knowledge about concepts such as symptoms, diseases and medicine. Given the very large and ever-growing scientific articles related to COVID-19, it is a daunting task even for experts to recognize the large set of concepts mentioned in these articles. In this paper, we address the problem of concept wikification for COVID-19, which is to automatically recognize mentions of concepts related to COVID-19 in text and resolve them into Wikipedia titles. We develop an approach to curate a COVID-19 concept wikification dataset by mining Wikipedia text and the associated intra-Wikipedia links. We also develop an end-to-end system for concept wikification for COVID-19. Preliminary experiments show very encouraging results. Our dataset, code and pre-trained model are available at github.com/panlybero/Covid19_wikification.","authors":["Panagiotis Lymperopoulos","Haoling Qiu","Bonan Min"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.29","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Concept Wikification for COVID-19","tldr":"Understanding scientific articles related to COVID-19 requires broad knowledge about concepts such as symptoms, diseases and medicine. Given the very large and ever-growing scientific articles related to COVID-19, it is a daunting task even for exper...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.29","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-29","speakers":"Panagiotis Lymperopoulos|Haoling Qiu|Bonan Min","title":"Concept Wikification for COVID-19"},{"content":{"abstract":"Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature.","authors":["Philip Resnik","Katherine E. Goodman","Mike Moran"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.30","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Developing a Curated Topic Model for COVID-19 Medical Research Literature","tldr":"Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desid...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.30","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-30","speakers":"Philip Resnik|Katherine E. Goodman|Mike Moran","title":"Developing a Curated Topic Model for COVID-19 Medical Research Literature"},{"content":{"abstract":"We release a dataset of over 2,100 COVID19 related Frequently asked Question-Answer pairs scraped from over 40 trusted websites. We include an additional 24, 000 questions pulled from online sources that have been aligned by experts with existing answered questions from our dataset. This paper describes our efforts in collecting the dataset and summarizes the resulting data. Our dataset is automatically updated daily and available at https://github.com/JHU-COVID-QA/ scraping-qas. So far, this data has been used to develop a chatbot providing users information about COVID-19. We encourage others to build analytics and tools upon this dataset as well.","authors":["Adam Poliak","Max Fleming","Cash Costello","Kenton W Murray","Mahsa Yarmohammadi","Shivani Pandya","Darius Irani","Milind Agarwal","Udit Sharma","Shuo Sun","Nicola Ivanov","Lingxi Shang","Kaushik Srinivasan","Seolhwa Lee","Xu Han","Smisha Agarwal","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.31","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Collecting Verified COVID-19 Question Answer Pairs","tldr":"We release a dataset of over 2,100 COVID19 related Frequently asked Question-Answer pairs scraped from over 40 trusted websites. We include an additional 24, 000 questions pulled from online sources that have been aligned by experts with existing ans...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.31","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-31","speakers":"Adam Poliak|Max Fleming|Cash Costello|Kenton W Murray|Mahsa Yarmohammadi|Shivani Pandya|Darius Irani|Milind Agarwal|Udit Sharma|Shuo Sun|Nicola Ivanov|Lingxi Shang|Kaushik Srinivasan|Seolhwa Lee|Xu Han|Smisha Agarwal|Jo\u00e3o Sedoc","title":"Collecting Verified COVID-19 Question Answer Pairs"},{"content":{"abstract":"The number of unique terms in the scientific literature used to refer to either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase rapidly despite well-established standardized terms. This high degree of term variation makes high recall identification of these important entities difficult. In this manuscript we present an extensive dictionary of terms used in the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based approach to iteratively generate new term variants, then locate these variants in a large text corpus. We compare our dictionary to an extensive collection of terminological resources, demonstrating that our resource provides a substantial number of additional terms. We use our dictionary to analyze the usage of SARS-CoV-2 and COVID-19 terms over time and show that the number of unique terms continues to grow rapidly. Our dictionary is freely available at https://github.com/ncbi-nlp/CovidTermVar.","authors":["Robert Leaman","Zhiyong Lu"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.32","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and SARS-CoV-2","tldr":"The number of unique terms in the scientific literature used to refer to either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase rapidly despite well-established standardized terms. This high degree of term variation makes hig...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.32","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-32","speakers":"Robert Leaman|Zhiyong Lu","title":"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and SARS-CoV-2"},{"content":{"abstract":"To combat misinformation regarding COVID- 19 during this unprecedented pandemic, we propose a conversational agent that answers questions related to COVID-19. We adapt the Poly-encoder (Humeau et al., 2020) model for informational retrieval from FAQs. We show that after fine-tuning, the Poly-encoder can achieve a higher F1 score. We make our code publicly available for other researchers to use.","authors":["Seolhwa Lee","Jo\u00e3o Sedoc"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.33","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Using the Poly-encoder for a COVID-19 Question Answering System","tldr":"To combat misinformation regarding COVID- 19 during this unprecedented pandemic, we propose a conversational agent that answers questions related to COVID-19. We adapt the Poly-encoder (Humeau et al., 2020) model for informational retrieval from FAQs...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.33","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-33","speakers":"Seolhwa Lee|Jo\u00e3o Sedoc","title":"Using the Poly-encoder for a COVID-19 Question Answering System"},{"content":{"abstract":"With the rapid development of COVID-19 around the world, people are requested to maintain \u201csocial distance\u201d and \u201cstay at home\u201d. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter and Sina Weibo. People generate posts to share information, express opinions and seek help during the pandemic outbreak, and these kinds of data on social media are valuable for studies to prevent COVID-19 transmissions, such as early warning and outbreaks detection. Therefore, in this paper, we release a novel and fine-grained large-scale COVID-19 social media dataset collected from Sina Weibo, named Weibo-COV, contains more than 40 million posts ranging from December 1, 2019 to April 30, 2020. Moreover, this dataset includes comprehensive information nuggets like post-level information, interactive information, location information, and repost network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and rapid researches to suppress the spread of this pandemic.","authors":["Yong Hu","Heyan Huang","Anfan Chen","Xian-Ling Mao"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.34","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Weibo-COV: A Large-Scale COVID-19 Social Media Dataset from Weibo","tldr":"With the rapid development of COVID-19 around the world, people are requested to maintain \u201csocial distance\u201d and \u201cstay at home\u201d. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter ...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.34","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-34","speakers":"Yong Hu|Heyan Huang|Anfan Chen|Xian-Ling Mao","title":"Weibo-COV: A Large-Scale COVID-19 Social Media Dataset from Weibo"},{"content":{"abstract":"In this paper, we present an iterative graph-based approach for the detection of symptoms of COVID-19, the pathology of which seems to be evolving. More generally, the method can be applied to finding context-specific words and texts (e.g. symptom mentions) in large imbalanced corpora (e.g. all tweets mentioning }#COVID-19). Given the novelty of COVID-19, we also test if the proposed approach generalizes to the problem of detecting Adverse Drug Reaction (ADR). We find that the approach applied to Twitter data can detect symptom mentions substantially before to their being reported by the Centers for Disease Control (CDC).","authors":["Roshan Santosh","H. Schwartz","Johannes Eichstaedt","Lyle Ungar","Sharath Chandra Guntuku"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.35","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Detecting Emerging Symptoms of COVID-19 using Context-based Twitter Embeddings","tldr":"In this paper, we present an iterative graph-based approach for the detection of symptoms of COVID-19, the pathology of which seems to be evolving. More generally, the method can be applied to finding context-specific words and texts (e.g. symptom me...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.35","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-35","speakers":"Roshan Santosh|H. Schwartz|Johannes Eichstaedt|Lyle Ungar|Sharath Chandra Guntuku","title":"Detecting Emerging Symptoms of COVID-19 using Context-based Twitter Embeddings"},{"content":{"abstract":"As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter, there are issues with their interpretability. We propose a novel use of learned feature importance which improves upon the performance of prior state-of-the-art text classification techniques, while producing more easily interpretable decisions. We also discuss both technical and practical challenges that remain for this task.","authors":["David Hardage","Peyman Najafirad"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.36","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI: Ongoing Applied Research","tldr":"As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identifica...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.36","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-36","speakers":"David Hardage|Peyman Najafirad","title":"Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI: Ongoing Applied Research"},{"content":{"abstract":"As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboard for the real-time classification, geolocation and interactive visualization of COVID-19 tweets that addresses these issues. We also describe a novel L2 classification layer that outperforms linear layers on a dataset of respiratory virus tweets.","authors":["Andrei Mircea"],"demo_url":null,"keywords":[],"material":null,"paper_type":"Workshop","pdf_url":"https://www.aclweb.org/anthology/2020.nlpcovid19-2.37","program":"workshop","sessions":[],"similar_paper_uids":[],"title":"Real-time Classification, Geolocation and Interactive Visualization of COVID-19 Information Shared on Social Media to Better Understand Global Developments","tldr":"As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboar...","track":"NLP for COVID-19 Workshop (Part 2)"},"id":"WS-26.2020.nlpcovid19-2.37","presentation_id":"","rocketchat_channel":"paper-nlp-covid19-emnlp-37","speakers":"Andrei Mircea","title":"Real-time Classification, Geolocation and Interactive Visualization of COVID-19 Information Shared on Social Media to Better Understand Global Developments"}],"prerecorded_talks":[],"rocketchat_channel":"workshop-nlp-covid19-emnlp","schedule":null,"sessions":[{"end_time":"Fri, 20 Nov 2020 08:10:00 GMT","hosts":"Karin Verspoor","link":"","session_name":"Welcome Speech","start_time":"Fri, 20 Nov 2020 08:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 10:30:00 GMT","hosts":"Karin Verspoor","link":"","session_name":"Session 1: Presentations","start_time":"Fri, 20 Nov 2020 08:10:00 GMT"},{"end_time":"Fri, 20 Nov 2020 11:30:00 GMT","hosts":"Karin Verspoor","link":"","session_name":"Poster session 1 (gather.town)","start_time":"Fri, 20 Nov 2020 10:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 16:00:00 GMT","hosts":"unhosted","link":"","session_name":"long break","start_time":"Fri, 20 Nov 2020 11:30:00 GMT"},{"end_time":"Fri, 20 Nov 2020 17:50:00 GMT","hosts":"Byron Wallace","link":"","session_name":"Session 2: Presentations","start_time":"Fri, 20 Nov 2020 16:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 18:50:00 GMT","hosts":"Mike Conway","link":"","session_name":"Poster session 1 (gather.town)","start_time":"Fri, 20 Nov 2020 17:50:00 GMT"},{"end_time":"Fri, 20 Nov 2020 19:00:00 GMT","hosts":"unhosted","link":"","session_name":"short break","start_time":"Fri, 20 Nov 2020 18:50:00 GMT"},{"end_time":"Fri, 20 Nov 2020 20:35:00 GMT","hosts":"Mike Conway","link":"","session_name":"Session 3: Presentations","start_time":"Fri, 20 Nov 2020 19:00:00 GMT"},{"end_time":"Fri, 20 Nov 2020 21:00:00 GMT","hosts":"Mike Conway","link":"","session_name":"Discussion","start_time":"Fri, 20 Nov 2020 20:35:00 GMT"}],"title":"NLP for COVID-19 Workshop (Part 2)","website":"https://www.nlpcovid19workshop.org/emnlp2020/","zoom_links":["https://zoom.us"]}]
